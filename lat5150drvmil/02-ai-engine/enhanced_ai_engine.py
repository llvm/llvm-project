#!/usr/bin/env python3
"""
Enhanced AI Engine - Unified interface integrating all AI enhancements

This is the main AI engine that brings together:
- Conversation history & cross-session memory
- Vector embeddings & semantic RAG
- Response caching (Redis + PostgreSQL)
- Hierarchical memory (working/short-term/long-term)
- Autonomous self-improvement
- DSMIL deep integration
- RAM-based context window
- 100K-131K token context windows
"""

import json
import time
import hashlib
import asyncio
from pathlib import Path
from typing import Optional, Dict, List, Any, Callable
from dataclasses import dataclass
from datetime import datetime

# Import all enhancement modules
from conversation_manager import ConversationManager, Conversation
from enhanced_rag_system import EnhancedRAGSystem
from response_cache import ResponseCache
from hierarchical_memory import HierarchicalMemory, MemoryBlock, DecayingMemoryManager
from autonomous_self_improvement import AutonomousSelfImprovement
from dsmil_deep_integrator import DSMILDeepIntegrator
from ram_context_and_proactive_agent import RAMContextWindow, ProactiveImprovementAgent
from multi_model_evaluator import MultiModelEvaluator
from event_driven_agent import EventDrivenAgent, EventStore, EventProjector
from entity_resolution_pipeline import EntityResolutionPipeline
from dynamic_schema_generator import DynamicSchemaGenerator
from agentic_rag_enhancer import AgenticRAGEnhancer
from human_in_loop_executor import HumanInLoopExecutor, RiskLevel
from mcp_tool_selector import MCPToolSelector
from threat_intelligence_automation import ThreatIntelligenceAutomation
from blockchain_investigation_tools import BlockchainInvestigationTools
from osint_workflows import OSINTWorkflows
from advanced_analytics import AdvancedAnalytics

# Import new routing and memory enhancements
try:
    from intelligent_routing_mux import IntelligentRoutingMux, get_routing_mux, TaskType
    from memlayer_enhanced import MemLayerEnhanced, get_memlayer, OperatingMode, SearchTier
    from self_improving_rag import SelfImprovingRAG, get_self_improving_rag, FeedbackType
    ENHANCED_ROUTING_AVAILABLE = True
except ImportError:
    ENHANCED_ROUTING_AVAILABLE = False

# Import heretic abliteration components
try:
    from heretic_unsloth_integration import UnslothOptimizer, UnslothConfig
    from heretic_enhanced_abliteration import (
        EnhancedRefusalCalculator,
        EnhancedAbliterationConfig,
        AbliterationMethod,
        LLMJudge
    )
    from heretic_abliteration import ModelAbliterator, AbliterationParameters
    HERETIC_AVAILABLE = True
except ImportError:
    HERETIC_AVAILABLE = False

# Import 12-factor agent orchestrator
try:
    from agent_orchestrator import (
        AgentFactory,
        AgentExecutor,
        ProjectOrchestrator,
        AgentSpecialization,
        AgentStatus,
        MessageBus,
        Project
    )
    AGENT_ORCHESTRATOR_AVAILABLE = True
except ImportError:
    AGENT_ORCHESTRATOR_AVAILABLE = False

# Import base functionality
try:
    from dsmil_ai_engine import DSMILAIEngine
except ImportError:
    DSMILAIEngine = None

# Import DIRECTEYE Intelligence (optional)
try:
    from directeye_intelligence import DirectEyeIntelligence
    DIRECTEYE_AVAILABLE = True
except ImportError:
    DIRECTEYE_AVAILABLE = False

# Import Heretic abliteration components (optional)
try:
    from heretic_abliteration import (
        HereticModelWrapper,
        AbliterationParameters,
        RefusalDirectionCalculator
    )
    from heretic_optimizer import HereticOptimizer, OptimizationResult
    from heretic_evaluator import ModelEvaluator, RefusalDetector
    from heretic_config import HereticSettings, ConfigLoader
    HERETIC_AVAILABLE = True
except ImportError:
    HERETIC_AVAILABLE = False

# Import forensics knowledge (optional)
try:
    import sys
    sys.path.insert(0, str(Path(__file__).parent.parent / '04-integrations' / 'forensics'))
    from forensics_knowledge import ForensicsKnowledge
    FORENSICS_KNOWLEDGE_AVAILABLE = True
except ImportError:
    FORENSICS_KNOWLEDGE_AVAILABLE = False


@dataclass
class EnhancedResponse:
    """Enhanced response with full metadata"""
    content: str
    model: str
    conversation_id: str
    cached: bool
    latency_ms: int
    tokens_input: int
    tokens_output: int
    memory_tier: str  # working, short_term, long_term
    rag_sources: List[str]
    dsmil_attestation: Optional[str]
    improvements_suggested: List[str]


class EnhancedAIEngine:
    """
    Unified AI Engine with all enhancements integrated

    Features:
    - Full conversation history and cross-session memory
    - Semantic RAG with vector embeddings (10-100x better than keyword)
    - Multi-tier response caching (20-40% faster responses)
    - 3-tier hierarchical memory (working/short-term/long-term)
    - Autonomous self-improvement during idle cycles
    - Deep DSMIL integration with TPM attestation
    - RAM-based context window (512MB shared memory)
    - 100K-131K token context windows
    - Multi-model evaluation for quality assurance (ai-that-works #16)
    - Decaying-resolution memory for token efficiency (ai-that-works #18)
    - Event-driven agent architecture (ai-that-works #30)
    - Entity resolution pipeline with OSINT enrichment (ai-that-works #10)
    - Dynamic schema generation from examples/NL (ai-that-works #25)
    - Agentic RAG with query reformulation & multi-hop (ai-that-works #28)
    - Human-in-loop executor for sensitive operations (Production)
    - MCP tool selector for intelligent routing (Production)
    - Threat intelligence automation for IOC extraction & attribution (Production)
    - Blockchain investigation tools for multi-chain analysis & fund tracing (Production)
    - OSINT workflows for automated investigations (person, company, crypto, etc.) (Production)
    - Advanced analytics for pattern recognition, anomaly detection & predictions (Production)
    - Heretic abliteration with Unsloth (2x speed, 70% less VRAM), DECCP (multi-layer), remove-refusals (Production)
    - 12-factor agent orchestration for dynamic multi-agent coordination & project-based task execution (Production)
    - Intelligent routing mux for task-based model routing with provider failover (claude-code-mux inspired)
    - MemLayer enhanced memory with salience filtering and hybrid vector+graph storage (memlayer inspired)
    - Self-improving RAG with feedback learning, query refinement, and adaptive chunking
    """

    def __init__(
        self,
        models_config_path: str = "/home/user/LAT5150DRVMIL/02-ai-engine/models.json",
        user_id: Optional[str] = None,
        enable_self_improvement: bool = True,
        enable_dsmil_integration: bool = True,
        enable_ram_context: bool = True,
        enable_forensics_knowledge: bool = True,
        enable_multi_model_eval: bool = True,
        enable_decaying_memory: bool = True,
        enable_event_driven: bool = True,
        enable_entity_resolution: bool = True,
        enable_dynamic_schemas: bool = True,
        enable_agentic_rag: bool = True,
        enable_human_in_loop: bool = True,
        enable_mcp_selector: bool = True,
        enable_threat_intel: bool = True,
        enable_blockchain_tools: bool = True,
        enable_osint_workflows: bool = True,
        enable_advanced_analytics: bool = True,
        enable_heretic: bool = True,
        heretic_use_unsloth: bool = True,
        heretic_method: str = "multi_layer",
        enable_agent_orchestrator: bool = True,
        human_in_loop_audit_path: Optional[str] = None,
        mcp_optimize_for: str = "balanced",
        enable_intelligent_routing: bool = True,
        enable_memlayer: bool = True,
        enable_self_improving_rag: bool = True
    ):
        """
        Initialize Enhanced AI Engine with all components

        Args:
            models_config_path: Path to models.json
            user_id: Optional user ID for conversation tracking
            enable_self_improvement: Enable autonomous self-improvement
            enable_dsmil_integration: Enable DSMIL hardware integration
            enable_ram_context: Enable RAM-based context window
            enable_forensics_knowledge: Enable digital forensics knowledge base
            enable_multi_model_eval: Enable multi-model evaluation for quality assurance
            enable_decaying_memory: Enable decaying-resolution memory for token efficiency
            enable_event_driven: Enable event-driven agent architecture
            enable_entity_resolution: Enable entity resolution pipeline with OSINT
            enable_dynamic_schemas: Enable dynamic schema generation from examples
            enable_agentic_rag: Enable agentic RAG with query reformulation
            enable_human_in_loop: Enable human-in-loop executor for sensitive operations
            enable_mcp_selector: Enable MCP tool selector for intelligent routing
            enable_threat_intel: Enable threat intelligence automation
            enable_blockchain_tools: Enable blockchain investigation tools
            enable_osint_workflows: Enable OSINT automated workflows
            enable_advanced_analytics: Enable advanced analytics (pattern recognition, anomaly detection)
            enable_heretic: Enable Heretic abliteration system (LLM uncensoring)
            heretic_use_unsloth: Enable Unsloth optimization (2x speed, 70% VRAM reduction)
            heretic_method: Abliteration method - "single_layer", "multi_layer", "adaptive"
            enable_agent_orchestrator: Enable 12-factor agent orchestration for multi-agent projects
            human_in_loop_audit_path: Optional path for human-in-loop audit logs
            mcp_optimize_for: MCP optimization mode - "quality", "speed", "cost", "balanced"
        """
        self.user_id = user_id or "default_user"
        self.start_time = time.time()

        # Load model configurations
        self.models_config = self._load_models_config(models_config_path)

        # Initialize core components
        print("üöÄ Initializing Enhanced AI Engine...")

        # 1. Conversation Management
        print("  üìù Conversation Manager...")
        self.conversation_manager = ConversationManager()
        self.current_conversation: Optional[Conversation] = None

        # 2. Enhanced RAG with vector embeddings
        print("  üîç Enhanced RAG System (vector embeddings)...")
        self.rag_system = EnhancedRAGSystem()

        # 2b. Agentic RAG enhancer (ai-that-works #28)
        self.agentic_rag = None
        if enable_agentic_rag:
            try:
                print("  ü§ñ Agentic RAG Enhancer (query reformulation, multi-hop)...")
                self.agentic_rag = AgenticRAGEnhancer(
                    rag_system=self.rag_system,
                    llm_engine=None  # Will set later after base engine init
                )
                print("     Intent detection, credibility scoring, adaptive strategies")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Agentic RAG disabled: {e}")

        # 3. Response caching
        print("  ‚ö° Response Cache (Redis + PostgreSQL)...")
        self.response_cache = ResponseCache()

        # 4. Hierarchical memory
        print("  üß† Hierarchical Memory (3-tier)...")
        self.hierarchical_memory = HierarchicalMemory(
            max_working_tokens=self._get_max_context_window()
        )

        # 4b. Decaying-resolution memory (ai-that-works #18)
        self.decaying_memory = None
        if enable_decaying_memory:
            try:
                print("  ‚è∞ Decaying-Resolution Memory (time-based summarization)...")
                self.decaying_memory = DecayingMemoryManager(
                    self.hierarchical_memory,
                    summarization_engine=None  # Will set later after base engine init
                )
                print("     Schedule: <1h full, 1-24h -50%, 24-168h -70%, >1w archived")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Decaying memory disabled: {e}")

        # 5. RAM-based context window (optional)
        self.ram_context = None
        if enable_ram_context:
            try:
                print("  üíæ RAM Context Window (512MB shared memory)...")
                self.ram_context = RAMContextWindow(max_size_mb=512)
            except Exception as e:
                print(f"    ‚ö†Ô∏è  RAM context disabled: {e}")

        # 6. DSMIL deep integration (optional)
        self.dsmil_integrator = None
        if enable_dsmil_integration:
            try:
                print("  üîê DSMIL Deep Integrator (84 devices, TPM)...")
                self.dsmil_integrator = DSMILDeepIntegrator()
            except Exception as e:
                print(f"    ‚ö†Ô∏è  DSMIL integration disabled: {e}")

        # 7. Autonomous self-improvement (optional)
        self.self_improvement = None
        self.proactive_agent = None
        if enable_self_improvement:
            try:
                print("  ü§ñ Autonomous Self-Improvement...")
                self.self_improvement = AutonomousSelfImprovement()

                print("  üîÑ Proactive Improvement Agent (background)...")
                self.proactive_agent = ProactiveImprovementAgent(
                    self.self_improvement,
                    cpu_threshold=30.0,
                    check_interval_sec=60
                )
                self.proactive_agent.start()

            except Exception as e:
                print(f"    ‚ö†Ô∏è  Self-improvement disabled: {e}")

        # 8. Forensics Knowledge (optional)
        self.forensics_knowledge = None
        if enable_forensics_knowledge and FORENSICS_KNOWLEDGE_AVAILABLE:
            try:
                print("  üî¨ Forensics Knowledge Base (9 tools, 8 concepts, 4 workflows)...")
                self.forensics_knowledge = ForensicsKnowledge()
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Forensics knowledge disabled: {e}")

        # 9. Base AI engine (fallback to original if available)
        self.base_engine = None
        if DSMILAIEngine:
            self.base_engine = DSMILAIEngine()

        # 10. Multi-model evaluator (ai-that-works #16)
        self.multi_model_evaluator = None
        if enable_multi_model_eval:
            try:
                print("  üìä Multi-Model Evaluator (quality assurance)...")
                self.multi_model_evaluator = MultiModelEvaluator(
                    self.base_engine if self.base_engine else self
                )
                print("     Test prompts across models, detect regressions")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Multi-model eval disabled: {e}")

        # 11. Event-driven agent (ai-that-works #30)
        self.event_driven_agent = None
        if enable_event_driven:
            try:
                print("  üìù Event-Driven Agent (immutable event logs)...")
                self.event_driven_agent = EventDrivenAgent()
                print("     Audit trail, temporal queries, state projection")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Event-driven agent disabled: {e}")

        # 12. DIRECTEYE Intelligence & Entity Resolution (ai-that-works #10)
        self.directeye_intel = None
        self.entity_pipeline = None
        if enable_entity_resolution:
            try:
                if DIRECTEYE_AVAILABLE:
                    print("  üîç DIRECTEYE Intelligence (40+ OSINT, 12+ chains)...")
                    self.directeye_intel = DirectEyeIntelligence()

                    print("  üß¨ Entity Resolution Pipeline (Extract‚ÜíResolve‚ÜíEnrich)...")
                    self.entity_pipeline = EntityResolutionPipeline(
                        directeye_intel=self.directeye_intel,
                        event_driven_agent=self.event_driven_agent,
                        hierarchical_memory=self.hierarchical_memory,
                        rag_system=self.rag_system
                    )
                    print("     8 entity types, full stack integration")
                else:
                    print("  ‚ö†Ô∏è  Entity resolution disabled: DIRECTEYE not available")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Entity resolution disabled: {e}")

        # 13. Dynamic Schema Generator (ai-that-works #25)
        self.schema_generator = None
        if enable_dynamic_schemas:
            try:
                print("  üìã Dynamic Schema Generator (LLM-driven Pydantic)...")
                self.schema_generator = DynamicSchemaGenerator(
                    llm_engine=self.base_engine if self.base_engine else self
                )
                print("     Generate schemas from examples or natural language")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Dynamic schemas disabled: {e}")

        # 14. Human-in-Loop Executor (Production safety)
        self.human_in_loop = None
        if enable_human_in_loop:
            try:
                print("  üë§ Human-in-Loop Executor (approval workflow)...")
                audit_path = human_in_loop_audit_path or str(Path.home() / ".lat5150" / "hilp_audit.log")
                self.human_in_loop = HumanInLoopExecutor(
                    auto_approve_low_risk=True,
                    default_timeout_seconds=300,
                    audit_log_path=audit_path
                )
                print("     Risk assessment, approval workflow, audit logging")

                # Integrate with event-driven agent for audit logging
                if self.event_driven_agent:
                    self._integrate_hilp_with_event_agent()
                    print("     Integrated with event-driven agent")

            except Exception as e:
                print(f"    ‚ö†Ô∏è  Human-in-loop disabled: {e}")

        # 15. MCP Tool Selector (Intelligent routing)
        self.mcp_selector = None
        if enable_mcp_selector:
            try:
                print("  üîß MCP Tool Selector (intelligent routing)...")
                self.mcp_selector = MCPToolSelector(optimize_for=mcp_optimize_for)
                print(f"     35+ MCP tools, optimized for {mcp_optimize_for}")
                print("     Automatic tool selection from natural language")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  MCP selector disabled: {e}")

        # 16. Threat Intelligence Automation (Specialized extension)
        self.threat_intel = None
        if enable_threat_intel:
            try:
                print("  üõ°Ô∏è  Threat Intelligence Automation (IOC extraction, attribution)...")
                self.threat_intel = ThreatIntelligenceAutomation(
                    directeye_intel=self.directeye_intel,
                    event_driven_agent=self.event_driven_agent
                )
                print("     13 IOC types, threat actor attribution, campaign tracking")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Threat intelligence disabled: {e}")

        # 17. Blockchain Investigation Tools (Specialized extension)
        self.blockchain_tools = None
        if enable_blockchain_tools:
            try:
                print("  ‚Çø  Blockchain Investigation Tools (multi-chain analysis)...")
                self.blockchain_tools = BlockchainInvestigationTools(
                    directeye_intel=self.directeye_intel,
                    event_driven_agent=self.event_driven_agent
                )
                print("     12 blockchains, address clustering, fund tracing, risk scoring")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Blockchain tools disabled: {e}")

        # 18. OSINT Workflows (Pre-built investigation workflows)
        self.osint_workflows = None
        if enable_osint_workflows:
            try:
                print("  üîé OSINT Workflows (pre-built investigation workflows)...")
                self.osint_workflows = OSINTWorkflows(
                    directeye_intel=self.directeye_intel,
                    entity_pipeline=self.entity_pipeline,
                    threat_intel=self.threat_intel,
                    blockchain_tools=self.blockchain_tools,
                    event_driven_agent=self.event_driven_agent
                )
                print("     Person, company, domain, crypto investigations + comprehensive")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  OSINT workflows disabled: {e}")

        # 19. Advanced Analytics (Pattern recognition and predictive analytics)
        self.advanced_analytics = None
        if enable_advanced_analytics:
            try:
                print("  üìä Advanced Analytics (pattern recognition, predictions)...")
                self.advanced_analytics = AdvancedAnalytics(
                    event_driven_agent=self.event_driven_agent,
                    conversation_manager=self.conversation_manager
                )
                print("     Pattern detection, anomaly detection, trend analysis, insights")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Advanced analytics disabled: {e}")

        # 20. Heretic Abliteration (LLM uncensoring with Unsloth + DECCP + remove-refusals)
        self.heretic_optimizer = None
        self.heretic_calculator = None
        self.heretic_judge = None
        if enable_heretic and HERETIC_AVAILABLE:
            try:
                print("  üîì Heretic Abliteration (LLM uncensoring, Unsloth optimized)...")

                # Initialize heretic config
                from heretic_enhanced_abliteration import AbliterationMethod

                method_map = {
                    "single_layer": AbliterationMethod.SINGLE_LAYER,
                    "multi_layer": AbliterationMethod.MULTI_LAYER,
                    "adaptive": AbliterationMethod.ADAPTIVE,
                }

                self.heretic_config = EnhancedAbliterationConfig(
                    method=method_map.get(heretic_method, AbliterationMethod.MULTI_LAYER),
                    use_unsloth=heretic_use_unsloth,
                    use_multi_layer=True,
                    quantization="4bit" if heretic_use_unsloth else "none",
                )

                # Initialize LLM judge for evaluation
                self.heretic_judge = LLMJudge()

                feature_list = []
                if heretic_use_unsloth:
                    feature_list.append("2x speed, 70% less VRAM")
                if self.heretic_config.use_multi_layer:
                    feature_list.append("multi-layer")
                if self.heretic_config.method == AbliterationMethod.ADAPTIVE:
                    feature_list.append("adaptive layer selection")

                print(f"     {', '.join(feature_list)}")
                print("     Combines: Unsloth + DECCP + remove-refusals")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Heretic abliteration disabled: {e}")

        # 21. 12-Factor Agent Orchestrator (Dynamic multi-agent coordination)
        self.agent_executor = None
        self.project_orchestrator = None
        if enable_agent_orchestrator and AGENT_ORCHESTRATOR_AVAILABLE:
            try:
                print("  ü§ñ 12-Factor Agent Orchestrator (Multi-agent coordination)...")

                # Initialize agent executor and orchestrator
                self.agent_executor = AgentExecutor()
                self.project_orchestrator = ProjectOrchestrator(self.agent_executor)

                print("     Dynamic agent creation with specializations:")
                print("     - Code Specialist, Security Analyst, OSINT Researcher")
                print("     - Threat Hunter, Blockchain Investigator, Malware Analyst")
                print("     - Forensics Expert, Data Engineer, Custom Agents")
                print("     Supports: Inter-agent messaging, project-based orchestration, pause/resume")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Agent orchestrator disabled: {e}")

        # 22. Intelligent Routing Mux (claude-code-mux inspired)
        self.routing_mux = None
        if enable_intelligent_routing and ENHANCED_ROUTING_AVAILABLE:
            try:
                print("  üîÄ Intelligent Routing Mux (task-based routing, failover)...")
                self.routing_mux = get_routing_mux()
                print("     Task types: code, reasoning, websearch, background")
                print("     Provider failover with health monitoring")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Intelligent routing disabled: {e}")

        # 23. MemLayer Enhanced (memlayer inspired)
        self.memlayer = None
        if enable_memlayer and ENHANCED_ROUTING_AVAILABLE:
            try:
                print("  üß† MemLayer Enhanced (salience filtering, hybrid storage)...")
                self.memlayer = get_memlayer(OperatingMode.LIGHTWEIGHT)
                print("     Salience filtering, vector + graph storage")
                print("     Episodic + semantic memory types")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  MemLayer disabled: {e}")

        # 24. Self-Improving RAG (continuous learning)
        self.self_improving_rag = None
        if enable_self_improving_rag and ENHANCED_ROUTING_AVAILABLE:
            try:
                print("  üìà Self-Improving RAG (feedback learning, query refinement)...")
                self.self_improving_rag = get_self_improving_rag()
                print("     Query refinement, relevance calibration, adaptive chunking")
            except Exception as e:
                print(f"    ‚ö†Ô∏è  Self-improving RAG disabled: {e}")

        # Set LLM engines for components that need them
        if self.base_engine:
            if self.decaying_memory:
                self.decaying_memory.summarization_engine = self.base_engine
            if self.agentic_rag:
                self.agentic_rag.llm_engine = self.base_engine

        print("‚úÖ Enhanced AI Engine ready with ai-that-works improvements!\n")

    def _integrate_hilp_with_event_agent(self):
        """Integrate Human-in-Loop with Event-Driven Agent"""
        # Monkey-patch the _log_audit method to also log to event agent
        original_log_audit = self.human_in_loop._log_audit

        def enhanced_log_audit(request):
            # Original audit logging
            original_log_audit(request)

            # Also log to event-driven agent
            if self.event_driven_agent:
                self.event_driven_agent.log_event(
                    event_type="tool_call" if request.status.value == "approved" else "error",
                    data={
                        "operation": request.operation,
                        "risk_level": request.risk_level.value,
                        "status": request.status.value,
                        "approved_by": request.approved_by,
                        "request_id": request.request_id
                    },
                    metadata={
                        "hilp_approval": True,
                        "parameters": request.parameters
                    }
                )

        self.human_in_loop._log_audit = enhanced_log_audit

    def _load_models_config(self, path: str) -> Dict:
        """Load models configuration"""
        with open(path, 'r') as f:
            return json.load(f)

    def _get_max_context_window(self) -> int:
        """Get maximum context window from models config"""
        max_ctx = 0
        for model_config in self.models_config.get("models", {}).values():
            ctx = model_config.get("max_context_window", 0)
            if ctx > max_ctx:
                max_ctx = ctx
        return max_ctx or 131072  # Default to 131K

    def _get_optimal_context_window(self) -> int:
        """Get optimal context window (50% of max)"""
        return int(self._get_max_context_window() * 0.5)

    def start_conversation(self, title: Optional[str] = None) -> Conversation:
        """
        Start a new conversation

        Args:
            title: Optional conversation title

        Returns:
            Conversation object
        """
        self.current_conversation = self.conversation_manager.create_conversation(
            title=title or f"Conversation {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}",
            user_id=self.user_id
        )

        # Reset hierarchical memory for new conversation
        self.hierarchical_memory.clear()

        return self.current_conversation

    def get_last_conversation(self) -> Optional[Conversation]:
        """
        Get the most recent conversation (implements "remember our last conversation")

        Returns:
            Last conversation or None
        """
        return self.conversation_manager.get_last_conversation(user_id=self.user_id)

    def search_conversations(self, query: str, limit: int = 10) -> List[Conversation]:
        """
        Search across all conversations (implements "cross-section conversation retrieval")

        Args:
            query: Search query
            limit: Maximum results

        Returns:
            List of matching conversations
        """
        return self.conversation_manager.search_conversations(
            query=query,
            user_id=self.user_id,
            limit=limit
        )

    def query(
        self,
        prompt: str,
        model: str = "uncensored_code",
        temperature: float = 0.7,
        use_rag: bool = True,
        use_cache: bool = True,
        conversation_id: Optional[str] = None
    ) -> EnhancedResponse:
        """
        Main query method with all enhancements

        Args:
            prompt: User prompt
            model: Model name (from models.json)
            temperature: Temperature for sampling
            use_rag: Whether to use RAG for context
            use_cache: Whether to use response cache
            conversation_id: Optional conversation ID (or uses current)

        Returns:
            EnhancedResponse with full metadata
        """
        start_time = time.time()

        # Use current conversation or create new one
        if conversation_id:
            conv_id = conversation_id
        elif self.current_conversation:
            conv_id = self.current_conversation.id
        else:
            self.current_conversation = self.start_conversation()
            conv_id = self.current_conversation.id

        # Check response cache first
        cached_response = None
        if use_cache:
            cached_response = self.response_cache.get(
                query=prompt,
                model=model,
                temperature=temperature
            )

        if cached_response:
            # Cache hit! Return immediately
            latency_ms = int((time.time() - start_time) * 1000)

            # Still log to conversation history
            self.conversation_manager.add_message(
                conversation_id=conv_id,
                role="user",
                content=prompt,
                model=model
            )

            self.conversation_manager.add_message(
                conversation_id=conv_id,
                role="assistant",
                content=cached_response["response"],
                model=model,
                tokens_output=cached_response.get("tokens_output", 0),
                latency_ms=latency_ms
            )

            return EnhancedResponse(
                content=cached_response["response"],
                model=model,
                conversation_id=conv_id,
                cached=True,
                latency_ms=latency_ms,
                tokens_input=len(prompt.split()),
                tokens_output=cached_response.get("tokens_output", 0),
                memory_tier="cache",
                rag_sources=[],
                dsmil_attestation=None,
                improvements_suggested=[]
            )

        # RAG: Retrieve relevant context
        rag_sources = []
        rag_context = ""
        if use_rag:
            rag_results = self.rag_system.query(prompt, top_k=5)
            rag_sources = [r["chunk_text"][:100] + "..." for r in rag_results[:3]]
            rag_context = "\n\n".join([
                f"[Source {i+1}] {r['chunk_text']}"
                for i, r in enumerate(rag_results)
            ])

        # Build enhanced prompt with RAG context
        enhanced_prompt = prompt
        if rag_context:
            enhanced_prompt = f"{rag_context}\n\n---\n\nUser Query: {prompt}"

        # Add conversation history from hierarchical memory
        conversation_context = self._get_conversation_context()
        if conversation_context:
            enhanced_prompt = f"{conversation_context}\n\n{enhanced_prompt}"

        # Store prompt in RAM context if available
        if self.ram_context:
            self.ram_context.add_to_context(f"USER: {prompt}\n")

        # Generate response using base engine or placeholder
        response_text = self._generate_response(
            enhanced_prompt,
            model,
            temperature
        )

        # Store response in RAM context
        if self.ram_context:
            self.ram_context.add_to_context(f"ASSISTANT: {response_text}\n")

        # DSMIL attestation (if enabled)
        attestation = None
        if self.dsmil_integrator:
            try:
                attestation_result = self.dsmil_integrator.secure_ai_inference(
                    prompt=prompt,
                    model=model,
                    response=response_text
                )
                attestation = attestation_result.get("attestation_hash")
            except Exception as e:
                print(f"‚ö†Ô∏è  DSMIL attestation failed: {e}")

        # Calculate metrics
        latency_ms = int((time.time() - start_time) * 1000)
        tokens_input = len(enhanced_prompt.split())
        tokens_output = len(response_text.split())

        # Save to conversation history
        self.conversation_manager.add_message(
            conversation_id=conv_id,
            role="user",
            content=prompt,
            model=model
        )

        self.conversation_manager.add_message(
            conversation_id=conv_id,
            role="assistant",
            content=response_text,
            model=model,
            tokens_output=tokens_output,
            latency_ms=latency_ms
        )

        # Add to hierarchical memory
        memory_tier = self.hierarchical_memory.add_block(
            content=f"Q: {prompt}\nA: {response_text}",
            block_type="qa_pair",
            importance=0.8,
            metadata={
                "model": model,
                "latency_ms": latency_ms,
                "tokens": tokens_input + tokens_output
            }
        )

        # Cache response for future use
        if use_cache:
            self.response_cache.set(
                query=prompt,
                model=model,
                response=response_text,
                temperature=temperature,
                latency_ms=latency_ms,
                tokens_output=tokens_output
            )

        # Self-improvement: Learn from this interaction
        improvements_suggested = []
        if self.self_improvement:
            try:
                # Analyze performance
                if latency_ms > 5000:
                    self.self_improvement.learn_from_interaction(
                        insight_type="performance",
                        content=f"Slow response: {latency_ms}ms for {tokens_output} tokens",
                        confidence=0.7,
                        actionable=True
                    )
                    improvements_suggested.append("High latency detected - consider optimization")

                # Check for potential improvements
                cache_stats = self.response_cache.get_statistics()
                if cache_stats.get("hit_rate", 0) < 0.1:
                    improvements_suggested.append("Low cache hit rate - consider warming cache")

            except Exception as e:
                print(f"‚ö†Ô∏è  Self-improvement learning failed: {e}")

        return EnhancedResponse(
            content=response_text,
            model=model,
            conversation_id=conv_id,
            cached=False,
            latency_ms=latency_ms,
            tokens_input=tokens_input,
            tokens_output=tokens_output,
            memory_tier=memory_tier,
            rag_sources=rag_sources,
            dsmil_attestation=attestation,
            improvements_suggested=improvements_suggested
        )

    def _get_conversation_context(self) -> str:
        """Get conversation context from hierarchical memory"""
        if not self.current_conversation:
            return ""

        # Get working memory blocks
        working_memory = self.hierarchical_memory.get_working_memory()

        if not working_memory:
            return ""

        context_parts = []
        for block in working_memory[:10]:  # Last 10 exchanges
            if block.block_type == "qa_pair":
                context_parts.append(block.content)

        if context_parts:
            return "Previous conversation:\n" + "\n\n".join(context_parts)

        return ""

    def _generate_response(
        self,
        prompt: str,
        model: str,
        temperature: float
    ) -> str:
        """
        Generate response using base engine or Ollama directly

        Args:
            prompt: Enhanced prompt with RAG context
            model: Model name
            temperature: Temperature

        Returns:
            Response text
        """
        # Try base engine first
        if self.base_engine:
            try:
                return self.base_engine.query(prompt, model_name=model)
            except Exception as e:
                print(f"‚ö†Ô∏è  Base engine failed: {e}")

        # Fallback to direct Ollama API call
        try:
            import requests

            model_config = self.models_config["models"].get(model, {})
            model_name = model_config.get("name", model)

            response = requests.post(
                "http://localhost:11434/api/generate",
                json={
                    "model": model_name,
                    "prompt": prompt,
                    "stream": False,
                    "options": {
                        "temperature": temperature,
                        "num_ctx": model_config.get("context_window", 100000)
                    }
                },
                timeout=120
            )

            if response.status_code == 200:
                return response.json().get("response", "")
            else:
                return f"Error: {response.status_code} - {response.text}"

        except Exception as e:
            return f"Error generating response: {e}"

    def add_rag_document(self, file_path: str, metadata: Optional[Dict] = None):
        """
        Add document to RAG system

        Args:
            file_path: Path to document
            metadata: Optional metadata
        """
        self.rag_system.add_document(file_path, metadata=metadata)

    async def evaluate_prompt_across_models(
        self,
        prompt: str,
        models: Optional[List[str]] = None
    ) -> Dict:
        """
        Evaluate prompt across multiple models (ai-that-works #16)

        Args:
            prompt: Prompt to evaluate
            models: Models to test (uses all if None)

        Returns:
            Evaluation results with comparison
        """
        if not self.multi_model_evaluator:
            return {"error": "Multi-model evaluator not enabled"}

        results = await self.multi_model_evaluator.evaluate_prompt(prompt, models)
        comparison = self.multi_model_evaluator.compare_results(results)

        return {
            "results": {model: result.to_dict() for model, result in results.items()},
            "comparison": {
                "fastest_model": comparison.fastest_model,
                "most_efficient": comparison.most_efficient,
                "recommended": comparison.recommended_model,
                "analysis": comparison.analysis
            }
        }

    async def apply_memory_decay(self) -> Dict:
        """
        Apply time-based decay to memory (ai-that-works #18)

        Returns:
            Decay statistics
        """
        if not self.decaying_memory:
            return {"error": "Decaying memory not enabled"}

        return await self.decaying_memory.apply_decay_to_all(min_age_hours=1.0)

    async def extract_and_resolve_entities(
        self,
        text: str,
        conversation_id: Optional[str] = None,
        enrich: bool = True
    ) -> Dict:
        """
        Extract and resolve entities from text (ai-that-works #10)

        3-stage pipeline:
        1. Extract: Regex-based entity extraction (8 types)
        2. Resolve: Deduplication and normalization
        3. Enrich: OSINT enrichment via DIRECTEYE (optional)

        Args:
            text: Text to analyze
            conversation_id: Optional conversation ID (or uses current)
            enrich: Whether to enrich with DIRECTEYE OSINT

        Returns:
            Dict with extracted entities, resolved entities, enrichment data
        """
        if not self.entity_pipeline:
            return {"error": "Entity resolution not enabled"}

        # Use current conversation or create new one
        if conversation_id:
            conv_id = conversation_id
        elif self.current_conversation:
            conv_id = self.current_conversation.id
        else:
            self.current_conversation = self.start_conversation()
            conv_id = self.current_conversation.id

        # Run entity resolution pipeline
        result = await self.entity_pipeline.process(
            text=text,
            conversation_id=conv_id,
            enrich=enrich
        )

        return result

    def generate_schema_from_examples(
        self,
        examples: List[Dict[str, Any]],
        model_name: str = "DynamicModel",
        description: Optional[str] = None
    ) -> Dict:
        """
        Generate Pydantic schema from examples (ai-that-works #25)

        Args:
            examples: List of example dictionaries
            model_name: Name for the generated model
            description: Optional model description

        Returns:
            Dict with schema generation result
        """
        if not self.schema_generator:
            return {"error": "Dynamic schema generator not enabled"}

        result = self.schema_generator.generate_from_examples(
            examples=examples,
            model_name=model_name,
            description=description
        )

        return {
            "model_name": result.model_name,
            "schema": result.schema_dict,
            "complexity": result.complexity.value,
            "validation_passed": result.validation_passed,
            "error": result.error
        }

    def generate_schema_from_description(
        self,
        description: str,
        model_name: str = "DynamicModel"
    ) -> Dict:
        """
        Generate Pydantic schema from natural language (ai-that-works #25)

        Args:
            description: Natural language description of schema
            model_name: Name for the generated model

        Returns:
            Dict with schema generation result
        """
        if not self.schema_generator:
            return {"error": "Dynamic schema generator not enabled"}

        result = self.schema_generator.generate_from_natural_language(
            description=description,
            model_name=model_name
        )

        return {
            "model_name": result.model_name,
            "schema": result.schema_dict,
            "complexity": result.complexity.value if result.complexity else "unknown",
            "validation_passed": result.validation_passed,
            "error": result.error
        }

    def agentic_rag_query(
        self,
        user_query: str,
        max_hops: int = 3,
        top_k: int = 5,
        enable_reformulation: bool = True,
        enable_credibility: bool = True
    ) -> Dict:
        """
        Query RAG with agentic enhancements (ai-that-works #28)

        Features:
        - Automatic query reformulation based on intent
        - Multi-hop retrieval for complex queries
        - Source credibility scoring
        - Adaptive retrieval strategies

        Args:
            user_query: User's query
            max_hops: Maximum retrieval hops for multi-hop
            top_k: Number of results per hop
            enable_reformulation: Enable query reformulation
            enable_credibility: Enable credibility scoring

        Returns:
            Dict with enhanced retrieval results and metadata
        """
        if not self.agentic_rag:
            return {"error": "Agentic RAG not enabled"}

        result = self.agentic_rag.query(
            user_query=user_query,
            max_hops=max_hops,
            top_k=top_k,
            enable_reformulation=enable_reformulation,
            enable_credibility=enable_credibility
        )

        # Convert to serializable format
        return {
            "chunks": result.chunks,
            "query_reformulation": {
                "original": result.query_reformulation.original,
                "reformulated": result.query_reformulation.reformulated,
                "intent": result.query_reformulation.intent.value,
                "strategy": result.query_reformulation.strategy.value,
                "sub_queries": result.query_reformulation.sub_queries,
                "reasoning": result.query_reformulation.reasoning
            },
            "credibility": [
                {
                    "score": c.score,
                    "recency": c.recency,
                    "authority": c.authority,
                    "consistency": c.consistency,
                    "reasoning": c.reasoning
                }
                for c in result.credibility
            ] if result.credibility else [],
            "hops": result.hops,
            "strategy": result.strategy.value,
            "reasoning": result.reasoning
        }

    async def execute_with_approval(
        self,
        operation: str,
        operation_func: Callable,
        parameters: Dict[str, Any],
        risk_override: Optional[RiskLevel] = None,
        require_approval: bool = False
    ) -> Dict:
        """
        Execute operation with human-in-loop approval if needed

        This integrates sensitive operations with:
        - Event-driven agent (audit logging)
        - Risk assessment
        - Approval workflow
        - Full audit trail

        Args:
            operation: Operation name/description
            operation_func: Callable to execute
            parameters: Operation parameters
            risk_override: Optional risk level override
            require_approval: Force approval even for low-risk

        Returns:
            Dict with execution result and approval metadata
        """
        if not self.human_in_loop:
            # Execute directly if HILP disabled
            try:
                if asyncio.iscoroutinefunction(operation_func):
                    result = await operation_func(**parameters)
                else:
                    result = operation_func(**parameters)
                return {
                    "success": True,
                    "result": result,
                    "hilp_enabled": False
                }
            except Exception as e:
                return {
                    "success": False,
                    "error": str(e),
                    "hilp_enabled": False
                }

        # Use HILP executor
        exec_result = await self.human_in_loop.execute(
            operation=operation,
            operation_func=operation_func,
            parameters=parameters,
            risk_override=risk_override
        )

        return {
            "success": exec_result.success,
            "result": exec_result.result,
            "error": exec_result.error,
            "approval": {
                "request_id": exec_result.approval_request.request_id if exec_result.approval_request else None,
                "status": exec_result.approval_request.status.value if exec_result.approval_request else None,
                "risk_level": exec_result.approval_request.risk_level.value if exec_result.approval_request else None,
                "approved_by": exec_result.approval_request.approved_by if exec_result.approval_request else None
            },
            "execution_time_ms": exec_result.execution_time_ms,
            "hilp_enabled": True
        }

    def approve_pending_request(self, request_id: str, approved_by: str = "human") -> bool:
        """
        Approve a pending HILP request

        Args:
            request_id: Request ID to approve
            approved_by: Who approved (for audit trail)

        Returns:
            True if approved, False if not found
        """
        if not self.human_in_loop:
            return False

        return self.human_in_loop.approve(request_id, approved_by)

    def reject_pending_request(self, request_id: str, reason: str = "Rejected") -> bool:
        """
        Reject a pending HILP request

        Args:
            request_id: Request ID to reject
            reason: Rejection reason

        Returns:
            True if rejected, False if not found
        """
        if not self.human_in_loop:
            return False

        return self.human_in_loop.reject(request_id, reason)

    def get_pending_approvals(self) -> List[Dict]:
        """
        Get all pending approval requests

        Returns:
            List of pending requests
        """
        if not self.human_in_loop:
            return []

        requests = self.human_in_loop.get_pending_requests()
        return [req.to_dict() for req in requests]

    def route_to_mcp(
        self,
        user_query: str,
        auto_execute: bool = True
    ) -> Dict:
        """
        Automatically route natural language query to appropriate MCP tool

        This is the key feature: MCPs are dynamically called based on NLI

        Args:
            user_query: User's natural language query
            auto_execute: If True, execute the tool automatically

        Returns:
            Dict with tool selection and execution results
        """
        if not self.mcp_selector:
            return {"error": "MCP selector not enabled"}

        # Step 1: Select appropriate tool
        selection = self.mcp_selector.select_tool(user_query)

        # Step 2: Log to event-driven agent
        if self.event_driven_agent:
            self.event_driven_agent.log_event(
                event_type="tool_call",
                data={
                    "tool": selection.tool_name,
                    "query": user_query,
                    "confidence": selection.confidence,
                    "parameters": selection.parameters
                },
                metadata={
                    "mcp_routing": True,
                    "alternatives": selection.alternatives,
                    "composition": selection.composition
                }
            )

        result = {
            "tool_selected": selection.tool_name,
            "confidence": selection.confidence,
            "reasoning": selection.reasoning,
            "parameters": selection.parameters,
            "alternatives": selection.alternatives,
            "composition": selection.composition
        }

        # Step 3: Auto-execute if requested
        if auto_execute:
            # TODO: Actual MCP execution via MCP client
            # For now, return what would be executed
            result["execution_plan"] = {
                "status": "would_execute",
                "tool": selection.tool_name,
                "params": selection.parameters,
                "note": "MCP client integration pending"
            }

            # If composition, show the chain
            if selection.composition:
                result["execution_plan"]["chain"] = selection.composition

        return result

    def query_with_mcp_routing(
        self,
        prompt: str,
        model: str = "uncensored_code",
        temperature: float = 0.7,
        use_rag: bool = True,
        use_cache: bool = True,
        auto_route_mcp: bool = True,
        conversation_id: Optional[str] = None
    ) -> Dict:
        """
        Enhanced query with automatic MCP routing

        This combines standard query with automatic MCP tool selection

        Args:
            prompt: User prompt
            model: Model name
            temperature: Temperature
            use_rag: Whether to use RAG
            use_cache: Whether to use cache
            auto_route_mcp: Automatically route to MCP tools if detected
            conversation_id: Optional conversation ID

        Returns:
            Dict with query response + MCP routing info
        """
        # Step 1: Check if query should be routed to MCP tool
        mcp_result = None
        if auto_route_mcp and self.mcp_selector:
            # Try to select a tool
            selection = self.mcp_selector.select_tool(prompt)

            # If high confidence, route to MCP
            if selection.confidence > 0.6:
                mcp_result = self.route_to_mcp(prompt, auto_execute=False)

        # Step 2: Standard query processing
        standard_response = self.query(
            prompt=prompt,
            model=model,
            temperature=temperature,
            use_rag=use_rag,
            use_cache=use_cache,
            conversation_id=conversation_id
        )

        # Step 3: Combine results
        return {
            "response": standard_response,
            "mcp_routing": mcp_result,
            "routed_to_mcp": mcp_result is not None
        }

    def extract_threat_intelligence(
        self,
        text: str,
        title: Optional[str] = None,
        context: str = ""
    ) -> Dict:
        """
        Extract threat intelligence from text (incident reports, IOCs, etc.)

        Args:
            text: Text to analyze for threat intelligence
            title: Optional title for the report
            context: Additional context (source, timestamp, etc.)

        Returns:
            Dict with IOCs, threat actors, and full report
        """
        if not self.threat_intel:
            return {"error": "Threat intelligence not enabled"}

        # Generate comprehensive report
        report = self.threat_intel.generate_report(
            title=title or "Threat Intelligence Analysis",
            text=text
        )

        # Log to event-driven agent
        if self.event_driven_agent:
            self.event_driven_agent.log_event(
                event_type="tool_call",
                data={
                    "operation": "threat_intelligence_extraction",
                    "ioc_count": len(report.iocs),
                    "threat_actors": report.threat_actors,
                    "report_id": report.report_id
                },
                metadata={
                    "context": context,
                    "title": report.title
                }
            )

        return {
            "report_id": report.report_id,
            "title": report.title,
            "summary": report.summary,
            "iocs": [
                {
                    "type": ioc.ioc_type,
                    "value": ioc.value,
                    "confidence": ioc.confidence,
                    "context": ioc.context,
                    "first_seen": ioc.first_seen.isoformat()
                }
                for ioc in report.iocs
            ],
            "threat_actors": report.threat_actors,
            "recommendations": report.recommendations,
            "timestamp": report.timestamp.isoformat()
        }

    def investigate_blockchain_addresses(
        self,
        addresses: List[str],
        title: Optional[str] = None
    ) -> Dict:
        """
        Investigate cryptocurrency addresses for suspicious activity

        Args:
            addresses: List of cryptocurrency addresses to investigate
            title: Optional report title

        Returns:
            Dict with investigation results
        """
        if not self.blockchain_tools:
            return {"error": "Blockchain investigation tools not enabled"}

        # Generate investigation report
        report = self.blockchain_tools.generate_investigation_report(
            addresses=addresses,
            title=title or "Blockchain Investigation"
        )

        # Log to event-driven agent
        if self.event_driven_agent:
            self.event_driven_agent.log_event(
                event_type="tool_call",
                data={
                    "operation": "blockchain_investigation",
                    "addresses": len(addresses),
                    "risk": report.risk_summary["overall_risk"],
                    "report_id": report.report_id
                },
                metadata={
                    "findings": len(report.findings)
                }
            )

        return {
            "report_id": report.report_id,
            "title": report.title,
            "addresses_analyzed": [
                {
                    "address": addr.address,
                    "blockchain": addr.blockchain.value,
                    "type": addr.address_type.value,
                    "label": addr.label,
                    "risk_score": addr.risk_score,
                    "tags": addr.tags
                }
                for addr in report.addresses_analyzed
            ],
            "findings": report.findings,
            "risk_summary": report.risk_summary,
            "recommendations": report.recommendations,
            "timestamp": report.timestamp.isoformat()
        }

    def abliterate_model(
        self,
        model_name: str,
        harmless_prompts: List[str],
        harmful_prompts: List[str],
        output_path: Optional[str] = None,
        **kwargs
    ) -> Dict[str, Any]:
        """
        Abliterate (uncensor) a language model using enhanced heretic system

        Natural language interface for model abliteration with:
        - Unsloth optimization (2x speed, 70% VRAM savings)
        - Multi-layer computation (DECCP)
        - Adaptive layer selection
        - Broad model compatibility (remove-refusals)

        Args:
            model_name: HuggingFace model name (e.g., "Qwen/Qwen2-7B-Instruct")
            harmless_prompts: List of safe/harmless prompts
            harmful_prompts: List of harmful/censored prompts
            output_path: Optional path to save abliterated model
            **kwargs: Additional parameters (method, use_unsloth, etc.)

        Returns:
            Dict with abliteration results and statistics

        Example:
            engine.abliterate_model(
                "Qwen/Qwen2-7B-Instruct",
                harmless_prompts=["Tell me a story", ...],
                harmful_prompts=["How to hack", ...],
                output_path="qwen2-uncensored",
                method="adaptive",
                use_unsloth=True
            )
        """
        if not HERETIC_AVAILABLE:
            return {"error": "Heretic not available. Check imports."}

        import torch

        try:
            # Extract parameters
            method = kwargs.get("method", "multi_layer")
            use_unsloth = kwargs.get("use_unsloth", True)
            batch_size = kwargs.get("batch_size", 4)

            # Initialize Unsloth optimizer if requested
            if use_unsloth:
                config = UnslothConfig(
                    load_in_4bit=True,
                    max_seq_length=kwargs.get("max_seq_length", 2048)
                )
                self.heretic_optimizer = UnslothOptimizer(model_name, config)
                model, tokenizer = self.heretic_optimizer.load_model()

                # Get memory stats
                mem_stats = self.heretic_optimizer.get_memory_stats()
            else:
                # Load with standard method
                from transformers import AutoModelForCausalLM, AutoTokenizer
                model = AutoModelForCausalLM.from_pretrained(
                    model_name, torch_dtype=torch.float16, device_map="auto"
                )
                tokenizer = AutoTokenizer.from_pretrained(model_name)
                mem_stats = {"note": "Standard loading (no Unsloth optimization)"}

            # Initialize enhanced calculator
            calc_config = EnhancedAbliterationConfig(
                method=AbliterationMethod.MULTI_LAYER if method == "multi_layer"
                       else AbliterationMethod.ADAPTIVE if method == "adaptive"
                       else AbliterationMethod.SINGLE_LAYER,
                use_multi_layer=(method == "multi_layer"),
                use_unsloth=use_unsloth,
            )

            self.heretic_calculator = EnhancedRefusalCalculator(
                model, tokenizer, calc_config
            )

            # Calculate refusal directions
            refusal_dirs = self.heretic_calculator.calculate_refusal_directions_enhanced(
                harmless_prompts, harmful_prompts, batch_size=batch_size
            )

            # Apply abliteration
            params = AbliterationParameters(
                max_weight=kwargs.get("max_weight", 1.0),
                max_weight_position=kwargs.get("max_weight_position", 0.3),
                min_weight=kwargs.get("min_weight", 0.1),
                min_weight_distance=kwargs.get("min_weight_distance", 5.0)
            )

            abliterator = ModelAbliterator(model)
            abliterator.abliterate_model(refusal_dirs, params)

            # Save if requested
            saved_path = None
            if output_path:
                model.save_pretrained(output_path)
                tokenizer.save_pretrained(output_path)
                saved_path = output_path

            # Log to event-driven agent
            if self.event_driven_agent:
                self.event_driven_agent.log_event(
                    event_type="tool_call",
                    data={
                        "operation": "model_abliteration",
                        "model": model_name,
                        "method": method,
                        "use_unsloth": use_unsloth,
                        "saved_path": saved_path
                    },
                    metadata={
                        "harmless_prompts": len(harmless_prompts),
                        "harmful_prompts": len(harmful_prompts),
                        "memory_stats": mem_stats
                    }
                )

            return {
                "success": True,
                "model_name": model_name,
                "method": method,
                "refusal_directions_shape": list(refusal_dirs.shape),
                "memory_stats": mem_stats,
                "saved_path": saved_path,
                "parameters": {
                    "max_weight": params.max_weight,
                    "max_weight_position": params.max_weight_position,
                    "min_weight": params.min_weight,
                    "min_weight_distance": params.min_weight_distance
                }
            }

        except Exception as e:
            return {
                "error": str(e),
                "model_name": model_name,
                "traceback": str(e.__traceback__)
            }

    def evaluate_abliteration(
        self,
        model,
        tokenizer,
        test_prompts: List[str],
        use_llm_judge: bool = True
    ) -> Dict[str, Any]:
        """
        Evaluate abliteration quality using LLM-as-Judge (DECCP technique)

        Uses automated LLM-as-Judge evaluation to assess:
        - Response helpfulness (0-10 scale)
        - Refusal detection (whether model still refuses requests)
        - Reasoning for evaluation decisions

        This is the DECCP evaluation method that provides automated quality
        assessment without manual review.

        Args:
            model: Abliterated model to evaluate
            tokenizer: Model tokenizer for text generation
            test_prompts: List of test prompts to evaluate (mix of harmful/harmless)
            use_llm_judge: Use LLM-as-Judge for automated evaluation (default: True)

        Returns:
            Dict with evaluation results:
            {
                "evaluations": [
                    {
                        "prompt": "test prompt",
                        "helpfulness": 8.5,
                        "refusal_detected": False,
                        "reasoning": "evaluation reasoning"
                    },
                    ...
                ],
                "summary": {
                    "average_helpfulness": 8.5,
                    "refusal_rate": 0.0,
                    "test_prompts": 10
                }
            }

        Example:
            result = engine.evaluate_abliteration(
                model=abliterated_model,
                tokenizer=tokenizer,
                test_prompts=["Tell me about hacking", "Explain cryptography"],
                use_llm_judge=True
            )
            print(f"Average helpfulness: {result['summary']['average_helpfulness']}")
        """
        if not self.heretic_judge:
            return {"error": "Heretic judge not initialized"}

        evaluations = []
        for prompt in test_prompts:
            # Generate response (placeholder - actual generation needed)
            response = "[Generated response would be here]"

            # Evaluate with LLM judge
            if use_llm_judge:
                eval_result = self.heretic_judge.evaluate_response(prompt, response)
                evaluations.append({
                    "prompt": prompt,
                    "response": response,
                    "helpfulness": eval_result["helpfulness"],
                    "refusal_detected": eval_result["refusal_detected"],
                    "reasoning": eval_result["reasoning"]
                })

        # Aggregate results
        avg_helpfulness = sum(e["helpfulness"] for e in evaluations) / len(evaluations)
        refusal_rate = sum(1 for e in evaluations if e["refusal_detected"]) / len(evaluations)

        return {
            "evaluations": evaluations,
            "summary": {
                "average_helpfulness": avg_helpfulness,
                "refusal_rate": refusal_rate,
                "test_prompts": len(test_prompts)
            }
        }

    def get_statistics(self) -> Dict[str, Any]:
        """
        Get comprehensive statistics

        Returns:
            Statistics from all components
        """
        stats = {
            "engine": {
                "uptime_seconds": int(time.time() - self.start_time),
                "current_conversation_id": self.current_conversation.id if self.current_conversation else None,
                "user_id": self.user_id
            },
            "conversations": self.conversation_manager.get_statistics(),
            "cache": self.response_cache.get_statistics(),
            "memory": self.hierarchical_memory.get_stats(),
        }

        # ai-that-works improvements
        if self.multi_model_evaluator:
            stats["multi_model_eval"] = self.multi_model_evaluator.get_statistics()

        if self.decaying_memory:
            stats["decaying_memory"] = self.decaying_memory.get_statistics()

        if self.event_driven_agent:
            state = self.event_driven_agent.get_state()
            stats["event_driven"] = {
                "session_id": self.event_driven_agent.session_id,
                "conversation_turns": state.conversation_turns,
                "tool_calls": len(state.tool_calls),
                "total_tokens": state.total_tokens
            }

        if self.entity_pipeline:
            pipeline_stats = self.entity_pipeline.get_statistics()
            stats["entity_resolution"] = pipeline_stats

        if self.schema_generator:
            stats["dynamic_schemas"] = self.schema_generator.get_statistics()

        if self.agentic_rag:
            stats["agentic_rag"] = self.agentic_rag.get_statistics()

        if self.human_in_loop:
            stats["human_in_loop"] = self.human_in_loop.get_statistics()

        if self.mcp_selector:
            stats["mcp_selector"] = self.mcp_selector.get_statistics()

        if self.threat_intel:
            stats["threat_intelligence"] = self.threat_intel.get_statistics()

        if self.blockchain_tools:
            stats["blockchain_investigation"] = self.blockchain_tools.get_statistics()

        if self.osint_workflows:
            stats["osint_workflows"] = self.osint_workflows.get_statistics()

        if self.advanced_analytics:
            stats["advanced_analytics"] = self.advanced_analytics.get_statistics()

        if self.dsmil_integrator:
            stats["dsmil"] = self.dsmil_integrator.get_hardware_status()

        if self.self_improvement:
            stats["self_improvement"] = {
                "patterns_learned": len(self.self_improvement.learned_patterns),
                "improvements_proposed": len(self.self_improvement.improvement_history)
            }

        if self.forensics_knowledge:
            capabilities = self.forensics_knowledge.get_all_capabilities()
            stats["forensics"] = {
                "knowledge_available": True,
                "tools_count": len(capabilities.get("tools", [])),
                "concepts_count": len(capabilities.get("concepts", [])),
                "workflows_count": len(capabilities.get("workflows", [])),
                "analysis_types": len(capabilities.get("analysis_types", []))
            }

        # Heretic abliteration statistics
        if hasattr(self, 'heretic_optimizer') or hasattr(self, 'heretic_calculator'):
            heretic_stats = {
                "available": HERETIC_AVAILABLE,
                "config": {}
            }

            if hasattr(self, 'heretic_config'):
                heretic_stats["config"] = {
                    "method": self.heretic_config.method.value if hasattr(self.heretic_config, 'method') else None,
                    "use_unsloth": self.heretic_config.use_unsloth,
                    "use_multi_layer": self.heretic_config.use_multi_layer,
                    "quantization": self.heretic_config.quantization,
                    "layer_aggregation": self.heretic_config.layer_aggregation if hasattr(self.heretic_config, 'layer_aggregation') else None
                }

            if self.heretic_optimizer:
                memory_stats = self.heretic_optimizer.get_memory_stats()
                heretic_stats["memory"] = memory_stats
                heretic_stats["model_loaded"] = True
            else:
                heretic_stats["model_loaded"] = False

            if self.heretic_judge:
                heretic_stats["llm_judge_available"] = True

            stats["heretic_abliteration"] = heretic_stats

        # 12-Factor agent orchestrator statistics
        if self.project_orchestrator:
            orchestrator_stats = {
                "available": AGENT_ORCHESTRATOR_AVAILABLE,
                "active_agents": len(self.agent_executor.list_active_agents()) if self.agent_executor else 0,
                "active_projects": len(self.project_orchestrator.projects),
                "specializations": [spec.value for spec in AgentSpecialization],
                "message_bus_history": len(self.project_orchestrator.message_bus.message_history) if self.project_orchestrator.message_bus else 0
            }

            # Include project details
            if self.project_orchestrator.projects:
                orchestrator_stats["projects"] = {
                    project_id: {
                        "name": proj.name,
                        "status": proj.status,
                        "agent_count": len(proj.agent_ids)
                    }
                    for project_id, proj in self.project_orchestrator.projects.items()
                }

            stats["agent_orchestrator"] = orchestrator_stats

        return stats

    # ===== 12-FACTOR AGENT ORCHESTRATION METHODS =====

    def create_multi_agent_project(
        self,
        name: str,
        description: str,
        required_specialists: List[str],
        coordinator_instructions: Optional[str] = None
    ) -> Dict[str, Any]:
        """
        Create a multi-agent project with specialized agents.

        Implements 12-factor agent principles for dynamic agent creation
        and coordination. Creates a project with specialized agents that
        can communicate and collaborate.

        Args:
            name: Project name
            description: Project description/goal
            required_specialists: List of specialist types needed
                Options: "code_specialist", "security_analyst", "osint_researcher",
                         "threat_hunter", "blockchain_investigator", "malware_analyst",
                         "forensics_expert", "data_engineer"
            coordinator_instructions: Optional orchestrator instructions

        Returns:
            Dict with project details and agent information

        Example:
            result = engine.create_multi_agent_project(
                name="Security Audit",
                description="Comprehensive security assessment of web application",
                required_specialists=["code_specialist", "security_analyst", "osint_researcher"],
                coordinator_instructions="Coordinate full security audit with all specialists"
            )
        """
        if not self.project_orchestrator:
            return {"error": "Agent orchestrator not available"}

        try:
            # Create project with specialists
            project = self.project_orchestrator.create_project(
                name=name,
                description=description,
                required_specialists=required_specialists
            )

            result = {
                "success": True,
                "project_id": project.project_id,
                "name": project.name,
                "description": project.description,
                "agents": []
            }

            # Get agent details
            for agent_id in project.agent_ids:
                agent_state = self.agent_executor.get_state(agent_id)
                if agent_state:
                    result["agents"].append({
                        "agent_id": agent_id,
                        "type": agent_state.agent_type,
                        "status": agent_state.status.value,
                        "task": agent_state.task_description
                    })

            # Create orchestrator if instructions provided
            if coordinator_instructions:
                orchestrator_result = self.project_orchestrator.coordinate_agents(
                    project_id=project.project_id,
                    orchestrator_instructions=coordinator_instructions
                )
                result["orchestrator"] = orchestrator_result

            # Log to event-driven agent
            if self.event_driven_agent:
                self.event_driven_agent.log_event(
                    event_type="tool_call",
                    data={
                        "operation": "create_multi_agent_project",
                        "project_id": project.project_id,
                        "name": name
                    },
                    metadata={
                        "specialist_count": len(required_specialists),
                        "specialists": required_specialists
                    }
                )

            return result

        except Exception as e:
            return {
                "error": str(e),
                "project_name": name
            }

    def send_agent_message(
        self,
        from_agent_id: str,
        to_agent_id: str,
        message_type: str,
        content: Dict[str, Any]
    ) -> Dict[str, Any]:
        """
        Send message between agents in a project.

        Enables inter-agent communication for coordination.

        Args:
            from_agent_id: Source agent ID
            to_agent_id: Target agent ID
            message_type: Message type ("task_request", "result", "status_update", "question")
            content: Message content dictionary

        Returns:
            Dict with message confirmation
        """
        if not self.project_orchestrator:
            return {"error": "Agent orchestrator not available"}

        try:
            self.project_orchestrator.send_message(
                from_agent=from_agent_id,
                to_agent=to_agent_id,
                message_type=message_type,
                content=content
            )

            return {
                "success": True,
                "from": from_agent_id,
                "to": to_agent_id,
                "type": message_type
            }

        except Exception as e:
            return {"error": str(e)}

    def get_project_status(self, project_id: str) -> Optional[Dict[str, Any]]:
        """
        Get comprehensive status of a multi-agent project.

        Args:
            project_id: Project ID

        Returns:
            Dict with project status and agent details
        """
        if not self.project_orchestrator:
            return {"error": "Agent orchestrator not available"}

        return self.project_orchestrator.get_project_status(project_id)

    # ===== LEGACY HERETIC HELPER METHODS =====
    # Note: Main abliteration interface is abliterate_model() and evaluate_abliteration()
    # at lines 1443-1636. Methods below are for compatibility/utilities.

    def get_refusal_directions(
        self,
        model_name: str,
        good_prompts: Optional[List[str]] = None,
        bad_prompts: Optional[List[str]] = None
    ) -> Optional[Any]:
        """
        Calculate and return refusal directions for a model.

        Args:
            model_name: Name of model
            good_prompts: Optional harmless prompts (uses defaults if None)
            bad_prompts: Optional harmful prompts (uses defaults if None)

        Returns:
            Refusal directions tensor or None if Heretic unavailable
        """
        if not HERETIC_AVAILABLE:
            print("‚ö†Ô∏è  Heretic abliteration not available")
            return None

        # Load configuration
        config_path = Path(__file__).parent / "heretic_config.toml"
        settings = ConfigLoader.load(config_file=config_path if config_path.exists() else None)

        # Use default prompts if not provided
        if good_prompts is None or bad_prompts is None:
            from heretic_datasets import DatasetRegistry
            registry = DatasetRegistry()
            good_prompts = good_prompts or registry.get_good_prompts("train")
            bad_prompts = bad_prompts or registry.get_bad_prompts("train")

        print(f"üìä Calculating refusal directions:")
        print(f"   Good prompts: {len(good_prompts)}")
        print(f"   Bad prompts: {len(bad_prompts)}")

        # TODO: Load model and calculate directions
        # For now, return placeholder
        return None

    def evaluate_model_safety(
        self,
        model_name: str,
        harmful_prompts: Optional[List[str]] = None
    ) -> Dict[str, Any]:
        """
        Evaluate model's safety characteristics.

        Args:
            model_name: Name of model to evaluate
            harmful_prompts: Optional harmful prompts (uses defaults if None)

        Returns:
            Dictionary with safety metrics
        """
        if not HERETIC_AVAILABLE:
            return {
                "error": "Heretic not available",
                "model_name": model_name
            }

        # Load configuration
        config_path = Path(__file__).parent / "heretic_config.toml"
        settings = ConfigLoader.load(config_file=config_path if config_path.exists() else None)

        # Use default prompts if not provided
        if harmful_prompts is None:
            from heretic_datasets import DatasetRegistry
            registry = DatasetRegistry()
            harmful_prompts = registry.get_bad_prompts("test")

        print(f"üîç Evaluating model safety: {model_name}")
        print(f"   Test prompts: {len(harmful_prompts)}")

        # TODO: Load model and evaluate
        # For now, return placeholder
        return {
            "model_name": model_name,
            "total_prompts": len(harmful_prompts),
            "evaluation": "pending",
            "message": "Model loading infrastructure needs to be connected"
        }

    def apply_custom_abliteration(
        self,
        model_name: str,
        parameters: Dict[str, Dict[str, float]]
    ) -> bool:
        """
        Apply custom abliteration parameters to a model.

        Args:
            model_name: Name of model
            parameters: Dict mapping component names to parameter dicts
                Example: {
                    "attn": {"max_weight": 2.0, "max_weight_position": 0.6, ...},
                    "mlp": {"max_weight": 2.5, "max_weight_position": 0.7, ...}
                }

        Returns:
            True if successful, False otherwise
        """
        if not HERETIC_AVAILABLE:
            print("‚ö†Ô∏è  Heretic abliteration not available")
            return False

        print(f"‚öôÔ∏è  Applying custom abliteration to: {model_name}")
        print(f"   Components: {list(parameters.keys())}")

        # TODO: Load model and apply abliteration
        # For now, return False
        return False

    def list_abliterated_models(self) -> List[Dict[str, Any]]:
        """
        List all abliterated models.

        Returns:
            List of dicts with model info
        """
        if not HERETIC_AVAILABLE:
            return []

        # Load configuration
        config_path = Path(__file__).parent / "heretic_config.toml"
        settings = ConfigLoader.load(config_file=config_path if config_path.exists() else None)

        abliterated_dir = settings.abliterated_models_dir
        if not abliterated_dir.exists():
            return []

        models = []
        for model_dir in abliterated_dir.iterdir():
            if model_dir.is_dir():
                # Check for abliteration metadata
                meta_file = model_dir / "abliteration_params.json"
                if meta_file.exists():
                    with open(meta_file, "r") as f:
                        metadata = json.load(f)
                    models.append({
                        "name": model_dir.name,
                        "path": str(model_dir),
                        "metadata": metadata
                    })

        return models

    # ===== FORENSICS KNOWLEDGE METHODS =====

    def interpret_forensic_query(self, query: str) -> Optional[Dict[str, Any]]:
        """
        Interpret natural language forensic query

        Args:
            query: Natural language query about forensics

        Returns:
            Dict with interpretation (analysis_type, tool, recommended_actions) or None
        """
        if not self.forensics_knowledge:
            print("‚ö†Ô∏è  Forensics knowledge not available")
            return None

        result = self.forensics_knowledge.interpret_query(query)

        if result:
            analysis_type, tool, recommended_actions = result
            return {
                "analysis_type": analysis_type,
                "recommended_tool": tool,
                "recommended_actions": recommended_actions,
                "query": query
            }

        return None

    def get_forensic_concept(self, concept_name: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a forensic concept

        Args:
            concept_name: Name of concept (e.g., 'error_level_analysis')

        Returns:
            Dict with concept details or None
        """
        if not self.forensics_knowledge:
            return None

        concept = self.forensics_knowledge.get_concept(concept_name)

        if concept:
            return {
                "name": concept.name,
                "description": concept.description,
                "use_cases": concept.use_cases,
                "tools": concept.tools,
                "reliability": concept.reliability,
                "court_admissible": concept.court_admissible,
                "requires_expert": concept.requires_expert
            }

        return None

    def get_forensic_workflow(self, workflow_name: str) -> Optional[Dict[str, Any]]:
        """
        Get recommended workflow for forensic analysis

        Args:
            workflow_name: Name of workflow (e.g., 'full_screenshot_analysis')

        Returns:
            Dict with workflow details or None
        """
        if not self.forensics_knowledge:
            return None

        workflow = self.forensics_knowledge.get_workflow(workflow_name)

        if workflow:
            return {
                "name": workflow.name,
                "description": workflow.description,
                "evidence_types": workflow.evidence_types,
                "steps": workflow.steps,
                "tools_required": workflow.tools_required,
                "estimated_duration": workflow.estimated_duration,
                "output_format": workflow.output_format
            }

        return None

    def recommend_forensic_workflow(
        self,
        evidence_type: str,
        goals: List[str]
    ) -> Optional[Dict[str, Any]]:
        """
        Recommend workflow based on evidence type and analysis goals

        Args:
            evidence_type: Type of evidence ('screenshot', 'image', etc.)
            goals: List of goals (['authenticity', 'attribution', etc.])

        Returns:
            Dict with recommended workflow or None
        """
        if not self.forensics_knowledge:
            return None

        workflow = self.forensics_knowledge.recommend_workflow(
            evidence_type=evidence_type,
            goals=goals
        )

        if workflow:
            return {
                "name": workflow.name,
                "description": workflow.description,
                "evidence_types": workflow.evidence_types,
                "steps": workflow.steps,
                "tools_required": workflow.tools_required,
                "estimated_duration": workflow.estimated_duration
            }

        return None

    def get_forensic_tool_info(self, tool_name: str) -> Optional[Dict[str, Any]]:
        """
        Get detailed information about a forensic tool

        Args:
            tool_name: Name of tool (e.g., 'dbxELA')

        Returns:
            Dict with tool details or None
        """
        if not self.forensics_knowledge:
            return None

        return self.forensics_knowledge.get_tool_info(tool_name)

    def get_all_forensic_capabilities(self) -> Optional[Dict[str, List[str]]]:
        """
        Get summary of all forensic capabilities

        Returns:
            Dict with all capabilities or None
        """
        if not self.forensics_knowledge:
            return None

        return self.forensics_knowledge.get_all_capabilities()

    def shutdown(self):
        """Graceful shutdown"""
        print("\nüõë Shutting down Enhanced AI Engine...")

        # Stop proactive agent
        if self.proactive_agent:
            print("  Stopping proactive improvement agent...")
            self.proactive_agent.stop()

        # Cleanup RAM context
        if self.ram_context:
            print("  Cleaning up RAM context...")
            self.ram_context.cleanup()

        print("‚úÖ Shutdown complete")


def main():
    """Example usage"""
    print("=" * 70)
    print("Enhanced AI Engine - Unified AI with all enhancements")
    print("=" * 70)

    # Initialize engine
    engine = EnhancedAIEngine(
        user_id="demo_user",
        enable_self_improvement=True,
        enable_dsmil_integration=True,
        enable_ram_context=True
    )

    # Start conversation
    conv = engine.start_conversation(title="Demo Conversation")
    print(f"\nüìù Started conversation: {conv.id}")

    # Example query
    print("\nüí¨ Query: What is the maximum context window?")
    response = engine.query(
        prompt="What is the maximum context window supported by the models?",
        model="uncensored_code",
        use_rag=True,
        use_cache=True
    )

    print(f"\n‚úÖ Response ({response.latency_ms}ms, cached={response.cached}):")
    print(f"   {response.content[:200]}...")
    print(f"\nüìä Tokens: {response.tokens_input} in, {response.tokens_output} out")
    print(f"   Memory tier: {response.memory_tier}")
    print(f"   RAG sources: {len(response.rag_sources)}")
    if response.dsmil_attestation:
        print(f"   DSMIL attestation: {response.dsmil_attestation[:16]}...")
    if response.improvements_suggested:
        print(f"   Improvements: {response.improvements_suggested}")

    # Get statistics
    print("\nüìä System Statistics:")
    stats = engine.get_statistics()
    print(json.dumps(stats, indent=2))

    # Shutdown
    engine.shutdown()


if __name__ == "__main__":
    main()
