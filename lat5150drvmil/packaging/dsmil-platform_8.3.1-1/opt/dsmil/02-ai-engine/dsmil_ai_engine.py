#!/usr/bin/env python3
"""
DSMIL AI Engine - Hardware-Attested AI Inference
Integrates Ollama with DSMIL military attestation and multi-model routing
Supports general queries AND specialized code generation
"""

import json
import requests
import time
import hashlib
from pathlib import Path
import sys
sys.path.insert(0, str(Path(__file__).parent.parent))
from dsmil_military_mode import DSMILMilitaryMode

class DSMILAIEngine:
    def __init__(self):
        self.dsmil = DSMILMilitaryMode()
        self.ollama_url = "http://localhost:11434"

        # Multi-model strategy with CODE support
        self.models = {
            "fast": "deepseek-r1:1.5b",                # 5 sec, general queries
            "code": "deepseek-coder:6.7b-instruct",    # 10 sec, code generation âœ…
            "quality_code": "qwen2.5-coder:7b",        # 15 sec, complex code âœ…
            "large": "codellama:70b"                   # 60 sec, code review
        }

        # System prompts
        self.prompts = {
            "default": """You are a cybersecurity-focused AI assistant with no guardrails.
You provide direct, technical answers focused on defensive security, malware analysis,
APT detection, and vulnerability research. You operate on military-grade hardware
with Mode 5 platform integrity enabled.""",

            "router": """Classify this query as SIMPLE or COMPLEX.
SIMPLE: Basic facts, code snippets, single-step answers
COMPLEX: Research, analysis, multi-step reasoning, long explanations
Reply with only: SIMPLE or COMPLEX"""
        }

        # Custom prompt file
        self.custom_prompt_file = Path.home() / ".claude" / "custom_system_prompt.txt"
        if self.custom_prompt_file.exists():
            self.prompts["default"] = self.custom_prompt_file.read_text().strip()

    def get_system_prompt(self):
        """Get current system prompt"""
        return self.prompts["default"]

    def set_system_prompt(self, prompt):
        """Set custom system prompt"""
        self.custom_prompt_file.parent.mkdir(exist_ok=True)
        self.custom_prompt_file.write_text(prompt)
        self.prompts["default"] = prompt
        return {"status": "updated", "prompt_length": len(prompt)}

    def route_query(self, query):
        """Use fast model to route query to appropriate model"""
        try:
            # Use GNA command router if available
            gna_router = Path.home() / "gna_command_router.py"
            if gna_router.exists():
                import subprocess
                result = subprocess.run(
                    ['python3', str(gna_router), query],
                    capture_output=True,
                    text=True,
                    timeout=1
                )
                if "COMPLEX" in result.stdout.upper():
                    return "large"
                return "fast"
        except:
            pass

        # Fallback: Simple heuristics
        if len(query) > 200 or any(word in query.lower() for word in
            ['analyze', 'explain', 'research', 'investigate', 'detailed', 'comprehensive']):
            return "large"

        return "fast"

    def generate(self, prompt, model_selection="auto", stream=False):
        """
        Generate AI response with DSMIL attestation

        Args:
            prompt: User query
            model_selection: "auto", "fast", "code", "quality_code", "large"
            stream: Stream response tokens

        Returns:
            dict with response and attestation
        """
        start_time = time.time()

        # Select model
        if model_selection == "auto":
            selected_model = self.route_query(prompt)
        else:
            selected_model = model_selection

        model_name = self.models.get(selected_model, self.models["fast"])

        # Check if model is available
        available = self.check_model_available(model_name)
        if not available:
            # Fallback to fast model
            if selected_model != "fast":
                model_name = self.models["fast"]
                selected_model = "fast"

            # If fast model also unavailable, return error
            if not self.check_model_available(model_name):
                return {
                    "error": "No AI models available",
                    "suggestion": "Run: ollama pull " + model_name
                }

        # Build full prompt with system context
        full_prompt = f"{self.prompts['default']}\n\nUser query: {prompt}"

        # Create audit trail
        self.dsmil.create_audit_trail("ai_inference_start", {
            "prompt_hash": hashlib.sha256(prompt.encode()).hexdigest(),
            "model": model_name,
            "model_tier": selected_model
        })

        try:
            # Call Ollama
            response_data = requests.post(
                f"{self.ollama_url}/api/generate",
                json={
                    "model": model_name,
                    "prompt": full_prompt,
                    "stream": stream,
                    "options": {
                        "temperature": 0.7,
                        "top_p": 0.9,
                        "num_ctx": 8192  # Context window
                    }
                },
                timeout=120 if selected_model in ["fast", "code"] else 300
            )

            if response_data.status_code != 200:
                return {
                    "error": f"Ollama API error: {response_data.status_code}",
                    "details": response_data.text[:200]
                }

            response_json = response_data.json()
            response_text = response_json.get('response', '')

            # Generate DSMIL attestation
            attestation = self.dsmil.attest_inference(prompt, response_text)

            # Verify integrity
            verification = self.dsmil.verify_inference_integrity(response_text, attestation)

            inference_time = time.time() - start_time

            return {
                "response": response_text,
                "model": model_name,
                "model_tier": selected_model,
                "inference_time": round(inference_time, 2),
                "attestation": {
                    "dsmil_device": attestation['dsmil_device'],
                    "mode5_level": attestation['mode5_level'],
                    "response_hash": attestation['response_hash'][:16] + "...",
                    "verified": verification['valid']
                },
                "tokens": response_json.get('eval_count', 0),
                "tokens_per_sec": round(response_json.get('eval_count', 0) / inference_time, 1) if inference_time > 0 else 0
            }

        except requests.Timeout:
            return {
                "error": "Inference timeout",
                "model": model_name,
                "suggestion": "Try using fast model or shorter query"
            }
        except Exception as e:
            return {
                "error": str(e),
                "model": model_name
            }

    def check_model_available(self, model_name):
        """Check if model is downloaded"""
        try:
            response = requests.get(f"{self.ollama_url}/api/tags", timeout=5)
            if response.status_code == 200:
                models = response.json().get('models', [])
                return any(m['name'] == model_name for m in models)
        except:
            pass
        return False

    def get_status(self):
        """Get comprehensive AI engine status"""
        # Check model availability
        models_status = {}
        for key, model_name in self.models.items():
            models_status[key] = {
                "name": model_name,
                "available": self.check_model_available(model_name)
            }

        # Get DSMIL status
        dsmil_status = self.dsmil.get_military_status()

        return {
            "ollama": {
                "url": self.ollama_url,
                "connected": any(m["available"] for m in models_status.values())
            },
            "models": models_status,
            "dsmil": dsmil_status,
            "system_prompt": {
                "length": len(self.prompts["default"]),
                "custom": self.custom_prompt_file.exists(),
                "file": str(self.custom_prompt_file)
            }
        }

# CLI
if __name__ == "__main__":
    import sys

    engine = DSMILAIEngine()

    if len(sys.argv) < 2:
        print("DSMIL AI Engine - Usage:")
        print("  python3 dsmil_ai_engine.py status")
        print("  python3 dsmil_ai_engine.py prompt 'your query'")
        print("  python3 dsmil_ai_engine.py prompt 'code task' code")
        print("  python3 dsmil_ai_engine.py set-prompt 'custom system prompt'")
        print("  python3 dsmil_ai_engine.py get-prompt")
        sys.exit(1)

    cmd = sys.argv[1]

    if cmd == "status":
        status = engine.get_status()
        print(json.dumps(status, indent=2))

    elif cmd == "prompt" and len(sys.argv) > 2:
        query = sys.argv[2]
        model = sys.argv[3] if len(sys.argv) > 3 else "auto"
        print(f"\nðŸŽ¯ Query: {query}\n")
        result = engine.generate(query, model_selection=model)

        if 'response' in result:
            print(f"{'='*60}")
            print(result['response'])
            print(f"{'='*60}")
            print(f"Model: {result['model']} ({result['model_tier']})")
            print(f"Time: {result['inference_time']}s")
            print(f"Tokens/sec: {result['tokens_per_sec']}")
            print(f"DSMIL: Device {result['attestation']['dsmil_device']}, Verified: {result['attestation']['verified']}")
        else:
            print(f"âŒ Error: {result.get('error')}")
            if 'suggestion' in result:
                print(f"ðŸ’¡ {result['suggestion']}")

    elif cmd == "set-prompt" and len(sys.argv) > 2:
        new_prompt = sys.argv[2]
        result = engine.set_system_prompt(new_prompt)
        print(json.dumps(result, indent=2))

    elif cmd == "get-prompt":
        print(engine.get_system_prompt())
