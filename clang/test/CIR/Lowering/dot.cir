// RUN: cir-opt %s -cir-to-llvm --reconcile-unrealized-casts -o %t.mlir
// RUN: FileCheck --input-file=%t.mlir %s -check-prefix=MLIR

!s32i = !cir.int<s, 32>
module {
  cir.func @dot(%arg0: !cir.ptr<!cir.double>, %arg1: !cir.ptr<!cir.double>, %arg2: !s32i) -> !cir.double {
    %0 = cir.alloca !cir.ptr<!cir.double>, !cir.ptr<!cir.ptr<!cir.double>>, ["a", init] {alignment = 8 : i64}
    %1 = cir.alloca !cir.ptr<!cir.double>, !cir.ptr<!cir.ptr<!cir.double>>, ["b", init] {alignment = 8 : i64}
    %2 = cir.alloca !s32i, !cir.ptr<!s32i>, ["size", init] {alignment = 4 : i64}
    %3 = cir.alloca !cir.double, !cir.ptr<!cir.double>, ["__retval"] {alignment = 8 : i64}
    %4 = cir.alloca !cir.double, !cir.ptr<!cir.double>, ["q", init] {alignment = 8 : i64}
    cir.store %arg0, %0 : !cir.ptr<!cir.double>, !cir.ptr<!cir.ptr<!cir.double>>
    cir.store %arg1, %1 : !cir.ptr<!cir.double>, !cir.ptr<!cir.ptr<!cir.double>>
    cir.store %arg2, %2 : !s32i, !cir.ptr<!s32i>
    %5 = cir.const #cir.fp<0.000000e+00> : !cir.double
    cir.store %5, %4 : !cir.double, !cir.ptr<!cir.double>
    cir.scope {
      %8 = cir.alloca !s32i, !cir.ptr<!s32i>, ["i", init] {alignment = 4 : i64}
      %9 = cir.const #cir.int<0> : !s32i
      cir.store %9, %8 : !s32i, !cir.ptr<!s32i>
      cir.for : cond {
        %10 = cir.load %8 : !cir.ptr<!s32i>, !s32i
        %11 = cir.load %2 : !cir.ptr<!s32i>, !s32i
        %12 = cir.cmp(lt, %10, %11) : !s32i, !s32i
        %13 = cir.cast(int_to_bool, %12 : !s32i), !cir.bool
        cir.condition(%13)
      } body {
        %10 = cir.load %0 : !cir.ptr<!cir.ptr<!cir.double>>, !cir.ptr<!cir.double>
        %11 = cir.load %8 : !cir.ptr<!s32i>, !s32i
        %12 = cir.ptr_stride(%10 : !cir.ptr<!cir.double>, %11 : !s32i), !cir.ptr<!cir.double>
        %13 = cir.load %12 : !cir.ptr<!cir.double>, !cir.double
        %14 = cir.load %1 : !cir.ptr<!cir.ptr<!cir.double>>, !cir.ptr<!cir.double>
        %15 = cir.load %8 : !cir.ptr<!s32i>, !s32i
        %16 = cir.ptr_stride(%14 : !cir.ptr<!cir.double>, %15 : !s32i), !cir.ptr<!cir.double>
        %17 = cir.load %16 : !cir.ptr<!cir.double>, !cir.double
        %18 = cir.binop(mul, %13, %17) : !cir.double
        %19 = cir.load %4 : !cir.ptr<!cir.double>, !cir.double
        %20 = cir.binop(add, %19, %18) : !cir.double
        cir.store %20, %4 : !cir.double, !cir.ptr<!cir.double>
        cir.yield
      } step {
        %10 = cir.load %8 : !cir.ptr<!s32i>, !s32i
        %11 = cir.unary(inc, %10) : !s32i, !s32i
        cir.store %11, %8 : !s32i, !cir.ptr<!s32i>
        cir.yield
      }
    }
    %6 = cir.load %4 : !cir.ptr<!cir.double>, !cir.double
    cir.store %6, %3 : !cir.double, !cir.ptr<!cir.double>
    %7 = cir.load %3 : !cir.ptr<!cir.double>, !cir.double
    cir.return %7 : !cir.double
  }
}

// MLIR-LABEL:   llvm.func @dot(
// MLIR:           %[[VAL_3:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_4:.*]] = llvm.alloca %[[VAL_3]] x !llvm.ptr {alignment = 8 : i64} : (i64) -> !llvm.ptr
// MLIR:           %[[VAL_5:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_6:.*]] = llvm.alloca %[[VAL_5]] x !llvm.ptr {alignment = 8 : i64} : (i64) -> !llvm.ptr
// MLIR:           %[[VAL_7:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_8:.*]] = llvm.alloca %[[VAL_7]] x i32 {alignment = 4 : i64} : (i64) -> !llvm.ptr
// MLIR:           %[[VAL_9:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_10:.*]] = llvm.alloca %[[VAL_9]] x f64 {alignment = 8 : i64} : (i64) -> !llvm.ptr
// MLIR:           %[[VAL_11:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_12:.*]] = llvm.alloca %[[VAL_11]] x f64 {alignment = 8 : i64} : (i64) -> !llvm.ptr
// MLIR:           llvm.store {{.*}}, %[[VAL_4]] {{.*}}: !llvm.ptr, !llvm.ptr
// MLIR:           llvm.store {{.*}}, %[[VAL_6]] {{.*}}: !llvm.ptr, !llvm.ptr
// MLIR:           llvm.store {{.*}}, %[[VAL_8]] {{.*}}: i32, !llvm.ptr
// MLIR:           %[[VAL_13:.*]] = llvm.mlir.constant(0.000000e+00 : f64) : f64
// MLIR:           llvm.store %[[VAL_13]], %[[VAL_12]] {{.*}}: f64, !llvm.ptr
// MLIR:           %[[VAL_14:.*]] = llvm.mlir.constant(1 : index) : i64
// MLIR:           %[[VAL_15:.*]] = llvm.alloca %[[VAL_14]] x i32 {alignment = 4 : i64} : (i64) -> !llvm.ptr
// MLIR:           %[[VAL_16:.*]] = llvm.mlir.constant(0 : i32) : i32
// MLIR:           llvm.store %[[VAL_16]], %[[VAL_15]] {{.*}}: i32, !llvm.ptr
// MLIR:           llvm.br ^bb1
// MLIR:         ^bb1:
// MLIR:           %[[VAL_17:.*]] = llvm.load %[[VAL_15]] {alignment = 4 : i64} : !llvm.ptr -> i32
// MLIR:           %[[VAL_18:.*]] = llvm.load %[[VAL_8]] {alignment = 4 : i64} : !llvm.ptr -> i32
// MLIR:           %[[VAL_19:.*]] = llvm.icmp "slt" %[[VAL_17]], %[[VAL_18]] : i32
// MLIR:           %[[VAL_20:.*]] = llvm.zext %[[VAL_19]] : i1 to i32
// MLIR:           %[[VAL_21:.*]] = llvm.mlir.constant(0 : i32) : i32
// MLIR:           %[[VAL_22:.*]] = llvm.icmp "ne" %[[VAL_20]], %[[VAL_21]] : i32
// MLIR:           llvm.cond_br %[[VAL_22]], ^bb2, ^bb3
// MLIR:         ^bb2:
// MLIR:           %[[VAL_23:.*]] = llvm.load %[[VAL_4]] {alignment = 8 : i64} : !llvm.ptr -> !llvm.ptr
// MLIR:           %[[VAL_24:.*]] = llvm.load %[[VAL_15]] {alignment = 4 : i64} : !llvm.ptr -> i32
// MLIR:           %[[VAL_25:.*]] = llvm.sext %[[VAL_24]] : i32 to i64
// MLIR:           %[[VAL_26:.*]] = llvm.getelementptr %[[VAL_23]]{{\[}}%[[VAL_25]]] : (!llvm.ptr, i64) -> !llvm.ptr, f64
// MLIR:           %[[VAL_27:.*]] = llvm.load %[[VAL_26]] {alignment = 8 : i64} : !llvm.ptr -> f64
// MLIR:           %[[VAL_28:.*]] = llvm.load %[[VAL_6]] {alignment = 8 : i64} : !llvm.ptr -> !llvm.ptr
// MLIR:           %[[VAL_29:.*]] = llvm.load %[[VAL_15]] {alignment = 4 : i64} : !llvm.ptr -> i32
// MLIR:           %[[VAL_30:.*]] = llvm.sext %[[VAL_29]] : i32 to i64
// MLIR:           %[[VAL_31:.*]] = llvm.getelementptr %[[VAL_28]]{{\[}}%[[VAL_30]]] : (!llvm.ptr, i64) -> !llvm.ptr, f64
// MLIR:           %[[VAL_32:.*]] = llvm.load %[[VAL_31]] {alignment = 8 : i64} : !llvm.ptr -> f64
// MLIR:           %[[VAL_33:.*]] = llvm.fmul %[[VAL_27]], %[[VAL_32]]  : f64
// MLIR:           %[[VAL_34:.*]] = llvm.load %[[VAL_12]] {alignment = 8 : i64} : !llvm.ptr -> f64
// MLIR:           %[[VAL_35:.*]] = llvm.fadd %[[VAL_34]], %[[VAL_33]]  : f64
// MLIR:           llvm.store %[[VAL_35]], %[[VAL_12]] {{.*}}: f64, !llvm.ptr
// MLIR:           %[[VAL_36:.*]] = llvm.load %[[VAL_15]] {alignment = 4 : i64} : !llvm.ptr -> i32
// MLIR:           %[[VAL_37:.*]] = llvm.mlir.constant(1 : i32) : i32
// MLIR:           %[[VAL_38:.*]] = llvm.add %[[VAL_36]], %[[VAL_37]] : i32
// MLIR:           llvm.store %[[VAL_38]], %[[VAL_15]] {{.*}}: i32, !llvm.ptr
// MLIR:           llvm.br ^bb1
// MLIR:         ^bb3:
// MLIR:           %[[VAL_39:.*]] = llvm.load %[[VAL_12]] {alignment = 8 : i64} : !llvm.ptr -> f64
// MLIR:           llvm.store %[[VAL_39]], %[[VAL_10]] {{.*}}: f64, !llvm.ptr
// MLIR:           %[[VAL_40:.*]] = llvm.load %[[VAL_10]] {alignment = 8 : i64} : !llvm.ptr -> f64
// MLIR:           llvm.return %[[VAL_40]] : f64
// MLIR:         }