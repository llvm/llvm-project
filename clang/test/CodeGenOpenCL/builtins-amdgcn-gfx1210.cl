// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -cl-std=CL2.0 -O0 -triple amdgcn-unknown-unknown -target-cpu gfx1210 -S -emit-llvm -o - %s | FileCheck %s
// REQUIRES: amdgpu-registered-target

#pragma OPENCL EXTENSION cl_khr_fp16 : enable

typedef unsigned int uint;
typedef unsigned short int ushort;
typedef unsigned int __attribute__((ext_vector_type(3))) uint3;
typedef __bf16 __attribute__((ext_vector_type(2))) bfloat2;
typedef __bf16 __attribute__((ext_vector_type(4))) bfloat4;
typedef __bf16 __attribute__((ext_vector_type(16))) bfloat16;
typedef half __attribute__((ext_vector_type(2))) half2;
typedef half __attribute__((ext_vector_type(4))) half4;
typedef half __attribute__((ext_vector_type(16))) half16;

// CHECK-LABEL: @test_setprio_inc_wg(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.amdgcn.s.setprio.inc.wg(i16 10)
// CHECK-NEXT:    ret void
//
void test_setprio_inc_wg() {
  __builtin_amdgcn_s_setprio_inc_wg(10);
}

// CHECK-LABEL: @test_s_monitor_sleep(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    call void @llvm.amdgcn.s.monitor.sleep(i16 10)
// CHECK-NEXT:    ret void
//
void test_s_monitor_sleep() {
  __builtin_amdgcn_s_monitor_sleep(10);
}

// CHECK-LABEL: @test_bitop3_b32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    store i32 [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[C:%.*]], ptr addrspace(5) [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(5) [[C_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.bitop3.i32(i32 [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i8 1)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr addrspace(1) [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
void test_bitop3_b32(global uint* out, uint a, uint b, uint c) {
  *out = __builtin_amdgcn_bitop3_b32(a, b, c, 1);
}

// CHECK-LABEL: @test_bitop3_b16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[C_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    store i16 [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    store i16 [[C:%.*]], ptr addrspace(5) [[C_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load i16, ptr addrspace(5) [[C_ADDR]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = call i16 @llvm.amdgcn.bitop3.i16(i16 [[TMP0]], i16 [[TMP1]], i16 [[TMP2]], i8 1)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[TMP3]], ptr addrspace(1) [[TMP4]], align 2
// CHECK-NEXT:    ret void
//
void test_bitop3_b16(global ushort* out, ushort a, ushort b, ushort c) {
  *out = __builtin_amdgcn_bitop3_b16(a, b, c, 1);
}

// CHECK-LABEL: @test_prng_b32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i32 @llvm.amdgcn.prng.b32(i32 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP1]], ptr addrspace(1) [[TMP2]], align 4
// CHECK-NEXT:    ret void
//
void test_prng_b32(global uint* out, uint a) {
  *out = __builtin_amdgcn_prng_b32(a);
}

// CHECK-LABEL: @test_tanh_f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store float [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.amdgcn.tanh.f32(float [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store float [[TMP1]], ptr addrspace(1) [[TMP2]], align 4
// CHECK-NEXT:    ret void
//
void test_tanh_f32(global float* out, float a)
{
  *out = __builtin_amdgcn_tanhf(a);
}

// CHECK-LABEL: @test_tanh_f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store ptr addrspace(1) [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP0:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[A_ADDR]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr addrspace(1) [[TMP0]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = call half @llvm.amdgcn.tanh.f16(half [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store half [[TMP2]], ptr addrspace(1) [[TMP3]], align 2
// CHECK-NEXT:    ret void
//
void test_tanh_f16(global half* out, global half* a)
{
  *out = __builtin_amdgcn_tanhh(*a);
}

// CHECK-LABEL: @test_tanh_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.tanh.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_tanh_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_tanh_bf16(a);
}

// CHECK-LABEL: @test_rcp_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.rcp.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_rcp_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_rcp_bf16(a);
}

// CHECK-LABEL: @test_sqrt_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.sqrt.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_sqrt_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_sqrt_bf16(a);
}

// CHECK-LABEL: @test_rsq_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.rsq.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_rsq_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_rsq_bf16(a);
}

// CHECK-LABEL: @test_log_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.log.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_log_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_log_bf16(a);
}

// CHECK-LABEL: @test_exp2_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.exp2.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_exp2_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_exp2_bf16(a);
}

// CHECK-LABEL: @test_sin_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.sin.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_sin_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_sin_bf16(a);
}

// CHECK-LABEL: @test_cos_bf16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca bfloat, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load bfloat, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call bfloat @llvm.amdgcn.cos.bf16(bfloat [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store bfloat [[TMP1]], ptr addrspace(1) [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
void test_cos_bf16(global __bf16* out, __bf16 a)
{
  *out = __builtin_amdgcn_cos_bf16(a);
}

// CHECK-LABEL: @test_cvt_pk_bf16_f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store float [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    store float [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <2 x bfloat> @llvm.amdgcn.cvt.pk.bf16.f32(float [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store <2 x bfloat> [[TMP2]], ptr addrspace(1) [[TMP3]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_pk_bf16_f32(global bfloat2* out, float a, float b)
{
  *out = __builtin_amdgcn_cvt_pk_bf16_f32(a, b);
}

// CHECK-LABEL: @test_cvt_sr_pk_bf16_f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store float [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    store float [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SR:%.*]], ptr addrspace(5) [[SR_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(5) [[SR_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x bfloat> @llvm.amdgcn.cvt.sr.pk.bf16.f32(float [[TMP0]], float [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store <2 x bfloat> [[TMP3]], ptr addrspace(1) [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_sr_pk_bf16_f32(global bfloat2* out, float a, float b, uint sr)
{
  *out = __builtin_amdgcn_cvt_sr_pk_bf16_f32(a, b, sr);
}

// CHECK-LABEL: @test_cvt_sr_pk_f16_f32(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store float [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    store float [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    store i32 [[SR:%.*]], ptr addrspace(5) [[SR_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr addrspace(5) [[B_ADDR]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(5) [[SR_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x half> @llvm.amdgcn.cvt.sr.pk.f16.f32(float [[TMP0]], float [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store <2 x half> [[TMP3]], ptr addrspace(1) [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_sr_pk_f16_f32(global half2* out, float a, float b, uint sr)
{
  *out = __builtin_amdgcn_cvt_sr_pk_f16_f32(a, b, sr);
}

// CHECK-LABEL: @test_cvt_f16_fp8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP2]], i64 0
// CHECK-NEXT:    store half [[TMP1]], ptr addrspace(1) [[ARRAYIDX]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP3]], i32 1)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP5]], i64 1
// CHECK-NEXT:    store half [[TMP4]], ptr addrspace(1) [[ARRAYIDX1]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP6]], i32 2)
// CHECK-NEXT:    [[TMP8:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP8]], i64 2
// CHECK-NEXT:    store half [[TMP7]], ptr addrspace(1) [[ARRAYIDX2]], align 2
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = call half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP9]], i32 3)
// CHECK-NEXT:    [[TMP11:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP11]], i64 3
// CHECK-NEXT:    store half [[TMP10]], ptr addrspace(1) [[ARRAYIDX3]], align 2
// CHECK-NEXT:    ret void
//
void test_cvt_f16_fp8(global half* out, int a)
{
  out[0] = __builtin_amdgcn_cvt_f16_fp8(a, 0);
  out[1] = __builtin_amdgcn_cvt_f16_fp8(a, 1);
  out[2] = __builtin_amdgcn_cvt_f16_fp8(a, 2);
  out[3] = __builtin_amdgcn_cvt_f16_fp8(a, 3);
}

// CHECK-LABEL: @test_cvt_f16_bf8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP2]], i64 0
// CHECK-NEXT:    store half [[TMP1]], ptr addrspace(1) [[ARRAYIDX]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP3]], i32 1)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP5]], i64 1
// CHECK-NEXT:    store half [[TMP4]], ptr addrspace(1) [[ARRAYIDX1]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP6]], i32 2)
// CHECK-NEXT:    [[TMP8:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP8]], i64 2
// CHECK-NEXT:    store half [[TMP7]], ptr addrspace(1) [[ARRAYIDX2]], align 2
// CHECK-NEXT:    [[TMP9:%.*]] = load i32, ptr addrspace(5) [[A_ADDR]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = call half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP9]], i32 3)
// CHECK-NEXT:    [[TMP11:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds half, ptr addrspace(1) [[TMP11]], i64 3
// CHECK-NEXT:    store half [[TMP10]], ptr addrspace(1) [[ARRAYIDX3]], align 2
// CHECK-NEXT:    ret void
//
void test_cvt_f16_bf8(global half* out, int a)
{
  out[0] = __builtin_amdgcn_cvt_f16_bf8(a, 0);
  out[1] = __builtin_amdgcn_cvt_f16_bf8(a, 1);
  out[2] = __builtin_amdgcn_cvt_f16_bf8(a, 2);
  out[3] = __builtin_amdgcn_cvt_f16_bf8(a, 3);
}

// CHECK-LABEL: @test_cvt_pk_f16_fp8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call <2 x half> @llvm.amdgcn.cvt.pk.f16.fp8(i16 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x half>, ptr addrspace(1) [[TMP2]], i64 0
// CHECK-NEXT:    store <2 x half> [[TMP1]], ptr addrspace(1) [[ARRAYIDX]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_pk_f16_fp8(global half2* out, short a)
{
  out[0] = __builtin_amdgcn_cvt_pk_f16_fp8(a);
}

// CHECK-LABEL: @test_cvt_pk_f16_bf8(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call <2 x half> @llvm.amdgcn.cvt.pk.f16.bf8(i16 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds <2 x half>, ptr addrspace(1) [[TMP2]], i64 0
// CHECK-NEXT:    store <2 x half> [[TMP1]], ptr addrspace(1) [[ARRAYIDX]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_pk_f16_bf8(global half2* out, short a)
{
  out[0] = __builtin_amdgcn_cvt_pk_f16_bf8(a);
}

// CHECK-LABEL: @test_cvt_pk_bf8_f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store half [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    store half [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = call i16 @llvm.amdgcn.cvt.pk.bf8.f16(half [[TMP0]], half [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[TMP2]], ptr addrspace(1) [[TMP3]], align 2
// CHECK-NEXT:    ret void
//
void test_cvt_pk_bf8_f16(global short* out, half a, half b)
{
  *out = __builtin_amdgcn_cvt_pk_bf8_f16(a, b);
}

// CHECK-LABEL: @test_cvt_pk_fp8_f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store half [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    store half [[B:%.*]], ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load half, ptr addrspace(5) [[B_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = call i16 @llvm.amdgcn.cvt.pk.fp8.f16(half [[TMP0]], half [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i16 [[TMP2]], ptr addrspace(1) [[TMP3]], align 2
// CHECK-NEXT:    ret void
//
void test_cvt_pk_fp8_f16(global short* out, half a, half b)
{
  *out = __builtin_amdgcn_cvt_pk_fp8_f16(a, b);
}

// CHECK-LABEL: @test_cvt_sr_bf8_f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[OLD_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store half [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    store i16 [[SR:%.*]], ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    store i32 [[OLD:%.*]], ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.cvt.sr.bf8.f16(half [[TMP0]], i16 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr addrspace(1) [[TMP4]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call i32 @llvm.amdgcn.cvt.sr.bf8.f16(half [[TMP5]], i16 [[TMP6]], i32 [[TMP7]], i32 1)
// CHECK-NEXT:    [[TMP9:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP8]], ptr addrspace(1) [[TMP9]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = call i32 @llvm.amdgcn.cvt.sr.bf8.f16(half [[TMP10]], i16 [[TMP11]], i32 [[TMP12]], i32 2)
// CHECK-NEXT:    [[TMP14:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP13]], ptr addrspace(1) [[TMP14]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP16:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = call i32 @llvm.amdgcn.cvt.sr.bf8.f16(half [[TMP15]], i16 [[TMP16]], i32 [[TMP17]], i32 3)
// CHECK-NEXT:    [[TMP19:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP18]], ptr addrspace(1) [[TMP19]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_sr_bf8_f16(global int* out, half a, short sr, int old)
{
  *out = __builtin_amdgcn_cvt_sr_bf8_f16(a, sr, old, 0);
  *out = __builtin_amdgcn_cvt_sr_bf8_f16(a, sr, old, 1);
  *out = __builtin_amdgcn_cvt_sr_bf8_f16(a, sr, old, 2);
  *out = __builtin_amdgcn_cvt_sr_bf8_f16(a, sr, old, 3);
}

// CHECK-LABEL: @test_cvt_sr_fp8_f16(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[OLD_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUT:%.*]], ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store half [[A:%.*]], ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    store i16 [[SR:%.*]], ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    store i32 [[OLD:%.*]], ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.cvt.sr.fp8.f16(half [[TMP0]], i16 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr addrspace(1) [[TMP4]], align 4
// CHECK-NEXT:    [[TMP5:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP6:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP7:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP8:%.*]] = call i32 @llvm.amdgcn.cvt.sr.fp8.f16(half [[TMP5]], i16 [[TMP6]], i32 [[TMP7]], i32 1)
// CHECK-NEXT:    [[TMP9:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP8]], ptr addrspace(1) [[TMP9]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP11:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = call i32 @llvm.amdgcn.cvt.sr.fp8.f16(half [[TMP10]], i16 [[TMP11]], i32 [[TMP12]], i32 2)
// CHECK-NEXT:    [[TMP14:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP13]], ptr addrspace(1) [[TMP14]], align 4
// CHECK-NEXT:    [[TMP15:%.*]] = load half, ptr addrspace(5) [[A_ADDR]], align 2
// CHECK-NEXT:    [[TMP16:%.*]] = load i16, ptr addrspace(5) [[SR_ADDR]], align 2
// CHECK-NEXT:    [[TMP17:%.*]] = load i32, ptr addrspace(5) [[OLD_ADDR]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = call i32 @llvm.amdgcn.cvt.sr.fp8.f16(half [[TMP15]], i16 [[TMP16]], i32 [[TMP17]], i32 3)
// CHECK-NEXT:    [[TMP19:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUT_ADDR]], align 8
// CHECK-NEXT:    store i32 [[TMP18]], ptr addrspace(1) [[TMP19]], align 4
// CHECK-NEXT:    ret void
//
void test_cvt_sr_fp8_f16(global int* out, half a, short sr, int old)
{
  *out = __builtin_amdgcn_cvt_sr_fp8_f16(a, sr, old, 0);
  *out = __builtin_amdgcn_cvt_sr_fp8_f16(a, sr, old, 1);
  *out = __builtin_amdgcn_cvt_sr_fp8_f16(a, sr, old, 2);
  *out = __builtin_amdgcn_cvt_sr_fp8_f16(a, sr, old, 3);
}

// CHECK-LABEL: @test_cvt_scale_pk(
// CHECK-NEXT:  entry:
// CHECK-NEXT:    [[OUTH16_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[OUTY16_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[SRC3_ADDR:%.*]] = alloca <3 x i32>, align 16, addrspace(5)
// CHECK-NEXT:    [[OUTH4_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[OUTY4_ADDR:%.*]] = alloca ptr addrspace(1), align 8, addrspace(5)
// CHECK-NEXT:    [[SRC1_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    store ptr addrspace(1) [[OUTH16:%.*]], ptr addrspace(5) [[OUTH16_ADDR]], align 8
// CHECK-NEXT:    store ptr addrspace(1) [[OUTY16:%.*]], ptr addrspace(5) [[OUTY16_ADDR]], align 8
// CHECK-NEXT:    [[EXTRACTVEC:%.*]] = shufflevector <3 x i32> [[SRC3:%.*]], <3 x i32> poison, <4 x i32> <i32 0, i32 1, i32 2, i32 poison>
// CHECK-NEXT:    store <4 x i32> [[EXTRACTVEC]], ptr addrspace(5) [[SRC3_ADDR]], align 16
// CHECK-NEXT:    store ptr addrspace(1) [[OUTH4:%.*]], ptr addrspace(5) [[OUTH4_ADDR]], align 8
// CHECK-NEXT:    store ptr addrspace(1) [[OUTY4:%.*]], ptr addrspace(5) [[OUTY4_ADDR]], align 8
// CHECK-NEXT:    store i32 [[SRC1:%.*]], ptr addrspace(5) [[SRC1_ADDR]], align 4
// CHECK-NEXT:    store float [[SCALE:%.*]], ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[LOADVEC4:%.*]] = load <4 x i32>, ptr addrspace(5) [[SRC3_ADDR]], align 16
// CHECK-NEXT:    [[EXTRACTVEC1:%.*]] = shufflevector <4 x i32> [[LOADVEC4]], <4 x i32> poison, <3 x i32> <i32 0, i32 1, i32 2>
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call <16 x half> @llvm.amdgcn.cvt.scale.pk.f16.fp6(<3 x i32> [[EXTRACTVEC1]], float [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTH16_ADDR]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP1]], ptr addrspace(1) [[TMP2]], align 32
// CHECK-NEXT:    [[LOADVEC42:%.*]] = load <4 x i32>, ptr addrspace(5) [[SRC3_ADDR]], align 16
// CHECK-NEXT:    [[EXTRACTVEC3:%.*]] = shufflevector <4 x i32> [[LOADVEC42]], <4 x i32> poison, <3 x i32> <i32 0, i32 1, i32 2>
// CHECK-NEXT:    [[TMP3:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call <16 x bfloat> @llvm.amdgcn.cvt.scale.pk.bf16.fp6(<3 x i32> [[EXTRACTVEC3]], float [[TMP3]])
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTY16_ADDR]], align 8
// CHECK-NEXT:    store <16 x bfloat> [[TMP4]], ptr addrspace(1) [[TMP5]], align 32
// CHECK-NEXT:    [[LOADVEC44:%.*]] = load <4 x i32>, ptr addrspace(5) [[SRC3_ADDR]], align 16
// CHECK-NEXT:    [[EXTRACTVEC5:%.*]] = shufflevector <4 x i32> [[LOADVEC44]], <4 x i32> poison, <3 x i32> <i32 0, i32 1, i32 2>
// CHECK-NEXT:    [[TMP6:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP7:%.*]] = call <16 x half> @llvm.amdgcn.cvt.scale.pk.f16.bf6(<3 x i32> [[EXTRACTVEC5]], float [[TMP6]])
// CHECK-NEXT:    [[TMP8:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTH16_ADDR]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP7]], ptr addrspace(1) [[TMP8]], align 32
// CHECK-NEXT:    [[LOADVEC46:%.*]] = load <4 x i32>, ptr addrspace(5) [[SRC3_ADDR]], align 16
// CHECK-NEXT:    [[EXTRACTVEC7:%.*]] = shufflevector <4 x i32> [[LOADVEC46]], <4 x i32> poison, <3 x i32> <i32 0, i32 1, i32 2>
// CHECK-NEXT:    [[TMP9:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP10:%.*]] = call <16 x bfloat> @llvm.amdgcn.cvt.scale.pk.bf16.bf6(<3 x i32> [[EXTRACTVEC7]], float [[TMP9]])
// CHECK-NEXT:    [[TMP11:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTY16_ADDR]], align 8
// CHECK-NEXT:    store <16 x bfloat> [[TMP10]], ptr addrspace(1) [[TMP11]], align 32
// CHECK-NEXT:    [[TMP12:%.*]] = load i32, ptr addrspace(5) [[SRC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP13:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP14:%.*]] = call <4 x half> @llvm.amdgcn.cvt.scale.pk.f16.fp8(i32 [[TMP12]], float [[TMP13]])
// CHECK-NEXT:    [[TMP15:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTH4_ADDR]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP14]], ptr addrspace(1) [[TMP15]], align 8
// CHECK-NEXT:    [[TMP16:%.*]] = load i32, ptr addrspace(5) [[SRC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP17:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP18:%.*]] = call <4 x bfloat> @llvm.amdgcn.cvt.scale.pk.bf16.fp8(i32 [[TMP16]], float [[TMP17]])
// CHECK-NEXT:    [[TMP19:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTY4_ADDR]], align 8
// CHECK-NEXT:    store <4 x bfloat> [[TMP18]], ptr addrspace(1) [[TMP19]], align 8
// CHECK-NEXT:    [[TMP20:%.*]] = load i32, ptr addrspace(5) [[SRC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP21:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP22:%.*]] = call <4 x half> @llvm.amdgcn.cvt.scale.pk.f16.bf8(i32 [[TMP20]], float [[TMP21]])
// CHECK-NEXT:    [[TMP23:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTH4_ADDR]], align 8
// CHECK-NEXT:    store <4 x half> [[TMP22]], ptr addrspace(1) [[TMP23]], align 8
// CHECK-NEXT:    [[TMP24:%.*]] = load i32, ptr addrspace(5) [[SRC1_ADDR]], align 4
// CHECK-NEXT:    [[TMP25:%.*]] = load float, ptr addrspace(5) [[SCALE_ADDR]], align 4
// CHECK-NEXT:    [[TMP26:%.*]] = call <4 x bfloat> @llvm.amdgcn.cvt.scale.pk.bf16.bf8(i32 [[TMP24]], float [[TMP25]])
// CHECK-NEXT:    [[TMP27:%.*]] = load ptr addrspace(1), ptr addrspace(5) [[OUTY4_ADDR]], align 8
// CHECK-NEXT:    store <4 x bfloat> [[TMP26]], ptr addrspace(1) [[TMP27]], align 8
// CHECK-NEXT:    ret void
//
void test_cvt_scale_pk(global half16 *outh16, global bfloat16 *outy16, uint3 src3,
                       global half4 *outh4, global bfloat4 *outy4, int src1,
                       float scale)
{
  *outh16 = __builtin_amdgcn_cvt_scale_pk_f16_fp6(src3, scale);
  *outy16 = __builtin_amdgcn_cvt_scale_pk_bf16_fp6(src3, scale);
  *outh16 = __builtin_amdgcn_cvt_scale_pk_f16_bf6(src3, scale);
  *outy16 = __builtin_amdgcn_cvt_scale_pk_bf16_bf6(src3, scale);
  *outh4 = __builtin_amdgcn_cvt_scale_pk_f16_fp8(src1, scale);
  *outy4 = __builtin_amdgcn_cvt_scale_pk_bf16_fp8(src1, scale);
  *outh4 = __builtin_amdgcn_cvt_scale_pk_f16_bf8(src1, scale);
  *outy4 = __builtin_amdgcn_cvt_scale_pk_bf16_bf8(src1, scale);
}
