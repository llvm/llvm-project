// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv32 -target-feature +experimental-p -emit-llvm %s -O2 -o - | FileCheck %s --check-prefix=RV32
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-p -emit-llvm %s -O2 -o - | FileCheck %s --check-prefix=RV64

#include <stdint.h>

typedef int8_t v4i8 __attribute__((vector_size(4)));
typedef int16_t v2i16 __attribute__((vector_size(4)));
typedef int8_t v8i8 __attribute__((vector_size(8)));
typedef int16_t v4i16 __attribute__((vector_size(8)));
typedef int32_t v2i32 __attribute__((vector_size(8)));

// RV32-LABEL: @test_padd_v4i8(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i32 [[A_COERCE:%.*]] to <4 x i8>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i32 [[B_COERCE:%.*]] to <4 x i8>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <4 x i8> @llvm.riscv.padd.v4i8(<4 x i8> [[TMP0]], <4 x i8> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <4 x i8> [[TMP2]] to i32
// RV32-NEXT:    ret i32 [[TMP3]]
//
// RV64-LABEL: @test_padd_v4i8(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[COERCE_VAL_II:%.*]] = trunc i64 [[A_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[COERCE_VAL_II]] to <4 x i8>
// RV64-NEXT:    [[COERCE_VAL_II1:%.*]] = trunc i64 [[B_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i32 [[COERCE_VAL_II1]] to <4 x i8>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <4 x i8> @llvm.riscv.padd.v4i8(<4 x i8> [[TMP0]], <4 x i8> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <4 x i8> [[TMP2]] to i32
// RV64-NEXT:    [[RETVAL_COERCE_0_INSERT_EXT:%.*]] = zext i32 [[TMP3]] to i64
// RV64-NEXT:    ret i64 [[RETVAL_COERCE_0_INSERT_EXT]]
//
v4i8 test_padd_v4i8(v4i8 a, v4i8 b) {
  return __builtin_riscv_padd_v4i8(a, b);
}

// RV32-LABEL: @test_padd_v2i16(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i32 [[A_COERCE:%.*]] to <2 x i16>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i32 [[B_COERCE:%.*]] to <2 x i16>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <2 x i16> @llvm.riscv.padd.v2i16(<2 x i16> [[TMP0]], <2 x i16> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <2 x i16> [[TMP2]] to i32
// RV32-NEXT:    ret i32 [[TMP3]]
//
// RV64-LABEL: @test_padd_v2i16(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[COERCE_VAL_II:%.*]] = trunc i64 [[A_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[COERCE_VAL_II]] to <2 x i16>
// RV64-NEXT:    [[COERCE_VAL_II1:%.*]] = trunc i64 [[B_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i32 [[COERCE_VAL_II1]] to <2 x i16>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <2 x i16> @llvm.riscv.padd.v2i16(<2 x i16> [[TMP0]], <2 x i16> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <2 x i16> [[TMP2]] to i32
// RV64-NEXT:    [[RETVAL_COERCE_0_INSERT_EXT:%.*]] = zext i32 [[TMP3]] to i64
// RV64-NEXT:    ret i64 [[RETVAL_COERCE_0_INSERT_EXT]]
//
v2i16 test_padd_v2i16(v2i16 a, v2i16 b) {
  return __builtin_riscv_padd_v2i16(a, b);
}

// RV32-LABEL: @test_padd_v8i8(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <8 x i8>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <8 x i8>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <8 x i8> @llvm.riscv.padd.v8i8(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_padd_v8i8(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <8 x i8>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <8 x i8>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <8 x i8> @llvm.riscv.padd.v8i8(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v8i8 test_padd_v8i8(v8i8 a, v8i8 b) {
  return __builtin_riscv_padd_v8i8(a, b);
}

// RV32-LABEL: @test_padd_v4i16(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <4 x i16>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <4 x i16>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <4 x i16> @llvm.riscv.padd.v4i16(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_padd_v4i16(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <4 x i16>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <4 x i16>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <4 x i16> @llvm.riscv.padd.v4i16(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v4i16 test_padd_v4i16(v4i16 a, v4i16 b) {
  return __builtin_riscv_padd_v4i16(a, b);
}

// RV32-LABEL: @test_padd_v2i32(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <2 x i32>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <2 x i32>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <2 x i32> @llvm.riscv.padd.v2i32(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_padd_v2i32(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <2 x i32>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <2 x i32>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <2 x i32> @llvm.riscv.padd.v2i32(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v2i32 test_padd_v2i32(v2i32 a, v2i32 b) {
  return __builtin_riscv_padd_v2i32(a, b);
}

// RV32-LABEL: @test_psub_v4i8(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i32 [[A_COERCE:%.*]] to <4 x i8>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i32 [[B_COERCE:%.*]] to <4 x i8>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <4 x i8> @llvm.riscv.psub.v4i8(<4 x i8> [[TMP0]], <4 x i8> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <4 x i8> [[TMP2]] to i32
// RV32-NEXT:    ret i32 [[TMP3]]
//
// RV64-LABEL: @test_psub_v4i8(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[COERCE_VAL_II:%.*]] = trunc i64 [[A_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[COERCE_VAL_II]] to <4 x i8>
// RV64-NEXT:    [[COERCE_VAL_II1:%.*]] = trunc i64 [[B_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i32 [[COERCE_VAL_II1]] to <4 x i8>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <4 x i8> @llvm.riscv.psub.v4i8(<4 x i8> [[TMP0]], <4 x i8> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <4 x i8> [[TMP2]] to i32
// RV64-NEXT:    [[RETVAL_COERCE_0_INSERT_EXT:%.*]] = zext i32 [[TMP3]] to i64
// RV64-NEXT:    ret i64 [[RETVAL_COERCE_0_INSERT_EXT]]
//
v4i8 test_psub_v4i8(v4i8 a, v4i8 b) {
  return __builtin_riscv_psub_v4i8(a, b);
}

// RV32-LABEL: @test_psub_v2i16(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i32 [[A_COERCE:%.*]] to <2 x i16>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i32 [[B_COERCE:%.*]] to <2 x i16>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <2 x i16> @llvm.riscv.psub.v2i16(<2 x i16> [[TMP0]], <2 x i16> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <2 x i16> [[TMP2]] to i32
// RV32-NEXT:    ret i32 [[TMP3]]
//
// RV64-LABEL: @test_psub_v2i16(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[COERCE_VAL_II:%.*]] = trunc i64 [[A_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i32 [[COERCE_VAL_II]] to <2 x i16>
// RV64-NEXT:    [[COERCE_VAL_II1:%.*]] = trunc i64 [[B_COERCE:%.*]] to i32
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i32 [[COERCE_VAL_II1]] to <2 x i16>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <2 x i16> @llvm.riscv.psub.v2i16(<2 x i16> [[TMP0]], <2 x i16> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <2 x i16> [[TMP2]] to i32
// RV64-NEXT:    [[RETVAL_COERCE_0_INSERT_EXT:%.*]] = zext i32 [[TMP3]] to i64
// RV64-NEXT:    ret i64 [[RETVAL_COERCE_0_INSERT_EXT]]
//
v2i16 test_psub_v2i16(v2i16 a, v2i16 b) {
  return __builtin_riscv_psub_v2i16(a, b);
}

// RV32-LABEL: @test_psub_v8i8(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <8 x i8>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <8 x i8>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <8 x i8> @llvm.riscv.psub.v8i8(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_psub_v8i8(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <8 x i8>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <8 x i8>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <8 x i8> @llvm.riscv.psub.v8i8(<8 x i8> [[TMP0]], <8 x i8> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <8 x i8> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v8i8 test_psub_v8i8(v8i8 a, v8i8 b) {
  return __builtin_riscv_psub_v8i8(a, b);
}

// RV32-LABEL: @test_psub_v4i16(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <4 x i16>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <4 x i16>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <4 x i16> @llvm.riscv.psub.v4i16(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_psub_v4i16(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <4 x i16>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <4 x i16>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <4 x i16> @llvm.riscv.psub.v4i16(<4 x i16> [[TMP0]], <4 x i16> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <4 x i16> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v4i16 test_psub_v4i16(v4i16 a, v4i16 b) {
  return __builtin_riscv_psub_v4i16(a, b);
}

// RV32-LABEL: @test_psub_v2i32(
// RV32-NEXT:  entry:
// RV32-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <2 x i32>
// RV32-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <2 x i32>
// RV32-NEXT:    [[TMP2:%.*]] = tail call <2 x i32> @llvm.riscv.psub.v2i32(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]])
// RV32-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP2]] to i64
// RV32-NEXT:    ret i64 [[TMP3]]
//
// RV64-LABEL: @test_psub_v2i32(
// RV64-NEXT:  entry:
// RV64-NEXT:    [[TMP0:%.*]] = bitcast i64 [[A_COERCE:%.*]] to <2 x i32>
// RV64-NEXT:    [[TMP1:%.*]] = bitcast i64 [[B_COERCE:%.*]] to <2 x i32>
// RV64-NEXT:    [[TMP2:%.*]] = tail call <2 x i32> @llvm.riscv.psub.v2i32(<2 x i32> [[TMP0]], <2 x i32> [[TMP1]])
// RV64-NEXT:    [[TMP3:%.*]] = bitcast <2 x i32> [[TMP2]] to i64
// RV64-NEXT:    ret i64 [[TMP3]]
//
v2i32 test_psub_v2i32(v2i32 a, v2i32 b) {
  return __builtin_riscv_psub_v2i32(a, b);
}
