// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py
// RUN: %clang_cc1 -triple riscv64 -target-feature +experimental-p -emit-llvm %s -o - \
// RUN:     -disable-O0-optnone | opt -S -passes=mem2reg \
// RUN:     | FileCheck %s  -check-prefix=RV64P

#include <riscv_simd.h>

// RV64P-LABEL: @pslli_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pslli.b.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pslli_b(uint64_t rs1, int64_t rs2) {
  return __riscv_pslli_b(rs1, 1);
}

// RV64P-LABEL: @pslli_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pslli.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pslli_h(uint64_t rs1, int64_t rs2) {
  return __riscv_pslli_h(rs1, 1);
}

// RV64P-LABEL: @pslli_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pslli.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pslli_w(uint64_t rs1, int64_t rs2) {
  return __riscv_pslli_w(rs1, 1);
}

// RV64P-LABEL: @psslai_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psslai.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psslai_h(uint64_t rs1, int64_t rs2) {
  return __riscv_psslai_h(rs1, 1);
}

// RV64P-LABEL: @psslai_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psslai.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psslai_w(uint64_t rs1, int64_t rs2) {
  return __riscv_psslai_w(rs1, 1);
}

// RV64P-LABEL: @psll_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psll.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psll_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psll_bs(rs1, rs2);
}

// RV64P-LABEL: @psll_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psll.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psll_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psll_hs(rs1, rs2);
}

// RV64P-LABEL: @psll_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psll.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psll_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_psll_ws(rs1, rs2);
}

// RV64P-LABEL: @padd_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_bs(rs1, rs2);
}

// RV64P-LABEL: @padd_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_hs(rs1, rs2);
}

// RV64P-LABEL: @padd_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_ws(rs1, rs2);
}

// RV64P-LABEL: @pusati_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pusati.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pusati_h(uint64_t rs1, int64_t rs2) {
  return __riscv_pusati_h(rs1, 1);
}

// RV64P-LABEL: @pusati_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pusati.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pusati_w(uint64_t rs1, int64_t rs2) {
  return __riscv_pusati_w(rs1, 1);
}

// RV64P-LABEL: @usati(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.usati.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t usati(uint64_t rs1, int64_t rs2) {
  return __riscv_usati(rs1, 1);
}

// RV64P-LABEL: @psrai_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrai.b.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrai_b(uint64_t rs1, int64_t rs2) {
  return __riscv_psrai_b(rs1, 1);
}

// RV64P-LABEL: @psrai_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrai.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrai_h(uint64_t rs1, int64_t rs2) {
  return __riscv_psrai_h(rs1, 1);
}

// RV64P-LABEL: @psrai_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrai.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrai_w(uint64_t rs1, int64_t rs2) {
  return __riscv_psrai_w(rs1, 1);
}

// RV64P-LABEL: @psrari_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrari.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrari_h(uint64_t rs1, int64_t rs2) {
  return __riscv_psrari_h(rs1, 1);
}

// RV64P-LABEL: @psrari_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrari.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrari_w(uint64_t rs1, int64_t rs2) {
  return __riscv_psrari_w(rs1, 1);
}

// RV64P-LABEL: @srari(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.srari.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t srari(int64_t rs1, int64_t rs2) {
  return __riscv_srari(rs1, 1);
}

// RV64P-LABEL: @psati_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psati.h.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psati_h(uint64_t rs1, int64_t rs2) {
  return __riscv_psati_h(rs1, 1);
}

// RV64P-LABEL: @psati_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psati.w.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psati_w(uint64_t rs1, int64_t rs2) {
  return __riscv_psati_w(rs1, 1);
}

// RV64P-LABEL: @sati(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[CONV_I:%.*]] = sext i32 1 to i64
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.sati.i64.i64(i64 [[RS1:%.*]], i64 [[CONV_I]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t sati(int64_t rs1, int64_t rs2) {
  return __riscv_sati(rs1, 1);
}

// RV64P-LABEL: @psrl_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrl.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrl_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psrl_bs(rs1, rs2);
}

// RV64P-LABEL: @psrl_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrl.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrl_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psrl_hs(rs1, rs2);
}

// RV64P-LABEL: @psrl_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psrl.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psrl_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_psrl_ws(rs1, rs2);
}

// RV64P-LABEL: @predsum_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsum.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsum_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsum_bs(rs1, rs2);
}

// RV64P-LABEL: @predsum_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsum.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsum_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsum_hs(rs1, rs2);
}

// RV64P-LABEL: @predsum_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsum.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsum_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsum_ws(rs1, rs2);
}

// RV64P-LABEL: @predsumu_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsumu.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsumu_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsumu_bs(rs1, rs2);
}

// RV64P-LABEL: @predsumu_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsumu.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsumu_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsumu_hs(rs1, rs2);
}

// RV64P-LABEL: @predsumu_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.predsumu.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t predsumu_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_predsumu_ws(rs1, rs2);
}

// RV64P-LABEL: @psra_bs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psra.bs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psra_bs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psra_bs(rs1, rs2);
}

// RV64P-LABEL: @psra_hs(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psra.hs.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psra_hs(uint64_t rs1, uint64_t rs2) {
  return __riscv_psra_hs(rs1, rs2);
}

// RV64P-LABEL: @psra_ws(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psra.ws.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psra_ws(uint64_t rs1, uint64_t rs2) {
  return __riscv_psra_ws(rs1, rs2);
}

// RV64P-LABEL: @padd_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_b(rs1, rs2);
}

// RV64P-LABEL: @padd_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_h(rs1, rs2);
}

// RV64P-LABEL: @padd_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.padd.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t padd_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_padd_w(rs1, rs2);
}

// RV64P-LABEL: @psadd_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psadd.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psadd_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_psadd_b(rs1, rs2);
}

// RV64P-LABEL: @psadd_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psadd.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psadd_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_psadd_h(rs1, rs2);
}

// RV64P-LABEL: @psadd_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psadd.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psadd_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_psadd_w(rs1, rs2);
}

// RV64P-LABEL: @paadd_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paadd.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paadd_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_paadd_b(rs1, rs2);
}

// RV64P-LABEL: @paadd_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paadd.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paadd_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_paadd_h(rs1, rs2);
}

// RV64P-LABEL: @paadd_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paadd.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paadd_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_paadd_w(rs1, rs2);
}

// RV64P-LABEL: @psaddu_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psaddu.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psaddu_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_psaddu_b(rs1, rs2);
}

// RV64P-LABEL: @psaddu_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psaddu.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psaddu_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_psaddu_h(rs1, rs2);
}

// RV64P-LABEL: @psaddu_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psaddu.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psaddu_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_psaddu_w(rs1, rs2);
}

// RV64P-LABEL: @paaddu_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paaddu.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paaddu_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_paaddu_b(rs1, rs2);
}

// RV64P-LABEL: @paaddu_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paaddu.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paaddu_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_paaddu_h(rs1, rs2);
}

// RV64P-LABEL: @paaddu_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.paaddu.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t paaddu_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_paaddu_w(rs1, rs2);
}

// RV64P-LABEL: @psub_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psub.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psub_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_psub_b(rs1, rs2);
}

// RV64P-LABEL: @psub_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psub.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psub_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_psub_h(rs1, rs2);
}

// RV64P-LABEL: @psub_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psub.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psub_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_psub_w(rs1, rs2);
}

// RV64P-LABEL: @pssub_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssub.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssub_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssub_b(rs1, rs2);
}

// RV64P-LABEL: @pssub_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssub.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssub_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssub_h(rs1, rs2);
}

// RV64P-LABEL: @pssub_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssub.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssub_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssub_w(rs1, rs2);
}

// RV64P-LABEL: @pasub_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasub.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasub_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasub_b(rs1, rs2);
}

// RV64P-LABEL: @pasub_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasub.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasub_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasub_h(rs1, rs2);
}

// RV64P-LABEL: @pasub_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasub.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasub_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasub_w(rs1, rs2);
}

// RV64P-LABEL: @pssubu_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssubu.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssubu_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssubu_b(rs1, rs2);
}

// RV64P-LABEL: @pssubu_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssubu.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssubu_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssubu_h(rs1, rs2);
}

// RV64P-LABEL: @pssubu_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssubu.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssubu_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssubu_w(rs1, rs2);
}

// RV64P-LABEL: @pasubu_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasubu.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasubu_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasubu_b(rs1, rs2);
}

// RV64P-LABEL: @pasubu_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasubu.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasubu_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasubu_h(rs1, rs2);
}

// RV64P-LABEL: @pasubu_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pasubu.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pasubu_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_pasubu_w(rs1, rs2);
}

// RV64P-LABEL: @pdif_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pdif.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pdif_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pdif_b(rs1, rs2);
}

// RV64P-LABEL: @pdif_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pdif.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pdif_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pdif_h(rs1, rs2);
}

// RV64P-LABEL: @pdifu_b(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pdifu.b.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pdifu_b(uint64_t rs1, uint64_t rs2) {
  return __riscv_pdifu_b(rs1, rs2);
}

// RV64P-LABEL: @pdifu_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pdifu.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pdifu_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pdifu_h(rs1, rs2);
}

// RV64P-LABEL: @pmul_h_b01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.h.b01.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_h_b01(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_h_b01(rs1, rs2);
}

// RV64P-LABEL: @pmul_w_h01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.w.h01.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_w_h01(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_w_h01(rs1, rs2);
}

// RV64P-LABEL: @pmulu_h_b01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.h.b01.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_h_b01(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_h_b01(rs1, rs2);
}

// RV64P-LABEL: @pmulu_w_h01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.w.h01.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_w_h01(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_w_h01(rs1, rs2);
}

// RV64P-LABEL: @mul_w01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mul.w01.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mul_w01(uint64_t rs1, uint64_t rs2) {
  return __riscv_mul_w01(rs1, rs2);
}

// RV64P-LABEL: @mulu_w01(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mulu.w01.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mulu_w01(uint64_t rs1, uint64_t rs2) {
  return __riscv_mulu_w01(rs1, rs2);
}

// RV64P-LABEL: @slx(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.slx.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t slx(uint64_t rs1, uint64_t rs2) {
  return __riscv_slx(rs1, rs2);
}

// RV64P-LABEL: @psh1add_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psh1add.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psh1add_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_psh1add_h(rs1, rs2);
}

// RV64P-LABEL: @psh1add_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.psh1add.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t psh1add_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_psh1add_w(rs1, rs2);
}

// RV64P-LABEL: @pssh1sadd_h(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssh1sadd.h.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssh1sadd_h(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssh1sadd_h(rs1, rs2);
}

// RV64P-LABEL: @pssh1sadd_w(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pssh1sadd.w.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pssh1sadd_w(uint64_t rs1, uint64_t rs2) {
  return __riscv_pssh1sadd_w(rs1, rs2);
}

// RV64P-LABEL: @unzip8p(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.unzip8p.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t unzip8p(int64_t rs1, int64_t rs2) {
  return __riscv_unzip8p(rs1, rs2);
}

// RV64P-LABEL: @unzip16p(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.unzip16p.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t unzip16p(int64_t rs1, int64_t rs2) {
  return __riscv_unzip16p(rs1, rs2);
}

// RV64P-LABEL: @unzip8hp(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.unzip8hp.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t unzip8hp(int64_t rs1, int64_t rs2) {
  return __riscv_unzip8hp(rs1, rs2);
}

// RV64P-LABEL: @unzip16hp(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.unzip16hp.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t unzip16hp(int64_t rs1, int64_t rs2) {
  return __riscv_unzip16hp(rs1, rs2);
}

// RV64P-LABEL: @zip8p(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.zip8p.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t zip8p(int64_t rs1, int64_t rs2) {
  return __riscv_zip8p(rs1, rs2);
}

// RV64P-LABEL: @zip16p(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.zip16p.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t zip16p(int64_t rs1, int64_t rs2) {
  return __riscv_zip16p(rs1, rs2);
}

// RV64P-LABEL: @zip8hp(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.zip8hp.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t zip8hp(int64_t rs1, int64_t rs2) {
  return __riscv_zip8hp(rs1, rs2);
}

// RV64P-LABEL: @zip16hp(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.zip16hp.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
int64_t zip16hp(int64_t rs1, int64_t rs2) {
  return __riscv_zip16hp(rs1, rs2);
}

// RV64P-LABEL: @pmul_h_b00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.h.b00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_h_b00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_h_b00(rs1, rs2);
}

// RV64P-LABEL: @pmul_w_h00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.w.h00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_w_h00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_w_h00(rs1, rs2);
}

// RV64P-LABEL: @pmul_h_b11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.h.b11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_h_b11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_h_b11(rs1, rs2);
}

// RV64P-LABEL: @pmul_w_h11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmul.w.h11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmul_w_h11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmul_w_h11(rs1, rs2);
}

// RV64P-LABEL: @pmulu_h_b00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.h.b00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_h_b00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_h_b00(rs1, rs2);
}

// RV64P-LABEL: @pmulu_w_h00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.w.h00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_w_h00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_w_h00(rs1, rs2);
}

// RV64P-LABEL: @pmulu_h_b11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.h.b11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_h_b11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_h_b11(rs1, rs2);
}

// RV64P-LABEL: @pmulu_w_h11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulu.w.h11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulu_w_h11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulu_w_h11(rs1, rs2);
}

// RV64P-LABEL: @pmulsu_h_b00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulsu.h.b00.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulsu_h_b00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulsu_h_b00(rs1, rs2);
}

// RV64P-LABEL: @pmulsu_w_h00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulsu.w.h00.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulsu_w_h00(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulsu_w_h00(rs1, rs2);
}

// RV64P-LABEL: @pmulsu_h_b11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulsu.h.b11.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulsu_h_b11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulsu_h_b11(rs1, rs2);
}

// RV64P-LABEL: @pmulsu_w_h11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.pmulsu.w.h11.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t pmulsu_w_h11(uint64_t rs1, uint64_t rs2) {
  return __riscv_pmulsu_w_h11(rs1, rs2);
}

// RV64P-LABEL: @mul_w00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mul.w00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mul_w00(uint64_t rs1, uint64_t rs2) {
  return __riscv_mul_w00(rs1, rs2);
}

// RV64P-LABEL: @mul_w11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mul.w11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mul_w11(uint64_t rs1, uint64_t rs2) {
  return __riscv_mul_w11(rs1, rs2);
}

// RV64P-LABEL: @mulu_w00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mulu.w00.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mulu_w00(uint64_t rs1, uint64_t rs2) {
  return __riscv_mulu_w00(rs1, rs2);
}

// RV64P-LABEL: @mulu_w11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mulu.w11.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mulu_w11(uint64_t rs1, uint64_t rs2) {
  return __riscv_mulu_w11(rs1, rs2);
}

// RV64P-LABEL: @mulsu_w00(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mulsu.w00.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mulsu_w00(uint64_t rs1, uint64_t rs2) {
  return __riscv_mulsu_w00(rs1, rs2);
}

// RV64P-LABEL: @mulsu_w11(
// RV64P-NEXT:  entry:
// RV64P-NEXT:    [[TMP0:%.*]] = call i64 @llvm.riscv.mulsu.w11.i64.i64.i64(i64 [[RS1:%.*]], i64 [[RS2:%.*]])
// RV64P-NEXT:    ret i64 [[TMP0]]
//
uint64_t mulsu_w11(uint64_t rs1, uint64_t rs2) {
  return __riscv_mulsu_w11(rs1, rs2);
}
