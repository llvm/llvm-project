; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; RUN: opt -mattr=+dotprod < %s -passes=msan -S | FileCheck %s
;
; Forked from llvm/test/CodeGen/AArch64/neon-dot-product.ll
;
; Strictly handled: (none)
;
; Heuristically handled: (none)

target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
target triple = "aarch64--linux-android9001"

declare <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32>, <8 x i8>, <8 x i8>)
declare <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32>, <16 x i8>, <16 x i8>)
declare <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32>, <8 x i8>, <8 x i8>)
declare <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32>, <16 x i8>, <16 x i8>)

define <2 x i32> @test_vdot_u32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_u32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_u32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_u32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}

define <2 x i32> @test_vdot_s32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_s32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_s32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_s32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}


define <2 x i32> @test_vdot_u32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_u32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], zeroinitializer
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    [[_MSPROP:%.*]] = or <2 x i32> [[TMP16]], [[TMP2]]
; CHECK-NEXT:    [[RET:%.*]] = add <2 x i32> [[VDOT1_I]], [[A]]
; CHECK-NEXT:    store <2 x i32> [[_MSPROP]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  %ret = add <2 x i32> %vdot1.i, %a
  ret <2 x i32> %ret
}

define <4 x i32> @test_vdotq_u32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_u32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], zeroinitializer
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    [[_MSPROP:%.*]] = or <4 x i32> [[TMP16]], [[TMP2]]
; CHECK-NEXT:    [[RET:%.*]] = add <4 x i32> [[VDOT1_I]], [[A]]
; CHECK-NEXT:    store <4 x i32> [[_MSPROP]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  %ret = add <4 x i32> %vdot1.i, %a
  ret <4 x i32> %ret
}

define <2 x i32> @test_vdot_s32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_s32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], zeroinitializer
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    [[_MSPROP:%.*]] = or <2 x i32> [[TMP16]], [[TMP2]]
; CHECK-NEXT:    [[RET:%.*]] = add <2 x i32> [[VDOT1_I]], [[A]]
; CHECK-NEXT:    store <2 x i32> [[_MSPROP]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  %ret = add <2 x i32> %vdot1.i, %a
  ret <2 x i32> %ret
}

define <4 x i32> @test_vdotq_s32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #0 sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_s32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP0]], zeroinitializer
; CHECK-NEXT:    [[TMP4:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP4]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], zeroinitializer
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    [[_MSPROP:%.*]] = or <4 x i32> [[TMP16]], [[TMP2]]
; CHECK-NEXT:    [[RET:%.*]] = add <4 x i32> [[VDOT1_I]], [[A]]
; CHECK-NEXT:    store <4 x i32> [[_MSPROP]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  %ret = add <4 x i32> %vdot1.i, %a
  ret <4 x i32> %ret
}


define <2 x i32> @test_vdot_lane_u32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_lane_u32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_lane_u32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_lane_u32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}

define <2 x i32> @test_vdot_laneq_u32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_laneq_u32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_laneq_u32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_laneq_u32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}


define <2 x i32> @test_vdot_lane_u32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_lane_u32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <8 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <8 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <8 x i1> [[TMP10]] to <8 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i8> [[TMP11]] to <2 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <2 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <2 x i1> [[TMP13]] to <2 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <2 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_lane_u32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_lane_u32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <16 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <16 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <16 x i1> [[TMP10]] to <16 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i8> [[TMP11]] to <4 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <4 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <4 x i1> [[TMP13]] to <4 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <4 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}

define <2 x i32> @test_vdot_laneq_u32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_laneq_u32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <8 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <8 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <8 x i1> [[TMP10]] to <8 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i8> [[TMP11]] to <2 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <2 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <2 x i1> [[TMP13]] to <2 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <2 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.udot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_laneq_u32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_laneq_u32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <16 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <16 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <16 x i1> [[TMP10]] to <16 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i8> [[TMP11]] to <4 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <4 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <4 x i1> [[TMP13]] to <4 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <4 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.udot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}


define <2 x i32> @test_vdot_lane_s32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_lane_s32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_lane_s32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_lane_s32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}

define <2 x i32> @test_vdot_laneq_s32(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_laneq_s32(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <2 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <8 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <8 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <8 x i1> [[TMP11]] to <8 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <8 x i8> [[TMP12]] to <2 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <2 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <2 x i1> [[TMP14]] to <2 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <2 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> [[A]], <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_laneq_s32(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_laneq_s32(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    [[TMP17:%.*]] = load <4 x i32>, ptr @__msan_param_tls, align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP2]], zeroinitializer
; CHECK-NEXT:    [[TMP18:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP3]], [[TMP18]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP5]], [[TMP18]]
; CHECK-NEXT:    [[TMP9:%.*]] = and <16 x i1> [[TMP3]], [[TMP6]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <16 x i1> [[TMP10]], [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = sext <16 x i1> [[TMP11]] to <16 x i8>
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast <16 x i8> [[TMP12]] to <4 x i32>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ne <4 x i32> [[TMP13]], zeroinitializer
; CHECK-NEXT:    [[TMP15:%.*]] = sext <4 x i1> [[TMP14]] to <4 x i32>
; CHECK-NEXT:    [[TMP16:%.*]] = or <4 x i32> [[TMP15]], [[TMP17]]
; CHECK-NEXT:    [[VDOT1_I:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> [[A]], <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP16]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[VDOT1_I]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}


define <2 x i32> @test_vdot_lane_s32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_lane_s32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <8 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <8 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <8 x i1> [[TMP10]] to <8 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i8> [[TMP11]] to <2 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <2 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <2 x i1> [[TMP13]] to <2 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <2 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_lane_s32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_lane_s32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <16 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <16 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <16 x i1> [[TMP10]] to <16 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i8> [[TMP11]] to <4 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <4 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <4 x i1> [[TMP13]] to <4 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <4 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}

define <2 x i32> @test_vdot_laneq_s32_zero(<2 x i32> %a, <8 x i8> %b, <8 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <2 x i32> @test_vdot_laneq_s32_zero(
; CHECK-SAME: <2 x i32> [[A:%.*]], <8 x i8> [[B:%.*]], <8 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 8), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <8 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <8 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <8 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <8 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <8 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <8 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <8 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <8 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <8 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <8 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <8 x i1> [[TMP10]] to <8 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <8 x i8> [[TMP11]] to <2 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <2 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <2 x i1> [[TMP13]] to <2 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <2 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> [[B]], <8 x i8> [[C]])
; CHECK-NEXT:    store <2 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <2 x i32> [[RET]]
;
entry:
  %vdot1.i = call <2 x i32> @llvm.aarch64.neon.sdot.v2i32.v8i8(<2 x i32> zeroinitializer, <8 x i8> %b, <8 x i8> %c) #2
  ret <2 x i32> %vdot1.i
}

define <4 x i32> @test_vdotq_laneq_s32_zero(<4 x i32> %a, <16 x i8> %b, <16 x i8> %c) sanitize_memory {
; CHECK-LABEL: define <4 x i32> @test_vdotq_laneq_s32_zero(
; CHECK-SAME: <4 x i32> [[A:%.*]], <16 x i8> [[B:%.*]], <16 x i8> [[C:%.*]]) #[[ATTR1]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 16), align 8
; CHECK-NEXT:    [[TMP4:%.*]] = load <16 x i8>, ptr getelementptr (i8, ptr @__msan_param_tls, i64 32), align 8
; CHECK-NEXT:    call void @llvm.donothing()
; CHECK-NEXT:    [[TMP2:%.*]] = icmp ne <16 x i8> [[TMP1]], zeroinitializer
; CHECK-NEXT:    [[TMP3:%.*]] = icmp ne <16 x i8> [[TMP4]], zeroinitializer
; CHECK-NEXT:    [[TMP16:%.*]] = icmp ne <16 x i8> [[B]], zeroinitializer
; CHECK-NEXT:    [[TMP5:%.*]] = icmp ne <16 x i8> [[C]], zeroinitializer
; CHECK-NEXT:    [[TMP6:%.*]] = and <16 x i1> [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = and <16 x i1> [[TMP16]], [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = and <16 x i1> [[TMP2]], [[TMP5]]
; CHECK-NEXT:    [[TMP9:%.*]] = or <16 x i1> [[TMP6]], [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = or <16 x i1> [[TMP9]], [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = sext <16 x i1> [[TMP10]] to <16 x i8>
; CHECK-NEXT:    [[TMP12:%.*]] = bitcast <16 x i8> [[TMP11]] to <4 x i32>
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ne <4 x i32> [[TMP12]], zeroinitializer
; CHECK-NEXT:    [[TMP14:%.*]] = sext <4 x i1> [[TMP13]] to <4 x i32>
; CHECK-NEXT:    [[TMP15:%.*]] = or <4 x i32> [[TMP14]], zeroinitializer
; CHECK-NEXT:    [[RET:%.*]] = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> [[B]], <16 x i8> [[C]])
; CHECK-NEXT:    store <4 x i32> [[TMP15]], ptr @__msan_retval_tls, align 8
; CHECK-NEXT:    ret <4 x i32> [[RET]]
;
entry:
  %vdot1.i = call <4 x i32> @llvm.aarch64.neon.sdot.v4i32.v16i8(<4 x i32> zeroinitializer, <16 x i8> %b, <16 x i8> %c) #2
  ret <4 x i32> %vdot1.i
}
