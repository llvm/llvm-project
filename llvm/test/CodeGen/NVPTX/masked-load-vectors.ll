; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -march=nvptx64 -mcpu=sm_90 -mattr=+ptx88 | FileCheck %s -check-prefixes=CHECK,SM90
; RUN: %if ptxas-sm_90 && ptxas-isa-8.8 %{ llc < %s -march=nvptx64 -mcpu=sm_90 -mattr=+ptx88 | %ptxas-verify -arch=sm_90 %}
; RUN: llc < %s -march=nvptx64 -mcpu=sm_100 -mattr=+ptx88 | FileCheck %s -check-prefixes=CHECK,SM100
; RUN: %if ptxas-sm_100 && ptxas-isa-8.8 %{ llc < %s -march=nvptx64 -mcpu=sm_100 -mattr=+ptx88 | %ptxas-verify -arch=sm_100 %}


; Different architectures are tested in this file for the following reasons:
; - SM90 does not have 256-bit load/store instructions
; - SM90 does not have masked store instructions
; - SM90 does not support packed f32x2 instructions

define void @global_8xi32(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; SM90-LABEL: global_8xi32(
; SM90:       {
; SM90-NEXT:    .reg .b32 %r<9>;
; SM90-NEXT:    .reg .b64 %rd<3>;
; SM90-EMPTY:
; SM90-NEXT:  // %bb.0:
; SM90-NEXT:    ld.param.b64 %rd1, [global_8xi32_param_0];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf000";
; SM90-NEXT:    ld.global.v4.b32 {%r1, %r2, %r3, %r4}, [%rd1+16];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf0f";
; SM90-NEXT:    ld.global.v4.b32 {%r5, %r6, %r7, %r8}, [%rd1];
; SM90-NEXT:    ld.param.b64 %rd2, [global_8xi32_param_1];
; SM90-NEXT:    st.global.b32 [%rd2], %r5;
; SM90-NEXT:    st.global.b32 [%rd2+8], %r7;
; SM90-NEXT:    st.global.b32 [%rd2+28], %r4;
; SM90-NEXT:    ret;
;
; SM100-LABEL: global_8xi32(
; SM100:       {
; SM100-NEXT:    .reg .b32 %r<9>;
; SM100-NEXT:    .reg .b64 %rd<3>;
; SM100-EMPTY:
; SM100-NEXT:  // %bb.0:
; SM100-NEXT:    ld.param.b64 %rd1, [global_8xi32_param_0];
; SM100-NEXT:    .pragma "used_bytes_mask 0xf0000f0f";
; SM100-NEXT:    ld.global.v8.b32 {%r1, %r2, %r3, %r4, %r5, %r6, %r7, %r8}, [%rd1];
; SM100-NEXT:    ld.param.b64 %rd2, [global_8xi32_param_1];
; SM100-NEXT:    st.global.v8.b32 [%rd2], {%r1, _, %r3, _, _, _, _, %r8};
; SM100-NEXT:    ret;
  %a.load = tail call <8 x i32> @llvm.masked.load.v8i32.p1(ptr addrspace(1) align 32 %a, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>, <8 x i32> poison)
  tail call void @llvm.masked.store.v8i32.p1(<8 x i32> %a.load, ptr addrspace(1) align 32 %b, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>)
  ret void
}

; Masked stores are only supported for 32-bit element types,
; while masked loads are supported for all element types.
define void @global_16xi16(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; SM90-LABEL: global_16xi16(
; SM90:       {
; SM90-NEXT:    .reg .b16 %rs<7>;
; SM90-NEXT:    .reg .b32 %r<9>;
; SM90-NEXT:    .reg .b64 %rd<3>;
; SM90-EMPTY:
; SM90-NEXT:  // %bb.0:
; SM90-NEXT:    ld.param.b64 %rd1, [global_16xi16_param_0];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf000";
; SM90-NEXT:    ld.global.v4.b32 {%r1, %r2, %r3, %r4}, [%rd1+16];
; SM90-NEXT:    mov.b32 {%rs1, %rs2}, %r4;
; SM90-NEXT:    .pragma "used_bytes_mask 0xf0f";
; SM90-NEXT:    ld.global.v4.b32 {%r5, %r6, %r7, %r8}, [%rd1];
; SM90-NEXT:    mov.b32 {%rs3, %rs4}, %r7;
; SM90-NEXT:    mov.b32 {%rs5, %rs6}, %r5;
; SM90-NEXT:    ld.param.b64 %rd2, [global_16xi16_param_1];
; SM90-NEXT:    st.global.b16 [%rd2], %rs5;
; SM90-NEXT:    st.global.b16 [%rd2+2], %rs6;
; SM90-NEXT:    st.global.b16 [%rd2+8], %rs3;
; SM90-NEXT:    st.global.b16 [%rd2+10], %rs4;
; SM90-NEXT:    st.global.b16 [%rd2+28], %rs1;
; SM90-NEXT:    st.global.b16 [%rd2+30], %rs2;
; SM90-NEXT:    ret;
;
; SM100-LABEL: global_16xi16(
; SM100:       {
; SM100-NEXT:    .reg .b16 %rs<7>;
; SM100-NEXT:    .reg .b32 %r<9>;
; SM100-NEXT:    .reg .b64 %rd<3>;
; SM100-EMPTY:
; SM100-NEXT:  // %bb.0:
; SM100-NEXT:    ld.param.b64 %rd1, [global_16xi16_param_0];
; SM100-NEXT:    .pragma "used_bytes_mask 0xf0000f0f";
; SM100-NEXT:    ld.global.v8.b32 {%r1, %r2, %r3, %r4, %r5, %r6, %r7, %r8}, [%rd1];
; SM100-NEXT:    mov.b32 {%rs1, %rs2}, %r8;
; SM100-NEXT:    mov.b32 {%rs3, %rs4}, %r3;
; SM100-NEXT:    mov.b32 {%rs5, %rs6}, %r1;
; SM100-NEXT:    ld.param.b64 %rd2, [global_16xi16_param_1];
; SM100-NEXT:    st.global.b16 [%rd2], %rs5;
; SM100-NEXT:    st.global.b16 [%rd2+2], %rs6;
; SM100-NEXT:    st.global.b16 [%rd2+8], %rs3;
; SM100-NEXT:    st.global.b16 [%rd2+10], %rs4;
; SM100-NEXT:    st.global.b16 [%rd2+28], %rs1;
; SM100-NEXT:    st.global.b16 [%rd2+30], %rs2;
; SM100-NEXT:    ret;
  %a.load = tail call <16 x i16> @llvm.masked.load.v16i16.p1(ptr addrspace(1) align 32 %a, <16 x i1> <i1 true, i1 true, i1 false, i1 false, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 true, i1 true>, <16 x i16> poison)
  tail call void @llvm.masked.store.v16i16.p1(<16 x i16> %a.load, ptr addrspace(1) align 32 %b, <16 x i1> <i1 true, i1 true, i1 false, i1 false, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 true, i1 true>)
  ret void
}

define void @global_8xi32_no_align(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_8xi32_no_align(
; CHECK:       {
; CHECK-NEXT:    .reg .b32 %r<4>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_8xi32_no_align_param_0];
; CHECK-NEXT:    ld.global.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_8xi32_no_align_param_1];
; CHECK-NEXT:    ld.global.b32 %r2, [%rd1+8];
; CHECK-NEXT:    ld.global.b32 %r3, [%rd1+28];
; CHECK-NEXT:    st.global.b32 [%rd2], %r1;
; CHECK-NEXT:    st.global.b32 [%rd2+8], %r2;
; CHECK-NEXT:    st.global.b32 [%rd2+28], %r3;
; CHECK-NEXT:    ret;
  %a.load = tail call <8 x i32> @llvm.masked.load.v8i32.p1(ptr addrspace(1) align 16 %a, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>, <8 x i32> poison)
  tail call void @llvm.masked.store.v8i32.p1(<8 x i32> %a.load, ptr addrspace(1) align 16 %b, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>)
  ret void
}


define void @global_8xi32_invariant(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; SM90-LABEL: global_8xi32_invariant(
; SM90:       {
; SM90-NEXT:    .reg .b32 %r<9>;
; SM90-NEXT:    .reg .b64 %rd<3>;
; SM90-EMPTY:
; SM90-NEXT:  // %bb.0:
; SM90-NEXT:    ld.param.b64 %rd1, [global_8xi32_invariant_param_0];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf000";
; SM90-NEXT:    ld.global.nc.v4.b32 {%r1, %r2, %r3, %r4}, [%rd1+16];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf0f";
; SM90-NEXT:    ld.global.nc.v4.b32 {%r5, %r6, %r7, %r8}, [%rd1];
; SM90-NEXT:    ld.param.b64 %rd2, [global_8xi32_invariant_param_1];
; SM90-NEXT:    st.global.b32 [%rd2], %r5;
; SM90-NEXT:    st.global.b32 [%rd2+8], %r7;
; SM90-NEXT:    st.global.b32 [%rd2+28], %r4;
; SM90-NEXT:    ret;
;
; SM100-LABEL: global_8xi32_invariant(
; SM100:       {
; SM100-NEXT:    .reg .b32 %r<9>;
; SM100-NEXT:    .reg .b64 %rd<3>;
; SM100-EMPTY:
; SM100-NEXT:  // %bb.0:
; SM100-NEXT:    ld.param.b64 %rd1, [global_8xi32_invariant_param_0];
; SM100-NEXT:    .pragma "used_bytes_mask 0xf0000f0f";
; SM100-NEXT:    ld.global.nc.v8.b32 {%r1, %r2, %r3, %r4, %r5, %r6, %r7, %r8}, [%rd1];
; SM100-NEXT:    ld.param.b64 %rd2, [global_8xi32_invariant_param_1];
; SM100-NEXT:    st.global.v8.b32 [%rd2], {%r1, _, %r3, _, _, _, _, %r8};
; SM100-NEXT:    ret;
  %a.load = tail call <8 x i32> @llvm.masked.load.v8i32.p1(ptr addrspace(1) align 32 %a, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>, <8 x i32> poison), !invariant.load !0
  tail call void @llvm.masked.store.v8i32.p1(<8 x i32> %a.load, ptr addrspace(1) align 32 %b, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true>)
  ret void
}

define void @global_2xi16(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_2xi16(
; CHECK:       {
; CHECK-NEXT:    .reg .b16 %rs<2>;
; CHECK-NEXT:    .reg .b32 %r<2>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_2xi16_param_0];
; CHECK-NEXT:    .pragma "used_bytes_mask 0x3";
; CHECK-NEXT:    ld.global.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_2xi16_param_1];
; CHECK-NEXT:    mov.b32 {%rs1, _}, %r1;
; CHECK-NEXT:    st.global.b16 [%rd2], %rs1;
; CHECK-NEXT:    ret;
  %a.load = tail call <2 x i16> @llvm.masked.load.v2i16.p1(ptr addrspace(1) align 4 %a, <2 x i1> <i1 true, i1 false>, <2 x i16> poison)
  tail call void @llvm.masked.store.v2i16.p1(<2 x i16> %a.load, ptr addrspace(1) align 4 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

define void @global_2xi16_invariant(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_2xi16_invariant(
; CHECK:       {
; CHECK-NEXT:    .reg .b16 %rs<2>;
; CHECK-NEXT:    .reg .b32 %r<2>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_2xi16_invariant_param_0];
; CHECK-NEXT:    .pragma "used_bytes_mask 0x3";
; CHECK-NEXT:    ld.global.nc.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_2xi16_invariant_param_1];
; CHECK-NEXT:    mov.b32 {%rs1, _}, %r1;
; CHECK-NEXT:    st.global.b16 [%rd2], %rs1;
; CHECK-NEXT:    ret;
  %a.load = tail call <2 x i16> @llvm.masked.load.v2i16.p1(ptr addrspace(1) align 4 %a, <2 x i1> <i1 true, i1 false>, <2 x i16> poison), !invariant.load !0
  tail call void @llvm.masked.store.v2i16.p1(<2 x i16> %a.load, ptr addrspace(1) align 4 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

define void @global_2xi16_no_align(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_2xi16_no_align(
; CHECK:       {
; CHECK-NEXT:    .reg .b16 %rs<2>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_2xi16_no_align_param_0];
; CHECK-NEXT:    ld.global.b16 %rs1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_2xi16_no_align_param_1];
; CHECK-NEXT:    st.global.b16 [%rd2], %rs1;
; CHECK-NEXT:    ret;
  %a.load = tail call <2 x i16> @llvm.masked.load.v2i16.p1(ptr addrspace(1) align 2 %a, <2 x i1> <i1 true, i1 false>, <2 x i16> poison)
  tail call void @llvm.masked.store.v2i16.p1(<2 x i16> %a.load, ptr addrspace(1) align 4 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

define void @global_4xi8(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_4xi8(
; CHECK:       {
; CHECK-NEXT:    .reg .b32 %r<3>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_4xi8_param_0];
; CHECK-NEXT:    .pragma "used_bytes_mask 0x5";
; CHECK-NEXT:    ld.global.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_4xi8_param_1];
; CHECK-NEXT:    st.global.b8 [%rd2], %r1;
; CHECK-NEXT:    prmt.b32 %r2, %r1, 0, 0x7772U;
; CHECK-NEXT:    st.global.b8 [%rd2+2], %r2;
; CHECK-NEXT:    ret;
  %a.load = tail call <4 x i8> @llvm.masked.load.v4i8.p1(ptr addrspace(1) align 4 %a, <4 x i1> <i1 true, i1 false, i1 true, i1 false>, <4 x i8> poison)
  tail call void @llvm.masked.store.v4i8.p1(<4 x i8> %a.load, ptr addrspace(1) align 4 %b, <4 x i1> <i1 true, i1 false, i1 true, i1 false>)
  ret void
}

define void @global_4xi8_invariant(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_4xi8_invariant(
; CHECK:       {
; CHECK-NEXT:    .reg .b32 %r<3>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_4xi8_invariant_param_0];
; CHECK-NEXT:    .pragma "used_bytes_mask 0x5";
; CHECK-NEXT:    ld.global.nc.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_4xi8_invariant_param_1];
; CHECK-NEXT:    st.global.b8 [%rd2], %r1;
; CHECK-NEXT:    prmt.b32 %r2, %r1, 0, 0x7772U;
; CHECK-NEXT:    st.global.b8 [%rd2+2], %r2;
; CHECK-NEXT:    ret;
  %a.load = tail call <4 x i8> @llvm.masked.load.v4i8.p1(ptr addrspace(1) align 4 %a, <4 x i1> <i1 true, i1 false, i1 true, i1 false>, <4 x i8> poison), !invariant.load !0
  tail call void @llvm.masked.store.v4i8.p1(<4 x i8> %a.load, ptr addrspace(1) align 4 %b, <4 x i1> <i1 true, i1 false, i1 true, i1 false>)
  ret void
}

define void @global_4xi8_no_align(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_4xi8_no_align(
; CHECK:       {
; CHECK-NEXT:    .reg .b16 %rs<3>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_4xi8_no_align_param_0];
; CHECK-NEXT:    ld.global.b8 %rs1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_4xi8_no_align_param_1];
; CHECK-NEXT:    ld.global.b8 %rs2, [%rd1+2];
; CHECK-NEXT:    st.global.b8 [%rd2], %rs1;
; CHECK-NEXT:    st.global.b8 [%rd2+2], %rs2;
; CHECK-NEXT:    ret;
  %a.load = tail call <4 x i8> @llvm.masked.load.v4i8.p1(ptr addrspace(1) align 2 %a, <4 x i1> <i1 true, i1 false, i1 true, i1 false>, <4 x i8> poison)
  tail call void @llvm.masked.store.v4i8.p1(<4 x i8> %a.load, ptr addrspace(1) align 4 %b, <4 x i1> <i1 true, i1 false, i1 true, i1 false>)
  ret void
}

; In sm100+, we pack 2xf32 loads into a single b64 load while lowering
define void @global_2xf32(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; SM90-LABEL: global_2xf32(
; SM90:       {
; SM90-NEXT:    .reg .b32 %r<3>;
; SM90-NEXT:    .reg .b64 %rd<3>;
; SM90-EMPTY:
; SM90-NEXT:  // %bb.0:
; SM90-NEXT:    ld.param.b64 %rd1, [global_2xf32_param_0];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf";
; SM90-NEXT:    ld.global.v2.b32 {%r1, %r2}, [%rd1];
; SM90-NEXT:    ld.param.b64 %rd2, [global_2xf32_param_1];
; SM90-NEXT:    st.global.b32 [%rd2], %r1;
; SM90-NEXT:    ret;
;
; SM100-LABEL: global_2xf32(
; SM100:       {
; SM100-NEXT:    .reg .b32 %r<2>;
; SM100-NEXT:    .reg .b64 %rd<4>;
; SM100-EMPTY:
; SM100-NEXT:  // %bb.0:
; SM100-NEXT:    ld.param.b64 %rd1, [global_2xf32_param_0];
; SM100-NEXT:    .pragma "used_bytes_mask 0xf";
; SM100-NEXT:    ld.global.b64 %rd2, [%rd1];
; SM100-NEXT:    ld.param.b64 %rd3, [global_2xf32_param_1];
; SM100-NEXT:    mov.b64 {%r1, _}, %rd2;
; SM100-NEXT:    st.global.b32 [%rd3], %r1;
; SM100-NEXT:    ret;
  %a.load = tail call <2 x float> @llvm.masked.load.v2f32.p1(ptr addrspace(1) align 8 %a, <2 x i1> <i1 true, i1 false>, <2 x float> poison)
  tail call void @llvm.masked.store.v2f32.p1(<2 x float> %a.load, ptr addrspace(1) align 8 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

define void @global_2xf32_invariant(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; SM90-LABEL: global_2xf32_invariant(
; SM90:       {
; SM90-NEXT:    .reg .b32 %r<3>;
; SM90-NEXT:    .reg .b64 %rd<3>;
; SM90-EMPTY:
; SM90-NEXT:  // %bb.0:
; SM90-NEXT:    ld.param.b64 %rd1, [global_2xf32_invariant_param_0];
; SM90-NEXT:    .pragma "used_bytes_mask 0xf";
; SM90-NEXT:    ld.global.nc.v2.b32 {%r1, %r2}, [%rd1];
; SM90-NEXT:    ld.param.b64 %rd2, [global_2xf32_invariant_param_1];
; SM90-NEXT:    st.global.b32 [%rd2], %r1;
; SM90-NEXT:    ret;
;
; SM100-LABEL: global_2xf32_invariant(
; SM100:       {
; SM100-NEXT:    .reg .b32 %r<2>;
; SM100-NEXT:    .reg .b64 %rd<4>;
; SM100-EMPTY:
; SM100-NEXT:  // %bb.0:
; SM100-NEXT:    ld.param.b64 %rd1, [global_2xf32_invariant_param_0];
; SM100-NEXT:    .pragma "used_bytes_mask 0xf";
; SM100-NEXT:    ld.global.nc.b64 %rd2, [%rd1];
; SM100-NEXT:    ld.param.b64 %rd3, [global_2xf32_invariant_param_1];
; SM100-NEXT:    mov.b64 {%r1, _}, %rd2;
; SM100-NEXT:    st.global.b32 [%rd3], %r1;
; SM100-NEXT:    ret;
  %a.load = tail call <2 x float> @llvm.masked.load.v2f32.p1(ptr addrspace(1) align 8 %a, <2 x i1> <i1 true, i1 false>, <2 x float> poison), !invariant.load !0
  tail call void @llvm.masked.store.v2f32.p1(<2 x float> %a.load, ptr addrspace(1) align 8 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

define void @global_2xf32_no_align(ptr addrspace(1) %a, ptr addrspace(1) %b) {
; CHECK-LABEL: global_2xf32_no_align(
; CHECK:       {
; CHECK-NEXT:    .reg .b32 %r<2>;
; CHECK-NEXT:    .reg .b64 %rd<3>;
; CHECK-EMPTY:
; CHECK-NEXT:  // %bb.0:
; CHECK-NEXT:    ld.param.b64 %rd1, [global_2xf32_no_align_param_0];
; CHECK-NEXT:    ld.global.b32 %r1, [%rd1];
; CHECK-NEXT:    ld.param.b64 %rd2, [global_2xf32_no_align_param_1];
; CHECK-NEXT:    st.global.b32 [%rd2], %r1;
; CHECK-NEXT:    ret;
  %a.load = tail call <2 x float> @llvm.masked.load.v2f32.p1(ptr addrspace(1) align 4 %a, <2 x i1> <i1 true, i1 false>, <2 x float> poison)
  tail call void @llvm.masked.store.v2f32.p1(<2 x float> %a.load, ptr addrspace(1) align 8 %b, <2 x i1> <i1 true, i1 false>)
  ret void
}

declare <8 x i32> @llvm.masked.load.v8i32.p1(ptr addrspace(1), <8 x i1>, <8 x i32>)
declare void @llvm.masked.store.v8i32.p1(<8 x i32>, ptr addrspace(1), <8 x i1>)
declare <16 x i16> @llvm.masked.load.v16i16.p1(ptr addrspace(1), <16 x i1>, <16 x i16>)
declare void @llvm.masked.store.v16i16.p1(<16 x i16>, ptr addrspace(1), <16 x i1>)
declare <2 x i16> @llvm.masked.load.v2i16.p1(ptr addrspace(1), <2 x i1>, <2 x i16>)
declare void @llvm.masked.store.v2i16.p1(<2 x i16>, ptr addrspace(1), <2 x i1>)
declare <4 x i8> @llvm.masked.load.v4i8.p1(ptr addrspace(1), <4 x i1>, <4 x i8>)
declare void @llvm.masked.store.v4i8.p1(<4 x i8>, ptr addrspace(1), <4 x i1>)
declare <2 x float> @llvm.masked.load.v2f32.p1(ptr addrspace(1), <2 x i1>, <2 x float>)
declare void @llvm.masked.store.v2f32.p1(<2 x float>, ptr addrspace(1), <2 x i1>)
!0 = !{}
