; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=arm -mattr=+neon %s -o - | FileCheck %s

; Check codegen for 64-bit icmp operations, which don't directly map to any
; instruction.

define <2 x i64> @vne(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vne:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    vceq.i32 q8, q9, q8
; CHECK-NEXT:    vrev64.32 q9, q8
; CHECK-NEXT:    vand q8, q8, q9
; CHECK-NEXT:    vmvn q8, q8
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    vmov r2, r3, d17
; CHECK-NEXT:    mov pc, lr
      %tmp1 = load <2 x i64>, ptr %A
      %tmp2 = load <2 x i64>, ptr %B
      %tmp3 = icmp ne <2 x i64> %tmp1, %tmp2
      %tmp4 = sext <2 x i1> %tmp3 to <2 x i64>
      ret <2 x i64> %tmp4
}

define <2 x i64> @veq(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: veq:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    vceq.i32 q8, q9, q8
; CHECK-NEXT:    vrev64.32 q9, q8
; CHECK-NEXT:    vand q8, q8, q9
; CHECK-NEXT:    vmov r0, r1, d16
; CHECK-NEXT:    vmov r2, r3, d17
; CHECK-NEXT:    mov pc, lr
    %tmp1 = load <2 x i64>, ptr %A
    %tmp2 = load <2 x i64>, ptr %B
    %tmp3 = icmp eq <2 x i64> %tmp1, %tmp2
    %tmp4 = sext <2 x i1> %tmp3 to <2 x i64>
    ret <2 x i64> %tmp4
}

; FIXME: We currently generate terrible code for this.
; (Atop < Btop) | ((ATop == BTop) & (ABottom < BBottom))
; would come out to roughly 6 instructions, but we currently
; scalarize it.
define <2 x i64> @vult(ptr %A, ptr %B) nounwind {
; CHECK-LABEL: vult:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    push {r4, r5, r6, lr}
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    mov r2, #0
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    vmov r0, r12, d16
; CHECK-NEXT:    vmov lr, r4, d17
; CHECK-NEXT:    vmov r3, r1, d18
; CHECK-NEXT:    vmov r5, r6, d19
; CHECK-NEXT:    cmp r3, r0
; CHECK-NEXT:    sbcs r0, r1, r12
; CHECK-NEXT:    mov r0, #0
; CHECK-NEXT:    movlo r0, #1
; CHECK-NEXT:    cmp r0, #0
; CHECK-NEXT:    mvnne r0, #0
; CHECK-NEXT:    cmp r5, lr
; CHECK-NEXT:    sbcs r1, r6, r4
; CHECK-NEXT:    movlo r2, #1
; CHECK-NEXT:    cmp r2, #0
; CHECK-NEXT:    mvnne r2, #0
; CHECK-NEXT:    mov r1, r0
; CHECK-NEXT:    mov r3, r2
; CHECK-NEXT:    pop {r4, r5, r6, lr}
; CHECK-NEXT:    mov pc, lr
    %tmp1 = load <2 x i64>, ptr %A
    %tmp2 = load <2 x i64>, ptr %B
    %tmp3 = icmp ult <2 x i64> %tmp1, %tmp2
    %tmp4 = sext <2 x i1> %tmp3 to <2 x i64>
    ret <2 x i64> %tmp4
}
