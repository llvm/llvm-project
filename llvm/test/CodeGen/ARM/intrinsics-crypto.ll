; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc < %s -mtriple=armv8 -mattr=+crypto | FileCheck %s

define arm_aapcs_vfpcc <16 x i8> @test_aesde(ptr %a, ptr %b) {
; CHECK-LABEL: test_aesde:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    vld1.64 {d16, d17}, [r1]
; CHECK-NEXT:    vld1.64 {d18, d19}, [r0]
; CHECK-NEXT:    aesd.8 q9, q8
; CHECK-NEXT:    aese.8 q9, q8
; CHECK-NEXT:    aesimc.8 q8, q9
; CHECK-NEXT:    aesmc.8 q0, q8
; CHECK-NEXT:    bx lr
  %tmp = load <16 x i8>, ptr %a
  %tmp2 = load <16 x i8>, ptr %b
  %tmp3 = call <16 x i8> @llvm.arm.neon.aesd(<16 x i8> %tmp, <16 x i8> %tmp2)
  %tmp4 = call <16 x i8> @llvm.arm.neon.aese(<16 x i8> %tmp3, <16 x i8> %tmp2)
  %tmp5 = call <16 x i8> @llvm.arm.neon.aesimc(<16 x i8> %tmp4)
  %tmp6 = call <16 x i8> @llvm.arm.neon.aesmc(<16 x i8> %tmp5)
  ret <16 x i8> %tmp6
}

define arm_aapcs_vfpcc <4 x i32> @test_sha(ptr %a, ptr %b, ptr %c) {
; CHECK-LABEL: test_sha:
; CHECK:       @ %bb.0:
; CHECK-NEXT:    ldr r0, [r0]
; CHECK-NEXT:    vld1.64 {d0, d1}, [r1]
; CHECK-NEXT:    vmov s4, r0
; CHECK-NEXT:    vld1.64 {d18, d19}, [r2]
; CHECK-NEXT:    sha1h.32 q2, q1
; CHECK-NEXT:    vmov r0, s8
; CHECK-NEXT:    vmov.32 d16[0], r0
; CHECK-NEXT:    sha1c.32 q0, q1, q8
; CHECK-NEXT:    sha1m.32 q0, q1, q8
; CHECK-NEXT:    sha1p.32 q0, q1, q8
; CHECK-NEXT:    sha1su0.32 q0, q9, q8
; CHECK-NEXT:    sha1su1.32 q0, q8
; CHECK-NEXT:    sha256h.32 q0, q9, q8
; CHECK-NEXT:    sha256h2.32 q0, q9, q8
; CHECK-NEXT:    sha256su1.32 q0, q9, q8
; CHECK-NEXT:    sha256su0.32 q0, q9
; CHECK-NEXT:    bx lr
  %tmp = load <4 x i32>, ptr %a
  %tmp2 = load <4 x i32>, ptr %b
  %tmp3 = load <4 x i32>, ptr %c
  %scalar = extractelement <4 x i32> %tmp, i32 0
  %resscalar = call i32 @llvm.arm.neon.sha1h(i32 %scalar)
  %res1 = insertelement <4 x i32> undef, i32 %resscalar, i32 0
  %res2 = call <4 x i32> @llvm.arm.neon.sha1c(<4 x i32> %tmp2, i32 %scalar, <4 x i32> %res1)
  %res3 = call <4 x i32> @llvm.arm.neon.sha1m(<4 x i32> %res2, i32 %scalar, <4 x i32> %res1)
  %res4 = call <4 x i32> @llvm.arm.neon.sha1p(<4 x i32> %res3, i32 %scalar, <4 x i32> %res1)
  %res5 = call <4 x i32> @llvm.arm.neon.sha1su0(<4 x i32> %res4, <4 x i32> %tmp3, <4 x i32> %res1)
  %res6 = call <4 x i32> @llvm.arm.neon.sha1su1(<4 x i32> %res5, <4 x i32> %res1)
  %res7 = call <4 x i32> @llvm.arm.neon.sha256h(<4 x i32> %res6, <4 x i32> %tmp3, <4 x i32> %res1)
  %res8 = call <4 x i32> @llvm.arm.neon.sha256h2(<4 x i32> %res7, <4 x i32> %tmp3, <4 x i32> %res1)
  %res9 = call <4 x i32> @llvm.arm.neon.sha256su1(<4 x i32> %res8, <4 x i32> %tmp3, <4 x i32> %res1)
  %res10 = call <4 x i32> @llvm.arm.neon.sha256su0(<4 x i32> %res9, <4 x i32> %tmp3)
  ret <4 x i32> %res10
}

declare <16 x i8> @llvm.arm.neon.aesd(<16 x i8>, <16 x i8>)
declare <16 x i8> @llvm.arm.neon.aese(<16 x i8>, <16 x i8>)
declare <16 x i8> @llvm.arm.neon.aesimc(<16 x i8>)
declare <16 x i8> @llvm.arm.neon.aesmc(<16 x i8>)
declare i32 @llvm.arm.neon.sha1h(i32)
declare <4 x i32> @llvm.arm.neon.sha1c(<4 x i32>, i32, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha1m(<4 x i32>, i32, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha1p(<4 x i32>, i32, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha1su0(<4 x i32>, <4 x i32>, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha256h(<4 x i32>, <4 x i32>, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha256h2(<4 x i32>, <4 x i32>, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha256su1(<4 x i32>, <4 x i32>, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha256su0(<4 x i32>, <4 x i32>)
declare <4 x i32> @llvm.arm.neon.sha1su1(<4 x i32>, <4 x i32>)
