; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=armv7-- -mattr=+neon     | FileCheck %s -check-prefix=ARM

define void @test_masked_store_success(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; ARM-LABEL: test_masked_store_success:
; ARM:       @ %bb.0:
; ARM-NEXT:    vldr d16, [sp, #24]
; ARM-NEXT:    vmov d21, r2, r3
; ARM-NEXT:    ldr r12, [sp, #16]
; ARM-NEXT:    vmov d20, r0, r1
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    mov r0, sp
; ARM-NEXT:    vld1.64 {d22, d23}, [r12:128]
; ARM-NEXT:    vmovl.u16 q9, d16
; ARM-NEXT:    vmovl.u16 q8, d17
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vbsl q9, q10, q11
; ARM-NEXT:    vld1.64 {d20, d21}, [r0]
; ARM-NEXT:    vst1.32 {d18, d19}, [r12:128]!
; ARM-NEXT:    vld1.64 {d18, d19}, [r12:128]
; ARM-NEXT:    vbsl q8, q10, q9
; ARM-NEXT:    vst1.64 {d16, d17}, [r12:128]
; ARM-NEXT:    bx lr
  %load = load <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_volatile_load(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; ARM-LABEL: test_masked_store_volatile_load:
; ARM:       @ %bb.0:
; ARM-NEXT:    vldr d16, [sp, #24]
; ARM-NEXT:    vmov d21, r2, r3
; ARM-NEXT:    ldr r12, [sp, #16]
; ARM-NEXT:    vmov d20, r0, r1
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    mov r0, sp
; ARM-NEXT:    vld1.64 {d22, d23}, [r12:128]
; ARM-NEXT:    vmovl.u16 q9, d16
; ARM-NEXT:    vmovl.u16 q8, d17
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vbsl q9, q10, q11
; ARM-NEXT:    vld1.64 {d20, d21}, [r0]
; ARM-NEXT:    vst1.32 {d18, d19}, [r12:128]!
; ARM-NEXT:    vld1.64 {d18, d19}, [r12:128]
; ARM-NEXT:    vbsl q8, q10, q9
; ARM-NEXT:    vst1.64 {d16, d17}, [r12:128]
; ARM-NEXT:    bx lr
  %load = load volatile <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_volatile_store(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; ARM-LABEL: test_masked_store_volatile_store:
; ARM:       @ %bb.0:
; ARM-NEXT:    vldr d16, [sp, #24]
; ARM-NEXT:    vmov d21, r2, r3
; ARM-NEXT:    ldr r12, [sp, #16]
; ARM-NEXT:    vmov d20, r0, r1
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    mov r1, sp
; ARM-NEXT:    vmovl.u16 q9, d16
; ARM-NEXT:    mov r0, r12
; ARM-NEXT:    vmovl.u16 q8, d17
; ARM-NEXT:    vld1.32 {d22, d23}, [r0:128]!
; ARM-NEXT:    vld1.64 {d24, d25}, [r0:128]
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vbsl q9, q10, q11
; ARM-NEXT:    vld1.64 {d20, d21}, [r1]
; ARM-NEXT:    vbsl q8, q10, q12
; ARM-NEXT:    vst1.64 {d18, d19}, [r12:128]
; ARM-NEXT:    vst1.64 {d16, d17}, [r0:128]
; ARM-NEXT:    bx lr
  %load = load <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store volatile <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

declare void @use_vec(<8 x i32>)

define void @test_masked_store_intervening(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; ARM-LABEL: test_masked_store_intervening:
; ARM:       @ %bb.0:
; ARM-NEXT:    push {r4, r5, r6, r7, r8, r9, r11, lr}
; ARM-NEXT:    vpush {d8, d9, d10, d11}
; ARM-NEXT:    sub sp, sp, #16
; ARM-NEXT:    ldr r8, [sp, #96]
; ARM-NEXT:    vmov.i32 q8, #0x0
; ARM-NEXT:    mov r9, r3
; ARM-NEXT:    mov r5, r2
; ARM-NEXT:    vld1.64 {d8, d9}, [r8:128]
; ARM-NEXT:    mov r6, r1
; ARM-NEXT:    mov r4, r8
; ARM-NEXT:    mov r7, r0
; ARM-NEXT:    vst1.32 {d16, d17}, [r4:128]!
; ARM-NEXT:    mov r0, #0
; ARM-NEXT:    mov r1, #0
; ARM-NEXT:    mov r2, #0
; ARM-NEXT:    vld1.64 {d10, d11}, [r4:128]
; ARM-NEXT:    mov r3, #0
; ARM-NEXT:    vst1.64 {d16, d17}, [r4:128]
; ARM-NEXT:    vst1.64 {d16, d17}, [sp]
; ARM-NEXT:    bl use_vec
; ARM-NEXT:    vldr d16, [sp, #104]
; ARM-NEXT:    vmov d21, r5, r9
; ARM-NEXT:    add r0, sp, #80
; ARM-NEXT:    vmov d20, r7, r6
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    vld1.64 {d22, d23}, [r0]
; ARM-NEXT:    vmovl.u16 q9, d17
; ARM-NEXT:    vmovl.u16 q8, d16
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vbsl q9, q11, q5
; ARM-NEXT:    vbsl q8, q10, q4
; ARM-NEXT:    vst1.64 {d18, d19}, [r4:128]
; ARM-NEXT:    vst1.64 {d16, d17}, [r8:128]
; ARM-NEXT:    add sp, sp, #16
; ARM-NEXT:    vpop {d8, d9, d10, d11}
; ARM-NEXT:    pop {r4, r5, r6, r7, r8, r9, r11, pc}
  %load = load <8 x i32>, ptr %ptr, align 32
  store <8 x i32> zeroinitializer, ptr %ptr, align 32
  %tmp = load <8 x i32>, ptr %ptr
  call void @use_vec(<8 x i32> %tmp)
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}


define void @test_masked_store_multiple(<8 x i32> %x, <8 x i32> %y, ptr %ptr1, ptr %ptr2, <8 x i1> %mask, <8 x i1> %mask2) {
; ARM-LABEL: test_masked_store_multiple:
; ARM:       @ %bb.0:
; ARM-NEXT:    push {r11, lr}
; ARM-NEXT:    vldr d16, [sp, #64]
; ARM-NEXT:    vmov d23, r2, r3
; ARM-NEXT:    ldr lr, [sp, #60]
; ARM-NEXT:    vmov d22, r0, r1
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    ldr r12, [sp, #56]
; ARM-NEXT:    add r1, sp, #8
; ARM-NEXT:    vmovl.u16 q9, d16
; ARM-NEXT:    vldr d16, [sp, #72]
; ARM-NEXT:    mov r0, lr
; ARM-NEXT:    vld1.64 {d20, d21}, [r12:128]
; ARM-NEXT:    vmovl.u16 q14, d17
; ARM-NEXT:    vld1.32 {d24, d25}, [r0:128]!
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vzip.8 d16, d26
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vmovl.u16 q8, d16
; ARM-NEXT:    vbsl q9, q11, q10
; ARM-NEXT:    vld1.64 {d22, d23}, [r1]
; ARM-NEXT:    vmovl.u16 q10, d26
; ARM-NEXT:    add r1, sp, #24
; ARM-NEXT:    vshl.i32 q13, q14, #31
; ARM-NEXT:    vld1.64 {d28, d29}, [r1]
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    add r1, sp, #40
; ARM-NEXT:    vst1.32 {d18, d19}, [r12:128]!
; ARM-NEXT:    vshl.i32 q10, q10, #31
; ARM-NEXT:    vld1.64 {d18, d19}, [r12:128]
; ARM-NEXT:    vshr.s32 q13, q13, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vld1.64 {d30, d31}, [r0:128]
; ARM-NEXT:    vshr.s32 q10, q10, #31
; ARM-NEXT:    vbit q9, q11, q13
; ARM-NEXT:    vld1.64 {d22, d23}, [r1]
; ARM-NEXT:    vbsl q8, q14, q12
; ARM-NEXT:    vbsl q10, q11, q15
; ARM-NEXT:    vst1.64 {d18, d19}, [r12:128]
; ARM-NEXT:    vst1.64 {d16, d17}, [lr:128]
; ARM-NEXT:    vst1.64 {d20, d21}, [r0:128]
; ARM-NEXT:    pop {r11, pc}
  %load = load <8 x i32>, ptr %ptr1, align 32
  %load2 = load <8 x i32>, ptr %ptr2, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  %sel2 = select <8 x i1> %mask2, <8 x i32> %y, <8 x i32> %load2
  store <8 x i32> %sel, ptr %ptr1, align 32
  store <8 x i32> %sel2, ptr %ptr2, align 32
  ret void
}

define void @test_masked_store_unaligned(<8 x i32> %data, ptr %ptr, <8 x i1> %mask) {
; ARM-LABEL: test_masked_store_unaligned:
; ARM:       @ %bb.0:
; ARM-NEXT:    vldr d16, [sp, #24]
; ARM-NEXT:    vmov d21, r2, r3
; ARM-NEXT:    ldr r12, [sp, #16]
; ARM-NEXT:    vmov d20, r0, r1
; ARM-NEXT:    vzip.8 d16, d17
; ARM-NEXT:    mov r0, sp
; ARM-NEXT:    vld1.8 {d22, d23}, [r12]
; ARM-NEXT:    vmovl.u16 q9, d16
; ARM-NEXT:    vmovl.u16 q8, d17
; ARM-NEXT:    vshl.i32 q9, q9, #31
; ARM-NEXT:    vshl.i32 q8, q8, #31
; ARM-NEXT:    vshr.s32 q9, q9, #31
; ARM-NEXT:    vshr.s32 q8, q8, #31
; ARM-NEXT:    vbsl q9, q10, q11
; ARM-NEXT:    vld1.64 {d20, d21}, [r0]
; ARM-NEXT:    vst1.8 {d18, d19}, [r12]!
; ARM-NEXT:    vld1.8 {d18, d19}, [r12]
; ARM-NEXT:    vbsl q8, q10, q9
; ARM-NEXT:    vst1.8 {d16, d17}, [r12]
; ARM-NEXT:    bx lr
  %ptr_i8 = getelementptr i8, ptr %ptr, i32 1
  %ptr_vec = bitcast ptr %ptr_i8 to ptr
  %load = load <8 x i32>, ptr %ptr, align 1
  %sel = select <8 x i1> %mask, <8 x i32> %data, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 1
  ret void
}
