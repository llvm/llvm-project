# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
# RUN: llc -mtriple=aarch64 -run-pass=legalizer %s -o - | FileCheck %s
---
name:            test_modf_f16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_f16
    ; CHECK: [[COPY:%[0-9]+]]:_(s16) = COPY $h0
    ; CHECK-NEXT: [[FPEXT:%[0-9]+]]:_(s32) = G_FPEXT [[COPY]](s16)
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[FPEXT]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX]](p0) :: (load (s32) from %stack.0)
    ; CHECK-NEXT: [[FPTRUNC:%[0-9]+]]:_(s16) = G_FPTRUNC [[LOAD]](s32)
    ; CHECK-NEXT: [[FPTRUNC1:%[0-9]+]]:_(s16) = G_FPTRUNC [[COPY1]](s32)
    ; CHECK-NEXT: $h0 = COPY [[FPTRUNC1]](s16)
    ; CHECK-NEXT: $h1 = COPY [[FPTRUNC]](s16)
    ; CHECK-NEXT: RET_ReallyLR implicit $h0, implicit $h1
    %0:_(s16) = COPY $h0
    %1:_(s16), %2:_(s16) = G_FMODF %0
    $h0 = COPY %1(s16)
    $h1 = COPY %2(s16)
    RET_ReallyLR implicit $h0, implicit $h1
...
---
name:            test_modf_f16_only_use_fractional_part
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_f16_only_use_fractional_part
    ; CHECK: [[COPY:%[0-9]+]]:_(s16) = COPY $h0
    ; CHECK-NEXT: [[FPEXT:%[0-9]+]]:_(s32) = G_FPEXT [[COPY]](s16)
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[FPEXT]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[FPTRUNC:%[0-9]+]]:_(s16) = G_FPTRUNC [[COPY1]](s32)
    ; CHECK-NEXT: $h0 = COPY [[FPTRUNC]](s16)
    ; CHECK-NEXT: RET_ReallyLR implicit $h0
    %0:_(s16) = COPY $h0
    %1:_(s16), %2:_(s16) = G_FMODF %0
    $h0 = COPY %1(s16)
    RET_ReallyLR implicit $h0
...
---
name:            test_modf_v2f16
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_v2f16
    ; CHECK: [[COPY:%[0-9]+]]:_(<4 x s16>) = COPY $d0
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s16), [[UV1:%[0-9]+]]:_(s16), [[UV2:%[0-9]+]]:_(s16), [[UV3:%[0-9]+]]:_(s16) = G_UNMERGE_VALUES [[COPY]](<4 x s16>)
    ; CHECK-NEXT: [[FPEXT:%[0-9]+]]:_(s32) = G_FPEXT [[UV]](s16)
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.1
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[FPEXT]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX]](p0) :: (load (s32) from %stack.1)
    ; CHECK-NEXT: [[FPTRUNC:%[0-9]+]]:_(s16) = G_FPTRUNC [[LOAD]](s32)
    ; CHECK-NEXT: [[FPTRUNC1:%[0-9]+]]:_(s16) = G_FPTRUNC [[COPY1]](s32)
    ; CHECK-NEXT: [[FPEXT1:%[0-9]+]]:_(s32) = G_FPEXT [[UV1]](s16)
    ; CHECK-NEXT: [[FRAME_INDEX1:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[FPEXT1]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX1]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD1:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX1]](p0) :: (load (s32) from %stack.0)
    ; CHECK-NEXT: [[FPTRUNC2:%[0-9]+]]:_(s16) = G_FPTRUNC [[LOAD1]](s32)
    ; CHECK-NEXT: [[FPTRUNC3:%[0-9]+]]:_(s16) = G_FPTRUNC [[COPY2]](s32)
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:_(s16) = G_IMPLICIT_DEF
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<4 x s16>) = G_BUILD_VECTOR [[FPTRUNC1]](s16), [[FPTRUNC3]](s16), [[DEF]](s16), [[DEF]](s16)
    ; CHECK-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<4 x s16>) = G_BUILD_VECTOR [[FPTRUNC]](s16), [[FPTRUNC2]](s16), [[DEF]](s16), [[DEF]](s16)
    ; CHECK-NEXT: $d0 = COPY [[BUILD_VECTOR]](<4 x s16>)
    ; CHECK-NEXT: $d1 = COPY [[BUILD_VECTOR1]](<4 x s16>)
    ; CHECK-NEXT: RET_ReallyLR implicit $d0, implicit $d1
    %1:_(<4 x s16>) = COPY $d0
    %0:_(<2 x s16>), %2:_(<2 x s16>) = G_UNMERGE_VALUES %1(<4 x s16>)
    %3:_(<2 x s16>), %4:_(<2 x s16>) = G_FMODF %0
    %5:_(s16), %6:_(s16) = G_UNMERGE_VALUES %3(<2 x s16>)
    %7:_(s16) = G_IMPLICIT_DEF
    %8:_(<4 x s16>) = G_BUILD_VECTOR %5(s16), %6(s16), %7(s16), %7(s16)
    %9:_(s16), %10:_(s16) = G_UNMERGE_VALUES %4(<2 x s16>)
    %11:_(<4 x s16>) = G_BUILD_VECTOR %9(s16), %10(s16), %7(s16), %7(s16)
    $d0 = COPY %8(<4 x s16>)
    $d1 = COPY %11(<4 x s16>)
    RET_ReallyLR implicit $d0, implicit $d1
...
---
name:            test_modf_v3f32
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_v3f32
    ; CHECK: [[COPY:%[0-9]+]]:_(<2 x s64>) = COPY $q0
    ; CHECK-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x s32>) = G_BITCAST [[COPY]](<2 x s64>)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s32), [[UV1:%[0-9]+]]:_(s32), [[UV2:%[0-9]+]]:_(s32), [[UV3:%[0-9]+]]:_(s32) = G_UNMERGE_VALUES [[BITCAST]](<4 x s32>)
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.2
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[UV]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX]](p0) :: (load (s32) from %stack.2)
    ; CHECK-NEXT: [[FRAME_INDEX1:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.1
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[UV1]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX1]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD1:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX1]](p0) :: (load (s32) from %stack.1)
    ; CHECK-NEXT: [[FRAME_INDEX2:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $s0 = COPY [[UV2]](s32)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX2]](p0)
    ; CHECK-NEXT: BL &modff, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $s0, implicit $x0, implicit-def $s0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:_(s32) = COPY $s0
    ; CHECK-NEXT: [[LOAD2:%[0-9]+]]:_(s32) = G_LOAD [[FRAME_INDEX2]](p0) :: (load (s32) from %stack.0)
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:_(s32) = G_IMPLICIT_DEF
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<4 x s32>) = G_BUILD_VECTOR [[COPY1]](s32), [[COPY2]](s32), [[COPY3]](s32), [[DEF]](s32)
    ; CHECK-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<4 x s32>) = G_BUILD_VECTOR [[LOAD]](s32), [[LOAD1]](s32), [[LOAD2]](s32), [[DEF]](s32)
    ; CHECK-NEXT: $q0 = COPY [[BUILD_VECTOR]](<4 x s32>)
    ; CHECK-NEXT: $q1 = COPY [[BUILD_VECTOR1]](<4 x s32>)
    ; CHECK-NEXT: RET_ReallyLR implicit $q0, implicit $q1
    %1:_(<2 x s64>) = COPY $q0
    %2:_(<4 x s32>) = G_BITCAST %1(<2 x s64>)
    %3:_(s32), %4:_(s32), %5:_(s32), %6:_(s32) = G_UNMERGE_VALUES %2(<4 x s32>)
    %0:_(<3 x s32>) = G_BUILD_VECTOR %3(s32), %4(s32), %5(s32)
    %7:_(<3 x s32>), %8:_(<3 x s32>) = G_FMODF %0
    %9:_(s32), %10:_(s32), %11:_(s32) = G_UNMERGE_VALUES %7(<3 x s32>)
    %12:_(s32) = G_IMPLICIT_DEF
    %13:_(<4 x s32>) = G_BUILD_VECTOR %9(s32), %10(s32), %11(s32), %12(s32)
    %14:_(s32), %15:_(s32), %16:_(s32) = G_UNMERGE_VALUES %8(<3 x s32>)
    %17:_(<4 x s32>) = G_BUILD_VECTOR %14(s32), %15(s32), %16(s32), %12(s32)
    $q0 = COPY %13(<4 x s32>)
    $q1 = COPY %17(<4 x s32>)
    RET_ReallyLR implicit $q0, implicit $q1
...
---
name:            test_modf_v2f64
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_v2f64
    ; CHECK: [[COPY:%[0-9]+]]:_(<2 x s64>) = COPY $q0
    ; CHECK-NEXT: [[UV:%[0-9]+]]:_(s64), [[UV1:%[0-9]+]]:_(s64) = G_UNMERGE_VALUES [[COPY]](<2 x s64>)
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.1
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $d0 = COPY [[UV]](s64)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modf, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $d0, implicit $x0, implicit-def $d0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s64) = COPY $d0
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(s64) = G_LOAD [[FRAME_INDEX]](p0) :: (load (s64) from %stack.1)
    ; CHECK-NEXT: [[FRAME_INDEX1:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $d0 = COPY [[UV1]](s64)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX1]](p0)
    ; CHECK-NEXT: BL &modf, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $d0, implicit $x0, implicit-def $d0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:_(s64) = COPY $d0
    ; CHECK-NEXT: [[LOAD1:%[0-9]+]]:_(s64) = G_LOAD [[FRAME_INDEX1]](p0) :: (load (s64) from %stack.0)
    ; CHECK-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x s64>) = G_BUILD_VECTOR [[COPY1]](s64), [[COPY2]](s64)
    ; CHECK-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<2 x s64>) = G_BUILD_VECTOR [[LOAD]](s64), [[LOAD1]](s64)
    ; CHECK-NEXT: $q0 = COPY [[BUILD_VECTOR]](<2 x s64>)
    ; CHECK-NEXT: $q1 = COPY [[BUILD_VECTOR1]](<2 x s64>)
    ; CHECK-NEXT: RET_ReallyLR implicit $q0, implicit $q1
    %0:_(<2 x s64>) = COPY $q0
    %1:_(<2 x s64>), %2:_(<2 x s64>) = G_FMODF %0
    $q0 = COPY %1(<2 x s64>)
    $q1 = COPY %2(<2 x s64>)
    RET_ReallyLR implicit $q0, implicit $q1
...
---
name:            test_modf_fp128
body:             |
  bb.0.entry:
    ; CHECK-LABEL: name: test_modf_fp128
    ; CHECK: [[COPY:%[0-9]+]]:_(s128) = COPY $q0
    ; CHECK-NEXT: [[FRAME_INDEX:%[0-9]+]]:_(p0) = G_FRAME_INDEX %stack.0
    ; CHECK-NEXT: ADJCALLSTACKDOWN 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: $q0 = COPY [[COPY]](s128)
    ; CHECK-NEXT: $x0 = COPY [[FRAME_INDEX]](p0)
    ; CHECK-NEXT: BL &modfl, csr_aarch64_aapcs, implicit-def $lr, implicit $sp, implicit $q0, implicit $x0, implicit-def $q0
    ; CHECK-NEXT: ADJCALLSTACKUP 0, 0, implicit-def $sp, implicit $sp
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:_(s128) = COPY $q0
    ; CHECK-NEXT: [[LOAD:%[0-9]+]]:_(s128) = G_LOAD [[FRAME_INDEX]](p0) :: (load (s128) from %stack.0)
    ; CHECK-NEXT: $q0 = COPY [[COPY1]](s128)
    ; CHECK-NEXT: $q1 = COPY [[LOAD]](s128)
    ; CHECK-NEXT: RET_ReallyLR implicit $q0, implicit $q1
    %0:_(s128) = COPY $q0
    %1:_(s128), %2:_(s128) = G_FMODF %0
    $q0 = COPY %1(s128)
    $q1 = COPY %2(s128)
    RET_ReallyLR implicit $q0, implicit $q1
...
