# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=aarch64-unknown-unknown -verify-machineinstrs -run-pass=legalizer %s -o - | FileCheck %s
...
---
name:            v8s8_legal
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $d0
    ; CHECK-LABEL: name: v8s8_legal
    ; CHECK: liveins: $d0
    ; CHECK: %copy:_(<8 x s8>) = COPY $d0
    ; CHECK: %ctpop:_(<8 x s8>) = G_CTPOP %copy(<8 x s8>)
    ; CHECK: $d0 = COPY %ctpop(<8 x s8>)
    ; CHECK: RET_ReallyLR implicit $d0
    %copy:_(<8 x s8>) = COPY $d0
    %ctpop:_(<8 x s8>) = G_CTPOP %copy(<8 x s8>)
    $d0 = COPY %ctpop(<8 x s8>)
    RET_ReallyLR implicit $d0

...
---
name:            v16s8_legal
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $q0
    ; CHECK-LABEL: name: v16s8_legal
    ; CHECK: liveins: $q0
    ; CHECK: %copy:_(<16 x s8>) = COPY $q0
    ; CHECK: %ctpop:_(<16 x s8>) = G_CTPOP %copy(<16 x s8>)
    ; CHECK: $q0 = COPY %ctpop(<16 x s8>)
    ; CHECK: RET_ReallyLR implicit $q0
    %copy:_(<16 x s8>) = COPY $q0
    %ctpop:_(<16 x s8>) = G_CTPOP %copy(<16 x s8>)
    $q0 = COPY %ctpop(<16 x s8>)
    RET_ReallyLR implicit $q0

...
---
name:            s32_lower
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $w0
    ; CHECK-LABEL: name: s32_lower
    ; CHECK: liveins: $w0
    ; CHECK: %copy:_(s32) = COPY $w0
    ; CHECK: [[ZEXT:%[0-9]+]]:_(s64) = G_ZEXT %copy(s32)
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST [[ZEXT]](s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: %ctpop:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: $w0 = COPY %ctpop(s32)
    ; CHECK: RET_ReallyLR implicit $w0
    %copy:_(s32) = COPY $w0
    %ctpop:_(s32) = G_CTPOP %copy(s32)
    $w0 = COPY %ctpop(s32)
    RET_ReallyLR implicit $w0

...
---
name:            s64_lower
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $x0
    ; CHECK-LABEL: name: s64_lower
    ; CHECK: liveins: $x0
    ; CHECK: %copy:_(s64) = COPY $x0
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST %copy(s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: [[INT:%[0-9]+]]:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: %ctpop:_(s64) = G_ZEXT [[INT]](s32)
    ; CHECK: $x0 = COPY %ctpop(s64)
    ; CHECK: RET_ReallyLR implicit $x0
    %copy:_(s64) = COPY $x0
    %ctpop:_(s64) = G_CTPOP %copy(s64)
    $x0 = COPY %ctpop(s64)
    RET_ReallyLR implicit $x0

...
---
name:            widen_s16
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $w0

    ; CHECK-LABEL: name: widen_s16
    ; CHECK: liveins: $w0
    ; CHECK: %copy:_(s32) = COPY $w0
    ; CHECK: [[C:%[0-9]+]]:_(s64) = G_CONSTANT i64 65535
    ; CHECK: [[ANYEXT:%[0-9]+]]:_(s64) = G_ANYEXT %copy(s32)
    ; CHECK: [[AND:%[0-9]+]]:_(s64) = G_AND [[ANYEXT]], [[C]]
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST [[AND]](s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: [[INT:%[0-9]+]]:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: [[COPY:%[0-9]+]]:_(s32) = COPY [[INT]](s32)
    ; CHECK: %ext:_(s32) = COPY [[COPY]](s32)
    ; CHECK: $w0 = COPY %ext(s32)
    ; CHECK: RET_ReallyLR implicit $w0
    %copy:_(s32) = COPY $w0
    %trunc:_(s16) = G_TRUNC %copy(s32)
    %ctpop:_(s16) = G_CTPOP %trunc(s16)
    %ext:_(s32) = G_ANYEXT %ctpop(s16)
    $w0 = COPY %ext(s32)
    RET_ReallyLR implicit $w0

...
---
name:            widen_s8
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $w0

    ; CHECK-LABEL: name: widen_s8
    ; CHECK: liveins: $w0
    ; CHECK: %copy:_(s32) = COPY $w0
    ; CHECK: [[C:%[0-9]+]]:_(s64) = G_CONSTANT i64 255
    ; CHECK: [[ANYEXT:%[0-9]+]]:_(s64) = G_ANYEXT %copy(s32)
    ; CHECK: [[AND:%[0-9]+]]:_(s64) = G_AND [[ANYEXT]], [[C]]
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST [[AND]](s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: [[INT:%[0-9]+]]:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: [[COPY:%[0-9]+]]:_(s32) = COPY [[INT]](s32)
    ; CHECK: %ext:_(s32) = COPY [[COPY]](s32)
    ; CHECK: $w0 = COPY %ext(s32)
    ; CHECK: RET_ReallyLR implicit $w0
    %copy:_(s32) = COPY $w0
    %trunc:_(s8) = G_TRUNC %copy(s32)
    %ctpop:_(s8) = G_CTPOP %trunc(s8)
    %ext:_(s32) = G_ANYEXT %ctpop(s8)
    $w0 = COPY %ext(s32)
    RET_ReallyLR implicit $w0

...
---
name:            widen_s3
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $w0

    ; CHECK-LABEL: name: widen_s3
    ; CHECK: liveins: $w0
    ; CHECK: %copy:_(s32) = COPY $w0
    ; CHECK: [[C:%[0-9]+]]:_(s64) = G_CONSTANT i64 7
    ; CHECK: [[ANYEXT:%[0-9]+]]:_(s64) = G_ANYEXT %copy(s32)
    ; CHECK: [[AND:%[0-9]+]]:_(s64) = G_AND [[ANYEXT]], [[C]]
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST [[AND]](s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: [[INT:%[0-9]+]]:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: [[COPY:%[0-9]+]]:_(s32) = COPY [[INT]](s32)
    ; CHECK: %ext:_(s32) = COPY [[COPY]](s32)
    ; CHECK: $w0 = COPY %ext(s32)
    ; CHECK: RET_ReallyLR implicit $w0
    %copy:_(s32) = COPY $w0
    %trunc:_(s3) = G_TRUNC %copy(s32)
    %ctpop:_(s3) = G_CTPOP %trunc(s3)
    %ext:_(s32) = G_ANYEXT %ctpop(s3)
    $w0 = COPY %ext(s32)
    RET_ReallyLR implicit $w0

...
---
name:            different_sizes
tracksRegLiveness: true
body:             |
  bb.0:
    liveins: $w0
    ; CHECK-LABEL: name: different_sizes
    ; CHECK: liveins: $w0
    ; CHECK: %copy:_(s32) = COPY $w0
    ; CHECK: [[C:%[0-9]+]]:_(s64) = G_CONSTANT i64 255
    ; CHECK: [[ANYEXT:%[0-9]+]]:_(s64) = G_ANYEXT %copy(s32)
    ; CHECK: [[AND:%[0-9]+]]:_(s64) = G_AND [[ANYEXT]], [[C]]
    ; CHECK: [[BITCAST:%[0-9]+]]:_(<8 x s8>) = G_BITCAST [[AND]](s64)
    ; CHECK: [[CTPOP:%[0-9]+]]:_(<8 x s8>) = G_CTPOP [[BITCAST]](<8 x s8>)
    ; CHECK: [[INT:%[0-9]+]]:_(s32) = G_INTRINSIC intrinsic(@llvm.aarch64.neon.uaddlv), [[CTPOP]](<8 x s8>)
    ; CHECK: [[COPY:%[0-9]+]]:_(s32) = COPY [[INT]](s32)
    ; CHECK: %ext:_(s32) = COPY [[COPY]](s32)
    ; CHECK: $w0 = COPY %ext(s32)
    ; CHECK: RET_ReallyLR implicit $w0
    %copy:_(s32) = COPY $w0
    %trunc:_(s8) = G_TRUNC %copy(s32)
    %ctpop:_(s16) = G_CTPOP %trunc(s8)
    %ext:_(s32) = G_ANYEXT %ctpop(s16)
    $w0 = COPY %ext(s32)
    RET_ReallyLR implicit $w0
