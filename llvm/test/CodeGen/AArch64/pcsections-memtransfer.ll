; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
; RUN: llc < %s -global-isel=0 -mtriple=aarch64-unknown-linux-gnu -stop-after=aarch64-expand-pseudo -verify-machineinstrs | FileCheck %s

define i64 @call_memcpy_intrinsic(ptr %src, ptr %dst, i64 %len) {
  ; CHECK-LABEL: name: call_memcpy_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0, $x1, $x2, $x19, $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STPXpre killed $lr, killed $x19, $sp, -2 :: (store (s64) into %stack.1), (store (s64) into %stack.0)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w19, -8
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x19 = ORRXrs $xzr, $x1, 0
  ; CHECK-NEXT:   BL &memcpy, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit-def $sp, implicit-def dead $x0, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x19, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr, $x19 = frame-destroy LDPXpost $sp, 2 :: (load (s64) from %stack.1), (load (s64) from %stack.0)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memcpy.p0.p0.i64(ptr %src, ptr %dst, i64 %len, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memcpy_intrinsic_sm(ptr %src, ptr %dst) {
  ; CHECK-LABEL: name: call_memcpy_intrinsic_sm
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0, $x1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   renamable $w8 = LDRBBui renamable $x1, 0, pcsections !0 :: (volatile load (s8) from %ir.dst)
  ; CHECK-NEXT:   STRBBui killed renamable $w8, killed renamable $x0, 0, pcsections !0 :: (volatile store (s8) into %ir.src)
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x1, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memcpy.p0.p0.i64(ptr %src, ptr %dst, i64 1, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memcpy_inline_intrinsic(ptr %src, ptr %dst) {
  ; CHECK-LABEL: name: call_memcpy_inline_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0, $x1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   renamable $w8 = LDRBBui renamable $x1, 0, pcsections !0 :: (volatile load (s8) from %ir.dst)
  ; CHECK-NEXT:   STRBBui killed renamable $w8, killed renamable $x0, 0, pcsections !0 :: (volatile store (s8) into %ir.src)
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x1, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memcpy.inline.p0.p0.i64(ptr %src, ptr %dst, i64 1, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memmove_intrinsic(ptr %src, ptr %dst, i64 %len) {
  ; CHECK-LABEL: name: call_memmove_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0, $x1, $x2, $x19, $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STPXpre killed $lr, killed $x19, $sp, -2 :: (store (s64) into %stack.1), (store (s64) into %stack.0)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w19, -8
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x19 = ORRXrs $xzr, $x1, 0
  ; CHECK-NEXT:   BL &memmove, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $x1, implicit $x2, implicit-def $sp, implicit-def dead $x0, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x19, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr, $x19 = frame-destroy LDPXpost $sp, 2 :: (load (s64) from %stack.1), (load (s64) from %stack.0)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memmove.p0.p0.i64(ptr %src, ptr %dst, i64 %len, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memset_intrinsic(ptr %dst, i64 %len) {
  ; CHECK-LABEL: name: call_memset_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0, $x1, $x19, $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STPXpre killed $lr, killed $x19, $sp, -2 :: (store (s64) into %stack.1), (store (s64) into %stack.0)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w19, -8
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x2 = ORRXrs $xzr, $x1, 0
  ; CHECK-NEXT:   $x19 = ORRXrs $xzr, $x0, 0
  ; CHECK-NEXT:   $w1 = ORRWrs $wzr, $wzr, 0
  ; CHECK-NEXT:   BL &memset, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit $w1, implicit $x2, implicit-def $sp, implicit-def dead $x0, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x19, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr, $x19 = frame-destroy LDPXpost $sp, 2 :: (load (s64) from %stack.1), (load (s64) from %stack.0)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memset.p0.p0.i64(ptr %dst, i8 0, i64 %len, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memset_inline_intrinsic(ptr %dst) {
  ; CHECK-LABEL: name: call_memset_inline_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $x0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   STRBBui $wzr, renamable $x0, 0, pcsections !0 :: (volatile store (s8) into %ir.dst)
  ; CHECK-NEXT:   renamable $x0 = LDRXui killed renamable $x0, 0 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  call void @llvm.memset.inline.p0.p0.i64(ptr %dst, i8 0, i64 1, i1 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memcpy_element_unordered_atomic_intrinsic() {
  ; CHECK-LABEL: name: call_memcpy_element_unordered_atomic_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STRXpre killed $lr, $sp, -16 :: (store (s64) into %stack.2)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x0 = ADDXri $sp, 12, 0
  ; CHECK-NEXT:   $x1 = ADDXri $sp, 8, 0
  ; CHECK-NEXT:   dead $w2 = MOVZWi 1, 0, implicit-def $x2
  ; CHECK-NEXT:   BL &__llvm_memcpy_element_unordered_atomic_1, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit killed $x1, implicit killed $x2, implicit-def $sp, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDRXui $sp, 1 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr = frame-destroy LDRXpost $sp, 16 :: (load (s64) from %stack.2)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  %src = alloca i32, align 1
  %dst = alloca i32, align 1
  call void @llvm.memcpy.element.unordered.atomic.p0.p0.i64(ptr align 1 %src, ptr align 1 %dst, i64 1, i32 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memmove_element_unordered_atomic_intrinsic() {
  ; CHECK-LABEL: name: call_memmove_element_unordered_atomic_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STRXpre killed $lr, $sp, -16 :: (store (s64) into %stack.2)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x0 = ADDXri $sp, 12, 0
  ; CHECK-NEXT:   $x1 = ADDXri $sp, 8, 0
  ; CHECK-NEXT:   dead $w2 = MOVZWi 1, 0, implicit-def $x2
  ; CHECK-NEXT:   BL &__llvm_memmove_element_unordered_atomic_1, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit killed $x1, implicit killed $x2, implicit-def $sp, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDRXui $sp, 1 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr = frame-destroy LDRXpost $sp, 16 :: (load (s64) from %stack.2)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  %src = alloca i32, align 1
  %dst = alloca i32, align 1
  call void @llvm.memmove.element.unordered.atomic.p0.p0.i64(ptr align 1 %src, ptr align 1 %dst, i64 1, i32 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}

define i64 @call_memset_element_unordered_atomic_intrinsic() {
  ; CHECK-LABEL: name: call_memset_element_unordered_atomic_intrinsic
  ; CHECK: bb.0 (%ir-block.0):
  ; CHECK-NEXT:   liveins: $lr
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber $sp = frame-setup STRXpre killed $lr, $sp, -16 :: (store (s64) into %stack.1)
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION def_cfa_offset 16
  ; CHECK-NEXT:   frame-setup CFI_INSTRUCTION offset $w30, -16
  ; CHECK-NEXT:   $x0 = ADDXri $sp, 12, 0
  ; CHECK-NEXT:   $w1 = ORRWrs $wzr, $wzr, 0
  ; CHECK-NEXT:   dead $w2 = MOVZWi 1, 0, implicit-def $x2
  ; CHECK-NEXT:   BL &__llvm_memset_element_unordered_atomic_1, csr_aarch64_aapcs, implicit-def dead $lr, implicit $sp, implicit $x0, implicit killed $w1, implicit killed $x2, implicit-def $sp, pcsections !0
  ; CHECK-NEXT:   renamable $x0 = LDURXi $sp, 12 :: (load (s64) from %ir.dst)
  ; CHECK-NEXT:   early-clobber $sp, $lr = frame-destroy LDRXpost $sp, 16 :: (load (s64) from %stack.1)
  ; CHECK-NEXT:   RET undef $lr, implicit $x0
  %dst = alloca i32, align 1
  call void @llvm.memset.element.unordered.atomic.p0.p0.i64(ptr align 1 %dst, i8 0, i64 1, i32 1), !pcsections !0
  %val = load i64, ptr %dst
  ret i64 %val
}


!0 = !{!"foo"}

declare void @llvm.memcpy.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i1)
declare void @llvm.memcpy.inline.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i1)
declare void @llvm.memmove.p0.p0.i64(ptr nocapture, ptr nocapture readonly, i64, i1)
declare void @llvm.memset.p0.p0.i64(ptr nocapture, i8, i64, i1)
declare void @llvm.memset.inline.p0.p0.i64(ptr nocapture, i8, i64, i1)
declare void @llvm.memcpy.element.unordered.atomic.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i32)
declare void @llvm.memmove.element.unordered.atomic.p0.p0.i64(ptr nocapture writeonly, ptr nocapture readonly, i64, i32)
declare void @llvm.memset.element.unordered.atomic.p0.p0.i64(ptr nocapture writeonly, i8, i64, i32)
