; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=aarch64-linux-gnu -mattr=+sve < %s | FileCheck --check-prefixes=CHECK,CHECK-SVE %s
; RUN: llc -mtriple=aarch64-linux-gnu -mattr=+sme -force-streaming < %s | FileCheck --check-prefixes=CHECK,CHECK-SME %s

; Test the optimization that folds lsl + lsr + orr to rev for half-width shifts

; Test case 1: 16-bit elements with 8-bit shift -> revh
define <vscale x 8 x i16> @lsl_lsr_orr_revh_i16(<vscale x 8 x i16> %x) {
; CHECK-LABEL: lsl_lsr_orr_revh_i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.h
; CHECK-NEXT:    revb z0.h, p0/m, z0.h
; CHECK-NEXT:    ret
  %lsl = shl <vscale x 8 x i16> %x, splat(i16 8)
  %lsr = lshr <vscale x 8 x i16> %x, splat(i16 8)
  %orr = or <vscale x 8 x i16> %lsl, %lsr
  ret <vscale x 8 x i16> %orr
}

; Test case 2: 32-bit elements with 16-bit shift -> revh
define <vscale x 4 x i32> @lsl_lsr_orr_revw_i32(<vscale x 4 x i32> %x) {
; CHECK-SVE-LABEL: lsl_lsr_orr_revw_i32:
; CHECK-SVE:       // %bb.0:
; CHECK-SVE-NEXT:    ptrue p0.s
; CHECK-SVE-NEXT:    revh z0.s, p0/m, z0.s
; CHECK-SVE-NEXT:    ret
;
; CHECK-SME-LABEL: lsl_lsr_orr_revw_i32:
; CHECK-SME:       // %bb.0:
; CHECK-SME-NEXT:    movi v1.2d, #0000000000000000
; CHECK-SME-NEXT:    xar z0.s, z0.s, z1.s, #16
; CHECK-SME-NEXT:    ret
  %lsl = shl <vscale x 4 x i32> %x, splat(i32 16)
  %lsr = lshr <vscale x 4 x i32> %x, splat(i32 16)
  %orr = or <vscale x 4 x i32> %lsl, %lsr
  ret <vscale x 4 x i32> %orr
}

; Test case 3: 64-bit elements with 32-bit shift -> revw
define <vscale x 2 x i64> @lsl_lsr_orr_revd_i64(<vscale x 2 x i64> %x) {
; CHECK-SVE-LABEL: lsl_lsr_orr_revd_i64:
; CHECK-SVE:       // %bb.0:
; CHECK-SVE-NEXT:    ptrue p0.d
; CHECK-SVE-NEXT:    revw z0.d, p0/m, z0.d
; CHECK-SVE-NEXT:    ret
;
; CHECK-SME-LABEL: lsl_lsr_orr_revd_i64:
; CHECK-SME:       // %bb.0:
; CHECK-SME-NEXT:    movi v1.2d, #0000000000000000
; CHECK-SME-NEXT:    xar z0.d, z0.d, z1.d, #32
; CHECK-SME-NEXT:    ret
  %lsl = shl <vscale x 2 x i64> %x, splat(i64 32)
  %lsr = lshr <vscale x 2 x i64> %x, splat(i64 32)
  %orr = or <vscale x 2 x i64> %lsl, %lsr
  ret <vscale x 2 x i64> %orr
}

; Test case 4: Order doesn't matter - lsr + lsl + orr -> revh
define <vscale x 8 x i16> @lsr_lsl_orr_revh_i16(<vscale x 8 x i16> %x) {
; CHECK-LABEL: lsr_lsl_orr_revh_i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p0.h
; CHECK-NEXT:    revb z0.h, p0/m, z0.h
; CHECK-NEXT:    ret
  %lsr = lshr <vscale x 8 x i16> %x, splat(i16 8)
  %lsl = shl <vscale x 8 x i16> %x, splat(i16 8)
  %orr = or <vscale x 8 x i16> %lsr, %lsl
  ret <vscale x 8 x i16> %orr
}

; Test case 5: Non-half-width shift should not be optimized
define <vscale x 8 x i16> @lsl_lsr_orr_no_opt_i16(<vscale x 8 x i16> %x) {
; CHECK-LABEL: lsl_lsr_orr_no_opt_i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    lsl z1.h, z0.h, #4
; CHECK-NEXT:    lsr z0.h, z0.h, #4
; CHECK-NEXT:    orr z0.d, z1.d, z0.d
; CHECK-NEXT:    ret
  %lsl = shl <vscale x 8 x i16> %x, splat(i16 4)
  %lsr = lshr <vscale x 8 x i16> %x, splat(i16 4)
  %orr = or <vscale x 8 x i16> %lsl, %lsr
  ret <vscale x 8 x i16> %orr
}

; Test case 6: Different shift amounts should not be optimized
define <vscale x 8 x i16> @lsl_lsr_orr_different_shifts_i16(<vscale x 8 x i16> %x) {
; CHECK-LABEL: lsl_lsr_orr_different_shifts_i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    lsl z1.h, z0.h, #8
; CHECK-NEXT:    lsr z0.h, z0.h, #4
; CHECK-NEXT:    orr z0.d, z1.d, z0.d
; CHECK-NEXT:    ret
  %lsl = shl <vscale x 8 x i16> %x, splat(i16 8)
  %lsr = lshr <vscale x 8 x i16> %x, splat(i16 4)
  %orr = or <vscale x 8 x i16> %lsl, %lsr
  ret <vscale x 8 x i16> %orr
}
