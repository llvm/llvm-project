; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mattr=+sve    < %s | FileCheck %s -check-prefix CHECK-SVE
; RUN: llc -mattr=+sve2p1 < %s | FileCheck %s -check-prefix CHECK-SVE2p1

target triple = "aarch64-unknown-linux"

; Byte elements, UF=0
define void @f_b_0(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_b_0:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB0_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    mov w9, w2
; CHECK-SVE-NEXT:    rdvl x10, #1
; CHECK-SVE-NEXT:    mov x8, xzr
; CHECK-SVE-NEXT:    whilelo p0.b, xzr, x9
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB0_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-SVE-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-SVE-NEXT:    add x8, x8, x10
; CHECK-SVE-NEXT:    whilelo p0.b, x8, x9
; CHECK-SVE-NEXT:    b.mi .LBB0_2
; CHECK-SVE-NEXT:  .LBB0_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_b_0:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB0_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    mov w9, w2
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    whilelo p0.b, xzr, x9
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB0_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-SVE2p1-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE2p1-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-SVE2p1-NEXT:    addvl x8, x8, #1
; CHECK-SVE2p1-NEXT:    whilelo p0.b, x8, x9
; CHECK-SVE2p1-NEXT:    b.mi .LBB0_2
; CHECK-SVE2p1-NEXT:  .LBB0_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds i8, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %2, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %3 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %4 = getelementptr inbounds i8, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %3, ptr %4, i32 1, <vscale x 16 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 16 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Halfword elements, UF=0
define void @f_h_0(ptr noalias %dst, ptr %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_h_0:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB1_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    mov w9, w2
; CHECK-SVE-NEXT:    cnth x10
; CHECK-SVE-NEXT:    mov x8, xzr
; CHECK-SVE-NEXT:    whilelo p0.h, xzr, x9
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB1_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1h { z0.h }, p0/z, [x1, x8, lsl #1]
; CHECK-SVE-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE-NEXT:    st1h { z0.h }, p0, [x0, x8, lsl #1]
; CHECK-SVE-NEXT:    add x8, x8, x10
; CHECK-SVE-NEXT:    whilelo p0.h, x8, x9
; CHECK-SVE-NEXT:    b.mi .LBB1_2
; CHECK-SVE-NEXT:  .LBB1_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_h_0:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB1_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    mov w9, w2
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    whilelo p0.h, xzr, x9
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB1_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1h { z0.h }, p0/z, [x1, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE2p1-NEXT:    st1h { z0.h }, p0, [x0, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    inch x8
; CHECK-SVE2p1-NEXT:    whilelo p0.h, x8, x9
; CHECK-SVE2p1-NEXT:    b.mi .LBB1_2
; CHECK-SVE2p1-NEXT:  .LBB1_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 3
  %active.lane.mask.entry = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 8 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds i16, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %2, i32 2, <vscale x 8 x i1> %active.lane.mask, <vscale x 8 x i16> poison)
  %3 = mul <vscale x 8 x i16> %wide.masked.load, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %4 = getelementptr inbounds i16, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %3, ptr %4, i32 2, <vscale x 8 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 8 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Double-precision float elements, UF=0
define void @f_d_0(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_d_0:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB2_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    mov w9, w2
; CHECK-SVE-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE-NEXT:    cntd x10
; CHECK-SVE-NEXT:    mov x8, xzr
; CHECK-SVE-NEXT:    whilelo p0.d, xzr, x9
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB2_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-SVE-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-SVE-NEXT:    add x8, x8, x10
; CHECK-SVE-NEXT:    whilelo p0.d, x8, x9
; CHECK-SVE-NEXT:    b.mi .LBB2_2
; CHECK-SVE-NEXT:  .LBB2_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_d_0:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB2_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    mov w9, w2
; CHECK-SVE2p1-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    whilelo p0.d, xzr, x9
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB2_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE2p1-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    incd x8
; CHECK-SVE2p1-NEXT:    whilelo p0.d, x8, x9
; CHECK-SVE2p1-NEXT:    b.mi .LBB2_2
; CHECK-SVE2p1-NEXT:  .LBB2_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 1
  %active.lane.mask.entry = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 0, i64 %wide.trip.count)
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %2 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %2, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %3 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %4 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %3, ptr %4, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 %index.next, i64 %wide.trip.count)
  %5 = extractelement <vscale x 2 x i1> %active.lane.mask.next, i64 0
  br i1 %5, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Byte elements, UF=2
define void @f_b_2(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_b_2:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB3_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    rdvl x11, #1
; CHECK-SVE-NEXT:    mov w8, w2
; CHECK-SVE-NEXT:    mov x9, xzr
; CHECK-SVE-NEXT:    add x10, x0, x11
; CHECK-SVE-NEXT:    whilelo p0.b, x11, x8
; CHECK-SVE-NEXT:    add x11, x1, x11
; CHECK-SVE-NEXT:    rdvl x12, #2
; CHECK-SVE-NEXT:    rdvl x13, #3
; CHECK-SVE-NEXT:    whilelo p1.b, xzr, x8
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB3_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1b { z0.b }, p1/z, [x1, x9]
; CHECK-SVE-NEXT:    ld1b { z1.b }, p0/z, [x11, x9]
; CHECK-SVE-NEXT:    add x14, x13, x9
; CHECK-SVE-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE-NEXT:    st1b { z0.b }, p1, [x0, x9]
; CHECK-SVE-NEXT:    st1b { z1.b }, p0, [x10, x9]
; CHECK-SVE-NEXT:    add x9, x12, x9
; CHECK-SVE-NEXT:    whilelo p0.b, x14, x8
; CHECK-SVE-NEXT:    whilelo p1.b, x9, x8
; CHECK-SVE-NEXT:    b.mi .LBB3_2
; CHECK-SVE-NEXT:  .LBB3_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_b_2:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB3_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    rdvl x10, #1
; CHECK-SVE2p1-NEXT:    mov w8, w2
; CHECK-SVE2p1-NEXT:    mov x9, xzr
; CHECK-SVE2p1-NEXT:    addvl x11, x1, #1
; CHECK-SVE2p1-NEXT:    whilelo p1.b, xzr, x8
; CHECK-SVE2p1-NEXT:    whilelo p0.b, x10, x8
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #1
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB3_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1b { z0.b }, p1/z, [x1, x9]
; CHECK-SVE2p1-NEXT:    ld1b { z1.b }, p0/z, [x11, x9]
; CHECK-SVE2p1-NEXT:    addvl x12, x9, #3
; CHECK-SVE2p1-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE2p1-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE2p1-NEXT:    st1b { z0.b }, p1, [x0, x9]
; CHECK-SVE2p1-NEXT:    st1b { z1.b }, p0, [x10, x9]
; CHECK-SVE2p1-NEXT:    addvl x9, x9, #2
; CHECK-SVE2p1-NEXT:    whilelo p0.b, x12, x8
; CHECK-SVE2p1-NEXT:    whilelo p1.b, x9, x8
; CHECK-SVE2p1-NEXT:    b.mi .LBB3_2
; CHECK-SVE2p1-NEXT:  .LBB3_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 5
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry10 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 4
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 4
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = shl nuw nsw i64 %8, 4
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next13, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry10, %for.body.preheader ], [ %active.lane.mask.next14, %vector.body ]
  %active.lane.mask11 = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %10 = getelementptr inbounds i8, ptr %src, i64 %index
  %11 = getelementptr inbounds i8, ptr %10, i64 %5
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %10, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %wide.masked.load12 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %11, i32 1, <vscale x 16 x i1> %active.lane.mask11, <vscale x 16 x i8> poison)
  %12 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %13 = mul <vscale x 16 x i8> %wide.masked.load12, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %14 = getelementptr inbounds i8, ptr %dst, i64 %index
  %15 = getelementptr inbounds i8, ptr %14, i64 %7
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %12, ptr %14, i32 1, <vscale x 16 x i1> %active.lane.mask)
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %13, ptr %15, i32 1, <vscale x 16 x i1> %active.lane.mask11)
  %index.next = add i64 %index, %1
  %index.next13 = add i64 %index, %1
  %16 = add i64 %index.next, %9
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %16, i64 %wide.trip.count)
  %active.lane.mask.next14 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %17 = extractelement <vscale x 16 x i1> %active.lane.mask.next14, i64 0
  br i1 %17, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Halfword elements, UF=2
define void @f_h_2(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_h_2:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB4_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    mov w9, w2
; CHECK-SVE-NEXT:    rdvl x10, #1
; CHECK-SVE-NEXT:    mov x8, xzr
; CHECK-SVE-NEXT:    add x11, x0, x10
; CHECK-SVE-NEXT:    whilelo p1.b, xzr, x9
; CHECK-SVE-NEXT:    add x12, x1, x10
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB4_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1h { z0.h }, p1/z, [x1, x8, lsl #1]
; CHECK-SVE-NEXT:    ld1h { z1.h }, p0/z, [x12, x8, lsl #1]
; CHECK-SVE-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE-NEXT:    mul z1.h, z1.h, #3
; CHECK-SVE-NEXT:    st1h { z0.h }, p1, [x0, x8, lsl #1]
; CHECK-SVE-NEXT:    st1h { z1.h }, p0, [x11, x8, lsl #1]
; CHECK-SVE-NEXT:    add x8, x10, x8
; CHECK-SVE-NEXT:    whilelo p1.b, x8, x9
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    mov z0.h, p1/z, #1 // =0x1
; CHECK-SVE-NEXT:    fmov w13, s0
; CHECK-SVE-NEXT:    tbnz w13, #0, .LBB4_2
; CHECK-SVE-NEXT:  .LBB4_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_h_2:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB4_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    mov w9, w2
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #1
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    whilelo { p0.h, p1.h }, xzr, x9
; CHECK-SVE2p1-NEXT:    addvl x11, x1, #1
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB4_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1h { z0.h }, p0/z, [x1, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    ld1h { z1.h }, p1/z, [x11, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE2p1-NEXT:    mul z1.h, z1.h, #3
; CHECK-SVE2p1-NEXT:    st1h { z0.h }, p0, [x0, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    st1h { z1.h }, p1, [x10, x8, lsl #1]
; CHECK-SVE2p1-NEXT:    addvl x8, x8, #1
; CHECK-SVE2p1-NEXT:    whilelo { p0.h, p1.h }, x8, x9
; CHECK-SVE2p1-NEXT:    mov z0.h, p0/z, #1 // =0x1
; CHECK-SVE2p1-NEXT:    fmov w12, s0
; CHECK-SVE2p1-NEXT:    tbnz w12, #0, .LBB4_2
; CHECK-SVE2p1-NEXT:  .LBB4_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry10 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry, i64 8)
  %active.lane.mask.entry11 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry, i64 0)
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 3
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 3
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next14, %vector.body ]
  %active.lane.mask = phi <vscale x 8 x i1> [ %active.lane.mask.entry11, %for.body.preheader ], [ %active.lane.mask.next16, %vector.body ]
  %active.lane.mask12 = phi <vscale x 8 x i1> [ %active.lane.mask.entry10, %for.body.preheader ], [ %active.lane.mask.next15, %vector.body ]
  %6 = getelementptr inbounds i16, ptr %src, i64 %index
  %7 = getelementptr inbounds i16, ptr %6, i64 %3
  %wide.masked.load = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %6, i32 2, <vscale x 8 x i1> %active.lane.mask, <vscale x 8 x i16> poison)
  %wide.masked.load13 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr nonnull %7, i32 2, <vscale x 8 x i1> %active.lane.mask12, <vscale x 8 x i16> poison)
  %8 = mul <vscale x 8 x i16> %wide.masked.load, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %9 = mul <vscale x 8 x i16> %wide.masked.load13, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %10 = getelementptr inbounds i16, ptr %dst, i64 %index
  %11 = getelementptr inbounds i16, ptr %10, i64 %5
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %8, ptr %10, i32 2, <vscale x 8 x i1> %active.lane.mask)
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %9, ptr %11, i32 2, <vscale x 8 x i1> %active.lane.mask12)
  %index.next = add i64 %index, %1
  %index.next14 = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next15 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next, i64 8)
  %active.lane.mask.next16 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next, i64 0)
  %12 = extractelement <vscale x 8 x i1> %active.lane.mask.next16, i64 0
  br i1 %12, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Double-precision float elements, UF=2
define void @f3(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f3:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB5_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    mov w9, w2
; CHECK-SVE-NEXT:    rdvl x11, #1
; CHECK-SVE-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE-NEXT:    mov x8, xzr
; CHECK-SVE-NEXT:    add x10, x0, x11
; CHECK-SVE-NEXT:    whilelo p1.s, xzr, x9
; CHECK-SVE-NEXT:    add x11, x1, x11
; CHECK-SVE-NEXT:    cntw x12
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB5_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1d { z1.d }, p1/z, [x1, x8, lsl #3]
; CHECK-SVE-NEXT:    ld1d { z2.d }, p0/z, [x11, x8, lsl #3]
; CHECK-SVE-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-SVE-NEXT:    st1d { z1.d }, p1, [x0, x8, lsl #3]
; CHECK-SVE-NEXT:    st1d { z2.d }, p0, [x10, x8, lsl #3]
; CHECK-SVE-NEXT:    add x8, x12, x8
; CHECK-SVE-NEXT:    whilelo p1.s, x8, x9
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    mov z1.d, p1/z, #1 // =0x1
; CHECK-SVE-NEXT:    fmov x13, d1
; CHECK-SVE-NEXT:    tbnz w13, #0, .LBB5_2
; CHECK-SVE-NEXT:  .LBB5_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f3:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB5_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    mov w9, w2
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #1
; CHECK-SVE2p1-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    whilelo { p0.d, p1.d }, xzr, x9
; CHECK-SVE2p1-NEXT:    addvl x11, x1, #1
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB5_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1d { z1.d }, p0/z, [x1, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    ld1d { z2.d }, p1/z, [x11, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE2p1-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-SVE2p1-NEXT:    st1d { z1.d }, p0, [x0, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    st1d { z2.d }, p1, [x10, x8, lsl #3]
; CHECK-SVE2p1-NEXT:    incw x8
; CHECK-SVE2p1-NEXT:    whilelo { p0.d, p1.d }, x8, x9
; CHECK-SVE2p1-NEXT:    mov z1.d, p0/z, #1 // =0x1
; CHECK-SVE2p1-NEXT:    fmov x12, d1
; CHECK-SVE2p1-NEXT:    tbnz w12, #0, .LBB5_2
; CHECK-SVE2p1-NEXT:  .LBB5_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry1 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry2 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 1
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next5, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry2, %for.body.preheader ], [ %active.lane.mask.next7, %vector.body ]
  %active.lane.mask3 = phi <vscale x 2 x i1> [ %active.lane.mask.entry1, %for.body.preheader ], [ %active.lane.mask.next6, %vector.body ]
  %6 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %6, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %7 = getelementptr inbounds double, ptr %6, i64 %3
  %wide.masked.load4 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %7, i32 8, <vscale x 2 x i1> %active.lane.mask3, <vscale x 2 x double> poison)
  %8 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %9 = fmul <vscale x 2 x double> %wide.masked.load4, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %10 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %8, ptr %10, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %11 = getelementptr inbounds double, ptr %10, i64 %5
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %9, ptr %11, i32 8, <vscale x 2 x i1> %active.lane.mask3)
  %index.next = add i64 %index, %1
  %index.next5 = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %12 = extractelement <vscale x 2 x i1> %active.lane.mask.next7, i64 0
  br i1 %12, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Byte elements, UF=4
define void @f_b_4(ptr noalias %dst, ptr %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_b_4:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB6_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    rdvl x14, #2
; CHECK-SVE-NEXT:    mov w8, w2
; CHECK-SVE-NEXT:    mov x9, xzr
; CHECK-SVE-NEXT:    add x11, x0, x14
; CHECK-SVE-NEXT:    rdvl x13, #3
; CHECK-SVE-NEXT:    rdvl x15, #1
; CHECK-SVE-NEXT:    add x10, x0, x13
; CHECK-SVE-NEXT:    add x12, x0, x15
; CHECK-SVE-NEXT:    rdvl x16, #4
; CHECK-SVE-NEXT:    rdvl x17, #5
; CHECK-SVE-NEXT:    rdvl x18, #6
; CHECK-SVE-NEXT:    rdvl x2, #7
; CHECK-SVE-NEXT:    whilelo p0.b, x13, x8
; CHECK-SVE-NEXT:    add x13, x1, x13
; CHECK-SVE-NEXT:    whilelo p1.b, x14, x8
; CHECK-SVE-NEXT:    add x14, x1, x14
; CHECK-SVE-NEXT:    whilelo p2.b, x15, x8
; CHECK-SVE-NEXT:    add x15, x1, x15
; CHECK-SVE-NEXT:    whilelo p3.b, xzr, x8
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB6_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1b { z0.b }, p3/z, [x1, x9]
; CHECK-SVE-NEXT:    ld1b { z1.b }, p2/z, [x15, x9]
; CHECK-SVE-NEXT:    add x3, x2, x9
; CHECK-SVE-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE-NEXT:    ld1b { z2.b }, p1/z, [x14, x9]
; CHECK-SVE-NEXT:    ld1b { z3.b }, p0/z, [x13, x9]
; CHECK-SVE-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE-NEXT:    mul z2.b, z2.b, #3
; CHECK-SVE-NEXT:    mul z3.b, z3.b, #3
; CHECK-SVE-NEXT:    st1b { z0.b }, p3, [x0, x9]
; CHECK-SVE-NEXT:    st1b { z1.b }, p2, [x12, x9]
; CHECK-SVE-NEXT:    st1b { z2.b }, p1, [x11, x9]
; CHECK-SVE-NEXT:    st1b { z3.b }, p0, [x10, x9]
; CHECK-SVE-NEXT:    whilelo p0.b, x3, x8
; CHECK-SVE-NEXT:    add x3, x18, x9
; CHECK-SVE-NEXT:    whilelo p1.b, x3, x8
; CHECK-SVE-NEXT:    add x3, x17, x9
; CHECK-SVE-NEXT:    add x9, x16, x9
; CHECK-SVE-NEXT:    whilelo p2.b, x3, x8
; CHECK-SVE-NEXT:    whilelo p3.b, x9, x8
; CHECK-SVE-NEXT:    b.mi .LBB6_2
; CHECK-SVE-NEXT:  .LBB6_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_b_4:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB6_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    rdvl x10, #1
; CHECK-SVE2p1-NEXT:    mov w8, w2
; CHECK-SVE2p1-NEXT:    mov x9, xzr
; CHECK-SVE2p1-NEXT:    rdvl x11, #2
; CHECK-SVE2p1-NEXT:    rdvl x12, #3
; CHECK-SVE2p1-NEXT:    addvl x13, x1, #3
; CHECK-SVE2p1-NEXT:    addvl x14, x1, #2
; CHECK-SVE2p1-NEXT:    addvl x15, x1, #1
; CHECK-SVE2p1-NEXT:    whilelo p3.b, xzr, x8
; CHECK-SVE2p1-NEXT:    whilelo p0.b, x12, x8
; CHECK-SVE2p1-NEXT:    addvl x12, x0, #1
; CHECK-SVE2p1-NEXT:    whilelo p1.b, x11, x8
; CHECK-SVE2p1-NEXT:    addvl x11, x0, #2
; CHECK-SVE2p1-NEXT:    whilelo p2.b, x10, x8
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #3
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB6_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1b { z0.b }, p3/z, [x1, x9]
; CHECK-SVE2p1-NEXT:    ld1b { z1.b }, p2/z, [x15, x9]
; CHECK-SVE2p1-NEXT:    addvl x16, x9, #5
; CHECK-SVE2p1-NEXT:    addvl x17, x9, #6
; CHECK-SVE2p1-NEXT:    ld1b { z2.b }, p1/z, [x14, x9]
; CHECK-SVE2p1-NEXT:    ld1b { z3.b }, p0/z, [x13, x9]
; CHECK-SVE2p1-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE2p1-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE2p1-NEXT:    mul z2.b, z2.b, #3
; CHECK-SVE2p1-NEXT:    mul z3.b, z3.b, #3
; CHECK-SVE2p1-NEXT:    addvl x18, x9, #7
; CHECK-SVE2p1-NEXT:    st1b { z0.b }, p3, [x0, x9]
; CHECK-SVE2p1-NEXT:    st1b { z1.b }, p2, [x12, x9]
; CHECK-SVE2p1-NEXT:    st1b { z2.b }, p1, [x11, x9]
; CHECK-SVE2p1-NEXT:    st1b { z3.b }, p0, [x10, x9]
; CHECK-SVE2p1-NEXT:    addvl x9, x9, #4
; CHECK-SVE2p1-NEXT:    whilelo p0.b, x18, x8
; CHECK-SVE2p1-NEXT:    whilelo p1.b, x17, x8
; CHECK-SVE2p1-NEXT:    whilelo p2.b, x16, x8
; CHECK-SVE2p1-NEXT:    whilelo p3.b, x9, x8
; CHECK-SVE2p1-NEXT:    b.mi .LBB6_2
; CHECK-SVE2p1-NEXT:  .LBB6_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 6
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 5
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = mul nuw nsw i64 %6, 48
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %7, i64 %wide.trip.count)
  %active.lane.mask.entry12 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %5, i64 %wide.trip.count)
  %active.lane.mask.entry13 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry14 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = shl nuw nsw i64 %8, 4
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 5
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = mul nuw nsw i64 %12, 48
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = shl nuw nsw i64 %14, 4
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 5
  %18 = tail call i64 @llvm.vscale.i64()
  %19 = mul nuw nsw i64 %18, 48
  %20 = tail call i64 @llvm.vscale.i64()
  %21 = shl nuw nsw i64 %20, 4
  %22 = tail call i64 @llvm.vscale.i64()
  %23 = shl nuw nsw i64 %22, 5
  %24 = tail call i64 @llvm.vscale.i64()
  %25 = mul nuw nsw i64 %24, 48
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next23, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %active.lane.mask.entry14, %for.body.preheader ], [ %active.lane.mask.next26, %vector.body ]
  %active.lane.mask15 = phi <vscale x 16 x i1> [ %active.lane.mask.entry13, %for.body.preheader ], [ %active.lane.mask.next25, %vector.body ]
  %active.lane.mask16 = phi <vscale x 16 x i1> [ %active.lane.mask.entry12, %for.body.preheader ], [ %active.lane.mask.next24, %vector.body ]
  %active.lane.mask17 = phi <vscale x 16 x i1> [ %active.lane.mask.entry, %for.body.preheader ], [ %active.lane.mask.next, %vector.body ]
  %26 = getelementptr inbounds i8, ptr %src, i64 %index
  %27 = getelementptr inbounds i8, ptr %26, i64 %9
  %28 = getelementptr inbounds i8, ptr %26, i64 %11
  %29 = getelementptr inbounds i8, ptr %26, i64 %13
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %26, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %wide.masked.load18 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %27, i32 1, <vscale x 16 x i1> %active.lane.mask15, <vscale x 16 x i8> poison)
  %wide.masked.load19 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %28, i32 1, <vscale x 16 x i1> %active.lane.mask16, <vscale x 16 x i8> poison)
  %wide.masked.load20 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %29, i32 1, <vscale x 16 x i1> %active.lane.mask17, <vscale x 16 x i8> poison)
  %30 = mul <vscale x 16 x i8> %wide.masked.load, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %31 = mul <vscale x 16 x i8> %wide.masked.load18, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %32 = mul <vscale x 16 x i8> %wide.masked.load19, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %33 = mul <vscale x 16 x i8> %wide.masked.load20, shufflevector (<vscale x 16 x i8> insertelement (<vscale x 16 x i8> poison, i8 3, i64 0), <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer)
  %34 = getelementptr inbounds i8, ptr %dst, i64 %index
  %35 = getelementptr inbounds i8, ptr %34, i64 %15
  %36 = getelementptr inbounds i8, ptr %34, i64 %17
  %37 = getelementptr inbounds i8, ptr %34, i64 %19
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %30, ptr %34, i32 1, <vscale x 16 x i1> %active.lane.mask)
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %31, ptr %35, i32 1, <vscale x 16 x i1> %active.lane.mask15)
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %32, ptr %36, i32 1, <vscale x 16 x i1> %active.lane.mask16)
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %33, ptr %37, i32 1, <vscale x 16 x i1> %active.lane.mask17)
  %index.next = add i64 %index, %1
  %index.next23 = add i64 %index, %1
  %38 = add i64 %index.next, %21
  %39 = add i64 %index.next, %23
  %40 = add i64 %index.next, %25
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %40, i64 %wide.trip.count)
  %active.lane.mask.next24 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %39, i64 %wide.trip.count)
  %active.lane.mask.next25 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %38, i64 %wide.trip.count)
  %active.lane.mask.next26 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %41 = extractelement <vscale x 16 x i1> %active.lane.mask.next26, i64 0
  br i1 %41, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

; Halfword elements, UF=4
define void @f_h_4(ptr noalias %dst, ptr %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_h_4:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB7_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    rdvl x17, #1
; CHECK-SVE-NEXT:    mov w8, w2
; CHECK-SVE-NEXT:    mov x10, xzr
; CHECK-SVE-NEXT:    add x14, x0, x17
; CHECK-SVE-NEXT:    whilelo p1.b, x17, x8
; CHECK-SVE-NEXT:    add x17, x1, x17
; CHECK-SVE-NEXT:    rdvl x9, #2
; CHECK-SVE-NEXT:    add x13, x0, x9
; CHECK-SVE-NEXT:    add x16, x1, x9
; CHECK-SVE-NEXT:    rdvl x11, #3
; CHECK-SVE-NEXT:    add x12, x0, x11
; CHECK-SVE-NEXT:    add x15, x1, x11
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    whilelo p3.b, xzr, x8
; CHECK-SVE-NEXT:    punpkhi p2.h, p3.b
; CHECK-SVE-NEXT:    punpklo p3.h, p3.b
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB7_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1h { z0.h }, p3/z, [x1, x10, lsl #1]
; CHECK-SVE-NEXT:    ld1h { z1.h }, p2/z, [x17, x10, lsl #1]
; CHECK-SVE-NEXT:    add x18, x11, x10
; CHECK-SVE-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE-NEXT:    ld1h { z2.h }, p1/z, [x16, x10, lsl #1]
; CHECK-SVE-NEXT:    ld1h { z3.h }, p0/z, [x15, x10, lsl #1]
; CHECK-SVE-NEXT:    mul z1.h, z1.h, #3
; CHECK-SVE-NEXT:    mul z2.h, z2.h, #3
; CHECK-SVE-NEXT:    mul z3.h, z3.h, #3
; CHECK-SVE-NEXT:    st1h { z0.h }, p3, [x0, x10, lsl #1]
; CHECK-SVE-NEXT:    st1h { z1.h }, p2, [x14, x10, lsl #1]
; CHECK-SVE-NEXT:    st1h { z2.h }, p1, [x13, x10, lsl #1]
; CHECK-SVE-NEXT:    st1h { z3.h }, p0, [x12, x10, lsl #1]
; CHECK-SVE-NEXT:    add x10, x9, x10
; CHECK-SVE-NEXT:    whilelo p1.b, x18, x8
; CHECK-SVE-NEXT:    whilelo p3.b, x10, x8
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    punpkhi p2.h, p3.b
; CHECK-SVE-NEXT:    punpklo p3.h, p3.b
; CHECK-SVE-NEXT:    mov z0.h, p3/z, #1 // =0x1
; CHECK-SVE-NEXT:    fmov w18, s0
; CHECK-SVE-NEXT:    tbnz w18, #0, .LBB7_2
; CHECK-SVE-NEXT:  .LBB7_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_h_4:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB7_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    rdvl x10, #1
; CHECK-SVE2p1-NEXT:    mov w8, w2
; CHECK-SVE2p1-NEXT:    mov x9, xzr
; CHECK-SVE2p1-NEXT:    whilelo { p2.h, p3.h }, xzr, x8
; CHECK-SVE2p1-NEXT:    addvl x11, x0, #2
; CHECK-SVE2p1-NEXT:    whilelo { p0.h, p1.h }, x10, x8
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #3
; CHECK-SVE2p1-NEXT:    addvl x12, x0, #1
; CHECK-SVE2p1-NEXT:    addvl x13, x1, #3
; CHECK-SVE2p1-NEXT:    addvl x14, x1, #2
; CHECK-SVE2p1-NEXT:    addvl x15, x1, #1
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB7_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1h { z0.h }, p2/z, [x1, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    ld1h { z1.h }, p3/z, [x15, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    addvl x16, x9, #3
; CHECK-SVE2p1-NEXT:    mul z0.h, z0.h, #3
; CHECK-SVE2p1-NEXT:    ld1h { z2.h }, p0/z, [x14, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    ld1h { z3.h }, p1/z, [x13, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    mul z1.h, z1.h, #3
; CHECK-SVE2p1-NEXT:    mul z2.h, z2.h, #3
; CHECK-SVE2p1-NEXT:    mul z3.h, z3.h, #3
; CHECK-SVE2p1-NEXT:    st1h { z0.h }, p2, [x0, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    st1h { z1.h }, p3, [x12, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    st1h { z2.h }, p0, [x11, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    st1h { z3.h }, p1, [x10, x9, lsl #1]
; CHECK-SVE2p1-NEXT:    addvl x9, x9, #2
; CHECK-SVE2p1-NEXT:    whilelo { p0.h, p1.h }, x16, x8
; CHECK-SVE2p1-NEXT:    whilelo { p2.h, p3.h }, x9, x8
; CHECK-SVE2p1-NEXT:    mov z0.h, p2/z, #1 // =0x1
; CHECK-SVE2p1-NEXT:    fmov w16, s0
; CHECK-SVE2p1-NEXT:    tbnz w16, #0, .LBB7_2
; CHECK-SVE2p1-NEXT:  .LBB7_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp7 = icmp sgt i32 %n, 0
  br i1 %cmp7, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 5
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %active.lane.mask.entry = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry12 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry13 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry, i64 8)
  %active.lane.mask.entry14 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry, i64 0)
  %active.lane.mask.entry15 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry12, i64 8)
  %active.lane.mask.entry16 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.entry12, i64 0)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 3
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 4
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = mul nuw nsw i64 %8, 24
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 3
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = shl nuw nsw i64 %12, 4
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = mul nuw nsw i64 %14, 24
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 4
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next25, %vector.body ]
  %active.lane.mask = phi <vscale x 8 x i1> [ %active.lane.mask.entry16, %for.body.preheader ], [ %active.lane.mask.next30, %vector.body ]
  %active.lane.mask17 = phi <vscale x 8 x i1> [ %active.lane.mask.entry15, %for.body.preheader ], [ %active.lane.mask.next29, %vector.body ]
  %active.lane.mask18 = phi <vscale x 8 x i1> [ %active.lane.mask.entry14, %for.body.preheader ], [ %active.lane.mask.next28, %vector.body ]
  %active.lane.mask19 = phi <vscale x 8 x i1> [ %active.lane.mask.entry13, %for.body.preheader ], [ %active.lane.mask.next27, %vector.body ]
  %18 = getelementptr inbounds i16, ptr %src, i64 %index
  %19 = getelementptr inbounds i16, ptr %18, i64 %5
  %20 = getelementptr inbounds i16, ptr %18, i64 %7
  %21 = getelementptr inbounds i16, ptr %18, i64 %9
  %wide.masked.load = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr %18, i32 2, <vscale x 8 x i1> %active.lane.mask, <vscale x 8 x i16> poison)
  %wide.masked.load20 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr nonnull %19, i32 2, <vscale x 8 x i1> %active.lane.mask17, <vscale x 8 x i16> poison)
  %wide.masked.load21 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr nonnull %20, i32 2, <vscale x 8 x i1> %active.lane.mask18, <vscale x 8 x i16> poison)
  %wide.masked.load22 = tail call <vscale x 8 x i16> @llvm.masked.load.nxv8i16.p0(ptr nonnull %21, i32 2, <vscale x 8 x i1> %active.lane.mask19, <vscale x 8 x i16> poison)
  %22 = mul <vscale x 8 x i16> %wide.masked.load, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %23 = mul <vscale x 8 x i16> %wide.masked.load20, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %24 = mul <vscale x 8 x i16> %wide.masked.load21, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %25 = mul <vscale x 8 x i16> %wide.masked.load22, shufflevector (<vscale x 8 x i16> insertelement (<vscale x 8 x i16> poison, i16 3, i64 0), <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer)
  %26 = getelementptr inbounds i16, ptr %dst, i64 %index
  %27 = getelementptr inbounds i16, ptr %26, i64 %11
  %28 = getelementptr inbounds i16, ptr %26, i64 %13
  %29 = getelementptr inbounds i16, ptr %26, i64 %15
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %22, ptr %26, i32 2, <vscale x 8 x i1> %active.lane.mask)
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %23, ptr %27, i32 2, <vscale x 8 x i1> %active.lane.mask17)
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %24, ptr %28, i32 2, <vscale x 8 x i1> %active.lane.mask18)
  tail call void @llvm.masked.store.nxv8i16.p0(<vscale x 8 x i16> %25, ptr %29, i32 2, <vscale x 8 x i1> %active.lane.mask19)
  %index.next = add i64 %index, %1
  %index.next25 = add i64 %index, %1
  %30 = add i64 %index.next, %17
  %active.lane.mask.next = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %30, i64 %wide.trip.count)
  %active.lane.mask.next26 = tail call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next27 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next, i64 8)
  %active.lane.mask.next28 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next, i64 0)
  %active.lane.mask.next29 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next26, i64 8)
  %active.lane.mask.next30 = tail call <vscale x 8 x i1> @llvm.vector.extract.nxv8i1.nxv16i1(<vscale x 16 x i1> %active.lane.mask.next26, i64 0)
  %31 = extractelement <vscale x 8 x i1> %active.lane.mask.next30, i64 0
  br i1 %31, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}


; Double-precision float elements, UF=4
define void @f_d_4(ptr noalias %dst, ptr readonly %src, i32 %n) #0 {
; CHECK-SVE-LABEL: f_d_4:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp w2, #1
; CHECK-SVE-NEXT:    b.lt .LBB8_3
; CHECK-SVE-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE-NEXT:    cntw x10
; CHECK-SVE-NEXT:    mov w8, w2
; CHECK-SVE-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE-NEXT:    mov x9, xzr
; CHECK-SVE-NEXT:    rdvl x13, #3
; CHECK-SVE-NEXT:    rdvl x14, #2
; CHECK-SVE-NEXT:    add x11, x0, x14
; CHECK-SVE-NEXT:    add x14, x1, x14
; CHECK-SVE-NEXT:    rdvl x15, #1
; CHECK-SVE-NEXT:    add x12, x0, x15
; CHECK-SVE-NEXT:    add x15, x1, x15
; CHECK-SVE-NEXT:    cnth x16
; CHECK-SVE-NEXT:    cntw x17, all, mul #3
; CHECK-SVE-NEXT:    whilelo p1.s, x10, x8
; CHECK-SVE-NEXT:    add x10, x0, x13
; CHECK-SVE-NEXT:    add x13, x1, x13
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    whilelo p3.s, xzr, x8
; CHECK-SVE-NEXT:    punpkhi p2.h, p3.b
; CHECK-SVE-NEXT:    punpklo p3.h, p3.b
; CHECK-SVE-NEXT:    .p2align 5, , 16
; CHECK-SVE-NEXT:  .LBB8_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    ld1d { z1.d }, p3/z, [x1, x9, lsl #3]
; CHECK-SVE-NEXT:    ld1d { z2.d }, p2/z, [x15, x9, lsl #3]
; CHECK-SVE-NEXT:    add x18, x17, x9
; CHECK-SVE-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE-NEXT:    ld1d { z3.d }, p1/z, [x14, x9, lsl #3]
; CHECK-SVE-NEXT:    ld1d { z4.d }, p0/z, [x13, x9, lsl #3]
; CHECK-SVE-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-SVE-NEXT:    fmul z3.d, z3.d, z0.d
; CHECK-SVE-NEXT:    fmul z4.d, z4.d, z0.d
; CHECK-SVE-NEXT:    st1d { z1.d }, p3, [x0, x9, lsl #3]
; CHECK-SVE-NEXT:    st1d { z2.d }, p2, [x12, x9, lsl #3]
; CHECK-SVE-NEXT:    st1d { z3.d }, p1, [x11, x9, lsl #3]
; CHECK-SVE-NEXT:    st1d { z4.d }, p0, [x10, x9, lsl #3]
; CHECK-SVE-NEXT:    add x9, x16, x9
; CHECK-SVE-NEXT:    whilelo p1.s, x18, x8
; CHECK-SVE-NEXT:    whilelo p3.s, x9, x8
; CHECK-SVE-NEXT:    punpkhi p0.h, p1.b
; CHECK-SVE-NEXT:    punpklo p1.h, p1.b
; CHECK-SVE-NEXT:    punpkhi p2.h, p3.b
; CHECK-SVE-NEXT:    punpklo p3.h, p3.b
; CHECK-SVE-NEXT:    mov z1.d, p3/z, #1 // =0x1
; CHECK-SVE-NEXT:    fmov x18, d1
; CHECK-SVE-NEXT:    tbnz w18, #0, .LBB8_2
; CHECK-SVE-NEXT:  .LBB8_3: // %for.cond.cleanup
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: f_d_4:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp w2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB8_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %for.body.preheader
; CHECK-SVE2p1-NEXT:    cntw x10
; CHECK-SVE2p1-NEXT:    mov w8, w2
; CHECK-SVE2p1-NEXT:    fmov z0.d, #3.00000000
; CHECK-SVE2p1-NEXT:    mov x9, xzr
; CHECK-SVE2p1-NEXT:    mov x15, xzr
; CHECK-SVE2p1-NEXT:    whilelo { p2.d, p3.d }, xzr, x8
; CHECK-SVE2p1-NEXT:    addvl x11, x0, #2
; CHECK-SVE2p1-NEXT:    whilelo { p0.d, p1.d }, x10, x8
; CHECK-SVE2p1-NEXT:    addvl x10, x0, #3
; CHECK-SVE2p1-NEXT:    addvl x12, x0, #1
; CHECK-SVE2p1-NEXT:    addvl x13, x1, #3
; CHECK-SVE2p1-NEXT:    addvl x14, x1, #2
; CHECK-SVE2p1-NEXT:    addvl x16, x1, #1
; CHECK-SVE2p1-NEXT:    .p2align 5, , 16
; CHECK-SVE2p1-NEXT:  .LBB8_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    ld1d { z1.d }, p2/z, [x1, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    ld1d { z2.d }, p3/z, [x16, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    inch x15
; CHECK-SVE2p1-NEXT:    fmul z1.d, z1.d, z0.d
; CHECK-SVE2p1-NEXT:    ld1d { z3.d }, p0/z, [x14, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    ld1d { z4.d }, p1/z, [x13, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    fmul z2.d, z2.d, z0.d
; CHECK-SVE2p1-NEXT:    fmul z3.d, z3.d, z0.d
; CHECK-SVE2p1-NEXT:    fmul z4.d, z4.d, z0.d
; CHECK-SVE2p1-NEXT:    st1d { z1.d }, p2, [x0, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    st1d { z2.d }, p3, [x12, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    whilelo { p2.d, p3.d }, x15, x8
; CHECK-SVE2p1-NEXT:    mov z1.d, p2/z, #1 // =0x1
; CHECK-SVE2p1-NEXT:    st1d { z3.d }, p0, [x11, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    st1d { z4.d }, p1, [x10, x9, lsl #3]
; CHECK-SVE2p1-NEXT:    incw x9, all, mul #3
; CHECK-SVE2p1-NEXT:    whilelo { p0.d, p1.d }, x9, x8
; CHECK-SVE2p1-NEXT:    mov x9, x15
; CHECK-SVE2p1-NEXT:    fmov x17, d1
; CHECK-SVE2p1-NEXT:    tbnz w17, #0, .LBB8_2
; CHECK-SVE2p1-NEXT:  .LBB8_3: // %for.cond.cleanup
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp6 = icmp sgt i32 %n, 0
  br i1 %cmp6, label %for.body.preheader, label %for.cond.cleanup

for.body.preheader:
  %wide.trip.count = zext nneg i32 %n to i64
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 3
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 2
  %active.lane.mask.entry = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %3, i64 %wide.trip.count)
  %active.lane.mask.entry3 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 %wide.trip.count)
  %active.lane.mask.entry4 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 2)
  %active.lane.mask.entry5 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry, i64 0)
  %active.lane.mask.entry6 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 2)
  %active.lane.mask.entry7 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.entry3, i64 0)
  %4 = tail call i64 @llvm.vscale.i64()
  %5 = shl nuw nsw i64 %4, 1
  %6 = tail call i64 @llvm.vscale.i64()
  %7 = shl nuw nsw i64 %6, 2
  %8 = tail call i64 @llvm.vscale.i64()
  %9 = mul nuw nsw i64 %8, 6
  %10 = tail call i64 @llvm.vscale.i64()
  %11 = shl nuw nsw i64 %10, 1
  %12 = tail call i64 @llvm.vscale.i64()
  %13 = shl nuw nsw i64 %12, 2
  %14 = tail call i64 @llvm.vscale.i64()
  %15 = mul nuw nsw i64 %14, 6
  %16 = tail call i64 @llvm.vscale.i64()
  %17 = shl nuw nsw i64 %16, 2
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %for.body.preheader ], [ %index.next16, %vector.body ]
  %active.lane.mask = phi <vscale x 2 x i1> [ %active.lane.mask.entry7, %for.body.preheader ], [ %active.lane.mask.next21, %vector.body ]
  %active.lane.mask8 = phi <vscale x 2 x i1> [ %active.lane.mask.entry6, %for.body.preheader ], [ %active.lane.mask.next20, %vector.body ]
  %active.lane.mask9 = phi <vscale x 2 x i1> [ %active.lane.mask.entry5, %for.body.preheader ], [ %active.lane.mask.next19, %vector.body ]
  %active.lane.mask10 = phi <vscale x 2 x i1> [ %active.lane.mask.entry4, %for.body.preheader ], [ %active.lane.mask.next18, %vector.body ]
  %18 = getelementptr inbounds double, ptr %src, i64 %index
  %wide.masked.load = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr %18, i32 8, <vscale x 2 x i1> %active.lane.mask, <vscale x 2 x double> poison)
  %19 = getelementptr inbounds double, ptr %18, i64 %5
  %wide.masked.load11 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %19, i32 8, <vscale x 2 x i1> %active.lane.mask8, <vscale x 2 x double> poison)
  %20 = getelementptr inbounds double, ptr %18, i64 %7
  %wide.masked.load12 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %20, i32 8, <vscale x 2 x i1> %active.lane.mask9, <vscale x 2 x double> poison)
  %21 = getelementptr inbounds double, ptr %18, i64 %9
  %wide.masked.load13 = tail call <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nonnull %21, i32 8, <vscale x 2 x i1> %active.lane.mask10, <vscale x 2 x double> poison)
  %22 = fmul <vscale x 2 x double> %wide.masked.load, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %23 = fmul <vscale x 2 x double> %wide.masked.load11, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %24 = fmul <vscale x 2 x double> %wide.masked.load12, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %25 = fmul <vscale x 2 x double> %wide.masked.load13, shufflevector (<vscale x 2 x double> insertelement (<vscale x 2 x double> poison, double 3.000000e+00, i64 0), <vscale x 2 x double> poison, <vscale x 2 x i32> zeroinitializer)
  %26 = getelementptr inbounds double, ptr %dst, i64 %index
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %22, ptr %26, i32 8, <vscale x 2 x i1> %active.lane.mask)
  %27 = getelementptr inbounds double, ptr %26, i64 %11
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %23, ptr %27, i32 8, <vscale x 2 x i1> %active.lane.mask8)
  %28 = getelementptr inbounds double, ptr %26, i64 %13
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %24, ptr %28, i32 8, <vscale x 2 x i1> %active.lane.mask9)
  %29 = getelementptr inbounds double, ptr %26, i64 %15
  tail call void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double> %25, ptr %29, i32 8, <vscale x 2 x i1> %active.lane.mask10)
  %index.next = add i64 %index, %1
  %index.next16 = add i64 %index, %1
  %30 = add i64 %index.next, %17
  %active.lane.mask.next = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %30, i64 %wide.trip.count)
  %active.lane.mask.next17 = tail call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 %index.next, i64 %wide.trip.count)
  %active.lane.mask.next18 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 2)
  %active.lane.mask.next19 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next, i64 0)
  %active.lane.mask.next20 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 2)
  %active.lane.mask.next21 = tail call <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1> %active.lane.mask.next17, i64 0)
  %31 = extractelement <vscale x 2 x i1> %active.lane.mask.next21, i64 0
  br i1 %31, label %vector.body, label %for.cond.cleanup

for.cond.cleanup:
  ret void
}

declare <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64, i64)
declare <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1>, i64 immarg)
declare <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nocapture, i32 immarg, <vscale x 16 x i1>, <vscale x 16 x i8>)
declare <vscale x 2 x double> @llvm.masked.load.nxv2f64.p0(ptr nocapture, i32 immarg, <vscale x 2 x i1>, <vscale x 2 x double>)
declare <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64, i64)
declare <vscale x 2 x i1> @llvm.vector.extract.nxv2i1.nxv4i1(<vscale x 4 x i1>, i64 immarg)
declare <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64, i64)
declare <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64, i64)
declare i64 @llvm.vscale.i64()
declare void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8>, ptr nocapture, i32 immarg, <vscale x 16 x i1>)
declare void @llvm.masked.store.nxv2f64.p0(<vscale x 2 x double>, ptr nocapture, i32 immarg, <vscale x 2 x i1>)

attributes #0 = { nounwind vscale_range(1,16) "target-cpu"="neoverse-v1" }

