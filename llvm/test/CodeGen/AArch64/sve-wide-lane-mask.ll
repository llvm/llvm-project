; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mattr=+sve    < %s | FileCheck %s -check-prefix CHECK-SVE
; RUN: llc -mattr=+sve2p1 < %s | FileCheck %s -check-prefix CHECK-SVE2p1

target triple = "aarch64-unknown-linux"

define void @scalable_wide_active_lane_mask(ptr %dst, ptr %src, i64 %n) #0 {
; CHECK-SVE-LABEL: scalable_wide_active_lane_mask:
; CHECK-SVE:       // %bb.0: // %entry
; CHECK-SVE-NEXT:    cmp x2, #1
; CHECK-SVE-NEXT:    b.lt .LBB0_3
; CHECK-SVE-NEXT:  // %bb.1: // %vector.ph
; CHECK-SVE-NEXT:    rdvl x8, #2
; CHECK-SVE-NEXT:    rdvl x9, #1
; CHECK-SVE-NEXT:    mov x11, xzr
; CHECK-SVE-NEXT:    subs x10, x2, x8
; CHECK-SVE-NEXT:    csel x10, xzr, x10, lo
; CHECK-SVE-NEXT:    whilelo p1.b, xzr, x2
; CHECK-SVE-NEXT:    whilelo p0.b, x9, x2
; CHECK-SVE-NEXT:  .LBB0_2: // %vector.body
; CHECK-SVE-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE-NEXT:    add x12, x1, x11
; CHECK-SVE-NEXT:    ld1b { z0.b }, p1/z, [x1, x11]
; CHECK-SVE-NEXT:    add x13, x0, x11
; CHECK-SVE-NEXT:    ld1b { z1.b }, p0/z, [x12, #1, mul vl]
; CHECK-SVE-NEXT:    adds x12, x11, x9
; CHECK-SVE-NEXT:    csinv x12, x12, xzr, lo
; CHECK-SVE-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE-NEXT:    st1b { z0.b }, p1, [x0, x11]
; CHECK-SVE-NEXT:    st1b { z1.b }, p0, [x13, #1, mul vl]
; CHECK-SVE-NEXT:    whilelo p0.b, x12, x10
; CHECK-SVE-NEXT:    whilelo p1.b, x11, x10
; CHECK-SVE-NEXT:    add x11, x11, x8
; CHECK-SVE-NEXT:    b.mi .LBB0_2
; CHECK-SVE-NEXT:  .LBB0_3: // %for.end
; CHECK-SVE-NEXT:    ret
;
; CHECK-SVE2p1-LABEL: scalable_wide_active_lane_mask:
; CHECK-SVE2p1:       // %bb.0: // %entry
; CHECK-SVE2p1-NEXT:    cmp x2, #1
; CHECK-SVE2p1-NEXT:    b.lt .LBB0_3
; CHECK-SVE2p1-NEXT:  // %bb.1: // %vector.ph
; CHECK-SVE2p1-NEXT:    rdvl x9, #2
; CHECK-SVE2p1-NEXT:    mov x8, xzr
; CHECK-SVE2p1-NEXT:    subs x9, x2, x9
; CHECK-SVE2p1-NEXT:    csel x9, xzr, x9, lo
; CHECK-SVE2p1-NEXT:    whilelo { p0.b, p1.b }, xzr, x2
; CHECK-SVE2p1-NEXT:  .LBB0_2: // %vector.body
; CHECK-SVE2p1-NEXT:    // =>This Inner Loop Header: Depth=1
; CHECK-SVE2p1-NEXT:    add x10, x1, x8
; CHECK-SVE2p1-NEXT:    ld1b { z0.b }, p0/z, [x1, x8]
; CHECK-SVE2p1-NEXT:    ld1b { z1.b }, p1/z, [x10, #1, mul vl]
; CHECK-SVE2p1-NEXT:    add x10, x0, x8
; CHECK-SVE2p1-NEXT:    mul z0.b, z0.b, #3
; CHECK-SVE2p1-NEXT:    mul z1.b, z1.b, #3
; CHECK-SVE2p1-NEXT:    st1b { z0.b }, p0, [x0, x8]
; CHECK-SVE2p1-NEXT:    st1b { z1.b }, p1, [x10, #1, mul vl]
; CHECK-SVE2p1-NEXT:    whilelo { p0.b, p1.b }, x8, x9
; CHECK-SVE2p1-NEXT:    incb x8, all, mul #2
; CHECK-SVE2p1-NEXT:    mov z0.b, p0/z, #1 // =0x1
; CHECK-SVE2p1-NEXT:    fmov w10, s0
; CHECK-SVE2p1-NEXT:    tbnz w10, #0, .LBB0_2
; CHECK-SVE2p1-NEXT:  .LBB0_3: // %for.end
; CHECK-SVE2p1-NEXT:    ret
entry:
  %cmp = icmp sgt i64 %n, 0
  br i1 %cmp, label %vector.ph, label %for.end

vector.ph:
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 5
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 5
  %4 = tail call i64 @llvm.usub.sat.i64(i64 %n, i64 %3)
  %active.lane.mask.entry = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 0, i64 %n)
  %5 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 0)
  %6 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.entry, i64 16)
  %7 = tail call i64 @llvm.vscale.i64()
  %8 = shl nuw nsw i64 %7, 4
  %9 = tail call i64 @llvm.vscale.i64()
  %10 = shl nuw nsw i64 %9, 4
  br label %vector.body

vector.body:
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %active.lane.mask = phi <vscale x 16 x i1> [ %5, %vector.ph ], [ %17, %vector.body ]
  %active.lane.mask2 = phi <vscale x 16 x i1> [ %6, %vector.ph ], [ %18, %vector.body ]
  %11 = getelementptr inbounds nuw i8, ptr %src, i64 %index
  %12 = getelementptr inbounds nuw i8, ptr %11, i64 %8
  %wide.masked.load = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr %11, i32 1, <vscale x 16 x i1> %active.lane.mask, <vscale x 16 x i8> poison)
  %wide.masked.load3 = tail call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr nonnull %12, i32 1, <vscale x 16 x i1> %active.lane.mask2, <vscale x 16 x i8> poison)
  %13 = mul <vscale x 16 x i8> %wide.masked.load, splat (i8 3)
  %14 = mul <vscale x 16 x i8> %wide.masked.load3, splat (i8 3)
  %15 = getelementptr inbounds nuw i8, ptr %dst, i64 %index
  %16 = getelementptr inbounds nuw i8, ptr %15, i64 %10
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %13, ptr %15, i32 1, <vscale x 16 x i1> %active.lane.mask)
  tail call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> %14, ptr %16, i32 1, <vscale x 16 x i1> %active.lane.mask2)
  %index.next = add i64 %index, %1
  %active.lane.mask.next = tail call <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64 %index, i64 %4)
  %17 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 0)
  %18 = tail call <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1> %active.lane.mask.next, i64 16)
  %19 = extractelement <vscale x 16 x i1> %17, i64 0
  br i1 %19, label %vector.body, label %for.end, !llvm.loop !0

for.end:
  ret void
}

declare i64 @llvm.vscale.i64()
declare <vscale x 32 x i1> @llvm.get.active.lane.mask.nxv32i1.i64(i64, i64)
declare <vscale x 16 x i1> @llvm.vector.extract.nxv16i1.nxv32i1(<vscale x 32 x i1>, i64 immarg)
declare <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr captures(none), i32 immarg, <vscale x 16 x i1>, <vscale x 16 x i8>)
declare void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8>, ptr captures(none), i32 immarg, <vscale x 16 x i1>)
declare i64 @llvm.usub.sat.i64(i64, i64)

attributes #0 = {  vscale_range(1,16) }

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.isvectorized", i32 1}
!2 = !{!"llvm.loop.unroll.runtime.disable"}
