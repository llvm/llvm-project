; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -aarch64-sve-vector-bits-max=0   < %s | FileCheck %s --check-prefix=CHECK-VLA
; RUN: llc -aarch64-sve-vector-bits-max=128 < %s | FileCheck %s --check-prefix=CHECK-128

target triple = "aarch64-unknown-linux-gnu"

define <vscale x 16 x i8> @ld_nxv16i8(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv16i8:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv16i8:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 16 x i8>, ptr %0, align 16
  ret <vscale x 16 x i8> %2
}

define void @st_nxv16i8(ptr %0, <vscale x 16 x i8> %1) #0 {
; CHECK-VLA-LABEL: st_nxv16i8:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv16i8:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 16 x i8> %1, ptr %0, align 16
  ret void
}

define <vscale x 8 x i16> @ld_nxv8i16(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv8i16:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv8i16:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 8 x i16>, ptr %0, align 16
  ret <vscale x 8 x i16> %2
}

define void @st_nxv8i16(ptr %0, <vscale x 8 x i16> %1) #0 {
; CHECK-VLA-LABEL: st_nxv8i16:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv8i16:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 8 x i16> %1, ptr %0, align 16
  ret void
}

define <vscale x 4 x i32> @ld_nxv4i32(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv4i32:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv4i32:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 4 x i32>, ptr %0, align 16
  ret <vscale x 4 x i32> %2
}

define void @st_nxv4i32(ptr %0, <vscale x 4 x i32> %1) #0 {
; CHECK-VLA-LABEL: st_nxv4i32:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv4i32:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 4 x i32> %1, ptr %0, align 16
  ret void
}

define <vscale x 2 x i64> @ld_nxv2i64(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv2i64:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv2i64:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 2 x i64>, ptr %0, align 16
  ret <vscale x 2 x i64> %2
}

define void @st_nxv2i64(ptr %0, <vscale x 2 x i64> %1) #0 {
; CHECK-VLA-LABEL: st_nxv2i64:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv2i64:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 2 x i64> %1, ptr %0, align 16
  ret void
}

define <vscale x 8 x half> @ld_nxv8f16(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv8f16:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv8f16:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 8 x half>, ptr %0, align 16
  ret <vscale x 8 x half> %2
}

define void @st_nxv8f16(ptr %0, <vscale x 8 x half> %1) #0 {
; CHECK-VLA-LABEL: st_nxv8f16:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv8f16:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 8 x half> %1, ptr %0, align 16
  ret void
}

define <vscale x 4 x float> @ld_nxv4f32(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv4f32:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv4f32:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 4 x float>, ptr %0, align 16
  ret <vscale x 4 x float> %2
}

define void @st_nxv4f32(ptr %0, <vscale x 4 x float> %1) #0 {
; CHECK-VLA-LABEL: st_nxv4f32:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv4f32:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 4 x float> %1, ptr %0, align 16
  ret void
}

define <vscale x 2 x double> @ld_nxv2f64(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv2f64:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv2f64:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0]
; CHECK-128-NEXT:    ret
  %2 = load <vscale x 2 x double>, ptr %0, align 16
  ret <vscale x 2 x double> %2
}

define void @st_nxv2f64(ptr %0, <vscale x 2 x double> %1) #0 {
; CHECK-VLA-LABEL: st_nxv2f64:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv2f64:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0]
; CHECK-128-NEXT:    ret
  store <vscale x 2 x double> %1, ptr %0, align 16
  ret void
}

define <vscale x 16 x i8> @ld_nxv16i8_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv16i8_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv16i8_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 16 x i8>, ptr %4, align 16
  ret <vscale x 16 x i8> %5
}

define void @st_nxv16i8_offset(ptr %0, <vscale x 16 x i8> %1) #0 {
; CHECK-VLA-LABEL: st_nxv16i8_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv16i8_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 16 x i8> %1, ptr %5, align 16
  ret void
}

define <vscale x 8 x i16> @ld_nxv8i16_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv8i16_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv8i16_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 8 x i16>, ptr %4, align 16
  ret <vscale x 8 x i16> %5
}

define void @st_nxv8i16_offset(ptr %0, <vscale x 8 x i16> %1) #0 {
; CHECK-VLA-LABEL: st_nxv8i16_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv8i16_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 8 x i16> %1, ptr %5, align 16
  ret void
}

define <vscale x 4 x i32> @ld_nxv4i32_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv4i32_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv4i32_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 4 x i32>, ptr %4, align 16
  ret <vscale x 4 x i32> %5
}

define void @st_nxv4i32_offset(ptr %0, <vscale x 4 x i32> %1) #0 {
; CHECK-VLA-LABEL: st_nxv4i32_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv4i32_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 4 x i32> %1, ptr %5, align 16
  ret void
}

define <vscale x 2 x i64> @ld_nxv2i64_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv2i64_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv2i64_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 2 x i64>, ptr %4, align 16
  ret <vscale x 2 x i64> %5
}

define void @st_nxv2i64_offset(ptr %0, <vscale x 2 x i64> %1) #0 {
; CHECK-VLA-LABEL: st_nxv2i64_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv2i64_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 2 x i64> %1, ptr %5, align 16
  ret void
}

define <vscale x 8 x half> @ld_nxv8f16_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv8f16_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv8f16_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 8 x half>, ptr %4, align 16
  ret <vscale x 8 x half> %5
}

define void @st_nxv8f16_offset(ptr %0, <vscale x 8 x half> %1) #0 {
; CHECK-VLA-LABEL: st_nxv8f16_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv8f16_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 8 x half> %1, ptr %5, align 16
  ret void
}

define <vscale x 4 x float> @ld_nxv4f32_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv4f32_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv4f32_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 4 x float>, ptr %4, align 16
  ret <vscale x 4 x float> %5
}

define void @st_nxv4f32_offset(ptr %0, <vscale x 4 x float> %1) #0 {
; CHECK-VLA-LABEL: st_nxv4f32_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv4f32_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 4 x float> %1, ptr %5, align 16
  ret void
}

define <vscale x 2 x double> @ld_nxv2f64_offset(ptr %0) #0 {
; CHECK-VLA-LABEL: ld_nxv2f64_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: ld_nxv2f64_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    ldr z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %2 = tail call i64 @llvm.vscale.i64()
  %3 = shl nuw nsw i64 %2, 4
  %4 = getelementptr inbounds nuw i8, ptr %0, i64 %3
  %5 = load <vscale x 2 x double>, ptr %4, align 16
  ret <vscale x 2 x double> %5
}

define void @st_nxv2f64_offset(ptr %0, <vscale x 2 x double> %1) #0 {
; CHECK-VLA-LABEL: st_nxv2f64_offset:
; CHECK-VLA:       // %bb.0:
; CHECK-VLA-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-VLA-NEXT:    ret
;
; CHECK-128-LABEL: st_nxv2f64_offset:
; CHECK-128:       // %bb.0:
; CHECK-128-NEXT:    str z0, [x0, #1, mul vl]
; CHECK-128-NEXT:    ret
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 4
  %5 = getelementptr inbounds nuw i8, ptr %0, i64 %4
  store <vscale x 2 x double> %1, ptr %5, align 16
  ret void
}

attributes #0 = { "target-features"="+sve" }
