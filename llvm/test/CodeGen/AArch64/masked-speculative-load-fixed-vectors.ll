; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=aarch64-linux-gnu < %s | FileCheck %s --check-prefixes=NEON
; RUN: llc -mtriple=aarch64-linux-gnu -mattr=+sve < %s | FileCheck %s --check-prefixes=SVE

define { <4 x i32>, <4 x i1> } @speculative_load_v4i32(ptr %p, <4 x i1> %mask) {
; NEON-LABEL: speculative_load_v4i32:
; NEON:       // %bb.0:
; NEON-NEXT:    // kill: def $d0 killed $d0 def $q0
; NEON-NEXT:    umov w8, v0.h[0]
; NEON-NEXT:    tbz w8, #0, .LBB0_2
; NEON-NEXT:  // %bb.1: // %speculative.load.first.lane
; NEON-NEXT:    adrp x8, .LCPI0_0
; NEON-NEXT:    ldr s0, [x0]
; NEON-NEXT:    ldr d1, [x8, :lo12:.LCPI0_0]
; NEON-NEXT:    // kill: def $d1 killed $d1 killed $q1
; NEON-NEXT:    ret
; NEON-NEXT:  .LBB0_2:
; NEON-NEXT:    movi v1.2d, #0000000000000000
; NEON-NEXT:    // implicit-def: $q0
; NEON-NEXT:    // kill: def $d1 killed $d1 killed $q1
; NEON-NEXT:    ret
;
; SVE-LABEL: speculative_load_v4i32:
; SVE:       // %bb.0:
; SVE-NEXT:    // kill: def $d0 killed $d0 def $q0
; SVE-NEXT:    umov w8, v0.h[0]
; SVE-NEXT:    tbz w8, #0, .LBB0_2
; SVE-NEXT:  // %bb.1: // %speculative.load.first.lane
; SVE-NEXT:    adrp x8, .LCPI0_0
; SVE-NEXT:    ldr s0, [x0]
; SVE-NEXT:    ldr d1, [x8, :lo12:.LCPI0_0]
; SVE-NEXT:    // kill: def $d1 killed $d1 killed $q1
; SVE-NEXT:    ret
; SVE-NEXT:  .LBB0_2:
; SVE-NEXT:    movi v1.2d, #0000000000000000
; SVE-NEXT:    // implicit-def: $q0
; SVE-NEXT:    // kill: def $d1 killed $d1 killed $q1
; SVE-NEXT:    ret
  %res = call { <4 x i32>, <4 x i1> } @llvm.masked.speculative.load.v4i32.p0(ptr %p, i32 16, <4 x i1> %mask)
  ret { <4 x i32>, <4 x i1> } %res
}

;; FIXME: If we know the input mask is all-true and the vector is fully aligned,
;;        we should be able to use a normal NEON load here.
define { <2 x double>, <2 x i1> } @speculative_load_v2f64_all_true_fully_aligned(ptr %p) {
; NEON-LABEL: speculative_load_v2f64_all_true_fully_aligned:
; NEON:       // %bb.0: // %speculative.load.first.lane
; NEON-NEXT:    adrp x8, .LCPI1_0
; NEON-NEXT:    ldr d0, [x0]
; NEON-NEXT:    ldr d1, [x8, :lo12:.LCPI1_0]
; NEON-NEXT:    ret
;
; SVE-LABEL: speculative_load_v2f64_all_true_fully_aligned:
; SVE:       // %bb.0: // %speculative.load.first.lane
; SVE-NEXT:    ldr d0, [x0]
; SVE-NEXT:    index z1.s, #1, #-1
; SVE-NEXT:    // kill: def $d1 killed $d1 killed $z1
; SVE-NEXT:    ret
  %res = call { <2 x double>, <2 x i1> } @llvm.masked.speculative.load.v2f64.p0(ptr %p, i32 16, <2 x i1> <i1 true, i1 true>)
  ret { <2 x double>, <2 x i1> } %res
}

define { <2 x double>, <2 x i1> } @speculative_load_v2f64_all_true_partially_aligned(ptr %p) {
; NEON-LABEL: speculative_load_v2f64_all_true_partially_aligned:
; NEON:       // %bb.0: // %speculative.load.first.lane
; NEON-NEXT:    adrp x8, .LCPI2_0
; NEON-NEXT:    ldr d0, [x0]
; NEON-NEXT:    ldr d1, [x8, :lo12:.LCPI2_0]
; NEON-NEXT:    ret
;
; SVE-LABEL: speculative_load_v2f64_all_true_partially_aligned:
; SVE:       // %bb.0: // %speculative.load.first.lane
; SVE-NEXT:    ldr d0, [x0]
; SVE-NEXT:    index z1.s, #1, #-1
; SVE-NEXT:    // kill: def $d1 killed $d1 killed $z1
; SVE-NEXT:    ret
  %res = call { <2 x double>, <2 x i1> } @llvm.masked.speculative.load.v2f64.p0(ptr %p, i32 8, <2 x i1> <i1 true, i1 true>)
  ret { <2 x double>, <2 x i1> } %res
}
