; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s --mattr=+complxnum,+sve,+fullfp16 -o - | FileCheck %s

target triple = "aarch64"

define <vscale x 4 x double> @simple_symmetric_muladd2(<vscale x 4 x double> %a, <vscale x 4 x double> %b) {
; CHECK-LABEL: simple_symmetric_muladd2:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    adrp x8, .LCPI0_0
; CHECK-NEXT:    add x8, x8, :lo12:.LCPI0_0
; CHECK-NEXT:    ld1rd { z4.d }, p0/z, [x8]
; CHECK-NEXT:    fmad z0.d, p0/m, z4.d, z2.d
; CHECK-NEXT:    fmad z1.d, p0/m, z4.d, z3.d
; CHECK-NEXT:    ret
entry:
  %strided.vec = tail call { <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave2.nxv4f64(<vscale x 4 x double> %a)
  %ext00 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 0
  %ext01 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 1
  %fmul0 = fmul fast <vscale x 2 x double> %ext00, splat (double 3.200000e+00)
  %strided.vec44 = tail call { <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave2.nxv4f64(<vscale x 4 x double> %b)
  %ext10 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 0
  %ext11 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 1
  %fadd0 = fadd fast <vscale x 2 x double> %ext10, %fmul0
  %fmul1 = fmul fast <vscale x 2 x double> %ext01, splat (double 3.200000e+00)
  %fadd1 = fadd fast <vscale x 2 x double> %ext11, %fmul1
  %interleaved.vec = tail call <vscale x 4 x double> @llvm.vector.interleave2.nxv4f64(<vscale x 2 x double> %fadd0, <vscale x 2 x double> %fadd1)
  ret <vscale x 4 x double> %interleaved.vec
}

define <vscale x 4 x double> @simple_symmetric_unary2(<vscale x 4 x double> %a) {
; CHECK-LABEL: simple_symmetric_unary2:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    fneg z0.d, p0/m, z0.d
; CHECK-NEXT:    fneg z1.d, p0/m, z1.d
; CHECK-NEXT:    ret
entry:
  %strided.vec = tail call { <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave2.nxv4f64(<vscale x 4 x double> %a)
  %ext00 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 0
  %ext01 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 1
  %fneg0 = fneg fast <vscale x 2 x double> %ext00
  %fneg1 = fneg fast <vscale x 2 x double> %ext01
  %interleaved.vec = tail call <vscale x 4 x double> @llvm.vector.interleave2.nxv4f64(<vscale x 2 x double> %fneg0, <vscale x 2 x double> %fneg1)
  ret <vscale x 4 x double> %interleaved.vec
}

define <vscale x 8 x double> @simple_symmetric_muladd4(<vscale x 8 x double> %a, <vscale x 8 x double> %b) {
; CHECK-LABEL: simple_symmetric_muladd4:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    adrp x8, .LCPI2_0
; CHECK-NEXT:    add x8, x8, :lo12:.LCPI2_0
; CHECK-NEXT:    ld1rd { z24.d }, p0/z, [x8]
; CHECK-NEXT:    fmad z0.d, p0/m, z24.d, z4.d
; CHECK-NEXT:    fmad z1.d, p0/m, z24.d, z5.d
; CHECK-NEXT:    fmad z2.d, p0/m, z24.d, z6.d
; CHECK-NEXT:    fmad z3.d, p0/m, z24.d, z7.d
; CHECK-NEXT:    ret
entry:
  %strided.vec = tail call { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave4.nxv8f64(<vscale x 8 x double> %a)
  %ext00 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 0
  %ext01 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 1
  %ext02 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 2
  %ext03 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 3
  %fmul0 = fmul fast <vscale x 2 x double> %ext00, splat (double 3.200000e+00)
  %strided.vec44 = tail call { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave4.nxv8f64(<vscale x 8 x double> %b)
  %ext10 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 0
  %ext11 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 1
  %ext12 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 2
  %ext13 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec44, 3
  %fadd0 = fadd fast <vscale x 2 x double> %ext10, %fmul0
  %fmul1 = fmul fast <vscale x 2 x double> %ext01, splat (double 3.200000e+00)
  %fadd1 = fadd fast <vscale x 2 x double> %ext11, %fmul1
  %fmul2 = fmul fast <vscale x 2 x double> %ext02, splat (double 3.200000e+00)
  %fadd2 = fadd fast <vscale x 2 x double> %ext12, %fmul2
  %fmul3 = fmul fast <vscale x 2 x double> %ext03, splat (double 3.200000e+00)
  %fadd3 = fadd fast <vscale x 2 x double> %ext13, %fmul3
  %interleaved.vec = tail call <vscale x 8 x double> @llvm.vector.interleave4.nxv8f64(<vscale x 2 x double> %fadd0, <vscale x 2 x double> %fadd1, <vscale x 2 x double> %fadd2, <vscale x 2 x double> %fadd3)
  ret <vscale x 8 x double> %interleaved.vec
}


define <vscale x 8 x double> @simple_symmetric_unary4(<vscale x 8 x double> %a) {
; CHECK-LABEL: simple_symmetric_unary4:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    fneg z0.d, p0/m, z0.d
; CHECK-NEXT:    fneg z1.d, p0/m, z1.d
; CHECK-NEXT:    fneg z2.d, p0/m, z2.d
; CHECK-NEXT:    fneg z3.d, p0/m, z3.d
; CHECK-NEXT:    ret
entry:
  %strided.vec = tail call { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } @llvm.vector.deinterleave4.nxv8f64(<vscale x 8 x double> %a)
  %ext00 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 0
  %ext01 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 1
  %ext02 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 2
  %ext03 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %strided.vec, 3
  %fneg0 = fneg fast <vscale x 2 x double> %ext00
  %fneg1 = fneg fast <vscale x 2 x double> %ext01
  %fneg2 = fneg fast <vscale x 2 x double> %ext02
  %fneg3 = fneg fast <vscale x 2 x double> %ext03
  %interleaved.vec = tail call <vscale x 8 x double> @llvm.vector.interleave4.nxv8f64(<vscale x 2 x double> %fneg0, <vscale x 2 x double> %fneg1, <vscale x 2 x double> %fneg2, <vscale x 2 x double> %fneg3)
  ret <vscale x 8 x double> %interleaved.vec
}
