; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
; RUN: llc %s -mtriple=aarch64 -mattr=+v8.3a,+fullfp16 -o - | FileCheck %s --check-prefixes=CHECK-LE,CHECK-LE-SD
; RUN: llc %s -mtriple=aarch64 -mattr=+v8.3a,+fullfp16 -global-isel -o - | FileCheck %s --check-prefixes=CHECK-LE,CHECK-LE-GI
; RUN: llc %s -mtriple=aarch64_be -mattr=+v8.3a,+fullfp16 -o - | FileCheck %s --check-prefix=CHECK-BE

define <4 x half> @test_16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_16x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #0
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot0.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c)
  ret <4 x half> %res
}

define <4 x half> @test_16x4_lane_1(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_16x4_lane_1:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    // kill: def $d2 killed $d2 def $q2
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.h[1], #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_16x4_lane_1:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.h[1], #0
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <4 x half> %c to <2 x i32>
  %c.dup = shufflevector <2 x i32> %c.cast , <2 x i32> undef, <2 x i32> <i32 1, i32 1>
  %c.res = bitcast <2 x i32> %c.dup to <4 x half>
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot0.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c.res)
  ret <4 x half> %res
}

define <4 x half> @test_rot90_16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_rot90_16x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #90
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot90.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c)
  ret <4 x half> %res
}

define <4 x half> @test_rot90_16x4_lane_0(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_rot90_16x4_lane_0:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    // kill: def $d2 killed $d2 def $q2
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_16x4_lane_0:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #90
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <4 x half> %c to <2 x i32>
  %c.dup = shufflevector <2 x i32> %c.cast , <2 x i32> undef, <2 x i32> <i32 0, i32 0>
  %c.res = bitcast <2 x i32> %c.dup to <4 x half>
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot90.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c.res)
  ret <4 x half> %res
}

define <4 x half> @test_rot180_16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_rot180_16x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #180
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot180.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c)
  ret <4 x half> %res
}

define <4 x half> @test_rot180_16x4_lane_0(<4 x half> %a, <4 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot180_16x4_lane_0:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_16x4_lane_0:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #180
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:

  %c.cast = bitcast <8 x half> %c to <4 x i32>
  %c.dup = shufflevector <4 x i32> %c.cast , <4 x i32> undef, <2 x i32> <i32 0, i32 0>
  %c.res = bitcast <2 x i32> %c.dup to <4 x half>
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot180.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c.res)
  ret <4 x half> %res
}

define <4 x half> @test_rot270_16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-LABEL: test_rot270_16x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #270
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot270.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c)
  ret <4 x half> %res
}

define <2 x float> @test_32x2(<2 x float> %a, <2 x float> %b, <2 x float> %c) {
; CHECK-LE-LABEL: test_32x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_32x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    rev64 v1.2s, v1.2s
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #0
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x float> @llvm.aarch64.neon.vcmla.rot0.v2f32(<2 x float> %a, <2 x float> %b, <2 x float> %c)
  ret <2 x float> %res
}

define <2 x float> @test_rot90_32x2(<2 x float> %a, <2 x float> %b, <2 x float> %c) {
; CHECK-LE-LABEL: test_rot90_32x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_32x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    rev64 v1.2s, v1.2s
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #90
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x float> @llvm.aarch64.neon.vcmla.rot90.v2f32(<2 x float> %a, <2 x float> %b, <2 x float> %c)
  ret <2 x float> %res
}

define <2 x float> @test_rot180_32x2(<2 x float> %a, <2 x float> %b, <2 x float> %c) {
; CHECK-LE-LABEL: test_rot180_32x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_32x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    rev64 v1.2s, v1.2s
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #180
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x float> @llvm.aarch64.neon.vcmla.rot180.v2f32(<2 x float> %a, <2 x float> %b, <2 x float> %c)
  ret <2 x float> %res
}

define <2 x float> @test_rot270_32x2(<2 x float> %a, <2 x float> %b, <2 x float> %c) {
; CHECK-LE-LABEL: test_rot270_32x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_32x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.2s, v2.2s
; CHECK-BE-NEXT:    rev64 v1.2s, v1.2s
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    fcmla v0.2s, v1.2s, v2.2s, #270
; CHECK-BE-NEXT:    rev64 v0.2s, v0.2s
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x float> @llvm.aarch64.neon.vcmla.rot270.v2f32(<2 x float> %a, <2 x float> %b, <2 x float> %c)
  ret <2 x float> %res
}

define <8 x half> @test_16x8(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_16x8:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_16x8:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.8h, v2.8h
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #0
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot0.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  ret <8 x half> %res
}

define <8 x half> @test_16x8_lane_0(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_16x8_lane_0:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.h[0], #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_16x8_lane_0:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.h[0], #0
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <8 x half> %c to <4 x i32>
  %c.dup = shufflevector <4 x i32> %c.cast , <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 0, i32 0>
  %c.res = bitcast <4 x i32> %c.dup to <8 x half>
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot0.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c.res)
  ret <8 x half> %res
}

define <8 x half> @test_rot90_16x8(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot90_16x8:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_16x8:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.8h, v2.8h
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #90
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot90.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  ret <8 x half> %res
}

define <8 x half> @test_rot90_16x8_lane_1(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot90_16x8_lane_1:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.h[1], #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_16x8_lane_1:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.h[1], #90
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <8 x half> %c to <4 x i32>
  %c.dup = shufflevector <4 x i32> %c.cast , <4 x i32> undef, <4 x i32> <i32 1, i32 1, i32 1, i32 1>
  %c.res = bitcast <4 x i32> %c.dup to <8 x half>
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot90.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c.res)
  ret <8 x half> %res
}

define <8 x half> @test_rot180_16x8(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot180_16x8:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_16x8:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.8h, v2.8h
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #180
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot180.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  ret <8 x half> %res
}

define <8 x half> @test_rot180_16x8_lane_1(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot180_16x8_lane_1:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.h[1], #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_16x8_lane_1:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.h[1], #180
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <8 x half> %c to <4 x i32>
  %c.dup = shufflevector <4 x i32> %c.cast , <4 x i32> undef, <4 x i32> <i32 1, i32 1, i32 1, i32 1>
  %c.res = bitcast <4 x i32> %c.dup to <8 x half>
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot180.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c.res)
  ret <8 x half> %res
}

define <8 x half> @test_rot270_16x8(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot270_16x8:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_16x8:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.8h, v2.8h
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.8h, #270
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot270.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c)
  ret <8 x half> %res
}

define <8 x half> @test_rot270_16x8_lane_0(<8 x half> %a, <8 x half> %b, <8 x half> %c) {
; CHECK-LE-LABEL: test_rot270_16x8_lane_0:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.8h, v1.8h, v2.h[0], #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_16x8_lane_0:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.8h, v1.8h
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.8h, v1.8h, v2.h[0], #270
; CHECK-BE-NEXT:    rev64 v0.8h, v0.8h
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <8 x half> %c to <4 x i32>
  %c.dup = shufflevector <4 x i32> %c.cast , <4 x i32> undef, <4 x i32> <i32 0, i32 0, i32 0, i32 0>
  %c.res = bitcast <4 x i32> %c.dup to <8 x half>
  %res = tail call <8 x half> @llvm.aarch64.neon.vcmla.rot270.v8f16(<8 x half> %a, <8 x half> %b, <8 x half> %c.res)
  ret <8 x half> %res
}

define <4 x float> @test_32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: test_32x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #0
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot0.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  ret <4 x float> %res
}

define <4 x float> @test_32x4_lane_0(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: test_32x4_lane_0:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4s, v1.4s, v2.s[0], #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_32x4_lane_0:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.s[0], #0
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %c.cast = bitcast <4 x float> %c to <2 x i64>
  %c.dup = shufflevector <2 x i64> %c.cast , <2 x i64> undef, <2 x i32> <i32 0, i32 0>
  %c.res = bitcast <2 x i64> %c.dup to <4 x float>
  %res = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot0.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c.res)
  ret <4 x float> %res
}

define <4 x float> @test_rot90_32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: test_rot90_32x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #90
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot90.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  ret <4 x float> %res
}

define <4 x float> @test_rot180_32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: test_rot180_32x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #180
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot180.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  ret <4 x float> %res
}

define <4 x float> @test_rot270_32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: test_rot270_32x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #270
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot270.v4f32(<4 x float> %a, <4 x float> %b, <4 x float> %c)
  ret <4 x float> %res
}

define <2 x double> @test_64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c) {
; CHECK-LE-LABEL: test_64x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #0
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #0
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot0.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  ret <2 x double> %res
}

define <2 x double> @test_rot90_64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c) {
; CHECK-LE-LABEL: test_rot90_64x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #90
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot90_64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #90
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot90.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  ret <2 x double> %res
}

define <2 x double> @test_rot180_64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c) {
; CHECK-LE-LABEL: test_rot180_64x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #180
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot180_64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #180
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot180.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  ret <2 x double> %res
}

define <2 x double> @test_rot270_64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c) {
; CHECK-LE-LABEL: test_rot270_64x2:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: test_rot270_64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %res = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot270.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  ret <2 x double> %res
}

define <4 x float> @reassoc_f32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-SD-LABEL: reassoc_f32x4:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #0
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_f32x4:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    movi v3.2d, #0000000000000000
; CHECK-LE-GI-NEXT:    fcmla v3.4s, v1.4s, v2.4s, #0
; CHECK-LE-GI-NEXT:    fadd v0.4s, v3.4s, v0.4s
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_f32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #0
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot0.v4f32(<4 x float> zeroinitializer, <4 x float> %b, <4 x float> %c)
  %res = fadd fast <4 x float> %d, %a
  ret <4 x float> %res
}

define <4 x float> @reassoc_c_f32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-SD-LABEL: reassoc_c_f32x4:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #90
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_c_f32x4:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    movi v3.2d, #0000000000000000
; CHECK-LE-GI-NEXT:    fcmla v3.4s, v1.4s, v2.4s, #90
; CHECK-LE-GI-NEXT:    fadd v0.4s, v0.4s, v3.4s
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_c_f32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4s, v1.4s, v2.4s, #90
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot90.v4f32(<4 x float> zeroinitializer, <4 x float> %b, <4 x float> %c)
  %res = fadd fast <4 x float> %a, %d
  ret <4 x float> %res
}

define <4 x half> @reassoc_f16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-SD-LABEL: reassoc_f16x4:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #180
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_f16x4:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    movi v3.2d, #0000000000000000
; CHECK-LE-GI-NEXT:    fcmla v3.4h, v1.4h, v2.4h, #180
; CHECK-LE-GI-NEXT:    fadd v0.4h, v3.4h, v0.4h
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_f16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #180
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot180.v4f16(<4 x half> zeroinitializer, <4 x half> %b, <4 x half> %c)
  %res = fadd fast <4 x half> %d, %a
  ret <4 x half> %res
}

define <4 x half> @reassoc_c_f16x4(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
; CHECK-LE-SD-LABEL: reassoc_c_f16x4:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #270
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_c_f16x4:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    movi v3.2d, #0000000000000000
; CHECK-LE-GI-NEXT:    fcmla v3.4h, v1.4h, v2.4h, #270
; CHECK-LE-GI-NEXT:    fadd v0.4h, v0.4h, v3.4h
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_c_f16x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4h, v2.4h
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.4h, #270
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <4 x half> @llvm.aarch64.neon.vcmla.rot270.v4f16(<4 x half> zeroinitializer, <4 x half> %b, <4 x half> %c)
  %res = fadd fast <4 x half> %a, %d
  ret <4 x half> %res
}

define <2 x double> @reassoc_f64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c, <2 x double> %g) {
; CHECK-LE-SD-LABEL: reassoc_f64x2:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-LE-SD-NEXT:    fcmla v0.2d, v2.2d, v3.2d, #270
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_f64x2:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    movi v4.2d, #0000000000000000
; CHECK-LE-GI-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-LE-GI-NEXT:    fcmla v4.2d, v2.2d, v3.2d, #270
; CHECK-LE-GI-NEXT:    fadd v0.2d, v4.2d, v0.2d
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_f64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v3.16b, v3.16b, v3.16b, #8
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-BE-NEXT:    fcmla v0.2d, v2.2d, v3.2d, #270
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot270.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  %e = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot270.v2f64(<2 x double> zeroinitializer, <2 x double> %c, <2 x double> %g)
  %res = fadd fast <2 x double> %e, %d
  ret <2 x double> %res
}

define <2 x double> @reassoc_c_f64x2(<2 x double> %a, <2 x double> %b, <2 x double> %c, <2 x double> %g) {
; CHECK-LE-SD-LABEL: reassoc_c_f64x2:
; CHECK-LE-SD:       // %bb.0: // %entry
; CHECK-LE-SD-NEXT:    fadd v0.2d, v0.2d, v0.2d
; CHECK-LE-SD-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-LE-SD-NEXT:    fcmla v0.2d, v2.2d, v3.2d, #270
; CHECK-LE-SD-NEXT:    ret
;
; CHECK-LE-GI-LABEL: reassoc_c_f64x2:
; CHECK-LE-GI:       // %bb.0: // %entry
; CHECK-LE-GI-NEXT:    mov v4.16b, v0.16b
; CHECK-LE-GI-NEXT:    fcmla v0.2d, v2.2d, v3.2d, #270
; CHECK-LE-GI-NEXT:    fcmla v4.2d, v1.2d, v2.2d, #270
; CHECK-LE-GI-NEXT:    fadd v0.2d, v0.2d, v4.2d
; CHECK-LE-GI-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_c_f64x2:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v3.16b, v3.16b, v3.16b, #8
; CHECK-BE-NEXT:    fadd v0.2d, v0.2d, v0.2d
; CHECK-BE-NEXT:    fcmla v0.2d, v1.2d, v2.2d, #270
; CHECK-BE-NEXT:    fcmla v0.2d, v2.2d, v3.2d, #270
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot270.v2f64(<2 x double> %a, <2 x double> %b, <2 x double> %c)
  %e = tail call <2 x double> @llvm.aarch64.neon.vcmla.rot270.v2f64(<2 x double> %a, <2 x double> %c, <2 x double> %g)
  %res = fadd fast <2 x double> %e, %d
  ret <2 x double> %res
}

define <4 x float> @reassoc_nonfast_f32x4(<4 x float> %a, <4 x float> %b, <4 x float> %c) {
; CHECK-LE-LABEL: reassoc_nonfast_f32x4:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    movi v3.2d, #0000000000000000
; CHECK-LE-NEXT:    fcmla v3.4s, v1.4s, v2.4s, #0
; CHECK-LE-NEXT:    fadd v0.4s, v3.4s, v0.4s
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: reassoc_nonfast_f32x4:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4s, v1.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    movi v3.2d, #0000000000000000
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    ext v1.16b, v1.16b, v1.16b, #8
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    fcmla v3.4s, v1.4s, v2.4s, #0
; CHECK-BE-NEXT:    fadd v0.4s, v3.4s, v0.4s
; CHECK-BE-NEXT:    rev64 v0.4s, v0.4s
; CHECK-BE-NEXT:    ext v0.16b, v0.16b, v0.16b, #8
; CHECK-BE-NEXT:    ret
entry:
  %d = tail call <4 x float> @llvm.aarch64.neon.vcmla.rot0.v4f32(<4 x float> zeroinitializer, <4 x float> %b, <4 x float> %c)
  %res = fadd <4 x float> %d, %a
  ret <4 x float> %res
}

define <4 x half> @be_vcmla_lane_f16(<4 x half> %0, <4 x half> %1, <4 x i32> %2) {
; CHECK-LE-LABEL: be_vcmla_lane_f16:
; CHECK-LE:       // %bb.0: // %entry
; CHECK-LE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #0
; CHECK-LE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-LE-NEXT:    ret
;
; CHECK-BE-LABEL: be_vcmla_lane_f16:
; CHECK-BE:       // %bb.0: // %entry
; CHECK-BE-NEXT:    rev64 v2.4s, v2.4s
; CHECK-BE-NEXT:    rev64 v1.4h, v1.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ext v2.16b, v2.16b, v2.16b, #8
; CHECK-BE-NEXT:    fcmla v0.4h, v1.4h, v2.h[0], #0
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    rev64 v0.4h, v0.4h
; CHECK-BE-NEXT:    ret
entry:
  %vecinit21 = shufflevector <4 x i32> %2, <4 x i32> poison, <2 x i32> zeroinitializer
  %3 = bitcast <2 x i32> %vecinit21 to <4 x half>
  %vcmla_f163.i = tail call noundef <4 x half> @llvm.aarch64.neon.vcmla.rot0.v4f16(<4 x half> %0, <4 x half> %1, <4 x half> %3)
  %shuffle22 = shufflevector <4 x half> %vcmla_f163.i, <4 x half> poison, <4 x i32> <i32 3, i32 2, i32 1, i32 0>
  ret <4 x half> %shuffle22
}
