; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=aarch64-- | FileCheck %s --check-prefixes=ALL

define void @lshr_4bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: lshr_4bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr w8, [x1]
; ALL-NEXT:    ldr w9, [x0]
; ALL-NEXT:    lsl w8, w8, #3
; ALL-NEXT:    lsr w8, w9, w8
; ALL-NEXT:    str w8, [x2]
; ALL-NEXT:    ret
  %src = load i32, ptr %src.ptr, align 1
  %byteOff = load i32, ptr %byteOff.ptr, align 1
  %bitOff = shl i32 %byteOff, 3
  %res = lshr i32 %src, %bitOff
  store i32 %res, ptr %dst, align 1
  ret void
}
define void @shl_4bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: shl_4bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr w8, [x1]
; ALL-NEXT:    ldr w9, [x0]
; ALL-NEXT:    lsl w8, w8, #3
; ALL-NEXT:    lsl w8, w9, w8
; ALL-NEXT:    str w8, [x2]
; ALL-NEXT:    ret
  %src = load i32, ptr %src.ptr, align 1
  %byteOff = load i32, ptr %byteOff.ptr, align 1
  %bitOff = shl i32 %byteOff, 3
  %res = shl i32 %src, %bitOff
  store i32 %res, ptr %dst, align 1
  ret void
}
define void @ashr_4bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: ashr_4bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr w8, [x1]
; ALL-NEXT:    ldr w9, [x0]
; ALL-NEXT:    lsl w8, w8, #3
; ALL-NEXT:    asr w8, w9, w8
; ALL-NEXT:    str w8, [x2]
; ALL-NEXT:    ret
  %src = load i32, ptr %src.ptr, align 1
  %byteOff = load i32, ptr %byteOff.ptr, align 1
  %bitOff = shl i32 %byteOff, 3
  %res = ashr i32 %src, %bitOff
  store i32 %res, ptr %dst, align 1
  ret void
}

define void @lshr_8bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: lshr_8bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldr x9, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    lsr x8, x9, x8
; ALL-NEXT:    str x8, [x2]
; ALL-NEXT:    ret
  %src = load i64, ptr %src.ptr, align 1
  %byteOff = load i64, ptr %byteOff.ptr, align 1
  %bitOff = shl i64 %byteOff, 3
  %res = lshr i64 %src, %bitOff
  store i64 %res, ptr %dst, align 1
  ret void
}
define void @shl_8bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: shl_8bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldr x9, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    lsl x8, x9, x8
; ALL-NEXT:    str x8, [x2]
; ALL-NEXT:    ret
  %src = load i64, ptr %src.ptr, align 1
  %byteOff = load i64, ptr %byteOff.ptr, align 1
  %bitOff = shl i64 %byteOff, 3
  %res = shl i64 %src, %bitOff
  store i64 %res, ptr %dst, align 1
  ret void
}
define void @ashr_8bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: ashr_8bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldr x9, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    asr x8, x9, x8
; ALL-NEXT:    str x8, [x2]
; ALL-NEXT:    ret
  %src = load i64, ptr %src.ptr, align 1
  %byteOff = load i64, ptr %byteOff.ptr, align 1
  %bitOff = shl i64 %byteOff, 3
  %res = ashr i64 %src, %bitOff
  store i64 %res, ptr %dst, align 1
  ret void
}

define void @lshr_16bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: lshr_16bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldp x10, x9, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    and x11, x8, #0x38
; ALL-NEXT:    mvn w12, w8
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsl x13, x9, #1
; ALL-NEXT:    lsr x10, x10, x11
; ALL-NEXT:    lsl x12, x13, x12
; ALL-NEXT:    lsr x9, x9, x11
; ALL-NEXT:    orr x8, x12, x10
; ALL-NEXT:    csel x10, xzr, x9, ne
; ALL-NEXT:    csel x8, x9, x8, ne
; ALL-NEXT:    stp x8, x10, [x2]
; ALL-NEXT:    ret
  %src = load i128, ptr %src.ptr, align 1
  %byteOff = load i128, ptr %byteOff.ptr, align 1
  %bitOff = shl i128 %byteOff, 3
  %res = lshr i128 %src, %bitOff
  store i128 %res, ptr %dst, align 1
  ret void
}
define void @shl_16bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: shl_16bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldp x9, x10, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    and x11, x8, #0x38
; ALL-NEXT:    mvn w12, w8
; ALL-NEXT:    lsr x13, x9, #1
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsl x10, x10, x11
; ALL-NEXT:    lsr x12, x13, x12
; ALL-NEXT:    lsl x9, x9, x11
; ALL-NEXT:    orr x8, x10, x12
; ALL-NEXT:    csel x10, xzr, x9, ne
; ALL-NEXT:    csel x8, x9, x8, ne
; ALL-NEXT:    stp x10, x8, [x2]
; ALL-NEXT:    ret
  %src = load i128, ptr %src.ptr, align 1
  %byteOff = load i128, ptr %byteOff.ptr, align 1
  %bitOff = shl i128 %byteOff, 3
  %res = shl i128 %src, %bitOff
  store i128 %res, ptr %dst, align 1
  ret void
}
define void @ashr_16bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: ashr_16bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x8, [x1]
; ALL-NEXT:    ldp x10, x9, [x0]
; ALL-NEXT:    lsl x8, x8, #3
; ALL-NEXT:    and x11, x8, #0x38
; ALL-NEXT:    mvn w12, w8
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsl x13, x9, #1
; ALL-NEXT:    asr x8, x9, #63
; ALL-NEXT:    lsr x10, x10, x11
; ALL-NEXT:    lsl x12, x13, x12
; ALL-NEXT:    asr x11, x9, x11
; ALL-NEXT:    orr x9, x12, x10
; ALL-NEXT:    csel x8, x8, x11, ne
; ALL-NEXT:    csel x9, x11, x9, ne
; ALL-NEXT:    stp x9, x8, [x2]
; ALL-NEXT:    ret
  %src = load i128, ptr %src.ptr, align 1
  %byteOff = load i128, ptr %byteOff.ptr, align 1
  %bitOff = shl i128 %byteOff, 3
  %res = ashr i128 %src, %bitOff
  store i128 %res, ptr %dst, align 1
  ret void
}

define void @lshr_32bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: lshr_32bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x9, [x1]
; ALL-NEXT:    mov w8, #128
; ALL-NEXT:    ldp x11, x10, [x0, #8]
; ALL-NEXT:    lsl x9, x9, #3
; ALL-NEXT:    ldr x12, [x0]
; ALL-NEXT:    sub x8, x8, x9
; ALL-NEXT:    ldr x13, [x0, #24]
; ALL-NEXT:    and x17, x8, #0x38
; ALL-NEXT:    mvn w0, w8
; ALL-NEXT:    lsr x14, x10, #1
; ALL-NEXT:    and x15, x9, #0x38
; ALL-NEXT:    mvn w16, w9
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsl x3, x13, x17
; ALL-NEXT:    lsr x14, x14, x0
; ALL-NEXT:    lsl x17, x10, x17
; ALL-NEXT:    orr x14, x3, x14
; ALL-NEXT:    lsl x18, x13, #1
; ALL-NEXT:    csel x0, xzr, x17, ne
; ALL-NEXT:    csel x14, x17, x14, ne
; ALL-NEXT:    lsl x17, x11, #1
; ALL-NEXT:    lsr x8, x10, x15
; ALL-NEXT:    lsl x1, x18, x16
; ALL-NEXT:    lsr x3, x12, x15
; ALL-NEXT:    lsl x16, x17, x16
; ALL-NEXT:    orr x8, x1, x8
; ALL-NEXT:    lsr x1, x13, x15
; ALL-NEXT:    tst x9, #0x40
; ALL-NEXT:    orr x16, x16, x3
; ALL-NEXT:    lsr x15, x11, x15
; ALL-NEXT:    csel x8, x1, x8, ne
; ALL-NEXT:    csel x16, x15, x16, ne
; ALL-NEXT:    csel x15, xzr, x15, ne
; ALL-NEXT:    csel x17, xzr, x1, ne
; ALL-NEXT:    subs x1, x9, #128
; ALL-NEXT:    and x3, x1, #0x38
; ALL-NEXT:    mvn w4, w1
; ALL-NEXT:    csel x17, x17, xzr, lo
; ALL-NEXT:    tst x1, #0x40
; ALL-NEXT:    orr x16, x16, x0
; ALL-NEXT:    orr x14, x15, x14
; ALL-NEXT:    lsr x10, x10, x3
; ALL-NEXT:    lsl x18, x18, x4
; ALL-NEXT:    orr x10, x18, x10
; ALL-NEXT:    lsr x13, x13, x3
; ALL-NEXT:    csel x10, x13, x10, ne
; ALL-NEXT:    csel x13, xzr, x13, ne
; ALL-NEXT:    cmp x9, #128
; ALL-NEXT:    csel x10, x16, x10, lo
; ALL-NEXT:    csel x8, x8, xzr, lo
; ALL-NEXT:    csel x13, x14, x13, lo
; ALL-NEXT:    cmp x9, #0
; ALL-NEXT:    csel x9, x12, x10, eq
; ALL-NEXT:    csel x10, x11, x13, eq
; ALL-NEXT:    stp x8, x17, [x2, #16]
; ALL-NEXT:    stp x9, x10, [x2]
; ALL-NEXT:    ret
  %src = load i256, ptr %src.ptr, align 1
  %byteOff = load i256, ptr %byteOff.ptr, align 1
  %bitOff = shl i256 %byteOff, 3
  %res = lshr i256 %src, %bitOff
  store i256 %res, ptr %dst, align 1
  ret void
}
define void @shl_32bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: shl_32bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x9, [x1]
; ALL-NEXT:    mov w8, #128
; ALL-NEXT:    ldp x10, x11, [x0, #8]
; ALL-NEXT:    lsl x9, x9, #3
; ALL-NEXT:    ldr x12, [x0, #24]
; ALL-NEXT:    sub x8, x8, x9
; ALL-NEXT:    ldr x13, [x0]
; ALL-NEXT:    and x17, x8, #0x38
; ALL-NEXT:    mvn w0, w8
; ALL-NEXT:    lsl x14, x10, #1
; ALL-NEXT:    and x15, x9, #0x38
; ALL-NEXT:    mvn w16, w9
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsr x3, x13, x17
; ALL-NEXT:    lsl x14, x14, x0
; ALL-NEXT:    lsr x17, x10, x17
; ALL-NEXT:    orr x14, x14, x3
; ALL-NEXT:    lsr x18, x13, #1
; ALL-NEXT:    csel x0, xzr, x17, ne
; ALL-NEXT:    csel x14, x17, x14, ne
; ALL-NEXT:    lsr x17, x11, #1
; ALL-NEXT:    lsl x8, x10, x15
; ALL-NEXT:    lsr x1, x18, x16
; ALL-NEXT:    lsl x3, x12, x15
; ALL-NEXT:    lsr x16, x17, x16
; ALL-NEXT:    orr x8, x8, x1
; ALL-NEXT:    lsl x1, x13, x15
; ALL-NEXT:    tst x9, #0x40
; ALL-NEXT:    orr x16, x3, x16
; ALL-NEXT:    lsl x15, x11, x15
; ALL-NEXT:    csel x8, x1, x8, ne
; ALL-NEXT:    csel x16, x15, x16, ne
; ALL-NEXT:    csel x15, xzr, x15, ne
; ALL-NEXT:    csel x17, xzr, x1, ne
; ALL-NEXT:    subs x1, x9, #128
; ALL-NEXT:    and x3, x1, #0x38
; ALL-NEXT:    mvn w4, w1
; ALL-NEXT:    csel x17, x17, xzr, lo
; ALL-NEXT:    tst x1, #0x40
; ALL-NEXT:    orr x16, x16, x0
; ALL-NEXT:    orr x14, x15, x14
; ALL-NEXT:    lsl x10, x10, x3
; ALL-NEXT:    lsr x18, x18, x4
; ALL-NEXT:    orr x10, x10, x18
; ALL-NEXT:    lsl x13, x13, x3
; ALL-NEXT:    csel x10, x13, x10, ne
; ALL-NEXT:    csel x13, xzr, x13, ne
; ALL-NEXT:    cmp x9, #128
; ALL-NEXT:    csel x10, x16, x10, lo
; ALL-NEXT:    csel x8, x8, xzr, lo
; ALL-NEXT:    csel x13, x14, x13, lo
; ALL-NEXT:    cmp x9, #0
; ALL-NEXT:    csel x9, x12, x10, eq
; ALL-NEXT:    csel x10, x11, x13, eq
; ALL-NEXT:    stp x17, x8, [x2]
; ALL-NEXT:    stp x10, x9, [x2, #16]
; ALL-NEXT:    ret
  %src = load i256, ptr %src.ptr, align 1
  %byteOff = load i256, ptr %byteOff.ptr, align 1
  %bitOff = shl i256 %byteOff, 3
  %res = shl i256 %src, %bitOff
  store i256 %res, ptr %dst, align 1
  ret void
}
define void @ashr_32bytes(ptr %src.ptr, ptr %byteOff.ptr, ptr %dst) nounwind {
; ALL-LABEL: ashr_32bytes:
; ALL:       // %bb.0:
; ALL-NEXT:    ldr x9, [x1]
; ALL-NEXT:    mov w8, #128
; ALL-NEXT:    ldp x11, x10, [x0, #8]
; ALL-NEXT:    lsl x9, x9, #3
; ALL-NEXT:    ldr x12, [x0]
; ALL-NEXT:    sub x8, x8, x9
; ALL-NEXT:    ldr x13, [x0, #24]
; ALL-NEXT:    and x18, x8, #0x38
; ALL-NEXT:    mvn w0, w8
; ALL-NEXT:    lsr x14, x10, #1
; ALL-NEXT:    and x15, x9, #0x38
; ALL-NEXT:    mvn w16, w9
; ALL-NEXT:    lsl x17, x13, #1
; ALL-NEXT:    lsl x4, x13, x18
; ALL-NEXT:    lsr x14, x14, x0
; ALL-NEXT:    tst x8, #0x40
; ALL-NEXT:    lsl x18, x10, x18
; ALL-NEXT:    orr x14, x4, x14
; ALL-NEXT:    lsr x8, x10, x15
; ALL-NEXT:    lsl x1, x17, x16
; ALL-NEXT:    csel x0, xzr, x18, ne
; ALL-NEXT:    csel x14, x18, x14, ne
; ALL-NEXT:    lsl x18, x11, #1
; ALL-NEXT:    orr x8, x1, x8
; ALL-NEXT:    lsr x1, x12, x15
; ALL-NEXT:    lsl x16, x18, x16
; ALL-NEXT:    asr x3, x13, x15
; ALL-NEXT:    tst x9, #0x40
; ALL-NEXT:    orr x16, x16, x1
; ALL-NEXT:    lsr x15, x11, x15
; ALL-NEXT:    asr x18, x13, #63
; ALL-NEXT:    csel x8, x3, x8, ne
; ALL-NEXT:    csel x16, x15, x16, ne
; ALL-NEXT:    csel x15, xzr, x15, ne
; ALL-NEXT:    csel x1, x18, x3, ne
; ALL-NEXT:    subs x3, x9, #128
; ALL-NEXT:    orr x16, x16, x0
; ALL-NEXT:    and x4, x3, #0x38
; ALL-NEXT:    mvn w5, w3
; ALL-NEXT:    orr x14, x15, x14
; ALL-NEXT:    lsr x10, x10, x4
; ALL-NEXT:    lsl x17, x17, x5
; ALL-NEXT:    orr x10, x17, x10
; ALL-NEXT:    csel x17, x1, x18, lo
; ALL-NEXT:    asr x13, x13, x4
; ALL-NEXT:    tst x3, #0x40
; ALL-NEXT:    csel x10, x13, x10, ne
; ALL-NEXT:    csel x13, x18, x13, ne
; ALL-NEXT:    cmp x9, #128
; ALL-NEXT:    csel x10, x16, x10, lo
; ALL-NEXT:    csel x8, x8, x18, lo
; ALL-NEXT:    csel x13, x14, x13, lo
; ALL-NEXT:    cmp x9, #0
; ALL-NEXT:    csel x9, x12, x10, eq
; ALL-NEXT:    csel x10, x11, x13, eq
; ALL-NEXT:    stp x8, x17, [x2, #16]
; ALL-NEXT:    stp x9, x10, [x2]
; ALL-NEXT:    ret
  %src = load i256, ptr %src.ptr, align 1
  %byteOff = load i256, ptr %byteOff.ptr, align 1
  %bitOff = shl i256 %byteOff, 3
  %res = ashr i256 %src, %bitOff
  store i256 %res, ptr %dst, align 1
  ret void
}
