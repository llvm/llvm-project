; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc < %s | FileCheck %s

target triple = "aarch64"

define void @partial_reduce_widen_v1i32_acc_v16i32_vec(ptr %accptr, ptr %resptr, ptr %vecptr) {
; CHECK-LABEL: partial_reduce_widen_v1i32_acc_v16i32_vec:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ldp q1, q0, [x2]
; CHECK-NEXT:    ldr s2, [x0]
; CHECK-NEXT:    ldp q5, q6, [x2, #32]
; CHECK-NEXT:    ext v3.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    ext v4.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    add v1.2s, v2.2s, v1.2s
; CHECK-NEXT:    ext v2.16b, v5.16b, v5.16b, #8
; CHECK-NEXT:    add v0.2s, v1.2s, v0.2s
; CHECK-NEXT:    add v1.2s, v4.2s, v3.2s
; CHECK-NEXT:    ext v3.16b, v6.16b, v6.16b, #8
; CHECK-NEXT:    add v0.2s, v0.2s, v5.2s
; CHECK-NEXT:    add v1.2s, v2.2s, v1.2s
; CHECK-NEXT:    add v0.2s, v0.2s, v6.2s
; CHECK-NEXT:    add v1.2s, v3.2s, v1.2s
; CHECK-NEXT:    add v0.2s, v1.2s, v0.2s
; CHECK-NEXT:    dup v1.2s, v0.s[1]
; CHECK-NEXT:    add v0.2s, v0.2s, v1.2s
; CHECK-NEXT:    str s0, [x1]
; CHECK-NEXT:    ret
  %acc = load <1 x i32>, ptr %accptr
  %vec = load <16 x i32>, ptr %vecptr
  %partial.reduce = call <1 x i32> @llvm.vector.partial.reduce.add(<1 x i32> %acc, <16 x i32> %vec)
  store <1 x i32> %partial.reduce, ptr %resptr
  ret void
}

define void @partial_reduce_widen_v3i32_acc_v12i32_vec(ptr %accptr, ptr %resptr, ptr %vecptr) {
; CHECK-LABEL: partial_reduce_widen_v3i32_acc_v12i32_vec:
; CHECK:       // %bb.0:
; CHECK-NEXT:    sub sp, sp, #128
; CHECK-NEXT:    .cfi_def_cfa_offset 128
; CHECK-NEXT:    ldp q1, q0, [x2]
; CHECK-NEXT:    ldr q2, [x0]
; CHECK-NEXT:    mov v2.s[3], wzr
; CHECK-NEXT:    add v0.4s, v1.4s, v0.4s
; CHECK-NEXT:    ldr q1, [x2, #32]
; CHECK-NEXT:    add v0.4s, v0.4s, v1.4s
; CHECK-NEXT:    add v0.4s, v2.4s, v0.4s
; CHECK-NEXT:    ext v1.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    add v0.2s, v0.2s, v1.2s
; CHECK-NEXT:    mov s1, v0.s[2]
; CHECK-NEXT:    str d0, [x1]
; CHECK-NEXT:    str s1, [x1, #8]
; CHECK-NEXT:    add sp, sp, #128
; CHECK-NEXT:    ret
  %acc = load <3 x i32>, ptr %accptr
  %vec = load <12 x i32>, ptr %vecptr
  %partial.reduce = call <3 x i32> @llvm.vector.partial.reduce.add(<3 x i32> %acc, <12 x i32> %vec)
  store <3 x i32> %partial.reduce, ptr %resptr
  ret void
}

define void @partial_reduce_widen_v4i32_acc_v20i32_vec(ptr %accptr, ptr %resptr, ptr %vecptr) {
; CHECK-LABEL: partial_reduce_widen_v4i32_acc_v20i32_vec:
; CHECK:       // %bb.0:
; CHECK-NEXT:    sub sp, sp, #272
; CHECK-NEXT:    str x29, [sp, #256] // 8-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 272
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    ldp q1, q0, [x2]
; CHECK-NEXT:    ldr s2, [x0]
; CHECK-NEXT:    ldp q5, q6, [x2, #32]
; CHECK-NEXT:    ldr x29, [sp, #256] // 8-byte Folded Reload
; CHECK-NEXT:    ext v3.16b, v1.16b, v1.16b, #8
; CHECK-NEXT:    ext v4.16b, v0.16b, v0.16b, #8
; CHECK-NEXT:    add v1.2s, v2.2s, v1.2s
; CHECK-NEXT:    ext v2.16b, v5.16b, v5.16b, #8
; CHECK-NEXT:    add v0.2s, v1.2s, v0.2s
; CHECK-NEXT:    add v1.2s, v4.2s, v3.2s
; CHECK-NEXT:    ext v3.16b, v6.16b, v6.16b, #8
; CHECK-NEXT:    ldr q4, [x2, #64]
; CHECK-NEXT:    add v0.2s, v0.2s, v5.2s
; CHECK-NEXT:    add v1.2s, v2.2s, v1.2s
; CHECK-NEXT:    ext v2.16b, v4.16b, v4.16b, #8
; CHECK-NEXT:    add v0.2s, v0.2s, v6.2s
; CHECK-NEXT:    add v1.2s, v3.2s, v1.2s
; CHECK-NEXT:    add v0.2s, v0.2s, v4.2s
; CHECK-NEXT:    add v1.2s, v2.2s, v1.2s
; CHECK-NEXT:    add v0.2s, v1.2s, v0.2s
; CHECK-NEXT:    dup v1.2s, v0.s[1]
; CHECK-NEXT:    add v0.2s, v0.2s, v1.2s
; CHECK-NEXT:    str s0, [x1]
; CHECK-NEXT:    add sp, sp, #272
; CHECK-NEXT:    ret
  %acc = load <1 x i32>, ptr %accptr
  %vec = load <20 x i32>, ptr %vecptr
  %partial.reduce = call <1 x i32> @llvm.vector.partial.reduce.add(<1 x i32> %acc, <20 x i32> %vec)
  store <1 x i32> %partial.reduce, ptr %resptr
  ret void
}
