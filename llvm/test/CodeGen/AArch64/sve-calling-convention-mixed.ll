; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple aarch64-linux-gnu -mattr=+sve | FileCheck %s

target datalayout = "e-m:e-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128"
target triple = "aarch64-unknown-linux-gnu"

; Make sure callers set up the arguments correctly - tests AArch64ISelLowering::LowerCALL

define float @foo1(double* %x0, double* %x1, double* %x2) nounwind {
; CHECK-LABEL: foo1:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-4
; CHECK-NEXT:    ptrue p0.b
; CHECK-NEXT:    fmov s0, #1.00000000
; CHECK-NEXT:    ld4d { z1.d, z2.d, z3.d, z4.d }, p0/z, [x0]
; CHECK-NEXT:    ld4d { z16.d, z17.d, z18.d, z19.d }, p0/z, [x1]
; CHECK-NEXT:    ld1d { z5.d }, p0/z, [x2]
; CHECK-NEXT:    mov x0, sp
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    st1d { z16.d }, p0, [sp]
; CHECK-NEXT:    st1d { z17.d }, p0, [sp, #1, mul vl]
; CHECK-NEXT:    st1d { z18.d }, p0, [sp, #2, mul vl]
; CHECK-NEXT:    st1d { z19.d }, p0, [sp, #3, mul vl]
; CHECK-NEXT:    bl callee1
; CHECK-NEXT:    addvl sp, sp, #4
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  %0 = call <vscale x 16 x i1> @llvm.aarch64.sve.ptrue.nxv16i1(i32 31)
  %1 = call <vscale x 2 x i1> @llvm.aarch64.sve.convert.from.svbool.nxv2i1(<vscale x 16 x i1> %0)
  %2 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x0)
  %3 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x1)
  %4 = call <vscale x 2 x double> @llvm.aarch64.sve.ld1.nxv2f64(<vscale x 2 x i1> %1, double* %x2)
  %5 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  0
  %6 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  1
  %7 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  2
  %8 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  3
  %9 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> poison, <vscale x 2 x double> %5, i64 0)
  %10 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %9, <vscale x 2 x double> %6, i64 2)
  %11 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %10, <vscale x 2 x double> %7, i64 4)
  %12 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %11, <vscale x 2 x double> %8, i64 6)
  %13 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  0
  %14 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  1
  %15 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  2
  %16 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  3
  %17 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> undef, <vscale x 2 x double> %13, i64 0)
  %18 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %17, <vscale x 2 x double> %14, i64 2)
  %19 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %18, <vscale x 2 x double> %15, i64 4)
  %20 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %19, <vscale x 2 x double> %16, i64 6)
  %call = call float @callee1(float 1.000000e+00, <vscale x 8 x double> %12, <vscale x 8 x double> %20, <vscale x 2 x double> %4)
  ret float %call
}

define float @foo2(double* %x0, double* %x1) nounwind {
; CHECK-LABEL: foo2:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-4
; CHECK-NEXT:    sub sp, sp, #16
; CHECK-NEXT:    ptrue p0.b
; CHECK-NEXT:    add x9, sp, #16
; CHECK-NEXT:    ld4d { z1.d, z2.d, z3.d, z4.d }, p0/z, [x0]
; CHECK-NEXT:    ld4d { z16.d, z17.d, z18.d, z19.d }, p0/z, [x1]
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    add x8, sp, #16
; CHECK-NEXT:    fmov s0, #1.00000000
; CHECK-NEXT:    mov w0, wzr
; CHECK-NEXT:    mov w1, #1
; CHECK-NEXT:    mov w2, #2
; CHECK-NEXT:    st1d { z16.d }, p0, [x9]
; CHECK-NEXT:    add x9, sp, #16
; CHECK-NEXT:    mov w3, #3
; CHECK-NEXT:    mov w4, #4
; CHECK-NEXT:    mov w5, #5
; CHECK-NEXT:    mov w6, #6
; CHECK-NEXT:    st1d { z17.d }, p0, [x9, #1, mul vl]
; CHECK-NEXT:    add x9, sp, #16
; CHECK-NEXT:    mov w7, #7
; CHECK-NEXT:    st1d { z18.d }, p0, [x9, #2, mul vl]
; CHECK-NEXT:    add x9, sp, #16
; CHECK-NEXT:    st1d { z19.d }, p0, [x9, #3, mul vl]
; CHECK-NEXT:    str x8, [sp]
; CHECK-NEXT:    bl callee2
; CHECK-NEXT:    addvl sp, sp, #4
; CHECK-NEXT:    add sp, sp, #16
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  %0 = call <vscale x 16 x i1> @llvm.aarch64.sve.ptrue.nxv16i1(i32 31)
  %1 = call <vscale x 2 x i1> @llvm.aarch64.sve.convert.from.svbool.nxv2i1(<vscale x 16 x i1> %0)
  %2 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x0)
  %3 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x1)
  %4 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  0
  %5 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  1
  %6 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  2
  %7 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  3
  %8 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> poison, <vscale x 2 x double> %4, i64 0)
  %9 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %8, <vscale x 2 x double> %5, i64 2)
  %10 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %9, <vscale x 2 x double> %6, i64 4)
  %11 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %10, <vscale x 2 x double> %7, i64 6)
  %12 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  0
  %13 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  1
  %14 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  2
  %15 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  3
  %16 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> poison, <vscale x 2 x double> %12, i64 0)
  %17 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %16, <vscale x 2 x double> %13, i64 2)
  %18 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %17, <vscale x 2 x double> %14, i64 4)
  %19 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %18, <vscale x 2 x double> %15, i64 6)
  %call = call float @callee2(i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, float 1.000000e+00, <vscale x 8 x double> %11, <vscale x 8 x double> %19)
  ret float %call
}

define float @foo3(double* %x0, double* %x1, double* %x2) nounwind {
; CHECK-LABEL: foo3:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    addvl sp, sp, #-3
; CHECK-NEXT:    ptrue p0.b
; CHECK-NEXT:    fmov s0, #1.00000000
; CHECK-NEXT:    ld4d { z2.d, z3.d, z4.d, z5.d }, p0/z, [x0]
; CHECK-NEXT:    ld3d { z16.d, z17.d, z18.d }, p0/z, [x1]
; CHECK-NEXT:    ld1d { z6.d }, p0/z, [x2]
; CHECK-NEXT:    fmov s1, #2.00000000
; CHECK-NEXT:    mov x0, sp
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    st1d { z16.d }, p0, [sp]
; CHECK-NEXT:    st1d { z17.d }, p0, [sp, #1, mul vl]
; CHECK-NEXT:    st1d { z18.d }, p0, [sp, #2, mul vl]
; CHECK-NEXT:    bl callee3
; CHECK-NEXT:    addvl sp, sp, #3
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
entry:
  %0 = call <vscale x 16 x i1> @llvm.aarch64.sve.ptrue.nxv16i1(i32 31)
  %1 = call <vscale x 2 x i1> @llvm.aarch64.sve.convert.from.svbool.nxv2i1(<vscale x 16 x i1> %0)
  %2 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x0)
  %3 = call {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld3.sret.nxv2f64(<vscale x 2 x i1> %1, double* %x1)
  %4 = call <vscale x 2 x double> @llvm.aarch64.sve.ld1.nxv2f64(<vscale x 2 x i1> %1, double* %x2)
  %5 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  0
  %6 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  1
  %7 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  2
  %8 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %2,  3
  %9 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> poison, <vscale x 2 x double> %5, i64 0)
  %10 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %9, <vscale x 2 x double> %6, i64 2)
  %11 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %10, <vscale x 2 x double> %7, i64 4)
  %12 = call <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double> %11, <vscale x 2 x double> %8, i64 6)
  %13 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} %3,  0
  %14 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  1
  %15 = extractvalue { <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double> } %3,  2
  %16 = call <vscale x 6 x double> @llvm.vector.insert.nxv6f64.nx2f64(<vscale x 6 x double> poison, <vscale x 2 x double> %13, i64 0)
  %17 = call <vscale x 6 x double> @llvm.vector.insert.nxv6f64.nx2f64(<vscale x 6 x double> %16 , <vscale x 2 x double> %14, i64 2)
  %18 = call <vscale x 6 x double> @llvm.vector.insert.nxv6f64.nx2f64(<vscale x 6 x double> %17 , <vscale x 2 x double> %15, i64 4)
  %call = call float @callee3(float 1.000000e+00, float 2.000000e+00, <vscale x 8 x double> %12, <vscale x 6 x double> %18, <vscale x 2 x double> %4)
  ret float %call
}

; Make sure callees read the arguments correctly - tests AArch64ISelLowering::LowerFormalArguments

define double @foo4(double %x0, double * %ptr1, double * %ptr2, double * %ptr3, <vscale x 8 x double> %x1, <vscale x 8 x double> %x2, <vscale x 2 x double> %x3) nounwind {
; CHECK-LABEL: foo4:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ld1d { z6.d }, p0/z, [x3, #1, mul vl]
; CHECK-NEXT:    ld1d { z7.d }, p0/z, [x3]
; CHECK-NEXT:    ld1d { z24.d }, p0/z, [x3, #3, mul vl]
; CHECK-NEXT:    ld1d { z25.d }, p0/z, [x3, #2, mul vl]
; CHECK-NEXT:    st1d { z4.d }, p0, [x0, #3, mul vl]
; CHECK-NEXT:    st1d { z3.d }, p0, [x0, #2, mul vl]
; CHECK-NEXT:    st1d { z2.d }, p0, [x0, #1, mul vl]
; CHECK-NEXT:    st1d { z1.d }, p0, [x0]
; CHECK-NEXT:    st1d { z25.d }, p0, [x1, #2, mul vl]
; CHECK-NEXT:    st1d { z24.d }, p0, [x1, #3, mul vl]
; CHECK-NEXT:    st1d { z7.d }, p0, [x1]
; CHECK-NEXT:    st1d { z6.d }, p0, [x1, #1, mul vl]
; CHECK-NEXT:    st1d { z5.d }, p0, [x2]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast double * %ptr1 to <vscale x 8 x double> *
  store volatile <vscale x 8 x double> %x1, <vscale x 8 x double>* %ptr1.bc
  %ptr2.bc = bitcast double * %ptr2 to <vscale x 8 x double> *
  store volatile <vscale x 8 x double> %x2, <vscale x 8 x double>* %ptr2.bc
  %ptr3.bc = bitcast double * %ptr3 to <vscale x 2 x double> *
  store volatile <vscale x 2 x double> %x3, <vscale x 2 x double>* %ptr3.bc
  ret double %x0
}

define double @foo5(i32 %i0, i32 %i1, i32 %i2, i32 %i3, i32 %i4, i32 %i5, double * %ptr1, double * %ptr2, double %x0, <vscale x 8 x double> %x1, <vscale x 8 x double> %x2) nounwind {
; CHECK-LABEL: foo5:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldr x8, [sp]
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ld1d { z5.d }, p0/z, [x8, #1, mul vl]
; CHECK-NEXT:    ld1d { z6.d }, p0/z, [x8]
; CHECK-NEXT:    ld1d { z7.d }, p0/z, [x8, #3, mul vl]
; CHECK-NEXT:    ld1d { z24.d }, p0/z, [x8, #2, mul vl]
; CHECK-NEXT:    st1d { z4.d }, p0, [x6, #3, mul vl]
; CHECK-NEXT:    st1d { z3.d }, p0, [x6, #2, mul vl]
; CHECK-NEXT:    st1d { z2.d }, p0, [x6, #1, mul vl]
; CHECK-NEXT:    st1d { z1.d }, p0, [x6]
; CHECK-NEXT:    st1d { z24.d }, p0, [x7, #2, mul vl]
; CHECK-NEXT:    st1d { z7.d }, p0, [x7, #3, mul vl]
; CHECK-NEXT:    st1d { z6.d }, p0, [x7]
; CHECK-NEXT:    st1d { z5.d }, p0, [x7, #1, mul vl]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast double * %ptr1 to <vscale x 8 x double> *
  store volatile <vscale x 8 x double> %x1, <vscale x 8 x double>* %ptr1.bc
  %ptr2.bc = bitcast double * %ptr2 to <vscale x 8 x double> *
  store volatile <vscale x 8 x double> %x2, <vscale x 8 x double>* %ptr2.bc
  ret double %x0
}

define double @foo6(double %x0, double %x1, double * %ptr1, double * %ptr2, <vscale x 8 x double> %x2, <vscale x 6 x double> %x3) nounwind {
; CHECK-LABEL: foo6:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ptrue p0.d
; CHECK-NEXT:    ld1d { z1.d }, p0/z, [x2]
; CHECK-NEXT:    ld1d { z6.d }, p0/z, [x2, #2, mul vl]
; CHECK-NEXT:    ld1d { z7.d }, p0/z, [x2, #1, mul vl]
; CHECK-NEXT:    st1d { z5.d }, p0, [x0, #3, mul vl]
; CHECK-NEXT:    st1d { z4.d }, p0, [x0, #2, mul vl]
; CHECK-NEXT:    st1d { z3.d }, p0, [x0, #1, mul vl]
; CHECK-NEXT:    st1d { z2.d }, p0, [x0]
; CHECK-NEXT:    st1d { z7.d }, p0, [x1, #1, mul vl]
; CHECK-NEXT:    st1d { z6.d }, p0, [x1, #2, mul vl]
; CHECK-NEXT:    st1d { z1.d }, p0, [x1]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast double * %ptr1 to <vscale x 8 x double> *
  store volatile <vscale x 8 x double> %x2, <vscale x 8 x double>* %ptr1.bc
  %ptr2.bc = bitcast double * %ptr2 to <vscale x 6 x double> *
  store volatile <vscale x 6 x double> %x3, <vscale x 6 x double>* %ptr2.bc
  ret double %x0
}

; Use AAVPCS, SVE register in z0-z7 used

define void @aavpcs1(i32 %s0, i32 %s1, i32 %s2, i32 %s3, i32 %s4, i32 %s5, i32 %s6, <vscale x 4 x i32> %s7, <vscale x 4 x i32> %s8, <vscale x 4 x i32> %s9, <vscale x 4 x i32> %s10, <vscale x 4 x i32> %s11, <vscale x 4 x i32> %s12, <vscale x 4 x i32> %s13, <vscale x 4 x i32> %s14, <vscale x 4 x i32> %s15, <vscale x 4 x i32> %s16, i32 * %ptr) nounwind {
; CHECK-LABEL: aavpcs1:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldp x8, x9, [sp]
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ld1w { z3.s }, p0/z, [x8]
; CHECK-NEXT:    ld1w { z24.s }, p0/z, [x7]
; CHECK-NEXT:    st1w { z0.s }, p0, [x9]
; CHECK-NEXT:    st1w { z1.s }, p0, [x9]
; CHECK-NEXT:    st1w { z2.s }, p0, [x9]
; CHECK-NEXT:    st1w { z4.s }, p0, [x9]
; CHECK-NEXT:    st1w { z5.s }, p0, [x9]
; CHECK-NEXT:    st1w { z6.s }, p0, [x9]
; CHECK-NEXT:    st1w { z7.s }, p0, [x9]
; CHECK-NEXT:    st1w { z24.s }, p0, [x9]
; CHECK-NEXT:    st1w { z3.s }, p0, [x9]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast i32 * %ptr to <vscale x 4 x i32> *
  store volatile <vscale x 4 x i32> %s7, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s8, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s9, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s11, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s12, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s13, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s14, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s15, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s16, <vscale x 4 x i32>* %ptr1.bc
  ret void
}

; Use AAVPCS, SVE register in z0-z7 used

define void @aavpcs2(float %s0, float %s1, float %s2, float %s3, float %s4, float %s5, float %s6, <vscale x 4 x float> %s7, <vscale x 4 x float> %s8, <vscale x 4 x float> %s9, <vscale x 4 x float> %s10, <vscale x 4 x float> %s11, <vscale x 4 x float> %s12,<vscale x 4 x float> %s13,<vscale x 4 x float> %s14,<vscale x 4 x float> %s15,<vscale x 4 x float> %s16,float * %ptr) nounwind {
; CHECK-LABEL: aavpcs2:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldp x8, x9, [sp]
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ld1w { z0.s }, p0/z, [x8]
; CHECK-NEXT:    ld1w { z1.s }, p0/z, [x7]
; CHECK-NEXT:    ld1w { z2.s }, p0/z, [x6]
; CHECK-NEXT:    ld1w { z3.s }, p0/z, [x5]
; CHECK-NEXT:    ld1w { z4.s }, p0/z, [x4]
; CHECK-NEXT:    ld1w { z5.s }, p0/z, [x3]
; CHECK-NEXT:    ld1w { z6.s }, p0/z, [x1]
; CHECK-NEXT:    ld1w { z24.s }, p0/z, [x0]
; CHECK-NEXT:    st1w { z7.s }, p0, [x9]
; CHECK-NEXT:    st1w { z24.s }, p0, [x9]
; CHECK-NEXT:    st1w { z6.s }, p0, [x9]
; CHECK-NEXT:    st1w { z5.s }, p0, [x9]
; CHECK-NEXT:    st1w { z4.s }, p0, [x9]
; CHECK-NEXT:    st1w { z3.s }, p0, [x9]
; CHECK-NEXT:    st1w { z2.s }, p0, [x9]
; CHECK-NEXT:    st1w { z1.s }, p0, [x9]
; CHECK-NEXT:    st1w { z0.s }, p0, [x9]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast float * %ptr to <vscale x 4 x float> *
  store volatile <vscale x 4 x float> %s7, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s8, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s9, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s11, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s12, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s13, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s14, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s15, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s16, <vscale x 4 x float>* %ptr1.bc
  ret void
}

; Use AAVPCS, no SVE register in z0-z7 used (floats occupy z0-z7) but predicate arg is used

define void @aavpcs3(float %s0, float %s1, float %s2, float %s3, float %s4, float %s5, float %s6, float %s7, <vscale x 4 x float> %s8, <vscale x 4 x float> %s9, <vscale x 4 x float> %s10, <vscale x 4 x float> %s11, <vscale x 4 x float> %s12, <vscale x 4 x float> %s13, <vscale x 4 x float> %s14, <vscale x 4 x float> %s15, <vscale x 4 x float> %s16, <vscale x 4 x float> %s17, <vscale x 16 x i1> %p0, float * %ptr) nounwind {
; CHECK-LABEL: aavpcs3:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldr x8, [sp]
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ld1w { z0.s }, p0/z, [x8]
; CHECK-NEXT:    ld1w { z1.s }, p0/z, [x7]
; CHECK-NEXT:    ld1w { z2.s }, p0/z, [x6]
; CHECK-NEXT:    ld1w { z3.s }, p0/z, [x5]
; CHECK-NEXT:    ld1w { z4.s }, p0/z, [x4]
; CHECK-NEXT:    ld1w { z5.s }, p0/z, [x3]
; CHECK-NEXT:    ld1w { z6.s }, p0/z, [x2]
; CHECK-NEXT:    ld1w { z7.s }, p0/z, [x1]
; CHECK-NEXT:    ld1w { z24.s }, p0/z, [x0]
; CHECK-NEXT:    ldr x8, [sp, #16]
; CHECK-NEXT:    st1w { z24.s }, p0, [x8]
; CHECK-NEXT:    st1w { z7.s }, p0, [x8]
; CHECK-NEXT:    st1w { z6.s }, p0, [x8]
; CHECK-NEXT:    st1w { z5.s }, p0, [x8]
; CHECK-NEXT:    st1w { z4.s }, p0, [x8]
; CHECK-NEXT:    st1w { z3.s }, p0, [x8]
; CHECK-NEXT:    st1w { z2.s }, p0, [x8]
; CHECK-NEXT:    st1w { z1.s }, p0, [x8]
; CHECK-NEXT:    st1w { z0.s }, p0, [x8]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast float * %ptr to <vscale x 4 x float> *
  store volatile <vscale x 4 x float> %s8, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s9, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s10, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s11, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s12, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s13, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s14, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s15, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s16, <vscale x 4 x float>* %ptr1.bc
  ret void
}

; use AAVPCS, SVE register in z0-z7 used (i32s dont occupy z0-z7)

define void @aavpcs4(i32 %s0, i32 %s1, i32 %s2, i32 %s3, i32 %s4, i32 %s5, i32 %s6, i32 %s7, <vscale x 4 x i32> %s8, <vscale x 4 x i32> %s9, <vscale x 4 x i32> %s10, <vscale x 4 x i32> %s11, <vscale x 4 x i32> %s12, <vscale x 4 x i32> %s13, <vscale x 4 x i32> %s14, <vscale x 4 x i32> %s15, <vscale x 4 x i32> %s16, <vscale x 4 x i32> %s17, i32 * %ptr) nounwind {
; CHECK-LABEL: aavpcs4:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldr x8, [sp]
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ldr x9, [sp, #16]
; CHECK-NEXT:    ld1w { z24.s }, p0/z, [x8]
; CHECK-NEXT:    st1w { z0.s }, p0, [x9]
; CHECK-NEXT:    st1w { z1.s }, p0, [x9]
; CHECK-NEXT:    st1w { z2.s }, p0, [x9]
; CHECK-NEXT:    st1w { z3.s }, p0, [x9]
; CHECK-NEXT:    st1w { z4.s }, p0, [x9]
; CHECK-NEXT:    st1w { z5.s }, p0, [x9]
; CHECK-NEXT:    st1w { z6.s }, p0, [x9]
; CHECK-NEXT:    st1w { z7.s }, p0, [x9]
; CHECK-NEXT:    st1w { z24.s }, p0, [x9]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast i32 * %ptr to <vscale x 4 x i32> *
  store volatile <vscale x 4 x i32> %s8, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s9, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s10, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s11, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s12, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s13, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s14, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s15, <vscale x 4 x i32>* %ptr1.bc
  store volatile <vscale x 4 x i32> %s16, <vscale x 4 x i32>* %ptr1.bc
  ret void
}

; Use AAPCS, no SVE register in z0-7 used (floats occupy z0-z7)

define void @aapcs1(float %s0, float %s1, float %s2, float %s3, float %s4, float %s5, float %s6, float %s7, <vscale x 4 x float> %s8, <vscale x 4 x float> %s9, <vscale x 4 x float> %s10, <vscale x 4 x float> %s11, <vscale x 4 x float> %s12, <vscale x 4 x float> %s13, <vscale x 4 x float> %s14, <vscale x 4 x float> %s15, <vscale x 4 x float> %s16, <vscale x 4 x float> %s17, float * %ptr) nounwind {
; CHECK-LABEL: aapcs1:
; CHECK:       // %bb.0: // %entry
; CHECK-NEXT:    ldr x8, [sp]
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    ld1w { z0.s }, p0/z, [x8]
; CHECK-NEXT:    ld1w { z1.s }, p0/z, [x7]
; CHECK-NEXT:    ld1w { z2.s }, p0/z, [x6]
; CHECK-NEXT:    ld1w { z3.s }, p0/z, [x5]
; CHECK-NEXT:    ld1w { z4.s }, p0/z, [x4]
; CHECK-NEXT:    ld1w { z5.s }, p0/z, [x3]
; CHECK-NEXT:    ld1w { z6.s }, p0/z, [x2]
; CHECK-NEXT:    ld1w { z7.s }, p0/z, [x1]
; CHECK-NEXT:    ld1w { z16.s }, p0/z, [x0]
; CHECK-NEXT:    ldr x8, [sp, #16]
; CHECK-NEXT:    st1w { z16.s }, p0, [x8]
; CHECK-NEXT:    st1w { z7.s }, p0, [x8]
; CHECK-NEXT:    st1w { z6.s }, p0, [x8]
; CHECK-NEXT:    st1w { z5.s }, p0, [x8]
; CHECK-NEXT:    st1w { z4.s }, p0, [x8]
; CHECK-NEXT:    st1w { z3.s }, p0, [x8]
; CHECK-NEXT:    st1w { z2.s }, p0, [x8]
; CHECK-NEXT:    st1w { z1.s }, p0, [x8]
; CHECK-NEXT:    st1w { z0.s }, p0, [x8]
; CHECK-NEXT:    ret
entry:
  %ptr1.bc = bitcast float * %ptr to <vscale x 4 x float> *
  store volatile <vscale x 4 x float> %s8, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s9, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s10, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s11, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s12, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s13, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s14, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s15, <vscale x 4 x float>* %ptr1.bc
  store volatile <vscale x 4 x float> %s16, <vscale x 4 x float>* %ptr1.bc
  ret void
}

define void @non_sve_callee_high_range(float %f0, float %f1, float %f2, float %f3, float %f4, float %f5, float %f6, float %f7, <vscale x 4 x float> %v0, <vscale x 4 x float> %v1)  {
; CHECK-LABEL: non_sve_callee_high_range:
; CHECK:       // %bb.0:
; CHECK-NEXT:    str d8, [sp, #-16]! // 8-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    .cfi_offset b8, -16
; CHECK-NEXT:    //APP
; CHECK-NEXT:    mov z8.d, #42 // =0x2a
; CHECK-NEXT:    //NO_APP
; CHECK-NEXT:    ldr d8, [sp], #16 // 8-byte Folded Reload
; CHECK-NEXT:    ret
  tail call void asm sideeffect "mov z8.d, #42", "~{z8}"()
  ret void
}

define <vscale x 4 x float> @sve_caller_non_sve_callee_high_range(<vscale x 4 x float> %v0, <vscale x 4 x float> %v1)  {
; CHECK-LABEL: sve_caller_non_sve_callee_high_range:
; CHECK:       // %bb.0:
; CHECK-NEXT:    stp x29, x30, [sp, #-16]! // 16-byte Folded Spill
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    .cfi_offset w30, -8
; CHECK-NEXT:    .cfi_offset w29, -16
; CHECK-NEXT:    addvl sp, sp, #-1
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x08, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 8 * VG
; CHECK-NEXT:    str z8, [sp] // 16-byte Folded Spill
; CHECK-NEXT:    .cfi_escape 0x10, 0x48, 0x0a, 0x11, 0x70, 0x22, 0x11, 0x78, 0x92, 0x2e, 0x00, 0x1e, 0x22 // $d8 @ cfa - 16 - 8 * VG
; CHECK-NEXT:    addvl sp, sp, #-2
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0c, 0x8f, 0x00, 0x11, 0x10, 0x22, 0x11, 0x18, 0x92, 0x2e, 0x00, 0x1e, 0x22 // sp + 16 + 24 * VG
; CHECK-NEXT:    mov z8.d, z0.d
; CHECK-NEXT:    movi d0, #0000000000000000
; CHECK-NEXT:    mov z24.d, z1.d
; CHECK-NEXT:    fmov s1, #1.00000000
; CHECK-NEXT:    fmov s2, #2.00000000
; CHECK-NEXT:    fmov s3, #3.00000000
; CHECK-NEXT:    fmov s4, #4.00000000
; CHECK-NEXT:    fmov s5, #5.00000000
; CHECK-NEXT:    fmov s6, #6.00000000
; CHECK-NEXT:    fmov s7, #7.00000000
; CHECK-NEXT:    mov x1, sp
; CHECK-NEXT:    addvl x0, sp, #1
; CHECK-NEXT:    ptrue p0.s
; CHECK-NEXT:    st1w { z24.s }, p0, [sp]
; CHECK-NEXT:    st1w { z8.s }, p0, [sp, #1, mul vl]
; CHECK-NEXT:    bl non_sve_callee_high_range
; CHECK-NEXT:    mov z0.d, z8.d
; CHECK-NEXT:    addvl sp, sp, #2
; CHECK-NEXT:    ldr z8, [sp] // 16-byte Folded Reload
; CHECK-NEXT:    addvl sp, sp, #1
; CHECK-NEXT:    ldp x29, x30, [sp], #16 // 16-byte Folded Reload
; CHECK-NEXT:    ret
  call void @non_sve_callee_high_range(float 0.0, float 1.0, float 2.0, float 3.0, float 4.0, float 5.0, float 6.0, float 7.0, <vscale x 4 x float> %v0, <vscale x 4 x float> %v1)
  ret <vscale x 4 x float> %v0
}

declare float @callee1(float, <vscale x 8 x double>, <vscale x 8 x double>, <vscale x 2 x double>)
declare float @callee2(i32, i32, i32, i32, i32, i32, i32, i32, float, <vscale x 8 x double>, <vscale x 8 x double>)
declare float @callee3(float, float, <vscale x 8 x double>, <vscale x 6 x double>, <vscale x 2 x double>)

declare <vscale x 16 x i1> @llvm.aarch64.sve.ptrue.nxv16i1(i32 immarg)
declare <vscale x 2 x i1> @llvm.aarch64.sve.convert.from.svbool.nxv2i1(<vscale x 16 x i1>)
declare {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld4.sret.nxv2f64(<vscale x 2 x i1>, double*)
declare {<vscale x 2 x double>, <vscale x 2 x double>, <vscale x 2 x double>} @llvm.aarch64.sve.ld3.sret.nxv2f64(<vscale x 2 x i1>, double*)
declare <vscale x 2 x double> @llvm.aarch64.sve.ld1.nxv2f64(<vscale x 2 x i1>, double*)
declare double @llvm.aarch64.sve.faddv.nxv2f64(<vscale x 2 x i1>, <vscale x 2 x double>)
declare <vscale x 8 x double> @llvm.vector.insert.nxv8f64.nx2f64(<vscale x 8 x double>, <vscale x 2 x double>, i64)
declare <vscale x 6 x double> @llvm.vector.insert.nxv6f64.nx2f64(<vscale x 6 x double>, <vscale x 2 x double>, i64)
