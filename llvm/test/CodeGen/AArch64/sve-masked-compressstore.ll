; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=aarch64 -mattr=+sve < %s | FileCheck %s

;; Full SVE vectors (supported with +sve)

define void @test_compressstore_nxv4i32(ptr %p, <vscale x 4 x i32> %vec, <vscale x 4 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv4i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.s
; CHECK-NEXT:    compact z0.s, p0, z0.s
; CHECK-NEXT:    cntp x8, p1, p0.s
; CHECK-NEXT:    whilelo p0.s, xzr, x8
; CHECK-NEXT:    st1w { z0.s }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv4i32(<vscale x 4 x i32> %vec, ptr align 4 %p, <vscale x 4 x i1> %mask)
  ret void
}

define void @test_compressstore_nxv2i64(ptr %p, <vscale x 2 x i64> %vec, <vscale x 2 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv2i64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.d
; CHECK-NEXT:    compact z0.d, p0, z0.d
; CHECK-NEXT:    cntp x8, p1, p0.d
; CHECK-NEXT:    whilelo p0.d, xzr, x8
; CHECK-NEXT:    st1d { z0.d }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv2i64(<vscale x 2 x i64> %vec, ptr align 8 %p, <vscale x 2 x i1> %mask)
  ret void
}

define void @test_compressstore_nxv4f32(ptr %p, <vscale x 4 x float> %vec, <vscale x 4 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv4f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.s
; CHECK-NEXT:    compact z0.s, p0, z0.s
; CHECK-NEXT:    cntp x8, p1, p0.s
; CHECK-NEXT:    whilelo p0.s, xzr, x8
; CHECK-NEXT:    st1w { z0.s }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv4f32(<vscale x 4 x float> %vec, ptr align 4 %p, <vscale x 4 x i1> %mask)
  ret void
}

; TODO: Legal and nonstreaming check
define void @test_compressstore_nxv2f64(ptr %p, <vscale x 2 x double> %vec, <vscale x 2 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv2f64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.d
; CHECK-NEXT:    compact z0.d, p0, z0.d
; CHECK-NEXT:    cntp x8, p1, p0.d
; CHECK-NEXT:    whilelo p0.d, xzr, x8
; CHECK-NEXT:    st1d { z0.d }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv2f64(<vscale x 2 x double> %vec, ptr align 8 %p, <vscale x 2 x i1> %mask)
  ret void
}

;; Promoted SVE vector types promoted to 32/64-bit (non-exhaustive)

define void @test_compressstore_nxv2i8(ptr %p, <vscale x 2 x i8> %vec, <vscale x 2 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv2i8:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.d
; CHECK-NEXT:    compact z0.d, p0, z0.d
; CHECK-NEXT:    cntp x8, p1, p0.d
; CHECK-NEXT:    whilelo p0.d, xzr, x8
; CHECK-NEXT:    st1b { z0.d }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv2i8(<vscale x 2 x i8> %vec, ptr align 1 %p, <vscale x 2 x i1> %mask)
  ret void
}

define void @test_compressstore_nxv4i16(ptr %p, <vscale x 4 x i16> %vec, <vscale x 4 x i1> %mask) {
; CHECK-LABEL: test_compressstore_nxv4i16:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ptrue p1.s
; CHECK-NEXT:    compact z0.s, p0, z0.s
; CHECK-NEXT:    cntp x8, p1, p0.s
; CHECK-NEXT:    whilelo p0.s, xzr, x8
; CHECK-NEXT:    st1h { z0.s }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.nxv4i16(<vscale x 4 x i16> %vec, ptr align 2 %p, <vscale x 4 x i1> %mask)
  ret void
}

;; NEON vector types (promoted to SVE)

define void @test_compressstore_v2f32(ptr %p, <2 x double> %vec, <2 x i1> %mask) {
; CHECK-LABEL: test_compressstore_v2f32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll v1.2d, v1.2s, #0
; CHECK-NEXT:    ptrue p0.d, vl2
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    ptrue p1.d
; CHECK-NEXT:    shl v1.2d, v1.2d, #63
; CHECK-NEXT:    cmpne p0.d, p0/z, z1.d, #0
; CHECK-NEXT:    cntp x8, p1, p0.d
; CHECK-NEXT:    compact z0.d, p0, z0.d
; CHECK-NEXT:    whilelo p0.d, xzr, x8
; CHECK-NEXT:    st1d { z0.d }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.v2f64(<2 x double> %vec, ptr align 8 %p, <2 x i1> %mask)
  ret void
}

define void @test_compressstore_v4i32(ptr %p, <4 x i32> %vec, <4 x i1> %mask) {
; CHECK-LABEL: test_compressstore_v4i32:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll v1.4s, v1.4h, #0
; CHECK-NEXT:    ptrue p0.s, vl4
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    ptrue p1.s
; CHECK-NEXT:    shl v1.4s, v1.4s, #31
; CHECK-NEXT:    cmpne p0.s, p0/z, z1.s, #0
; CHECK-NEXT:    cntp x8, p1, p0.s
; CHECK-NEXT:    compact z0.s, p0, z0.s
; CHECK-NEXT:    whilelo p0.s, xzr, x8
; CHECK-NEXT:    st1w { z0.s }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.v4i32(<4 x i32> %vec, ptr align 4 %p, <4 x i1> %mask)
  ret void
}

define void @test_compressstore_v2i64(ptr %p, <2 x i64> %vec, <2 x i1> %mask) {
; CHECK-LABEL: test_compressstore_v2i64:
; CHECK:       // %bb.0:
; CHECK-NEXT:    ushll v1.2d, v1.2s, #0
; CHECK-NEXT:    ptrue p0.d, vl2
; CHECK-NEXT:    // kill: def $q0 killed $q0 def $z0
; CHECK-NEXT:    ptrue p1.d
; CHECK-NEXT:    shl v1.2d, v1.2d, #63
; CHECK-NEXT:    cmpne p0.d, p0/z, z1.d, #0
; CHECK-NEXT:    cntp x8, p1, p0.d
; CHECK-NEXT:    compact z0.d, p0, z0.d
; CHECK-NEXT:    whilelo p0.d, xzr, x8
; CHECK-NEXT:    st1d { z0.d }, p0, [x0]
; CHECK-NEXT:    ret
  tail call void @llvm.masked.compressstore.v2i64(<2 x i64> %vec, ptr align 8 %p, <2 x i1> %mask)
  ret void
}
