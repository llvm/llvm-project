; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv64 -mattr=+v        -O3 | FileCheck %s --check-prefix=RV64
; RUN: llc < %s -mtriple=riscv32 -mattr=+v        -O3 | FileCheck %s --check-prefix=RV32
; RUN: llc < %s -mtriple=riscv32 -mattr=+v,+zvl128b -O3 | FileCheck %s --check-prefix=RV32-V128
; RUN: llc < %s -mtriple=riscv64 -mattr=+v,+zvl256b -O3 | FileCheck %s --check-prefix=RV64-V256


; Basic pass-through select on nxv4i32
define <vscale x 4 x i32> @ctsel_nxv4i32_basic(i1 %cond, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b) {
; RV64-LABEL: ctsel_nxv4i32_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 %cond, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b)
  ret <vscale x 4 x i32> %r
}

; Select with loads (aligned)
define <vscale x 4 x i32> @ctsel_nxv4i32_load(i1 %cond, ptr %p1, ptr %p2) {
; RV64-LABEL: ctsel_nxv4i32_load:
; RV64:       # %bb.0:
; RV64-NEXT:    vl2re32.v v8, (a1)
; RV64-NEXT:    vl2re32.v v10, (a2)
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_load:
; RV32:       # %bb.0:
; RV32-NEXT:    vl2re32.v v8, (a1)
; RV32-NEXT:    vl2re32.v v10, (a2)
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_load:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    vl2re32.v v8, (a1)
; RV32-V128-NEXT:    vl2re32.v v10, (a2)
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_load:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    vl2re32.v v8, (a1)
; RV64-V256-NEXT:    vl2re32.v v10, (a2)
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %a = load <vscale x 4 x i32>, ptr %p1, align 16
  %b = load <vscale x 4 x i32>, ptr %p2, align 16
  %r = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 %cond, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b)
  ret <vscale x 4 x i32> %r
}

; Mixed: do arithmetic first, then select, then store
define void @ctsel_nxv4i32_mixed(i1 %cond, ptr %p1, ptr %p2, ptr %out) {
; RV64-LABEL: ctsel_nxv4i32_mixed:
; RV64:       # %bb.0:
; RV64-NEXT:    vl2re32.v v8, (a1)
; RV64-NEXT:    vl2re32.v v10, (a2)
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vadd.vv v8, v8, v8
; RV64-NEXT:    vadd.vv v10, v10, v10
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    vs2r.v v8, (a3)
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_mixed:
; RV32:       # %bb.0:
; RV32-NEXT:    vl2re32.v v8, (a1)
; RV32-NEXT:    vl2re32.v v10, (a2)
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vadd.vv v8, v8, v8
; RV32-NEXT:    vadd.vv v10, v10, v10
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    vs2r.v v8, (a3)
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_mixed:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    vl2re32.v v8, (a1)
; RV32-V128-NEXT:    vl2re32.v v10, (a2)
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vadd.vv v8, v8, v8
; RV32-V128-NEXT:    vadd.vv v10, v10, v10
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    vs2r.v v8, (a3)
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_mixed:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    vl2re32.v v8, (a1)
; RV64-V256-NEXT:    vl2re32.v v10, (a2)
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vadd.vv v8, v8, v8
; RV64-V256-NEXT:    vadd.vv v10, v10, v10
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    vs2r.v v8, (a3)
; RV64-V256-NEXT:    ret
  %a = load <vscale x 4 x i32>, ptr %p1, align 16
  %b = load <vscale x 4 x i32>, ptr %p2, align 16
  ; avoid scalable vector constants: use %a+%a and %b+%b
  %a2 = add <vscale x 4 x i32> %a, %a
  %b2 = add <vscale x 4 x i32> %b, %b
  %r  = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 %cond, <vscale x 4 x i32> %a2, <vscale x 4 x i32> %b2)
  store <vscale x 4 x i32> %r, ptr %out, align 16
  ret void
}

; Const-true/false fold smoke tests
define <vscale x 4 x i32> @ctsel_nxv4i32_true(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b) {
; RV64-LABEL: ctsel_nxv4i32_true:
; RV64:       # %bb.0:
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_true:
; RV32:       # %bb.0:
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_true:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_true:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    ret
  %r = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 true,  <vscale x 4 x i32> %a, <vscale x 4 x i32> %b)
  ret <vscale x 4 x i32> %r
}

define <vscale x 4 x i32> @ctsel_nxv4i32_false(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b) {
; RV64-LABEL: ctsel_nxv4i32_false:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 1, e8, m1, ta, ma
; RV64-NEXT:    vmv2r.v v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_false:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 1, e8, m1, ta, ma
; RV32-NEXT:    vmv2r.v v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_false:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    vsetivli zero, 1, e8, m1, ta, ma
; RV32-V128-NEXT:    vmv2r.v v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_false:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    vsetivli zero, 1, e8, m1, ta, ma
; RV64-V256-NEXT:    vmv2r.v v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 false, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b)
  ret <vscale x 4 x i32> %r
}

; Chain two selects to ensure masks don’t get merged away
define <vscale x 4 x i32> @ctsel_nxv4i32_chain(i1 %c1, i1 %c2,
; RV64-LABEL: ctsel_nxv4i32_chain:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v14, 0
; RV64-NEXT:    andi a1, a1, 1
; RV64-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v16, a0
; RV64-NEXT:    vmsne.vi v0, v16, 0
; RV64-NEXT:    vmv.v.x v16, a1
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmerge.vim v18, v14, -1, v0
; RV64-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmsne.vi v0, v16, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmerge.vim v14, v14, -1, v0
; RV64-NEXT:    vand.vv v8, v18, v8
; RV64-NEXT:    vnot.v v16, v18
; RV64-NEXT:    vand.vv v10, v16, v10
; RV64-NEXT:    vnot.v v16, v14
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    vand.vv v8, v14, v8
; RV64-NEXT:    vand.vv v10, v16, v12
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4i32_chain:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v14, 0
; RV32-NEXT:    andi a1, a1, 1
; RV32-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v16, a0
; RV32-NEXT:    vmsne.vi v0, v16, 0
; RV32-NEXT:    vmv.v.x v16, a1
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmerge.vim v18, v14, -1, v0
; RV32-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmsne.vi v0, v16, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmerge.vim v14, v14, -1, v0
; RV32-NEXT:    vand.vv v8, v18, v8
; RV32-NEXT:    vnot.v v16, v18
; RV32-NEXT:    vand.vv v10, v16, v10
; RV32-NEXT:    vnot.v v16, v14
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    vand.vv v8, v14, v8
; RV32-NEXT:    vand.vv v10, v16, v12
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4i32_chain:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v14, 0
; RV32-V128-NEXT:    andi a1, a1, 1
; RV32-V128-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v16, a0
; RV32-V128-NEXT:    vmsne.vi v0, v16, 0
; RV32-V128-NEXT:    vmv.v.x v16, a1
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmerge.vim v18, v14, -1, v0
; RV32-V128-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmsne.vi v0, v16, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmerge.vim v14, v14, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v18, v8
; RV32-V128-NEXT:    vnot.v v16, v18
; RV32-V128-NEXT:    vand.vv v10, v16, v10
; RV32-V128-NEXT:    vnot.v v16, v14
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    vand.vv v8, v14, v8
; RV32-V128-NEXT:    vand.vv v10, v16, v12
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4i32_chain:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a2, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v14, 0
; RV64-V256-NEXT:    andi a1, a1, 1
; RV64-V256-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v16, a0
; RV64-V256-NEXT:    vmsne.vi v0, v16, 0
; RV64-V256-NEXT:    vmv.v.x v16, a1
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmerge.vim v18, v14, -1, v0
; RV64-V256-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmsne.vi v0, v16, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmerge.vim v14, v14, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v18, v8
; RV64-V256-NEXT:    vnot.v v16, v18
; RV64-V256-NEXT:    vand.vv v10, v16, v10
; RV64-V256-NEXT:    vnot.v v16, v14
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    vand.vv v8, v14, v8
; RV64-V256-NEXT:    vand.vv v10, v16, v12
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
                                               <vscale x 4 x i32> %a,
                                               <vscale x 4 x i32> %b,
                                               <vscale x 4 x i32> %c) {
  %t  = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 %c1, <vscale x 4 x i32> %a, <vscale x 4 x i32> %b)
  %r  = call <vscale x 4 x i32> @llvm.ct.select.nxv4i32(i1 %c2, <vscale x 4 x i32> %t, <vscale x 4 x i32> %c)
  ret <vscale x 4 x i32> %r
}

; A different element width
define <vscale x 8 x i16> @ctsel_nxv8i16_basic(i1 %cond, <vscale x 8 x i16> %a, <vscale x 8 x i16> %b) {
; RV64-LABEL: ctsel_nxv8i16_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, m1, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv8i16_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, m1, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv8i16_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, m1, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv8i16_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, m1, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 8 x i16> @llvm.ct.select.nxv8i16(i1 %cond, <vscale x 8 x i16> %a, <vscale x 8 x i16> %b)
  ret <vscale x 8 x i16> %r
}

define <vscale x 16 x i8> @ctsel_nxv16i8_basic(i1 %cond, <vscale x 16 x i8> %a, <vscale x 16 x i8> %b) {
; RV64-LABEL: ctsel_nxv16i8_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, m2, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv16i8_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, m2, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv16i8_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv16i8_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 16 x i8> @llvm.ct.select.nxv16i8(i1 %cond, <vscale x 16 x i8> %a, <vscale x 16 x i8> %b)
  ret <vscale x 16 x i8> %r
}

; 64-bit elements (useful on RV64)
define <vscale x 2 x i64> @ctsel_nxv2i64_basic(i1 %cond, <vscale x 2 x i64> %a, <vscale x 2 x i64> %b) {
; RV64-LABEL: ctsel_nxv2i64_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv2i64_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv2i64_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv2i64_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 2 x i64> @llvm.ct.select.nxv2i64(i1 %cond, <vscale x 2 x i64> %a, <vscale x 2 x i64> %b)
  ret <vscale x 2 x i64> %r
}

; Floating-point scalable vectors (bitcasted in your fallback)
define <vscale x 4 x float> @ctsel_nxv4f32_basic(i1 %cond, <vscale x 4 x float> %a, <vscale x 4 x float> %b) {
; RV64-LABEL: ctsel_nxv4f32_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4f32_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4f32_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4f32_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 4 x float> @llvm.ct.select.nxv4f32(i1 %cond, <vscale x 4 x float> %a, <vscale x 4 x float> %b)
  ret <vscale x 4 x float> %r
}

; FP arithmetic around select
define <vscale x 4 x float> @ctsel_nxv4f32_arith(i1 %cond, <vscale x 4 x float> %x, <vscale x 4 x float> %y) {
; RV64-LABEL: ctsel_nxv4f32_arith:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV64-NEXT:    vfadd.vv v12, v8, v10
; RV64-NEXT:    vfsub.vv v8, v8, v10
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-NEXT:    vmv.v.x v10, a0
; RV64-NEXT:    vmsne.vi v0, v10, 0
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-NEXT:    vmv.v.i v10, 0
; RV64-NEXT:    vmerge.vim v10, v10, -1, v0
; RV64-NEXT:    vand.vv v12, v10, v12
; RV64-NEXT:    vnot.v v10, v10
; RV64-NEXT:    vand.vv v8, v10, v8
; RV64-NEXT:    vor.vv v8, v12, v8
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv4f32_arith:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32-NEXT:    vfadd.vv v12, v8, v10
; RV32-NEXT:    vfsub.vv v8, v8, v10
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-NEXT:    vmv.v.x v10, a0
; RV32-NEXT:    vmsne.vi v0, v10, 0
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-NEXT:    vmv.v.i v10, 0
; RV32-NEXT:    vmerge.vim v10, v10, -1, v0
; RV32-NEXT:    vand.vv v12, v10, v12
; RV32-NEXT:    vnot.v v10, v10
; RV32-NEXT:    vand.vv v8, v10, v8
; RV32-NEXT:    vor.vv v8, v12, v8
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv4f32_arith:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vfadd.vv v12, v8, v10
; RV32-V128-NEXT:    vfsub.vv v8, v8, v10
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV32-V128-NEXT:    vmv.v.x v10, a0
; RV32-V128-NEXT:    vmsne.vi v0, v10, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v10, 0
; RV32-V128-NEXT:    vmerge.vim v10, v10, -1, v0
; RV32-V128-NEXT:    vand.vv v12, v10, v12
; RV32-V128-NEXT:    vnot.v v10, v10
; RV32-V128-NEXT:    vand.vv v8, v10, v8
; RV32-V128-NEXT:    vor.vv v8, v12, v8
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv4f32_arith:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vfadd.vv v12, v8, v10
; RV64-V256-NEXT:    vfsub.vv v8, v8, v10
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; RV64-V256-NEXT:    vmv.v.x v10, a0
; RV64-V256-NEXT:    vmsne.vi v0, v10, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v10, 0
; RV64-V256-NEXT:    vmerge.vim v10, v10, -1, v0
; RV64-V256-NEXT:    vand.vv v12, v10, v12
; RV64-V256-NEXT:    vnot.v v10, v10
; RV64-V256-NEXT:    vand.vv v8, v10, v8
; RV64-V256-NEXT:    vor.vv v8, v12, v8
; RV64-V256-NEXT:    ret
  %sum  = fadd <vscale x 4 x float> %x, %y
  %diff = fsub <vscale x 4 x float> %x, %y
  %r    = call <vscale x 4 x float> @llvm.ct.select.nxv4f32(i1 %cond, <vscale x 4 x float> %sum, <vscale x 4 x float> %diff)
  ret <vscale x 4 x float> %r
}

define <vscale x 2 x double> @ctsel_nxv2f64_basic(i1 %cond, <vscale x 2 x double> %a, <vscale x 2 x double> %b) {
; RV64-LABEL: ctsel_nxv2f64_basic:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV64-NEXT:    vmv.v.x v12, a0
; RV64-NEXT:    vmsne.vi v0, v12, 0
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64-NEXT:    vmv.v.i v12, 0
; RV64-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vnot.v v12, v12
; RV64-NEXT:    vand.vv v10, v12, v10
; RV64-NEXT:    vor.vv v8, v8, v10
; RV64-NEXT:    ret
;
; RV32-LABEL: ctsel_nxv2f64_basic:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV32-NEXT:    vmv.v.x v12, a0
; RV32-NEXT:    vmsne.vi v0, v12, 0
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32-NEXT:    vmv.v.i v12, 0
; RV32-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-NEXT:    vand.vv v8, v12, v8
; RV32-NEXT:    vnot.v v12, v12
; RV32-NEXT:    vand.vv v10, v12, v10
; RV32-NEXT:    vor.vv v8, v8, v10
; RV32-NEXT:    ret
;
; RV32-V128-LABEL: ctsel_nxv2f64_basic:
; RV32-V128:       # %bb.0:
; RV32-V128-NEXT:    andi a0, a0, 1
; RV32-V128-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV32-V128-NEXT:    vmv.v.x v12, a0
; RV32-V128-NEXT:    vmsne.vi v0, v12, 0
; RV32-V128-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32-V128-NEXT:    vmv.v.i v12, 0
; RV32-V128-NEXT:    vmerge.vim v12, v12, -1, v0
; RV32-V128-NEXT:    vand.vv v8, v12, v8
; RV32-V128-NEXT:    vnot.v v12, v12
; RV32-V128-NEXT:    vand.vv v10, v12, v10
; RV32-V128-NEXT:    vor.vv v8, v8, v10
; RV32-V128-NEXT:    ret
;
; RV64-V256-LABEL: ctsel_nxv2f64_basic:
; RV64-V256:       # %bb.0:
; RV64-V256-NEXT:    andi a0, a0, 1
; RV64-V256-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; RV64-V256-NEXT:    vmv.v.x v12, a0
; RV64-V256-NEXT:    vmsne.vi v0, v12, 0
; RV64-V256-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64-V256-NEXT:    vmv.v.i v12, 0
; RV64-V256-NEXT:    vmerge.vim v12, v12, -1, v0
; RV64-V256-NEXT:    vand.vv v8, v12, v8
; RV64-V256-NEXT:    vnot.v v12, v12
; RV64-V256-NEXT:    vand.vv v10, v12, v10
; RV64-V256-NEXT:    vor.vv v8, v8, v10
; RV64-V256-NEXT:    ret
  %r = call <vscale x 2 x double> @llvm.ct.select.nxv2f64(i1 %cond, <vscale x 2 x double> %a, <vscale x 2 x double> %b)
  ret <vscale x 2 x double> %r
}

declare <vscale x 4 x i32>   @llvm.ct.select.nxv4i32(i1, <vscale x 4 x i32>,   <vscale x 4 x i32>)
declare <vscale x 8 x i16>   @llvm.ct.select.nxv8i16(i1, <vscale x 8 x i16>,   <vscale x 8 x i16>)
declare <vscale x 16 x i8>   @llvm.ct.select.nxv16i8(i1, <vscale x 16 x i8>,   <vscale x 16 x i8>)
declare <vscale x 2 x i64>   @llvm.ct.select.nxv2i64(i1, <vscale x 2 x i64>,   <vscale x 2 x i64>)
declare <vscale x 4 x float> @llvm.ct.select.nxv4f32(i1, <vscale x 4 x float>, <vscale x 4 x float>)
declare <vscale x 2 x double>@llvm.ct.select.nxv2f64(i1, <vscale x 2 x double>,<vscale x 2 x double>)
