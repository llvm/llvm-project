; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv64 -O3 | FileCheck %s --check-prefix=RV64
; RUN: llc < %s -mtriple=riscv32 -O3 | FileCheck %s --check-prefix=RV32

; Test basic ct.select functionality for scalar types
define i8 @test_ctselect_i8(i1 %cond, i8 %a, i8 %b) {
; RV64-LABEL: test_ctselect_i8:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_i8:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a2
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a2, a0
; RV32-NEXT:    ret
  %result = call i8 @llvm.ct.select.i8(i1 %cond, i8 %a, i8 %b)
  ret i8 %result
}
define i32 @test_ctselect_i32(i1 %cond, i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_i32:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_i32:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a2
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a2, a0
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}

define i64 @test_ctselect_i64(i1 %cond, i64 %a, i64 %b) {
; RV64-LABEL: test_ctselect_i64:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_i64:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a3
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    xor a2, a2, a4
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a1, a1, a0
; RV32-NEXT:    and a2, a2, a0
; RV32-NEXT:    xor a0, a3, a1
; RV32-NEXT:    xor a1, a4, a2
; RV32-NEXT:    ret
  %result = call i64 @llvm.ct.select.i64(i1 %cond, i64 %a, i64 %b)
  ret i64 %result
}

define ptr @test_ctselect_ptr(i1 %cond, ptr %a, ptr %b) {
; RV64-LABEL: test_ctselect_ptr:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_ptr:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a2
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a2, a0
; RV32-NEXT:    ret
  %result = call ptr @llvm.ct.select.p0(i1 %cond, ptr %a, ptr %b)
  ret ptr %result
}

; Test with constant conditions
define i32 @test_ctselect_const_true(i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_const_true:
; RV64:       # %bb.0:
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_const_true:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a0, a0, a1
; RV32-NEXT:    xor a0, a1, a0
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 true, i32 %a, i32 %b)
  ret i32 %result
}

define i32 @test_ctselect_const_false(i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_const_false:
; RV64:       # %bb.0:
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_const_false:
; RV32:       # %bb.0:
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 false, i32 %a, i32 %b)
  ret i32 %result
}

; Test with comparison conditions
define i32 @test_ctselect_icmp_eq(i32 %x, i32 %y, i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_icmp_eq:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a1
; RV64-NEXT:    sext.w a0, a0
; RV64-NEXT:    xor a0, a0, a1
; RV64-NEXT:    snez a0, a0
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    addi a0, a0, -1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    xor a0, a3, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_icmp_eq:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a0, a0, a1
; RV32-NEXT:    snez a0, a0
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    xor a0, a3, a0
; RV32-NEXT:    ret
  %cond = icmp eq i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}
define i32 @test_ctselect_icmp_ult(i32 %x, i32 %y, i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_icmp_ult:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a1
; RV64-NEXT:    sext.w a0, a0
; RV64-NEXT:    sltu a0, a0, a1
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    neg a0, a0
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    xor a0, a3, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_icmp_ult:
; RV32:       # %bb.0:
; RV32-NEXT:    sltu a0, a0, a1
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    neg a0, a0
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    xor a0, a3, a0
; RV32-NEXT:    ret
  %cond = icmp ult i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}

; Test with memory operands
define i32 @test_ctselect_load(i1 %cond, ptr %p1, ptr %p2) {
; RV64-LABEL: test_ctselect_load:
; RV64:       # %bb.0:
; RV64-NEXT:    lw a1, 0(a1)
; RV64-NEXT:    lw a2, 0(a2)
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_load:
; RV32:       # %bb.0:
; RV32-NEXT:    lw a1, 0(a1)
; RV32-NEXT:    lw a2, 0(a2)
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    xor a1, a1, a2
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a2, a0
; RV32-NEXT:    ret
  %a = load i32, ptr %p1
  %b = load i32, ptr %p2
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}

; Test nested CT_SELECT pattern with AND merging on i1 values
; Pattern: ct_select C0, (ct_select C1, X, Y), Y -> ct_select (C0 & C1), X, Y
define i32 @test_ctselect_nested_and_i1_to_i32(i1 %c0, i1 %c1, i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_nested_and_i1_to_i32:
; RV64:       # %bb.0:
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    xor a0, a3, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_nested_and_i1_to_i32:
; RV32:       # %bb.0:
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    xor a0, a3, a0
; RV32-NEXT:    ret
  %inner = call i1 @llvm.ct.select.i1(i1 %c1, i1 true, i1 false)
  %cond = call i1 @llvm.ct.select.i1(i1 %c0, i1 %inner, i1 false)
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %x, i32 %y)
  ret i32 %result
}

; Test nested CT_SELECT pattern with OR merging on i1 values
; Pattern: ct_select C0, X, (ct_select C1, X, Y) -> ct_select (C0 | C1), X, Y
define i32 @test_ctselect_nested_or_i1_to_i32(i1 %c0, i1 %c1, i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_nested_or_i1_to_i32:
; RV64:       # %bb.0:
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    xor a0, a3, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_nested_or_i1_to_i32:
; RV32:       # %bb.0:
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    xor a0, a3, a0
; RV32-NEXT:    ret
  %inner = call i1 @llvm.ct.select.i1(i1 %c1, i1 true, i1 false)
  %cond = call i1 @llvm.ct.select.i1(i1 %c0, i1 true, i1 %inner)
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %x, i32 %y)
  ret i32 %result
}

; Test double nested CT_SELECT with recursive AND merging
; Pattern: ct_select C0, (ct_select C1, (ct_select C2, X, Y), Y), Y
;   -> ct_select (C0 & C1 & C2), X, Y
define i32 @test_ctselect_double_nested_and_i1(i1 %c0, i1 %c1, i1 %c2, i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_double_nested_and_i1:
; RV64:       # %bb.0:
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a3, a3, a4
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a3, a0
; RV64-NEXT:    xor a0, a4, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_double_nested_and_i1:
; RV32:       # %bb.0:
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a3, a3, a4
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a3, a0
; RV32-NEXT:    xor a0, a4, a0
; RV32-NEXT:    ret
  %inner2 = call i1 @llvm.ct.select.i1(i1 %c2, i1 true, i1 false)
  %inner1 = call i1 @llvm.ct.select.i1(i1 %c1, i1 %inner2, i1 false)
  %cond = call i1 @llvm.ct.select.i1(i1 %c0, i1 %inner1, i1 false)
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %x, i32 %y)
  ret i32 %result
}

; Test double nested CT_SELECT with mixed AND/OR patterns
define i32 @test_ctselect_double_nested_mixed_i1(i1 %c0, i1 %c1, i1 %c2, i32 %x, i32 %y, i32 %z) {
; RV64-LABEL: test_ctselect_double_nested_mixed_i1:
; RV64:       # %bb.0:
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a3, a3, a4
; RV64-NEXT:    or a0, a0, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a3, a3, a0
; RV64-NEXT:    xor a4, a4, a5
; RV64-NEXT:    xor a3, a4, a3
; RV64-NEXT:    and a0, a3, a0
; RV64-NEXT:    xor a0, a5, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_double_nested_mixed_i1:
; RV32:       # %bb.0:
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a3, a3, a4
; RV32-NEXT:    or a0, a0, a2
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a3, a3, a0
; RV32-NEXT:    xor a4, a4, a5
; RV32-NEXT:    xor a3, a4, a3
; RV32-NEXT:    and a0, a3, a0
; RV32-NEXT:    xor a0, a5, a0
; RV32-NEXT:    ret
  %inner1 = call i1 @llvm.ct.select.i1(i1 %c1, i1 true, i1 false)
  %and_cond = call i1 @llvm.ct.select.i1(i1 %c0, i1 %inner1, i1 false)
  %inner2 = call i1 @llvm.ct.select.i1(i1 %c2, i1 true, i1 false)
  %or_cond = call i1 @llvm.ct.select.i1(i1 %and_cond, i1 true, i1 %inner2)
  %inner_result = call i32 @llvm.ct.select.i32(i1 %or_cond, i32 %x, i32 %y)
  %result = call i32 @llvm.ct.select.i32(i1 %or_cond, i32 %inner_result, i32 %z)
  ret i32 %result
}

; Test nested ct_select calls
define i32 @test_ctselect_nested(i1 %cond1, i1 %cond2, i32 %a, i32 %b, i32 %c) {
; RV64-LABEL: test_ctselect_nested:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    slli a1, a1, 63
; RV64-NEXT:    xor a3, a3, a4
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a1, a1, 63
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    xor a1, a3, a1
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a4, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_nested:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    xor a3, a3, a4
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    xor a1, a3, a1
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a4, a0
; RV32-NEXT:    ret
  %inner = call i32 @llvm.ct.select.i32(i1 %cond2, i32 %a, i32 %b)
  %result = call i32 @llvm.ct.select.i32(i1 %cond1, i32 %inner, i32 %c)
  ret i32 %result
}

; Test floating-point ct.select selecting between NaN and Inf
define float @test_ctselect_f32_nan_inf(i1 %cond) {
; RV64-LABEL: test_ctselect_f32_nan_inf:
; RV64:       # %bb.0:
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    lui a1, 1024
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a0, a1
; RV64-NEXT:    lui a1, 522240
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_f32_nan_inf:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    lui a1, 1024
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a0, a1
; RV32-NEXT:    lui a1, 522240
; RV32-NEXT:    xor a0, a0, a1
; RV32-NEXT:    ret
  %result = call float @llvm.ct.select.f32(i1 %cond, float 0x7FF8000000000000, float 0x7FF0000000000000)
  ret float %result
}

define double @test_ctselect_f64_nan_inf(i1 %cond) {
; RV64-LABEL: test_ctselect_f64_nan_inf:
; RV64:       # %bb.0:
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    li a1, 1
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    slli a1, a1, 51
; RV64-NEXT:    and a0, a0, a1
; RV64-NEXT:    li a1, 2047
; RV64-NEXT:    slli a1, a1, 52
; RV64-NEXT:    xor a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_f64_nan_inf:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    lui a1, 128
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a0, a1
; RV32-NEXT:    lui a1, 524032
; RV32-NEXT:    or a1, a0, a1
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    ret
  %result = call double @llvm.ct.select.f64(i1 %cond, double 0x7FF8000000000000, double 0x7FF0000000000000)
  ret double %result
}

; Test basic floating-point ct.select
define float @test_ctselect_f32(i1 %cond, float %a, float %b) {
; RV64-LABEL: test_ctselect_f32:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_f32:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a2
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    xor a0, a2, a0
; RV32-NEXT:    ret
  %result = call float @llvm.ct.select.f32(i1 %cond, float %a, float %b)
  ret float %result
}

define double @test_ctselect_f64(i1 %cond, double %a, double %b) {
; RV64-LABEL: test_ctselect_f64:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a1, a1, a2
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    xor a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_f64:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a1, a1, a3
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    xor a2, a2, a4
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a1, a1, a0
; RV32-NEXT:    and a2, a2, a0
; RV32-NEXT:    xor a0, a3, a1
; RV32-NEXT:    xor a1, a4, a2
; RV32-NEXT:    ret
  %result = call double @llvm.ct.select.f64(i1 %cond, double %a, double %b)
  ret double %result
}

; Test vector ct.select with integer vectors
define <4 x i32> @test_ctselect_v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b) {
; RV64-LABEL: test_ctselect_v4i32:
; RV64:       # %bb.0:
; RV64-NEXT:    lw a4, 0(a3)
; RV64-NEXT:    lw a5, 8(a3)
; RV64-NEXT:    lw a6, 16(a3)
; RV64-NEXT:    lw a3, 24(a3)
; RV64-NEXT:    lw a7, 0(a2)
; RV64-NEXT:    lw t0, 8(a2)
; RV64-NEXT:    lw t1, 16(a2)
; RV64-NEXT:    lw a2, 24(a2)
; RV64-NEXT:    slli a1, a1, 63
; RV64-NEXT:    srai a1, a1, 63
; RV64-NEXT:    xor a7, a7, a4
; RV64-NEXT:    xor t0, t0, a5
; RV64-NEXT:    xor t1, t1, a6
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    and a7, a7, a1
; RV64-NEXT:    and t0, t0, a1
; RV64-NEXT:    and t1, t1, a1
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    xor a2, a4, a7
; RV64-NEXT:    xor a4, a5, t0
; RV64-NEXT:    xor a5, a6, t1
; RV64-NEXT:    xor a1, a3, a1
; RV64-NEXT:    sw a2, 0(a0)
; RV64-NEXT:    sw a4, 4(a0)
; RV64-NEXT:    sw a5, 8(a0)
; RV64-NEXT:    sw a1, 12(a0)
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_v4i32:
; RV32:       # %bb.0:
; RV32-NEXT:    lw a4, 0(a3)
; RV32-NEXT:    lw a5, 4(a3)
; RV32-NEXT:    lw a6, 8(a3)
; RV32-NEXT:    lw a3, 12(a3)
; RV32-NEXT:    lw a7, 0(a2)
; RV32-NEXT:    lw t0, 4(a2)
; RV32-NEXT:    lw t1, 8(a2)
; RV32-NEXT:    lw a2, 12(a2)
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    xor a7, a7, a4
; RV32-NEXT:    xor t0, t0, a5
; RV32-NEXT:    xor t1, t1, a6
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    and a7, a7, a1
; RV32-NEXT:    and t0, t0, a1
; RV32-NEXT:    and t1, t1, a1
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    xor a2, a4, a7
; RV32-NEXT:    xor a4, a5, t0
; RV32-NEXT:    xor a5, a6, t1
; RV32-NEXT:    xor a1, a3, a1
; RV32-NEXT:    sw a2, 0(a0)
; RV32-NEXT:    sw a4, 4(a0)
; RV32-NEXT:    sw a5, 8(a0)
; RV32-NEXT:    sw a1, 12(a0)
; RV32-NEXT:    ret
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}
define <4 x float> @test_ctselect_v4f32(i1 %cond, <4 x float> %a, <4 x float> %b) {
; RV64-LABEL: test_ctselect_v4f32:
; RV64:       # %bb.0:
; RV64-NEXT:    lw a4, 0(a3)
; RV64-NEXT:    lw a5, 8(a3)
; RV64-NEXT:    lw a6, 16(a3)
; RV64-NEXT:    lw a3, 24(a3)
; RV64-NEXT:    lw a7, 0(a2)
; RV64-NEXT:    lw t0, 8(a2)
; RV64-NEXT:    lw t1, 16(a2)
; RV64-NEXT:    lw a2, 24(a2)
; RV64-NEXT:    slli a1, a1, 63
; RV64-NEXT:    srai a1, a1, 63
; RV64-NEXT:    xor a7, a7, a4
; RV64-NEXT:    xor t0, t0, a5
; RV64-NEXT:    xor t1, t1, a6
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    and a7, a7, a1
; RV64-NEXT:    and t0, t0, a1
; RV64-NEXT:    and t1, t1, a1
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    xor a2, a4, a7
; RV64-NEXT:    xor a4, a5, t0
; RV64-NEXT:    xor a5, a6, t1
; RV64-NEXT:    xor a1, a3, a1
; RV64-NEXT:    sw a2, 0(a0)
; RV64-NEXT:    sw a4, 4(a0)
; RV64-NEXT:    sw a5, 8(a0)
; RV64-NEXT:    sw a1, 12(a0)
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_v4f32:
; RV32:       # %bb.0:
; RV32-NEXT:    lw a4, 0(a3)
; RV32-NEXT:    lw a5, 4(a3)
; RV32-NEXT:    lw a6, 8(a3)
; RV32-NEXT:    lw a3, 12(a3)
; RV32-NEXT:    lw a7, 0(a2)
; RV32-NEXT:    lw t0, 4(a2)
; RV32-NEXT:    lw t1, 8(a2)
; RV32-NEXT:    lw a2, 12(a2)
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    xor a7, a7, a4
; RV32-NEXT:    xor t0, t0, a5
; RV32-NEXT:    xor t1, t1, a6
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    and a7, a7, a1
; RV32-NEXT:    and t0, t0, a1
; RV32-NEXT:    and t1, t1, a1
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    xor a2, a4, a7
; RV32-NEXT:    xor a4, a5, t0
; RV32-NEXT:    xor a5, a6, t1
; RV32-NEXT:    xor a1, a3, a1
; RV32-NEXT:    sw a2, 0(a0)
; RV32-NEXT:    sw a4, 4(a0)
; RV32-NEXT:    sw a5, 8(a0)
; RV32-NEXT:    sw a1, 12(a0)
; RV32-NEXT:    ret
  %result = call <4 x float> @llvm.ct.select.v4f32(i1 %cond, <4 x float> %a, <4 x float> %b)
  ret <4 x float> %result
}
define <8 x i32> @test_ctselect_v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b) {
; RV64-LABEL: test_ctselect_v8i32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    .cfi_def_cfa_offset 32
; RV64-NEXT:    sd s0, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset s0, -8
; RV64-NEXT:    .cfi_offset s1, -16
; RV64-NEXT:    .cfi_offset s2, -24
; RV64-NEXT:    lw a7, 32(a3)
; RV64-NEXT:    lw a6, 40(a3)
; RV64-NEXT:    lw a5, 48(a3)
; RV64-NEXT:    lw a4, 56(a3)
; RV64-NEXT:    lw t0, 32(a2)
; RV64-NEXT:    lw t1, 40(a2)
; RV64-NEXT:    lw t2, 48(a2)
; RV64-NEXT:    lw t3, 56(a2)
; RV64-NEXT:    lw t4, 0(a3)
; RV64-NEXT:    lw t5, 8(a3)
; RV64-NEXT:    lw t6, 16(a3)
; RV64-NEXT:    lw a3, 24(a3)
; RV64-NEXT:    lw s0, 0(a2)
; RV64-NEXT:    lw s1, 8(a2)
; RV64-NEXT:    lw s2, 16(a2)
; RV64-NEXT:    lw a2, 24(a2)
; RV64-NEXT:    slli a1, a1, 63
; RV64-NEXT:    srai a1, a1, 63
; RV64-NEXT:    xor s0, s0, t4
; RV64-NEXT:    xor s1, s1, t5
; RV64-NEXT:    xor s2, s2, t6
; RV64-NEXT:    xor a2, a2, a3
; RV64-NEXT:    xor t0, t0, a7
; RV64-NEXT:    xor t1, t1, a6
; RV64-NEXT:    xor t2, t2, a5
; RV64-NEXT:    xor t3, t3, a4
; RV64-NEXT:    and s0, s0, a1
; RV64-NEXT:    and s1, s1, a1
; RV64-NEXT:    and s2, s2, a1
; RV64-NEXT:    and a2, a2, a1
; RV64-NEXT:    and t0, t0, a1
; RV64-NEXT:    and t1, t1, a1
; RV64-NEXT:    and t2, t2, a1
; RV64-NEXT:    and a1, t3, a1
; RV64-NEXT:    xor t3, t4, s0
; RV64-NEXT:    xor t4, t5, s1
; RV64-NEXT:    xor t5, t6, s2
; RV64-NEXT:    xor a2, a3, a2
; RV64-NEXT:    xor a3, a7, t0
; RV64-NEXT:    xor a6, a6, t1
; RV64-NEXT:    xor a5, a5, t2
; RV64-NEXT:    xor a1, a4, a1
; RV64-NEXT:    sw a3, 16(a0)
; RV64-NEXT:    sw a6, 20(a0)
; RV64-NEXT:    sw a5, 24(a0)
; RV64-NEXT:    sw a1, 28(a0)
; RV64-NEXT:    sw t3, 0(a0)
; RV64-NEXT:    sw t4, 4(a0)
; RV64-NEXT:    sw t5, 8(a0)
; RV64-NEXT:    sw a2, 12(a0)
; RV64-NEXT:    ld s0, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore s0
; RV64-NEXT:    .cfi_restore s1
; RV64-NEXT:    .cfi_restore s2
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_v8i32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw s0, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 8(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset s0, -4
; RV32-NEXT:    .cfi_offset s1, -8
; RV32-NEXT:    .cfi_offset s2, -12
; RV32-NEXT:    lw a7, 16(a3)
; RV32-NEXT:    lw a6, 20(a3)
; RV32-NEXT:    lw a5, 24(a3)
; RV32-NEXT:    lw a4, 28(a3)
; RV32-NEXT:    lw t0, 16(a2)
; RV32-NEXT:    lw t1, 20(a2)
; RV32-NEXT:    lw t2, 24(a2)
; RV32-NEXT:    lw t3, 28(a2)
; RV32-NEXT:    lw t4, 0(a3)
; RV32-NEXT:    lw t5, 4(a3)
; RV32-NEXT:    lw t6, 8(a3)
; RV32-NEXT:    lw a3, 12(a3)
; RV32-NEXT:    lw s0, 0(a2)
; RV32-NEXT:    lw s1, 4(a2)
; RV32-NEXT:    lw s2, 8(a2)
; RV32-NEXT:    lw a2, 12(a2)
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    xor s0, s0, t4
; RV32-NEXT:    xor s1, s1, t5
; RV32-NEXT:    xor s2, s2, t6
; RV32-NEXT:    xor a2, a2, a3
; RV32-NEXT:    xor t0, t0, a7
; RV32-NEXT:    xor t1, t1, a6
; RV32-NEXT:    xor t2, t2, a5
; RV32-NEXT:    xor t3, t3, a4
; RV32-NEXT:    and s0, s0, a1
; RV32-NEXT:    and s1, s1, a1
; RV32-NEXT:    and s2, s2, a1
; RV32-NEXT:    and a2, a2, a1
; RV32-NEXT:    and t0, t0, a1
; RV32-NEXT:    and t1, t1, a1
; RV32-NEXT:    and t2, t2, a1
; RV32-NEXT:    and a1, t3, a1
; RV32-NEXT:    xor t3, t4, s0
; RV32-NEXT:    xor t4, t5, s1
; RV32-NEXT:    xor t5, t6, s2
; RV32-NEXT:    xor a2, a3, a2
; RV32-NEXT:    xor a3, a7, t0
; RV32-NEXT:    xor a6, a6, t1
; RV32-NEXT:    xor a5, a5, t2
; RV32-NEXT:    xor a1, a4, a1
; RV32-NEXT:    sw a3, 16(a0)
; RV32-NEXT:    sw a6, 20(a0)
; RV32-NEXT:    sw a5, 24(a0)
; RV32-NEXT:    sw a1, 28(a0)
; RV32-NEXT:    sw t3, 0(a0)
; RV32-NEXT:    sw t4, 4(a0)
; RV32-NEXT:    sw t5, 8(a0)
; RV32-NEXT:    sw a2, 12(a0)
; RV32-NEXT:    lw s0, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 8(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
  %result = call <8 x i32> @llvm.ct.select.v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %result
}

; Declare the intrinsics
declare i1 @llvm.ct.select.i1(i1, i1, i1)
declare i8 @llvm.ct.select.i8(i1, i8, i8)
declare i16 @llvm.ct.select.i16(i1, i16, i16)
declare i32 @llvm.ct.select.i32(i1, i32, i32)
declare i64 @llvm.ct.select.i64(i1, i64, i64)
declare ptr @llvm.ct.select.p0(i1, ptr, ptr)
declare float @llvm.ct.select.f32(i1, float, float)
declare double @llvm.ct.select.f64(i1, double, double)

; Vector intrinsics
declare <4 x i32> @llvm.ct.select.v4i32(i1, <4 x i32>, <4 x i32>)
declare <2 x i64> @llvm.ct.select.v2i64(i1, <2 x i64>, <2 x i64>)
declare <8 x i16> @llvm.ct.select.v8i16(i1, <8 x i16>, <8 x i16>)
declare <16 x i8> @llvm.ct.select.v16i8(i1, <16 x i8>, <16 x i8>)
declare <4 x float> @llvm.ct.select.v4f32(i1, <4 x float>, <4 x float>)
declare <2 x double> @llvm.ct.select.v2f64(i1, <2 x double>, <2 x double>)
declare <8 x i32> @llvm.ct.select.v8i32(i1, <8 x i32>, <8 x i32>)
