; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=riscv32 -mattr=+m | FileCheck %s --check-prefixes=RISCV32

define { i128, i8 } @muloti_test(i128 %l, i128 %r) #0 {
; RISCV32-LABEL: muloti_test:
; RISCV32:       # %bb.0: # %overflow.entry
; RISCV32-NEXT:    addi sp, sp, -32
; RISCV32-NEXT:    sw s0, 28(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s1, 24(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s2, 20(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s3, 16(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s4, 12(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s5, 8(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s6, 4(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    sw s7, 0(sp) # 4-byte Folded Spill
; RISCV32-NEXT:    lw a3, 0(a1)
; RISCV32-NEXT:    lw t0, 4(a1)
; RISCV32-NEXT:    lw a4, 8(a1)
; RISCV32-NEXT:    lw a6, 12(a1)
; RISCV32-NEXT:    lw a1, 0(a2)
; RISCV32-NEXT:    lw a7, 4(a2)
; RISCV32-NEXT:    lw a5, 8(a2)
; RISCV32-NEXT:    lw a2, 12(a2)
; RISCV32-NEXT:    or t4, a4, a6
; RISCV32-NEXT:    beqz t4, .LBB0_5
; RISCV32-NEXT:  # %bb.1: # %overflow.lhs
; RISCV32-NEXT:    or t5, a5, a2
; RISCV32-NEXT:    beqz t5, .LBB0_9
; RISCV32-NEXT:  # %bb.2: # %overflow
; RISCV32-NEXT:    mulhu t1, a3, a1
; RISCV32-NEXT:    mul t2, t0, a1
; RISCV32-NEXT:    mulhu t3, t0, a1
; RISCV32-NEXT:    mul t6, a3, a7
; RISCV32-NEXT:    mulhu s0, a3, a7
; RISCV32-NEXT:    mul s4, t0, a7
; RISCV32-NEXT:    mul s1, a5, a3
; RISCV32-NEXT:    mul s5, a4, a1
; RISCV32-NEXT:    mul s2, t0, a5
; RISCV32-NEXT:    mul s3, a2, a3
; RISCV32-NEXT:    mul s6, a7, a4
; RISCV32-NEXT:    add s3, s3, s2
; RISCV32-NEXT:    mul s2, a6, a1
; RISCV32-NEXT:    add s6, s2, s6
; RISCV32-NEXT:    mulhu s7, t0, a7
; RISCV32-NEXT:    add t1, t2, t1
; RISCV32-NEXT:    sltu t2, t1, t2
; RISCV32-NEXT:    add t2, t3, t2
; RISCV32-NEXT:    mulhu s2, a5, a3
; RISCV32-NEXT:    add t1, t6, t1
; RISCV32-NEXT:    sltu t3, t1, t6
; RISCV32-NEXT:    add t3, s0, t3
; RISCV32-NEXT:    mulhu s0, a4, a1
; RISCV32-NEXT:    add t6, s5, s1
; RISCV32-NEXT:    add s3, s2, s3
; RISCV32-NEXT:    add s1, s0, s6
; RISCV32-NEXT:    sltu s5, t6, s5
; RISCV32-NEXT:    add t3, t2, t3
; RISCV32-NEXT:    sltu t2, t3, t2
; RISCV32-NEXT:    add s7, s7, t2
; RISCV32-NEXT:    add s6, s1, s3
; RISCV32-NEXT:    add t3, s4, t3
; RISCV32-NEXT:    add t2, t3, t6
; RISCV32-NEXT:    sltu s4, t3, s4
; RISCV32-NEXT:    sltu t6, t2, t3
; RISCV32-NEXT:    add s4, s7, s4
; RISCV32-NEXT:    add s5, s6, s5
; RISCV32-NEXT:    add t3, s4, s5
; RISCV32-NEXT:    add t3, t3, t6
; RISCV32-NEXT:    beq t3, s4, .LBB0_4
; RISCV32-NEXT:  # %bb.3: # %overflow
; RISCV32-NEXT:    sltu t6, t3, s4
; RISCV32-NEXT:  .LBB0_4: # %overflow
; RISCV32-NEXT:    sltu s2, s3, s2
; RISCV32-NEXT:    snez s3, t0
; RISCV32-NEXT:    snez s4, a2
; RISCV32-NEXT:    mulhu a2, a2, a3
; RISCV32-NEXT:    mulhu a5, t0, a5
; RISCV32-NEXT:    sltu t0, s1, s0
; RISCV32-NEXT:    snez s0, a7
; RISCV32-NEXT:    snez s1, a6
; RISCV32-NEXT:    mulhu a6, a6, a1
; RISCV32-NEXT:    mulhu a4, a7, a4
; RISCV32-NEXT:    snez a7, t5
; RISCV32-NEXT:    snez t4, t4
; RISCV32-NEXT:    and t5, s4, s3
; RISCV32-NEXT:    snez a2, a2
; RISCV32-NEXT:    snez a5, a5
; RISCV32-NEXT:    and s0, s1, s0
; RISCV32-NEXT:    snez a6, a6
; RISCV32-NEXT:    snez a4, a4
; RISCV32-NEXT:    and a7, t4, a7
; RISCV32-NEXT:    or a2, t5, a2
; RISCV32-NEXT:    or a6, s0, a6
; RISCV32-NEXT:    or a2, a2, a5
; RISCV32-NEXT:    or a4, a6, a4
; RISCV32-NEXT:    or a2, a2, s2
; RISCV32-NEXT:    or a4, a4, t0
; RISCV32-NEXT:    or a4, a7, a4
; RISCV32-NEXT:    or a2, a4, a2
; RISCV32-NEXT:    or t4, a2, t6
; RISCV32-NEXT:    j .LBB0_14
; RISCV32-NEXT:  .LBB0_5: # %overflow.no.lhs
; RISCV32-NEXT:    or t1, a5, a2
; RISCV32-NEXT:    beqz t1, .LBB0_13
; RISCV32-NEXT:  # %bb.6: # %overflow.no.lhs.only
; RISCV32-NEXT:    mulhu t1, a3, a1
; RISCV32-NEXT:    mul t6, t0, a1
; RISCV32-NEXT:    mulhu s0, t0, a1
; RISCV32-NEXT:    mul t4, a3, a7
; RISCV32-NEXT:    mulhu t5, a3, a7
; RISCV32-NEXT:    mul t2, t0, a7
; RISCV32-NEXT:    mulhu t3, t0, a7
; RISCV32-NEXT:    mulhu s1, a1, a4
; RISCV32-NEXT:    mul s2, a1, a6
; RISCV32-NEXT:    mul a7, a7, a4
; RISCV32-NEXT:    add s1, s1, s2
; RISCV32-NEXT:    mulhu s2, a5, a4
; RISCV32-NEXT:    mul a6, a5, a6
; RISCV32-NEXT:    add a6, s2, a6
; RISCV32-NEXT:    mulhu s2, a3, a5
; RISCV32-NEXT:    add a7, s1, a7
; RISCV32-NEXT:    mul s1, a2, a4
; RISCV32-NEXT:    add a6, a6, s1
; RISCV32-NEXT:    mul s1, t0, a5
; RISCV32-NEXT:    add t1, t6, t1
; RISCV32-NEXT:    sltu t6, t1, t6
; RISCV32-NEXT:    add t6, s0, t6
; RISCV32-NEXT:    mulhu s0, t0, a5
; RISCV32-NEXT:    add s2, s1, s2
; RISCV32-NEXT:    sltu s1, s2, s1
; RISCV32-NEXT:    add s0, s0, s1
; RISCV32-NEXT:    mul s1, a3, a2
; RISCV32-NEXT:    add t1, t4, t1
; RISCV32-NEXT:    sltu t4, t1, t4
; RISCV32-NEXT:    add t4, t5, t4
; RISCV32-NEXT:    mul t5, t0, a2
; RISCV32-NEXT:    mulhu t0, t0, a2
; RISCV32-NEXT:    mulhu a2, a3, a2
; RISCV32-NEXT:    add s2, s1, s2
; RISCV32-NEXT:    sltu s1, s2, s1
; RISCV32-NEXT:    add a2, a2, s1
; RISCV32-NEXT:    mul s1, a1, a4
; RISCV32-NEXT:    mul a4, a5, a4
; RISCV32-NEXT:    mul a5, a3, a5
; RISCV32-NEXT:    add t4, t6, t4
; RISCV32-NEXT:    add a2, s0, a2
; RISCV32-NEXT:    sltu t6, t4, t6
; RISCV32-NEXT:    add t4, t2, t4
; RISCV32-NEXT:    sltu s0, a2, s0
; RISCV32-NEXT:    add s3, t5, a2
; RISCV32-NEXT:    add s1, t4, s1
; RISCV32-NEXT:    sltu t2, t4, t2
; RISCV32-NEXT:    add t3, t3, t6
; RISCV32-NEXT:    add a2, s3, a4
; RISCV32-NEXT:    sltu a4, s3, t5
; RISCV32-NEXT:    add t0, t0, s0
; RISCV32-NEXT:    sltu t4, s1, t4
; RISCV32-NEXT:    add t3, t3, t2
; RISCV32-NEXT:    sltu t5, a2, s3
; RISCV32-NEXT:    add a4, t0, a4
; RISCV32-NEXT:    add t2, s1, a5
; RISCV32-NEXT:    add a7, t3, a7
; RISCV32-NEXT:    add a5, a4, a6
; RISCV32-NEXT:    sltu a4, t2, s1
; RISCV32-NEXT:    add a6, a7, t4
; RISCV32-NEXT:    add t3, s2, a4
; RISCV32-NEXT:    add t3, a6, t3
; RISCV32-NEXT:    add a5, a5, t5
; RISCV32-NEXT:    beq t3, a6, .LBB0_8
; RISCV32-NEXT:  # %bb.7: # %overflow.no.lhs.only
; RISCV32-NEXT:    sltu a4, t3, a6
; RISCV32-NEXT:  .LBB0_8: # %overflow.no.lhs.only
; RISCV32-NEXT:    mul a1, a3, a1
; RISCV32-NEXT:    j .LBB0_12
; RISCV32-NEXT:  .LBB0_9: # %overflow.no.rhs.only
; RISCV32-NEXT:    mulhu t1, a1, a3
; RISCV32-NEXT:    mul t6, a7, a3
; RISCV32-NEXT:    mulhu s0, a7, a3
; RISCV32-NEXT:    mul t4, a1, t0
; RISCV32-NEXT:    mulhu t5, a1, t0
; RISCV32-NEXT:    mul t2, a7, t0
; RISCV32-NEXT:    mulhu t3, a7, t0
; RISCV32-NEXT:    mulhu s1, a3, a5
; RISCV32-NEXT:    mul s2, a3, a2
; RISCV32-NEXT:    mul t0, t0, a5
; RISCV32-NEXT:    add s1, s1, s2
; RISCV32-NEXT:    mulhu s2, a4, a5
; RISCV32-NEXT:    mul a2, a4, a2
; RISCV32-NEXT:    add a2, s2, a2
; RISCV32-NEXT:    mulhu s2, a1, a4
; RISCV32-NEXT:    add t0, s1, t0
; RISCV32-NEXT:    mul s1, a6, a5
; RISCV32-NEXT:    add s1, a2, s1
; RISCV32-NEXT:    mul a2, a7, a4
; RISCV32-NEXT:    add t1, t6, t1
; RISCV32-NEXT:    sltu t6, t1, t6
; RISCV32-NEXT:    add t6, s0, t6
; RISCV32-NEXT:    mulhu s0, a7, a4
; RISCV32-NEXT:    add s2, a2, s2
; RISCV32-NEXT:    sltu a2, s2, a2
; RISCV32-NEXT:    add a2, s0, a2
; RISCV32-NEXT:    mul s0, a1, a6
; RISCV32-NEXT:    add t1, t4, t1
; RISCV32-NEXT:    sltu t4, t1, t4
; RISCV32-NEXT:    add t4, t5, t4
; RISCV32-NEXT:    mul t5, a7, a6
; RISCV32-NEXT:    mulhu a7, a7, a6
; RISCV32-NEXT:    mulhu a6, a1, a6
; RISCV32-NEXT:    add s2, s0, s2
; RISCV32-NEXT:    sltu s0, s2, s0
; RISCV32-NEXT:    add a6, a6, s0
; RISCV32-NEXT:    mul s0, a3, a5
; RISCV32-NEXT:    mul a5, a4, a5
; RISCV32-NEXT:    mul a4, a1, a4
; RISCV32-NEXT:    add t4, t6, t4
; RISCV32-NEXT:    add a6, a2, a6
; RISCV32-NEXT:    sltu t6, t4, t6
; RISCV32-NEXT:    add t4, t2, t4
; RISCV32-NEXT:    sltu s3, a6, a2
; RISCV32-NEXT:    add a6, t5, a6
; RISCV32-NEXT:    add s0, t4, s0
; RISCV32-NEXT:    sltu t2, t4, t2
; RISCV32-NEXT:    add t3, t3, t6
; RISCV32-NEXT:    add a2, a6, a5
; RISCV32-NEXT:    sltu a5, a6, t5
; RISCV32-NEXT:    add a7, a7, s3
; RISCV32-NEXT:    sltu t4, s0, t4
; RISCV32-NEXT:    add t3, t3, t2
; RISCV32-NEXT:    sltu t5, a2, a6
; RISCV32-NEXT:    add a5, a7, a5
; RISCV32-NEXT:    add t2, s0, a4
; RISCV32-NEXT:    add a6, t3, t0
; RISCV32-NEXT:    add a5, a5, s1
; RISCV32-NEXT:    sltu a4, t2, s0
; RISCV32-NEXT:    add a6, a6, t4
; RISCV32-NEXT:    add t3, s2, a4
; RISCV32-NEXT:    add t3, a6, t3
; RISCV32-NEXT:    add a5, a5, t5
; RISCV32-NEXT:    beq t3, a6, .LBB0_11
; RISCV32-NEXT:  # %bb.10: # %overflow.no.rhs.only
; RISCV32-NEXT:    sltu a4, t3, a6
; RISCV32-NEXT:  .LBB0_11: # %overflow.no.rhs.only
; RISCV32-NEXT:    mul a1, a1, a3
; RISCV32-NEXT:  .LBB0_12: # %overflow.res
; RISCV32-NEXT:    add a4, a2, a4
; RISCV32-NEXT:    sltu a2, a4, a2
; RISCV32-NEXT:    add a2, a5, a2
; RISCV32-NEXT:    or a2, a4, a2
; RISCV32-NEXT:    snez t4, a2
; RISCV32-NEXT:    j .LBB0_15
; RISCV32-NEXT:  .LBB0_13: # %overflow.no
; RISCV32-NEXT:    li t4, 0
; RISCV32-NEXT:    mulhu t1, a3, a1
; RISCV32-NEXT:    mul t2, t0, a1
; RISCV32-NEXT:    mulhu t3, t0, a1
; RISCV32-NEXT:    mul t5, a3, a7
; RISCV32-NEXT:    mulhu t6, a3, a7
; RISCV32-NEXT:    mul s0, t0, a7
; RISCV32-NEXT:    mul s1, a5, t0
; RISCV32-NEXT:    mulhu s2, a5, a3
; RISCV32-NEXT:    add s1, s2, s1
; RISCV32-NEXT:    mul s2, a1, a4
; RISCV32-NEXT:    mul a5, a5, a3
; RISCV32-NEXT:    mulhu t0, t0, a7
; RISCV32-NEXT:    mul a2, a2, a3
; RISCV32-NEXT:    mul a7, a7, a4
; RISCV32-NEXT:    mulhu a4, a1, a4
; RISCV32-NEXT:    mul a6, a1, a6
; RISCV32-NEXT:    add t1, t2, t1
; RISCV32-NEXT:    add s2, a5, s2
; RISCV32-NEXT:    add a4, a4, a6
; RISCV32-NEXT:    sltu a6, t1, t2
; RISCV32-NEXT:    add t1, t5, t1
; RISCV32-NEXT:    add a2, s1, a2
; RISCV32-NEXT:    add a4, a4, a7
; RISCV32-NEXT:    sltu a5, s2, a5
; RISCV32-NEXT:    add a6, t3, a6
; RISCV32-NEXT:    sltu a7, t1, t5
; RISCV32-NEXT:    add a2, a2, a4
; RISCV32-NEXT:    add a7, t6, a7
; RISCV32-NEXT:    add a2, a2, a5
; RISCV32-NEXT:    add a7, a6, a7
; RISCV32-NEXT:    add a4, s0, a7
; RISCV32-NEXT:    sltu a5, a7, a6
; RISCV32-NEXT:    add t2, a4, s2
; RISCV32-NEXT:    sltu a6, a4, s0
; RISCV32-NEXT:    add a5, t0, a5
; RISCV32-NEXT:    sltu t3, t2, a4
; RISCV32-NEXT:    add a5, a5, a6
; RISCV32-NEXT:    add a2, a5, a2
; RISCV32-NEXT:    add t3, a2, t3
; RISCV32-NEXT:  .LBB0_14: # %overflow.res
; RISCV32-NEXT:    mul a1, a3, a1
; RISCV32-NEXT:  .LBB0_15: # %overflow.res
; RISCV32-NEXT:    andi a2, t4, 1
; RISCV32-NEXT:    sw a1, 0(a0)
; RISCV32-NEXT:    sw t1, 4(a0)
; RISCV32-NEXT:    sw t2, 8(a0)
; RISCV32-NEXT:    sw t3, 12(a0)
; RISCV32-NEXT:    sb a2, 16(a0)
; RISCV32-NEXT:    lw s0, 28(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s1, 24(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s2, 20(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s3, 16(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s4, 12(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s5, 8(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s6, 4(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    lw s7, 0(sp) # 4-byte Folded Reload
; RISCV32-NEXT:    addi sp, sp, 32
; RISCV32-NEXT:    ret
start:
  %0 = tail call { i128, i1 } @llvm.umul.with.overflow.i128(i128 %l, i128 %r) #2
  %1 = extractvalue { i128, i1 } %0, 0
  %2 = extractvalue { i128, i1 } %0, 1
  %3 = zext i1 %2 to i8
  %4 = insertvalue { i128, i8 } undef, i128 %1, 0
  %5 = insertvalue { i128, i8 } %4, i8 %3, 1
  ret { i128, i8 } %5
}

; Function Attrs: nounwind readnone speculatable
declare { i128, i1 } @llvm.umul.with.overflow.i128(i128, i128) #1

attributes #0 = { nounwind readnone }
attributes #1 = { nounwind readnone speculatable }
attributes #2 = { nounwind }
