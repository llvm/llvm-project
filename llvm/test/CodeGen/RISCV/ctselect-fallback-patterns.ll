; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv64 -O3 | FileCheck %s --check-prefix=RV64
; RUN: llc < %s -mtriple=riscv32 -O3 | FileCheck %s --check-prefix=RV32

; Test smin(x, 0) pattern
define i32 @test_ctselect_smin_zero(i32 %x) {
; RV64-LABEL: test_ctselect_smin_zero:
; RV64:       # %bb.0:
; RV64-NEXT:    sraiw a1, a0, 31
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_smin_zero:
; RV32:       # %bb.0:
; RV32-NEXT:    srai a1, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
  %cmp = icmp slt i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 0)
  ret i32 %result
}

; Test smax(x, 0) pattern
define i32 @test_ctselect_smax_zero(i32 %x) {
; RV64-LABEL: test_ctselect_smax_zero:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a0
; RV64-NEXT:    sgtz a1, a1
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_smax_zero:
; RV32:       # %bb.0:
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
  %cmp = icmp sgt i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 0)
  ret i32 %result
}

; Test generic smin pattern
define i32 @test_ctselect_smin_generic(i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_smin_generic:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a2, a1
; RV64-NEXT:    sext.w a3, a0
; RV64-NEXT:    slt a2, a3, a2
; RV64-NEXT:    addi a3, a2, -1
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a1, a3, a1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_smin_generic:
; RV32:       # %bb.0:
; RV32-NEXT:    slt a2, a0, a1
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %cmp = icmp slt i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 %y)
  ret i32 %result
}

; Test generic smax pattern
define i32 @test_ctselect_smax_generic(i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_smax_generic:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a2, a0
; RV64-NEXT:    sext.w a3, a1
; RV64-NEXT:    slt a2, a3, a2
; RV64-NEXT:    addi a3, a2, -1
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a1, a3, a1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_smax_generic:
; RV32:       # %bb.0:
; RV32-NEXT:    slt a2, a1, a0
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %cmp = icmp sgt i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 %y)
  ret i32 %result
}

; Test umin pattern
define i32 @test_ctselect_umin_generic(i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_umin_generic:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a2, a1
; RV64-NEXT:    sext.w a3, a0
; RV64-NEXT:    sltu a2, a3, a2
; RV64-NEXT:    addi a3, a2, -1
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a1, a3, a1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_umin_generic:
; RV32:       # %bb.0:
; RV32-NEXT:    sltu a2, a0, a1
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %cmp = icmp ult i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 %y)
  ret i32 %result
}

; Test umax pattern
define i32 @test_ctselect_umax_generic(i32 %x, i32 %y) {
; RV64-LABEL: test_ctselect_umax_generic:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a2, a0
; RV64-NEXT:    sext.w a3, a1
; RV64-NEXT:    sltu a2, a3, a2
; RV64-NEXT:    addi a3, a2, -1
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a1, a3, a1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_umax_generic:
; RV32:       # %bb.0:
; RV32-NEXT:    sltu a2, a1, a0
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %cmp = icmp ugt i32 %x, %y
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 %y)
  ret i32 %result
}

; Test abs pattern
define i32 @test_ctselect_abs(i32 %x) {
; RV64-LABEL: test_ctselect_abs:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a0
; RV64-NEXT:    negw a2, a0
; RV64-NEXT:    slti a1, a1, 0
; RV64-NEXT:    negw a3, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a2, a3, a2
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    or a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_abs:
; RV32:       # %bb.0:
; RV32-NEXT:    neg a1, a0
; RV32-NEXT:    srai a2, a0, 31
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    ret
  %neg = sub i32 0, %x
  %cmp = icmp slt i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %neg, i32 %x)
  ret i32 %result
}

; Test nabs pattern (negative abs)
define i32 @test_ctselect_nabs(i32 %x) {
; RV64-LABEL: test_ctselect_nabs:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a0
; RV64-NEXT:    negw a2, a0
; RV64-NEXT:    slti a1, a1, 0
; RV64-NEXT:    addi a3, a1, -1
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a2, a3, a2
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    or a0, a0, a2
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_nabs:
; RV32:       # %bb.0:
; RV32-NEXT:    neg a1, a0
; RV32-NEXT:    srai a2, a0, 31
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %neg = sub i32 0, %x
  %cmp = icmp slt i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 %x, i32 %neg)
  ret i32 %result
}

; Test sign extension pattern
define i32 @test_ctselect_sign_extend(i32 %x) {
; RV64-LABEL: test_ctselect_sign_extend:
; RV64:       # %bb.0:
; RV64-NEXT:    sraiw a0, a0, 31
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_sign_extend:
; RV32:       # %bb.0:
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    ret
  %cmp = icmp slt i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 -1, i32 0)
  ret i32 %result
}

; Test zero extension pattern
define i32 @test_ctselect_zero_extend(i32 %x) {
; RV64-LABEL: test_ctselect_zero_extend:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a0, a0
; RV64-NEXT:    snez a0, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_zero_extend:
; RV32:       # %bb.0:
; RV32-NEXT:    seqz a0, a0
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    ret
  %cmp = icmp ne i32 %x, 0
  %result = call i32 @llvm.ct.select.i32(i1 %cmp, i32 1, i32 0)
  ret i32 %result
}

; Test constant folding with known condition
define i32 @test_ctselect_constant_folding_true(i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_constant_folding_true:
; RV64:       # %bb.0:
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_constant_folding_true:
; RV32:       # %bb.0:
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 true, i32 %a, i32 %b)
  ret i32 %result
}

define i32 @test_ctselect_constant_folding_false(i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_constant_folding_false:
; RV64:       # %bb.0:
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_constant_folding_false:
; RV32:       # %bb.0:
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 false, i32 %a, i32 %b)
  ret i32 %result
}

; Test with identical operands
define i32 @test_ctselect_identical_operands(i1 %cond, i32 %x) {
; RV64-LABEL: test_ctselect_identical_operands:
; RV64:       # %bb.0:
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_identical_operands:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a2, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a1
; RV32-NEXT:    or a0, a2, a0
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %x, i32 %x)
  ret i32 %result
}

; Test with inverted condition
define i32 @test_ctselect_inverted_condition(i32 %x, i32 %y, i32 %a, i32 %b) {
; RV64-LABEL: test_ctselect_inverted_condition:
; RV64:       # %bb.0:
; RV64-NEXT:    sext.w a1, a1
; RV64-NEXT:    sext.w a0, a0
; RV64-NEXT:    xor a0, a0, a1
; RV64-NEXT:    snez a0, a0
; RV64-NEXT:    addi a1, a0, -1
; RV64-NEXT:    neg a0, a0
; RV64-NEXT:    and a1, a1, a3
; RV64-NEXT:    and a0, a0, a2
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_inverted_condition:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a0, a0, a1
; RV32-NEXT:    seqz a0, a0
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a2, a0, a2
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    or a0, a2, a0
; RV32-NEXT:    ret
  %cmp = icmp eq i32 %x, %y
  %not_cmp = xor i1 %cmp, true
  %result = call i32 @llvm.ct.select.i32(i1 %not_cmp, i32 %a, i32 %b)
  ret i32 %result
}

; Test chain of ct.select operations
define i32 @test_ctselect_chain(i1 %c1, i1 %c2, i1 %c3, i32 %a, i32 %b, i32 %c, i32 %d) {
; RV64-LABEL: test_ctselect_chain:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    andi a1, a1, 1
; RV64-NEXT:    andi a2, a2, 1
; RV64-NEXT:    addi a7, a0, -1
; RV64-NEXT:    neg a0, a0
; RV64-NEXT:    and a4, a7, a4
; RV64-NEXT:    neg a7, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    neg a3, a2
; RV64-NEXT:    addi a2, a2, -1
; RV64-NEXT:    and a1, a1, a5
; RV64-NEXT:    or a0, a0, a4
; RV64-NEXT:    and a0, a7, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    and a0, a3, a0
; RV64-NEXT:    and a1, a2, a6
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_chain:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    andi a1, a1, 1
; RV32-NEXT:    andi a2, a2, 1
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    slli a2, a2, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    srai a2, a2, 31
; RV32-NEXT:    and a3, a0, a3
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    not a4, a1
; RV32-NEXT:    and a4, a4, a5
; RV32-NEXT:    not a5, a2
; RV32-NEXT:    or a0, a3, a0
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    or a0, a0, a4
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    and a1, a5, a6
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %sel1 = call i32 @llvm.ct.select.i32(i1 %c1, i32 %a, i32 %b)
  %sel2 = call i32 @llvm.ct.select.i32(i1 %c2, i32 %sel1, i32 %c)
  %sel3 = call i32 @llvm.ct.select.i32(i1 %c3, i32 %sel2, i32 %d)
  ret i32 %sel3
}

; Test for 64-bit operations (supported on all 64-bit architectures)
define i64 @test_ctselect_i64_smin_zero(i64 %x) {
; RV64-LABEL: test_ctselect_i64_smin_zero:
; RV64:       # %bb.0:
; RV64-NEXT:    srai a1, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_i64_smin_zero:
; RV32:       # %bb.0:
; RV32-NEXT:    srai a2, a1, 31
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    ret
  %cmp = icmp slt i64 %x, 0
  %result = call i64 @llvm.ct.select.i64(i1 %cmp, i64 %x, i64 0)
  ret i64 %result
}

; Declare the intrinsics
declare i32 @llvm.ct.select.i32(i1, i32, i32)
declare i64 @llvm.ct.select.i64(i1, i64, i64)
