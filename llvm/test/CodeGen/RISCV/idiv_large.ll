; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=riscv32 < %s | FileCheck %s --check-prefix=RV32
; RUN: llc -mtriple=riscv64 < %s | FileCheck %s --check-prefix=RV64

define i64 @udiv_i64(i64 %x, i64 %y) nounwind {
; RV32-LABEL: udiv_i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    call __udivdi3
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: udiv_i64:
; RV64:       # %bb.0:
; RV64-NEXT:    tail __udivdi3
  %res = udiv i64 %x, %y
  ret i64 %res
}

define i65 @udiv_i65(i65 %x, i65 %y) nounwind {
; RV32-LABEL: udiv_i65:
; RV32:       # %bb.0: # %_udiv-special-cases
; RV32-NEXT:    lw a3, 4(a2)
; RV32-NEXT:    lw t1, 8(a2)
; RV32-NEXT:    lw a2, 0(a2)
; RV32-NEXT:    lui a4, 349525
; RV32-NEXT:    lui a5, 209715
; RV32-NEXT:    lui a7, 61681
; RV32-NEXT:    addi t0, a4, 1365
; RV32-NEXT:    srli a4, a3, 1
; RV32-NEXT:    slli t2, t1, 31
; RV32-NEXT:    addi a6, a5, 819
; RV32-NEXT:    or t3, t2, a4
; RV32-NEXT:    slli a4, a3, 31
; RV32-NEXT:    srli a5, a2, 1
; RV32-NEXT:    addi a7, a7, -241
; RV32-NEXT:    or t5, a5, a4
; RV32-NEXT:    bnez t3, .LBB1_2
; RV32-NEXT:  # %bb.1: # %_udiv-special-cases
; RV32-NEXT:    srli a4, t5, 1
; RV32-NEXT:    or a4, t5, a4
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 8
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 16
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    not a4, a4
; RV32-NEXT:    srli a5, a4, 1
; RV32-NEXT:    and a5, a5, t0
; RV32-NEXT:    sub a4, a4, a5
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    and a4, a4, a6
; RV32-NEXT:    and a5, a5, a6
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    and a4, a4, a7
; RV32-NEXT:    slli a5, a4, 8
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    slli a5, a4, 16
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli a4, a4, 24
; RV32-NEXT:    addi t2, a4, 32
; RV32-NEXT:    j .LBB1_3
; RV32-NEXT:  .LBB1_2:
; RV32-NEXT:    srli a4, t3, 1
; RV32-NEXT:    or a4, t3, a4
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 8
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 16
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    not a4, a4
; RV32-NEXT:    srli a5, a4, 1
; RV32-NEXT:    and a5, a5, t0
; RV32-NEXT:    sub a4, a4, a5
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    and a4, a4, a6
; RV32-NEXT:    and a5, a5, a6
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    and a4, a4, a7
; RV32-NEXT:    slli a5, a4, 8
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    slli a5, a4, 16
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli t2, a4, 24
; RV32-NEXT:  .LBB1_3: # %_udiv-special-cases
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    sw s0, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 72(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 68(sp) # 4-byte Folded Spill
; RV32-NEXT:    slli a4, a2, 31
; RV32-NEXT:    li t4, 64
; RV32-NEXT:    bnez a4, .LBB1_5
; RV32-NEXT:  # %bb.4: # %_udiv-special-cases
; RV32-NEXT:    li s0, 64
; RV32-NEXT:    j .LBB1_6
; RV32-NEXT:  .LBB1_5:
; RV32-NEXT:    srli a5, a4, 1
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 8
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    srli a5, a4, 16
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    not a4, a4
; RV32-NEXT:    srli a5, a4, 1
; RV32-NEXT:    and a5, a5, t0
; RV32-NEXT:    sub a4, a4, a5
; RV32-NEXT:    srli a5, a4, 2
; RV32-NEXT:    and a4, a4, a6
; RV32-NEXT:    and a5, a5, a6
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli a5, a4, 4
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    and a4, a4, a7
; RV32-NEXT:    slli a5, a4, 8
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    slli a5, a4, 16
; RV32-NEXT:    add a4, a4, a5
; RV32-NEXT:    srli s0, a4, 24
; RV32-NEXT:  .LBB1_6: # %_udiv-special-cases
; RV32-NEXT:    lw a5, 0(a1)
; RV32-NEXT:    lw a4, 4(a1)
; RV32-NEXT:    lw a1, 8(a1)
; RV32-NEXT:    or t3, t5, t3
; RV32-NEXT:    addi s2, s0, 64
; RV32-NEXT:    bnez t3, .LBB1_8
; RV32-NEXT:  # %bb.7: # %_udiv-special-cases
; RV32-NEXT:    mv t2, s2
; RV32-NEXT:  .LBB1_8: # %_udiv-special-cases
; RV32-NEXT:    srli t5, a4, 1
; RV32-NEXT:    slli t6, a1, 31
; RV32-NEXT:    or t5, t6, t5
; RV32-NEXT:    slli t6, a4, 31
; RV32-NEXT:    srli s1, a5, 1
; RV32-NEXT:    snez s3, t3
; RV32-NEXT:    or t6, s1, t6
; RV32-NEXT:    bnez t5, .LBB1_10
; RV32-NEXT:  # %bb.9: # %_udiv-special-cases
; RV32-NEXT:    srli t3, t6, 1
; RV32-NEXT:    or t3, t6, t3
; RV32-NEXT:    srli s1, t3, 2
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 4
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 8
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 16
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    not t3, t3
; RV32-NEXT:    srli s1, t3, 1
; RV32-NEXT:    and s1, s1, t0
; RV32-NEXT:    sub t3, t3, s1
; RV32-NEXT:    srli s1, t3, 2
; RV32-NEXT:    and t3, t3, a6
; RV32-NEXT:    and s1, s1, a6
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    srli s1, t3, 4
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    and t3, t3, a7
; RV32-NEXT:    slli s1, t3, 8
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    slli s1, t3, 16
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    srli t3, t3, 24
; RV32-NEXT:    addi s1, t3, 32
; RV32-NEXT:    j .LBB1_11
; RV32-NEXT:  .LBB1_10:
; RV32-NEXT:    srli t3, t5, 1
; RV32-NEXT:    or t3, t5, t3
; RV32-NEXT:    srli s1, t3, 2
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 4
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 8
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    srli s1, t3, 16
; RV32-NEXT:    or t3, t3, s1
; RV32-NEXT:    not t3, t3
; RV32-NEXT:    srli s1, t3, 1
; RV32-NEXT:    and s1, s1, t0
; RV32-NEXT:    sub t3, t3, s1
; RV32-NEXT:    srli s1, t3, 2
; RV32-NEXT:    and t3, t3, a6
; RV32-NEXT:    and s1, s1, a6
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    srli s1, t3, 4
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    and t3, t3, a7
; RV32-NEXT:    slli s1, t3, 8
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    slli s1, t3, 16
; RV32-NEXT:    add t3, t3, s1
; RV32-NEXT:    srli s1, t3, 24
; RV32-NEXT:  .LBB1_11: # %_udiv-special-cases
; RV32-NEXT:    andi t3, a1, 1
; RV32-NEXT:    andi a1, t1, 1
; RV32-NEXT:    or t1, a2, a3
; RV32-NEXT:    sltu s0, s2, s0
; RV32-NEXT:    slli s4, a5, 31
; RV32-NEXT:    or s2, a5, a4
; RV32-NEXT:    addi s3, s3, -1
; RV32-NEXT:    beqz s4, .LBB1_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    srli t4, s4, 1
; RV32-NEXT:    or t4, s4, t4
; RV32-NEXT:    srli s4, t4, 2
; RV32-NEXT:    or t4, t4, s4
; RV32-NEXT:    srli s4, t4, 4
; RV32-NEXT:    or t4, t4, s4
; RV32-NEXT:    srli s4, t4, 8
; RV32-NEXT:    or t4, t4, s4
; RV32-NEXT:    srli s4, t4, 16
; RV32-NEXT:    or t4, t4, s4
; RV32-NEXT:    not t4, t4
; RV32-NEXT:    srli s4, t4, 1
; RV32-NEXT:    and t0, s4, t0
; RV32-NEXT:    sub t0, t4, t0
; RV32-NEXT:    srli t4, t0, 2
; RV32-NEXT:    and t0, t0, a6
; RV32-NEXT:    and a6, t4, a6
; RV32-NEXT:    add a6, t0, a6
; RV32-NEXT:    srli t0, a6, 4
; RV32-NEXT:    add a6, a6, t0
; RV32-NEXT:    and a6, a6, a7
; RV32-NEXT:    slli a7, a6, 8
; RV32-NEXT:    add a6, a6, a7
; RV32-NEXT:    slli a7, a6, 16
; RV32-NEXT:    add a6, a6, a7
; RV32-NEXT:    srli t4, a6, 24
; RV32-NEXT:  .LBB1_13: # %_udiv-special-cases
; RV32-NEXT:    or t0, t1, a1
; RV32-NEXT:    or a7, s2, t3
; RV32-NEXT:    or t5, t6, t5
; RV32-NEXT:    and a6, s3, s0
; RV32-NEXT:    addi t6, t4, 64
; RV32-NEXT:    bnez t5, .LBB1_15
; RV32-NEXT:  # %bb.14: # %_udiv-special-cases
; RV32-NEXT:    mv s1, t6
; RV32-NEXT:  .LBB1_15: # %_udiv-special-cases
; RV32-NEXT:    seqz t0, t0
; RV32-NEXT:    snez t5, t5
; RV32-NEXT:    sltu t4, t6, t4
; RV32-NEXT:    addi t5, t5, -1
; RV32-NEXT:    and t6, t5, t4
; RV32-NEXT:    sltu s0, t2, s1
; RV32-NEXT:    seqz a7, a7
; RV32-NEXT:    mv t4, s0
; RV32-NEXT:    beq a6, t6, .LBB1_17
; RV32-NEXT:  # %bb.16: # %_udiv-special-cases
; RV32-NEXT:    sltu t4, a6, t6
; RV32-NEXT:  .LBB1_17: # %_udiv-special-cases
; RV32-NEXT:    or t5, t0, a7
; RV32-NEXT:    andi t4, t4, 1
; RV32-NEXT:    sub a6, a6, t6
; RV32-NEXT:    sub a7, a6, s0
; RV32-NEXT:    sub t0, t2, s1
; RV32-NEXT:    beqz t4, .LBB1_19
; RV32-NEXT:  # %bb.18: # %_udiv-special-cases
; RV32-NEXT:    mv a6, t4
; RV32-NEXT:    j .LBB1_20
; RV32-NEXT:  .LBB1_19:
; RV32-NEXT:    sltiu a6, t0, 65
; RV32-NEXT:    xori a6, a6, 1
; RV32-NEXT:    snez t2, a7
; RV32-NEXT:    or a6, a6, t2
; RV32-NEXT:  .LBB1_20: # %_udiv-special-cases
; RV32-NEXT:    or t6, t5, a6
; RV32-NEXT:    addi t5, t6, -1
; RV32-NEXT:    and a6, t3, t5
; RV32-NEXT:    and t2, t5, a4
; RV32-NEXT:    and t5, t5, a5
; RV32-NEXT:    bnez t6, .LBB1_29
; RV32-NEXT:  # %bb.21: # %_udiv-special-cases
; RV32-NEXT:    xori t6, t0, 64
; RV32-NEXT:    or s0, t4, a7
; RV32-NEXT:    or t6, t6, s0
; RV32-NEXT:    beqz t6, .LBB1_29
; RV32-NEXT:  # %bb.22: # %udiv-bb1
; RV32-NEXT:    addi a6, t0, 1
; RV32-NEXT:    li t2, 64
; RV32-NEXT:    seqz t5, a6
; RV32-NEXT:    sub t2, t2, t0
; RV32-NEXT:    add a7, a7, t5
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    srli t5, t2, 3
; RV32-NEXT:    andi t5, t5, 12
; RV32-NEXT:    addi t6, sp, 48
; RV32-NEXT:    sw a5, 48(sp)
; RV32-NEXT:    sw a4, 52(sp)
; RV32-NEXT:    sw t3, 56(sp)
; RV32-NEXT:    sub s4, t6, t5
; RV32-NEXT:    or t5, a6, a7
; RV32-NEXT:    lw t6, 0(s4)
; RV32-NEXT:    seqz s0, t5
; RV32-NEXT:    add t4, t4, s0
; RV32-NEXT:    lw s2, 4(s4)
; RV32-NEXT:    andi t4, t4, 1
; RV32-NEXT:    andi t2, t2, 31
; RV32-NEXT:    xori s1, t2, 31
; RV32-NEXT:    srli t2, t6, 1
; RV32-NEXT:    srl t2, t2, s1
; RV32-NEXT:    neg s3, t0
; RV32-NEXT:    sll t0, s2, s3
; RV32-NEXT:    or t5, t5, t4
; RV32-NEXT:    or t0, t0, t2
; RV32-NEXT:    sll t2, t6, s3
; RV32-NEXT:    beqz t5, .LBB1_28
; RV32-NEXT:  # %bb.23: # %udiv-preheader
; RV32-NEXT:    li t5, 0
; RV32-NEXT:    li t6, 0
; RV32-NEXT:    li s0, 0
; RV32-NEXT:    lw s4, 8(s4)
; RV32-NEXT:    srli s5, a6, 3
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    andi s5, s5, 12
; RV32-NEXT:    mv s6, sp
; RV32-NEXT:    sw a5, 0(sp)
; RV32-NEXT:    sw a4, 4(sp)
; RV32-NEXT:    sw t3, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    add s5, s6, s5
; RV32-NEXT:    lw a4, 4(s5)
; RV32-NEXT:    srli a5, s2, 1
; RV32-NEXT:    lw t3, 8(s5)
; RV32-NEXT:    srl a5, a5, s1
; RV32-NEXT:    sll s1, s4, s3
; RV32-NEXT:    lw s2, 0(s5)
; RV32-NEXT:    or a5, s1, a5
; RV32-NEXT:    srl s1, a4, a6
; RV32-NEXT:    andi s3, a6, 31
; RV32-NEXT:    slli t3, t3, 1
; RV32-NEXT:    xori s3, s3, 31
; RV32-NEXT:    sll t3, t3, s3
; RV32-NEXT:    slli a4, a4, 1
; RV32-NEXT:    or t3, s1, t3
; RV32-NEXT:    sll a4, a4, s3
; RV32-NEXT:    andi s1, a5, 1
; RV32-NEXT:    srl a5, s2, a6
; RV32-NEXT:    or s2, a5, a4
; RV32-NEXT:    snez a4, t1
; RV32-NEXT:    seqz a5, a2
; RV32-NEXT:    add a4, a1, a4
; RV32-NEXT:    sub a1, a3, a5
; RV32-NEXT:    addi a4, a4, 1
; RV32-NEXT:    andi a4, a4, 1
; RV32-NEXT:    addi a5, a2, -1
; RV32-NEXT:    j .LBB1_26
; RV32-NEXT:  .LBB1_24: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB1_26 Depth=1
; RV32-NEXT:    sltu s1, a1, t1
; RV32-NEXT:  .LBB1_25: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB1_26 Depth=1
; RV32-NEXT:    srli t3, t3, 31
; RV32-NEXT:    sub t3, a4, t3
; RV32-NEXT:    sub t3, t3, s1
; RV32-NEXT:    slli t3, t3, 31
; RV32-NEXT:    srai t3, t3, 31
; RV32-NEXT:    and s3, t3, a3
; RV32-NEXT:    srli s1, t2, 31
; RV32-NEXT:    slli s4, t0, 1
; RV32-NEXT:    slli t2, t2, 1
; RV32-NEXT:    or s1, s4, s1
; RV32-NEXT:    srli s4, t0, 31
; RV32-NEXT:    or t2, t5, t2
; RV32-NEXT:    or t0, t6, s1
; RV32-NEXT:    or s1, s0, s4
; RV32-NEXT:    andi s1, s1, 1
; RV32-NEXT:    and t6, t3, a2
; RV32-NEXT:    sltu s0, s2, t6
; RV32-NEXT:    sub t1, t1, s3
; RV32-NEXT:    andi t5, t3, 1
; RV32-NEXT:    sub t3, t1, s0
; RV32-NEXT:    sub s2, s2, t6
; RV32-NEXT:    or t1, a6, a7
; RV32-NEXT:    seqz t6, a6
; RV32-NEXT:    snez t1, t1
; RV32-NEXT:    sub a7, a7, t6
; RV32-NEXT:    add t1, t4, t1
; RV32-NEXT:    addi t1, t1, 1
; RV32-NEXT:    addi a6, a6, -1
; RV32-NEXT:    andi t4, t1, 1
; RV32-NEXT:    or t1, a6, a7
; RV32-NEXT:    or t1, t1, t4
; RV32-NEXT:    li t6, 0
; RV32-NEXT:    li s0, 0
; RV32-NEXT:    beqz t1, .LBB1_28
; RV32-NEXT:  .LBB1_26: # %udiv-do-while
; RV32-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32-NEXT:    srli t1, s2, 31
; RV32-NEXT:    slli s3, t3, 1
; RV32-NEXT:    or t1, s3, t1
; RV32-NEXT:    slli s2, s2, 1
; RV32-NEXT:    andi s1, s1, 1
; RV32-NEXT:    or s2, s2, s1
; RV32-NEXT:    bne a1, t1, .LBB1_24
; RV32-NEXT:  # %bb.27: # in Loop: Header=BB1_26 Depth=1
; RV32-NEXT:    sltu s1, a5, s2
; RV32-NEXT:    j .LBB1_25
; RV32-NEXT:  .LBB1_28: # %udiv-loop-exit
; RV32-NEXT:    srli a1, t2, 31
; RV32-NEXT:    slli a2, t0, 1
; RV32-NEXT:    slli a3, t2, 1
; RV32-NEXT:    or t2, a2, a1
; RV32-NEXT:    srli a6, t0, 31
; RV32-NEXT:    or t5, t5, a3
; RV32-NEXT:  .LBB1_29: # %udiv-end
; RV32-NEXT:    sw t5, 0(a0)
; RV32-NEXT:    sw t2, 4(a0)
; RV32-NEXT:    sb a6, 8(a0)
; RV32-NEXT:    lw s0, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 72(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 68(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: udiv_i65:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    andi a1, a1, 1
; RV64-NEXT:    andi a3, a3, 1
; RV64-NEXT:    call __udivti3
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    ret
  %res = udiv i65 %x, %y
  ret i65 %res
}

define i128 @udiv_i128(i128 %x, i128 %y) nounwind {
; RV32-LABEL: udiv_i128:
; RV32:       # %bb.0: # %_udiv-special-cases
; RV32-NEXT:    addi sp, sp, -144
; RV32-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    lw ra, 0(a2)
; RV32-NEXT:    lw a5, 4(a2)
; RV32-NEXT:    lw s10, 8(a2)
; RV32-NEXT:    lw s11, 12(a2)
; RV32-NEXT:    lui t4, 349525
; RV32-NEXT:    addi t4, t4, 1365
; RV32-NEXT:    lui a0, 209715
; RV32-NEXT:    lui a6, 61681
; RV32-NEXT:    addi t3, a0, 819
; RV32-NEXT:    addi t2, a6, -241
; RV32-NEXT:    bnez a5, .LBB2_2
; RV32-NEXT:  # %bb.1: # %_udiv-special-cases
; RV32-NEXT:    srli a0, ra, 1
; RV32-NEXT:    or a0, ra, a0
; RV32-NEXT:    srli a6, a0, 2
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 4
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 8
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 16
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a6, a0, 1
; RV32-NEXT:    and a6, a6, t4
; RV32-NEXT:    sub a0, a0, a6
; RV32-NEXT:    srli a6, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a6, a6, t3
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    srli a6, a0, 4
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a6, a0, 8
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    slli a6, a0, 16
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    addi t6, a0, 32
; RV32-NEXT:    j .LBB2_3
; RV32-NEXT:  .LBB2_2:
; RV32-NEXT:    srli a0, a5, 1
; RV32-NEXT:    or a0, a5, a0
; RV32-NEXT:    srli a6, a0, 2
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 4
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 8
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 16
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a6, a0, 1
; RV32-NEXT:    and a6, a6, t4
; RV32-NEXT:    sub a0, a0, a6
; RV32-NEXT:    srli a6, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a6, a6, t3
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    srli a6, a0, 4
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a6, a0, 8
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    slli a6, a0, 16
; RV32-NEXT:    add a0, a0, a6
; RV32-NEXT:    srli t6, a0, 24
; RV32-NEXT:  .LBB2_3: # %_udiv-special-cases
; RV32-NEXT:    lw a6, 4(a1)
; RV32-NEXT:    or s1, s10, s11
; RV32-NEXT:    bnez s11, .LBB2_5
; RV32-NEXT:  # %bb.4: # %_udiv-special-cases
; RV32-NEXT:    srli a0, s10, 1
; RV32-NEXT:    or a0, s10, a0
; RV32-NEXT:    srli a7, a0, 2
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 4
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 8
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 16
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a7, a0, 1
; RV32-NEXT:    and a7, a7, t4
; RV32-NEXT:    sub a0, a0, a7
; RV32-NEXT:    srli a7, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a7, a7, t3
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    srli a7, a0, 4
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a7, a0, 8
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    slli a7, a0, 16
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    addi t5, a0, 32
; RV32-NEXT:    j .LBB2_6
; RV32-NEXT:  .LBB2_5:
; RV32-NEXT:    srli a0, s11, 1
; RV32-NEXT:    or a0, s11, a0
; RV32-NEXT:    srli a7, a0, 2
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 4
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 8
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    srli a7, a0, 16
; RV32-NEXT:    or a0, a0, a7
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a7, a0, 1
; RV32-NEXT:    and a7, a7, t4
; RV32-NEXT:    sub a0, a0, a7
; RV32-NEXT:    srli a7, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a7, a7, t3
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    srli a7, a0, 4
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a7, a0, 8
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    slli a7, a0, 16
; RV32-NEXT:    add a0, a0, a7
; RV32-NEXT:    srli t5, a0, 24
; RV32-NEXT:  .LBB2_6: # %_udiv-special-cases
; RV32-NEXT:    lw a7, 12(a1)
; RV32-NEXT:    addi s0, t6, 64
; RV32-NEXT:    bnez s1, .LBB2_8
; RV32-NEXT:  # %bb.7: # %_udiv-special-cases
; RV32-NEXT:    mv t5, s0
; RV32-NEXT:  .LBB2_8: # %_udiv-special-cases
; RV32-NEXT:    lw t1, 0(a1)
; RV32-NEXT:    lw t0, 8(a1)
; RV32-NEXT:    snez s4, s1
; RV32-NEXT:    bnez a6, .LBB2_10
; RV32-NEXT:  # %bb.9: # %_udiv-special-cases
; RV32-NEXT:    srli a0, t1, 1
; RV32-NEXT:    or a0, t1, a0
; RV32-NEXT:    srli a1, a0, 2
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 8
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 16
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a1, a0, 1
; RV32-NEXT:    and a1, a1, t4
; RV32-NEXT:    sub a0, a0, a1
; RV32-NEXT:    srli a1, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a1, a1, t3
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    srli a1, a0, 4
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a1, a0, 8
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    slli a1, a0, 16
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    j .LBB2_11
; RV32-NEXT:  .LBB2_10:
; RV32-NEXT:    srli a0, a6, 1
; RV32-NEXT:    or a0, a6, a0
; RV32-NEXT:    srli a1, a0, 2
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 8
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    srli a1, a0, 16
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli a1, a0, 1
; RV32-NEXT:    and a1, a1, t4
; RV32-NEXT:    sub a0, a0, a1
; RV32-NEXT:    srli a1, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and a1, a1, t3
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    srli a1, a0, 4
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli a1, a0, 8
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    slli a1, a0, 16
; RV32-NEXT:    add a0, a0, a1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:  .LBB2_11: # %_udiv-special-cases
; RV32-NEXT:    or a1, a5, s11
; RV32-NEXT:    or s1, ra, s10
; RV32-NEXT:    or s2, a6, a7
; RV32-NEXT:    or s3, t1, t0
; RV32-NEXT:    sltu t6, s0, t6
; RV32-NEXT:    addi s4, s4, -1
; RV32-NEXT:    or s5, t0, a7
; RV32-NEXT:    addi s0, a0, 64
; RV32-NEXT:    snez s7, s5
; RV32-NEXT:    sltu s6, s0, a0
; RV32-NEXT:    addi s7, s7, -1
; RV32-NEXT:    bnez a7, .LBB2_13
; RV32-NEXT:  # %bb.12: # %_udiv-special-cases
; RV32-NEXT:    srli a0, t0, 1
; RV32-NEXT:    or a0, t0, a0
; RV32-NEXT:    srli s8, a0, 2
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 4
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 8
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 16
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli s8, a0, 1
; RV32-NEXT:    and t4, s8, t4
; RV32-NEXT:    sub a0, a0, t4
; RV32-NEXT:    srli t4, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and t3, t4, t3
; RV32-NEXT:    add a0, a0, t3
; RV32-NEXT:    srli t3, a0, 4
; RV32-NEXT:    add a0, a0, t3
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli t2, a0, 8
; RV32-NEXT:    add a0, a0, t2
; RV32-NEXT:    slli t2, a0, 16
; RV32-NEXT:    add a0, a0, t2
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    addi t2, a0, 32
; RV32-NEXT:    j .LBB2_14
; RV32-NEXT:  .LBB2_13:
; RV32-NEXT:    srli a0, a7, 1
; RV32-NEXT:    or a0, a7, a0
; RV32-NEXT:    srli s8, a0, 2
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 4
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 8
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    srli s8, a0, 16
; RV32-NEXT:    or a0, a0, s8
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    srli s8, a0, 1
; RV32-NEXT:    and t4, s8, t4
; RV32-NEXT:    sub a0, a0, t4
; RV32-NEXT:    srli t4, a0, 2
; RV32-NEXT:    and a0, a0, t3
; RV32-NEXT:    and t3, t4, t3
; RV32-NEXT:    add a0, a0, t3
; RV32-NEXT:    srli t3, a0, 4
; RV32-NEXT:    add a0, a0, t3
; RV32-NEXT:    and a0, a0, t2
; RV32-NEXT:    slli t2, a0, 8
; RV32-NEXT:    add a0, a0, t2
; RV32-NEXT:    slli t2, a0, 16
; RV32-NEXT:    add a0, a0, t2
; RV32-NEXT:    srli t2, a0, 24
; RV32-NEXT:  .LBB2_14: # %_udiv-special-cases
; RV32-NEXT:    or a1, s1, a1
; RV32-NEXT:    or t3, s3, s2
; RV32-NEXT:    and t4, s4, t6
; RV32-NEXT:    and s1, s7, s6
; RV32-NEXT:    bnez s5, .LBB2_16
; RV32-NEXT:  # %bb.15: # %_udiv-special-cases
; RV32-NEXT:    mv t2, s0
; RV32-NEXT:  .LBB2_16: # %_udiv-special-cases
; RV32-NEXT:    seqz a0, a1
; RV32-NEXT:    seqz a1, t3
; RV32-NEXT:    sltu t3, t5, t2
; RV32-NEXT:    sub t6, t4, s1
; RV32-NEXT:    mv s0, t3
; RV32-NEXT:    beq t4, s1, .LBB2_18
; RV32-NEXT:  # %bb.17: # %_udiv-special-cases
; RV32-NEXT:    sltu s0, t4, s1
; RV32-NEXT:  .LBB2_18: # %_udiv-special-cases
; RV32-NEXT:    seqz t4, s0
; RV32-NEXT:    neg s0, s0
; RV32-NEXT:    addi t4, t4, -1
; RV32-NEXT:    sub t3, t6, t3
; RV32-NEXT:    or s1, s0, t4
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    sub t6, t5, t2
; RV32-NEXT:    beqz s1, .LBB2_20
; RV32-NEXT:  # %bb.19: # %_udiv-special-cases
; RV32-NEXT:    snez a1, s1
; RV32-NEXT:    j .LBB2_21
; RV32-NEXT:  .LBB2_20:
; RV32-NEXT:    sltiu a1, t6, 128
; RV32-NEXT:    snez t2, t3
; RV32-NEXT:    xori a1, a1, 1
; RV32-NEXT:    or a1, a1, t2
; RV32-NEXT:  .LBB2_21: # %_udiv-special-cases
; RV32-NEXT:    or a1, a0, a1
; RV32-NEXT:    addi a0, a1, -1
; RV32-NEXT:    and s1, a0, a7
; RV32-NEXT:    and t5, a0, t0
; RV32-NEXT:    and t2, a0, a6
; RV32-NEXT:    and a0, a0, t1
; RV32-NEXT:    bnez a1, .LBB2_25
; RV32-NEXT:  # %bb.22: # %_udiv-special-cases
; RV32-NEXT:    xori a1, t6, 127
; RV32-NEXT:    or a1, a1, s0
; RV32-NEXT:    or s2, t3, t4
; RV32-NEXT:    or a1, a1, s2
; RV32-NEXT:    beqz a1, .LBB2_25
; RV32-NEXT:  # %bb.23: # %udiv-bb1
; RV32-NEXT:    sw a3, 8(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi t2, t6, 1
; RV32-NEXT:    seqz a1, t2
; RV32-NEXT:    add t3, t3, a1
; RV32-NEXT:    or a1, t2, t3
; RV32-NEXT:    sw zero, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    seqz t5, a1
; RV32-NEXT:    li a1, 127
; RV32-NEXT:    add t5, s0, t5
; RV32-NEXT:    sub a1, a1, t6
; RV32-NEXT:    sltu a3, t5, s0
; RV32-NEXT:    srli a4, a1, 3
; RV32-NEXT:    add t4, t4, a3
; RV32-NEXT:    andi a4, a4, 12
; RV32-NEXT:    or a3, t3, t4
; RV32-NEXT:    addi s0, sp, 72
; RV32-NEXT:    sw t1, 72(sp)
; RV32-NEXT:    sw a6, 76(sp)
; RV32-NEXT:    sw t0, 80(sp)
; RV32-NEXT:    sw a7, 84(sp)
; RV32-NEXT:    sub s0, s0, a4
; RV32-NEXT:    lw s1, 4(s0)
; RV32-NEXT:    lw a4, 8(s0)
; RV32-NEXT:    lw s3, 0(s0)
; RV32-NEXT:    lw s0, 12(s0)
; RV32-NEXT:    or s2, t2, t5
; RV32-NEXT:    andi a1, a1, 31
; RV32-NEXT:    or a3, s2, a3
; RV32-NEXT:    xori a1, a1, 31
; RV32-NEXT:    srli s2, a4, 1
; RV32-NEXT:    not t6, t6
; RV32-NEXT:    srl s2, s2, a1
; RV32-NEXT:    sll s0, s0, t6
; RV32-NEXT:    or s0, s0, s2
; RV32-NEXT:    srli s2, s1, 1
; RV32-NEXT:    srl s2, s2, a1
; RV32-NEXT:    sll a4, a4, t6
; RV32-NEXT:    or a4, a4, s2
; RV32-NEXT:    srli s2, s3, 1
; RV32-NEXT:    srl a1, s2, a1
; RV32-NEXT:    sll s1, s1, t6
; RV32-NEXT:    or s2, s1, a1
; RV32-NEXT:    sll t6, s3, t6
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    bnez a3, .LBB2_26
; RV32-NEXT:  .LBB2_24: # %udiv-loop-exit
; RV32-NEXT:    srli a1, t6, 31
; RV32-NEXT:    slli a2, s2, 1
; RV32-NEXT:    srli a3, s2, 31
; RV32-NEXT:    or t2, a2, a1
; RV32-NEXT:    slli a1, a4, 1
; RV32-NEXT:    or t5, a1, a3
; RV32-NEXT:    srli s1, a4, 31
; RV32-NEXT:    slli s0, s0, 1
; RV32-NEXT:    slli t6, t6, 1
; RV32-NEXT:    or s1, s0, s1
; RV32-NEXT:    or a0, a0, t6
; RV32-NEXT:    lw a3, 8(sp) # 4-byte Folded Reload
; RV32-NEXT:  .LBB2_25: # %udiv-end
; RV32-NEXT:    sw a0, 0(a3)
; RV32-NEXT:    sw t2, 4(a3)
; RV32-NEXT:    sw t5, 8(a3)
; RV32-NEXT:    sw s1, 12(a3)
; RV32-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 144
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB2_26: # %udiv-preheader
; RV32-NEXT:    li s3, 0
; RV32-NEXT:    li s4, 0
; RV32-NEXT:    li s5, 0
; RV32-NEXT:    srli a1, t2, 3
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    andi a1, a1, 12
; RV32-NEXT:    addi a3, sp, 24
; RV32-NEXT:    sw t1, 24(sp)
; RV32-NEXT:    sw a6, 28(sp)
; RV32-NEXT:    sw t0, 32(sp)
; RV32-NEXT:    sw a7, 36(sp)
; RV32-NEXT:    add a1, a3, a1
; RV32-NEXT:    lw a3, 12(a1)
; RV32-NEXT:    lw a6, 8(a1)
; RV32-NEXT:    lw a7, 4(a1)
; RV32-NEXT:    lw a1, 0(a1)
; RV32-NEXT:    andi t0, t2, 31
; RV32-NEXT:    slli t1, a3, 1
; RV32-NEXT:    xori t0, t0, 31
; RV32-NEXT:    srl s1, a6, t2
; RV32-NEXT:    sll t1, t1, t0
; RV32-NEXT:    or s6, s1, t1
; RV32-NEXT:    slli a6, a6, 1
; RV32-NEXT:    srl t1, a7, t2
; RV32-NEXT:    sll a6, a6, t0
; RV32-NEXT:    or s8, t1, a6
; RV32-NEXT:    slli a7, a7, 1
; RV32-NEXT:    srl a1, a1, t2
; RV32-NEXT:    sll a6, a7, t0
; RV32-NEXT:    or s9, a1, a6
; RV32-NEXT:    srl s7, a3, t2
; RV32-NEXT:    seqz a1, ra
; RV32-NEXT:    or a3, ra, a5
; RV32-NEXT:    sw a5, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sub a6, a5, a1
; RV32-NEXT:    seqz a1, a3
; RV32-NEXT:    sub a7, s10, a1
; RV32-NEXT:    mv a2, s10
; RV32-NEXT:    sltu a1, s10, a1
; RV32-NEXT:    mv t0, s11
; RV32-NEXT:    sub a1, s11, a1
; RV32-NEXT:    sw a1, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi a1, ra, -1
; RV32-NEXT:    sw a1, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    j .LBB2_28
; RV32-NEXT:  .LBB2_27: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB2_28 Depth=1
; RV32-NEXT:    srli s1, a4, 31
; RV32-NEXT:    slli s0, s0, 1
; RV32-NEXT:    srli a3, s2, 31
; RV32-NEXT:    slli a4, a4, 1
; RV32-NEXT:    or s0, s0, s1
; RV32-NEXT:    or a3, a4, a3
; RV32-NEXT:    srli a4, t6, 31
; RV32-NEXT:    slli s2, s2, 1
; RV32-NEXT:    or a4, s2, a4
; RV32-NEXT:    slli t6, t6, 1
; RV32-NEXT:    or t6, a0, t6
; RV32-NEXT:    or s2, s3, a4
; RV32-NEXT:    or a4, s4, a3
; RV32-NEXT:    or s0, s5, s0
; RV32-NEXT:    andi a0, a1, 1
; RV32-NEXT:    and a3, a1, a2
; RV32-NEXT:    and a1, a1, t0
; RV32-NEXT:    sub s1, s7, a3
; RV32-NEXT:    sltu a3, s7, a3
; RV32-NEXT:    sub a1, s6, a1
; RV32-NEXT:    sltu s3, s1, s8
; RV32-NEXT:    sub a1, a1, a3
; RV32-NEXT:    sub s6, s1, s8
; RV32-NEXT:    sub s7, a1, s3
; RV32-NEXT:    sub a1, s10, ra
; RV32-NEXT:    or a3, t2, t3
; RV32-NEXT:    sub s8, a1, t1
; RV32-NEXT:    seqz a1, a3
; RV32-NEXT:    seqz a3, t2
; RV32-NEXT:    sltu t1, t5, a1
; RV32-NEXT:    sub t5, t5, a1
; RV32-NEXT:    sub t3, t3, a3
; RV32-NEXT:    sub t4, t4, t1
; RV32-NEXT:    addi t2, t2, -1
; RV32-NEXT:    or a1, t3, t4
; RV32-NEXT:    or a3, t2, t5
; RV32-NEXT:    sub s9, s9, s11
; RV32-NEXT:    or a1, a3, a1
; RV32-NEXT:    li s3, 0
; RV32-NEXT:    li s4, 0
; RV32-NEXT:    li s5, 0
; RV32-NEXT:    mv ra, a5
; RV32-NEXT:    beqz a1, .LBB2_24
; RV32-NEXT:  .LBB2_28: # %udiv-do-while
; RV32-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32-NEXT:    srli a1, s9, 31
; RV32-NEXT:    slli t1, s8, 1
; RV32-NEXT:    or s10, t1, a1
; RV32-NEXT:    slli s9, s9, 1
; RV32-NEXT:    srli a1, s0, 31
; RV32-NEXT:    or s9, s9, a1
; RV32-NEXT:    beq a6, s10, .LBB2_30
; RV32-NEXT:  # %bb.29: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB2_28 Depth=1
; RV32-NEXT:    sltu a1, a6, s10
; RV32-NEXT:    j .LBB2_31
; RV32-NEXT:  .LBB2_30: # in Loop: Header=BB2_28 Depth=1
; RV32-NEXT:    lw a1, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    sltu a1, a1, s9
; RV32-NEXT:  .LBB2_31: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB2_28 Depth=1
; RV32-NEXT:    srli t1, s6, 31
; RV32-NEXT:    srli s8, s8, 31
; RV32-NEXT:    slli s6, s6, 1
; RV32-NEXT:    slli s11, s7, 1
; RV32-NEXT:    or s7, s6, s8
; RV32-NEXT:    or s6, s11, t1
; RV32-NEXT:    sub t1, a7, s7
; RV32-NEXT:    sltu s8, a7, s7
; RV32-NEXT:    lw a3, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    sub s11, a3, s6
; RV32-NEXT:    sltu a1, t1, a1
; RV32-NEXT:    sub t1, s11, s8
; RV32-NEXT:    sub a1, t1, a1
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    mv a5, ra
; RV32-NEXT:    and s11, a1, ra
; RV32-NEXT:    lw a3, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    and ra, a1, a3
; RV32-NEXT:    sltu t1, s9, s11
; RV32-NEXT:    mv s8, t1
; RV32-NEXT:    beq s10, ra, .LBB2_27
; RV32-NEXT:  # %bb.32: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB2_28 Depth=1
; RV32-NEXT:    sltu s8, s10, ra
; RV32-NEXT:    j .LBB2_27
;
; RV64-LABEL: udiv_i128:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    call __udivti3
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    ret
  %res = udiv i128 %x, %y
  ret i128 %res
}

define i129 @udiv_i129(i129 %x, i129 %y) nounwind {
; RV32-LABEL: udiv_i129:
; RV32:       # %bb.0: # %_udiv-special-cases
; RV32-NEXT:    addi sp, sp, -224
; RV32-NEXT:    sw ra, 220(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 216(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 212(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 208(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 204(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 200(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 196(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 192(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 180(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 176(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 172(sp) # 4-byte Folded Spill
; RV32-NEXT:    mv s9, a0
; RV32-NEXT:    lw a5, 4(a2)
; RV32-NEXT:    lw a4, 8(a2)
; RV32-NEXT:    lui a0, 349525
; RV32-NEXT:    lui a3, 209715
; RV32-NEXT:    lw t2, 16(a2)
; RV32-NEXT:    lw a6, 0(a2)
; RV32-NEXT:    lw a2, 12(a2)
; RV32-NEXT:    sw a2, 20(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw a4, 28(sp) # 4-byte Folded Spill
; RV32-NEXT:    slli a2, a4, 31
; RV32-NEXT:    srli a4, a5, 1
; RV32-NEXT:    addi t5, a0, 1365
; RV32-NEXT:    or a0, a4, a2
; RV32-NEXT:    addi t4, a3, 819
; RV32-NEXT:    lui a2, 61681
; RV32-NEXT:    sw a5, 16(sp) # 4-byte Folded Spill
; RV32-NEXT:    slli a3, a5, 31
; RV32-NEXT:    sw a6, 24(sp) # 4-byte Folded Spill
; RV32-NEXT:    srli a4, a6, 1
; RV32-NEXT:    addi t3, a2, -241
; RV32-NEXT:    or a6, a4, a3
; RV32-NEXT:    bnez a0, .LBB3_2
; RV32-NEXT:  # %bb.1: # %_udiv-special-cases
; RV32-NEXT:    srli a2, a6, 1
; RV32-NEXT:    or a2, a6, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a2, a2, 24
; RV32-NEXT:    addi t0, a2, 32
; RV32-NEXT:    j .LBB3_3
; RV32-NEXT:  .LBB3_2:
; RV32-NEXT:    srli a2, a0, 1
; RV32-NEXT:    or a2, a0, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli t0, a2, 24
; RV32-NEXT:  .LBB3_3: # %_udiv-special-cases
; RV32-NEXT:    lw a7, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    srli a2, a7, 1
; RV32-NEXT:    slli a4, t2, 31
; RV32-NEXT:    slli a7, a7, 31
; RV32-NEXT:    lw a3, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    slli a3, a3, 31
; RV32-NEXT:    lw a5, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    srli a5, a5, 1
; RV32-NEXT:    li s3, 64
; RV32-NEXT:    bnez a3, .LBB3_5
; RV32-NEXT:  # %bb.4: # %_udiv-special-cases
; RV32-NEXT:    li t1, 64
; RV32-NEXT:    j .LBB3_6
; RV32-NEXT:  .LBB3_5:
; RV32-NEXT:    srli t1, a3, 1
; RV32-NEXT:    or t1, a3, t1
; RV32-NEXT:    srli t6, t1, 2
; RV32-NEXT:    or t1, t1, t6
; RV32-NEXT:    srli t6, t1, 4
; RV32-NEXT:    or t1, t1, t6
; RV32-NEXT:    srli t6, t1, 8
; RV32-NEXT:    or t1, t1, t6
; RV32-NEXT:    srli t6, t1, 16
; RV32-NEXT:    or t1, t1, t6
; RV32-NEXT:    not t1, t1
; RV32-NEXT:    srli t6, t1, 1
; RV32-NEXT:    and t6, t6, t5
; RV32-NEXT:    sub t1, t1, t6
; RV32-NEXT:    srli t6, t1, 2
; RV32-NEXT:    and t1, t1, t4
; RV32-NEXT:    and t6, t6, t4
; RV32-NEXT:    add t1, t1, t6
; RV32-NEXT:    srli t6, t1, 4
; RV32-NEXT:    add t1, t1, t6
; RV32-NEXT:    and t1, t1, t3
; RV32-NEXT:    slli t6, t1, 8
; RV32-NEXT:    add t1, t1, t6
; RV32-NEXT:    slli t6, t1, 16
; RV32-NEXT:    add t1, t1, t6
; RV32-NEXT:    srli t1, t1, 24
; RV32-NEXT:  .LBB3_6: # %_udiv-special-cases
; RV32-NEXT:    or a2, a4, a2
; RV32-NEXT:    or a4, a5, a7
; RV32-NEXT:    bnez a3, .LBB3_8
; RV32-NEXT:  # %bb.7: # %_udiv-special-cases
; RV32-NEXT:    li t1, 128
; RV32-NEXT:  .LBB3_8: # %_udiv-special-cases
; RV32-NEXT:    or a7, a4, a2
; RV32-NEXT:    or a3, a0, a2
; RV32-NEXT:    or s1, a6, a4
; RV32-NEXT:    addi a0, t1, 128
; RV32-NEXT:    or s1, s1, a3
; RV32-NEXT:    addi a6, t0, 64
; RV32-NEXT:    sltu s0, a0, t1
; RV32-NEXT:    bnez s1, .LBB3_11
; RV32-NEXT:  # %bb.9: # %_udiv-special-cases
; RV32-NEXT:    mv t6, s0
; RV32-NEXT:    beqz a2, .LBB3_12
; RV32-NEXT:  .LBB3_10:
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli s2, a2, 24
; RV32-NEXT:    beqz a7, .LBB3_13
; RV32-NEXT:    j .LBB3_14
; RV32-NEXT:  .LBB3_11:
; RV32-NEXT:    snez a3, a7
; RV32-NEXT:    sltu a5, a6, t0
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and t6, a3, a5
; RV32-NEXT:    bnez a2, .LBB3_10
; RV32-NEXT:  .LBB3_12: # %_udiv-special-cases
; RV32-NEXT:    srli a2, a4, 1
; RV32-NEXT:    or a2, a4, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a2, a2, 24
; RV32-NEXT:    addi s2, a2, 32
; RV32-NEXT:    bnez a7, .LBB3_14
; RV32-NEXT:  .LBB3_13: # %_udiv-special-cases
; RV32-NEXT:    mv s2, a6
; RV32-NEXT:  .LBB3_14: # %_udiv-special-cases
; RV32-NEXT:    lw a7, 0(a1)
; RV32-NEXT:    lw t0, 4(a1)
; RV32-NEXT:    lw a6, 8(a1)
; RV32-NEXT:    bnez s1, .LBB3_16
; RV32-NEXT:  # %bb.15: # %_udiv-special-cases
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:  .LBB3_16: # %_udiv-special-cases
; RV32-NEXT:    lw t1, 12(a1)
; RV32-NEXT:    lw a1, 16(a1)
; RV32-NEXT:    slli a0, a6, 31
; RV32-NEXT:    srli a2, t0, 1
; RV32-NEXT:    or a0, a2, a0
; RV32-NEXT:    slli a2, t0, 31
; RV32-NEXT:    srli a3, a7, 1
; RV32-NEXT:    or s4, a3, a2
; RV32-NEXT:    bnez a0, .LBB3_18
; RV32-NEXT:  # %bb.17: # %_udiv-special-cases
; RV32-NEXT:    srli a2, s4, 1
; RV32-NEXT:    or a2, s4, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a2, a2, 24
; RV32-NEXT:    addi s8, a2, 32
; RV32-NEXT:    j .LBB3_19
; RV32-NEXT:  .LBB3_18:
; RV32-NEXT:    srli a2, a0, 1
; RV32-NEXT:    or a2, a0, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli s8, a2, 24
; RV32-NEXT:  .LBB3_19: # %_udiv-special-cases
; RV32-NEXT:    srli a2, t1, 1
; RV32-NEXT:    slli a3, a1, 31
; RV32-NEXT:    slli a4, a7, 31
; RV32-NEXT:    slli a5, t1, 31
; RV32-NEXT:    srli s5, a6, 1
; RV32-NEXT:    beqz a4, .LBB3_21
; RV32-NEXT:  # %bb.20:
; RV32-NEXT:    srli s3, a4, 1
; RV32-NEXT:    or s3, a4, s3
; RV32-NEXT:    srli s6, s3, 2
; RV32-NEXT:    or s3, s3, s6
; RV32-NEXT:    srli s6, s3, 4
; RV32-NEXT:    or s3, s3, s6
; RV32-NEXT:    srli s6, s3, 8
; RV32-NEXT:    or s3, s3, s6
; RV32-NEXT:    srli s6, s3, 16
; RV32-NEXT:    or s3, s3, s6
; RV32-NEXT:    not s3, s3
; RV32-NEXT:    srli s6, s3, 1
; RV32-NEXT:    and s6, s6, t5
; RV32-NEXT:    sub s3, s3, s6
; RV32-NEXT:    srli s6, s3, 2
; RV32-NEXT:    and s3, s3, t4
; RV32-NEXT:    and s6, s6, t4
; RV32-NEXT:    add s3, s3, s6
; RV32-NEXT:    srli s6, s3, 4
; RV32-NEXT:    add s3, s3, s6
; RV32-NEXT:    and s3, s3, t3
; RV32-NEXT:    slli s6, s3, 8
; RV32-NEXT:    add s3, s3, s6
; RV32-NEXT:    slli s6, s3, 16
; RV32-NEXT:    add s3, s3, s6
; RV32-NEXT:    srli s3, s3, 24
; RV32-NEXT:  .LBB3_21: # %_udiv-special-cases
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    or a3, s5, a5
; RV32-NEXT:    bnez a4, .LBB3_23
; RV32-NEXT:  # %bb.22: # %_udiv-special-cases
; RV32-NEXT:    li s3, 128
; RV32-NEXT:  .LBB3_23: # %_udiv-special-cases
; RV32-NEXT:    or s7, a3, a2
; RV32-NEXT:    or a0, a0, a2
; RV32-NEXT:    or a4, s4, a3
; RV32-NEXT:    addi s6, s3, 128
; RV32-NEXT:    or s5, a4, a0
; RV32-NEXT:    addi s4, s8, 64
; RV32-NEXT:    sltu s3, s6, s3
; RV32-NEXT:    bnez s5, .LBB3_26
; RV32-NEXT:  # %bb.24: # %_udiv-special-cases
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    xori s8, s0, 1
; RV32-NEXT:    beqz a2, .LBB3_27
; RV32-NEXT:  .LBB3_25:
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli t4, a2, 24
; RV32-NEXT:    j .LBB3_28
; RV32-NEXT:  .LBB3_26:
; RV32-NEXT:    snez a0, s7
; RV32-NEXT:    sltu a4, s4, s8
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    xori s8, s0, 1
; RV32-NEXT:    bnez a2, .LBB3_25
; RV32-NEXT:  .LBB3_27: # %_udiv-special-cases
; RV32-NEXT:    srli a2, a3, 1
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 8
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    srli a3, a2, 16
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    not a2, a2
; RV32-NEXT:    srli a3, a2, 1
; RV32-NEXT:    and a3, a3, t5
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    srli a3, a2, 2
; RV32-NEXT:    and a2, a2, t4
; RV32-NEXT:    and a3, a3, t4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a3, a2, 4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    and a2, a2, t3
; RV32-NEXT:    slli a3, a2, 8
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    slli a3, a2, 16
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    srli a2, a2, 24
; RV32-NEXT:    addi t4, a2, 32
; RV32-NEXT:  .LBB3_28: # %_udiv-special-cases
; RV32-NEXT:    neg a2, s8
; RV32-NEXT:    snez a3, s1
; RV32-NEXT:    bnez s7, .LBB3_30
; RV32-NEXT:  # %bb.29: # %_udiv-special-cases
; RV32-NEXT:    mv t4, s4
; RV32-NEXT:  .LBB3_30: # %_udiv-special-cases
; RV32-NEXT:    andi s11, a1, 1
; RV32-NEXT:    andi s7, t2, 1
; RV32-NEXT:    lw a1, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw a4, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    or s4, a4, a1
; RV32-NEXT:    or a1, a7, a6
; RV32-NEXT:    and s0, a2, s0
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    bnez s5, .LBB3_32
; RV32-NEXT:  # %bb.31: # %_udiv-special-cases
; RV32-NEXT:    mv t4, s6
; RV32-NEXT:  .LBB3_32: # %_udiv-special-cases
; RV32-NEXT:    lw a2, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw a4, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    or s6, a2, a4
; RV32-NEXT:    or a2, s4, s7
; RV32-NEXT:    or t2, t0, t1
; RV32-NEXT:    and a3, a3, s0
; RV32-NEXT:    xori a4, s3, 1
; RV32-NEXT:    or t3, a1, s11
; RV32-NEXT:    neg a1, a4
; RV32-NEXT:    and a4, a1, s3
; RV32-NEXT:    snez a1, s5
; RV32-NEXT:    addi a5, a1, -1
; RV32-NEXT:    sltu a1, s2, t4
; RV32-NEXT:    and a5, a5, a4
; RV32-NEXT:    mv a4, a1
; RV32-NEXT:    beq t6, a0, .LBB3_34
; RV32-NEXT:  # %bb.33: # %_udiv-special-cases
; RV32-NEXT:    sltu a4, t6, a0
; RV32-NEXT:  .LBB3_34: # %_udiv-special-cases
; RV32-NEXT:    or a2, a2, s6
; RV32-NEXT:    or t3, t3, t2
; RV32-NEXT:    sltu t2, a3, a5
; RV32-NEXT:    mv t5, a4
; RV32-NEXT:    beq a3, a5, .LBB3_36
; RV32-NEXT:  # %bb.35: # %_udiv-special-cases
; RV32-NEXT:    mv t5, t2
; RV32-NEXT:  .LBB3_36: # %_udiv-special-cases
; RV32-NEXT:    seqz a2, a2
; RV32-NEXT:    sub s0, a3, a5
; RV32-NEXT:    seqz a3, t3
; RV32-NEXT:    sltu a5, s0, a4
; RV32-NEXT:    andi t5, t5, 1
; RV32-NEXT:    add a5, t2, a5
; RV32-NEXT:    neg t3, a5
; RV32-NEXT:    sub s0, s0, a4
; RV32-NEXT:    sub a0, t6, a0
; RV32-NEXT:    or a4, s0, t3
; RV32-NEXT:    sub t2, a0, a1
; RV32-NEXT:    sub a0, s2, t4
; RV32-NEXT:    beqz a4, .LBB3_38
; RV32-NEXT:  # %bb.37: # %_udiv-special-cases
; RV32-NEXT:    snez a1, a4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    bnez t5, .LBB3_39
; RV32-NEXT:    j .LBB3_40
; RV32-NEXT:  .LBB3_38:
; RV32-NEXT:    sltiu a1, a0, 129
; RV32-NEXT:    snez a4, t2
; RV32-NEXT:    xori a1, a1, 1
; RV32-NEXT:    or a1, a1, a4
; RV32-NEXT:    or a2, a2, a3
; RV32-NEXT:    beqz t5, .LBB3_40
; RV32-NEXT:  .LBB3_39: # %_udiv-special-cases
; RV32-NEXT:    mv a1, t5
; RV32-NEXT:  .LBB3_40: # %_udiv-special-cases
; RV32-NEXT:    or t4, a2, a1
; RV32-NEXT:    addi a5, t4, -1
; RV32-NEXT:    and a4, s11, a5
; RV32-NEXT:    and a3, a5, t1
; RV32-NEXT:    and a1, a5, a6
; RV32-NEXT:    and a2, a5, t0
; RV32-NEXT:    and a5, a5, a7
; RV32-NEXT:    bnez t4, .LBB3_57
; RV32-NEXT:  # %bb.41: # %_udiv-special-cases
; RV32-NEXT:    xori t4, a0, 128
; RV32-NEXT:    or t6, t5, s0
; RV32-NEXT:    or s1, t2, t3
; RV32-NEXT:    or t4, t4, t6
; RV32-NEXT:    or t4, t4, s1
; RV32-NEXT:    beqz t4, .LBB3_57
; RV32-NEXT:  # %bb.42: # %udiv-bb1
; RV32-NEXT:    sw s9, 0(sp) # 4-byte Folded Spill
; RV32-NEXT:    addi a1, a0, 1
; RV32-NEXT:    seqz a2, a1
; RV32-NEXT:    add t2, t2, a2
; RV32-NEXT:    or a2, a1, t2
; RV32-NEXT:    seqz t4, a2
; RV32-NEXT:    sw zero, 120(sp)
; RV32-NEXT:    sw zero, 124(sp)
; RV32-NEXT:    sw zero, 128(sp)
; RV32-NEXT:    sw zero, 132(sp)
; RV32-NEXT:    add t4, s0, t4
; RV32-NEXT:    sltu a2, t4, s0
; RV32-NEXT:    sw zero, 104(sp)
; RV32-NEXT:    sw zero, 108(sp)
; RV32-NEXT:    sw zero, 112(sp)
; RV32-NEXT:    sw zero, 116(sp)
; RV32-NEXT:    add t3, t3, a2
; RV32-NEXT:    or a2, a1, t4
; RV32-NEXT:    or a3, t2, t3
; RV32-NEXT:    li a4, 128
; RV32-NEXT:    or a5, a2, a3
; RV32-NEXT:    sub a4, a4, a0
; RV32-NEXT:    sw a7, 136(sp)
; RV32-NEXT:    sw t0, 140(sp)
; RV32-NEXT:    sw a6, 144(sp)
; RV32-NEXT:    sw t1, 148(sp)
; RV32-NEXT:    srli a2, a4, 3
; RV32-NEXT:    andi a2, a2, 28
; RV32-NEXT:    addi a3, sp, 136
; RV32-NEXT:    sw s11, 152(sp)
; RV32-NEXT:    sub a2, a3, a2
; RV32-NEXT:    lw t6, 4(a2)
; RV32-NEXT:    lw s0, 8(a2)
; RV32-NEXT:    lw s1, 0(a2)
; RV32-NEXT:    lw a3, 12(a2)
; RV32-NEXT:    seqz s2, a5
; RV32-NEXT:    andi a4, a4, 31
; RV32-NEXT:    add s2, t5, s2
; RV32-NEXT:    xori s5, a4, 31
; RV32-NEXT:    srli a4, s0, 1
; RV32-NEXT:    neg a0, a0
; RV32-NEXT:    srl a4, a4, s5
; RV32-NEXT:    sll t5, a3, a0
; RV32-NEXT:    or t5, t5, a4
; RV32-NEXT:    srli a4, t6, 1
; RV32-NEXT:    srl a4, a4, s5
; RV32-NEXT:    sll s0, s0, a0
; RV32-NEXT:    or s0, s0, a4
; RV32-NEXT:    srli a4, s1, 1
; RV32-NEXT:    andi s3, s2, 1
; RV32-NEXT:    srl a4, a4, s5
; RV32-NEXT:    sll t6, t6, a0
; RV32-NEXT:    or a5, a5, s3
; RV32-NEXT:    or s2, t6, a4
; RV32-NEXT:    sll t6, s1, a0
; RV32-NEXT:    beqz a5, .LBB3_55
; RV32-NEXT:  # %bb.43: # %udiv-preheader
; RV32-NEXT:    sw s7, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    li s7, 0
; RV32-NEXT:    li s8, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li s10, 0
; RV32-NEXT:    srli a3, a3, 1
; RV32-NEXT:    sw a3, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    lw ra, 16(a2)
; RV32-NEXT:    sw zero, 88(sp)
; RV32-NEXT:    sw zero, 92(sp)
; RV32-NEXT:    sw zero, 96(sp)
; RV32-NEXT:    sw zero, 100(sp)
; RV32-NEXT:    sw zero, 72(sp)
; RV32-NEXT:    sw zero, 76(sp)
; RV32-NEXT:    sw zero, 80(sp)
; RV32-NEXT:    sw zero, 84(sp)
; RV32-NEXT:    srli a2, a1, 3
; RV32-NEXT:    sw s11, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    andi a2, a2, 28
; RV32-NEXT:    addi a3, sp, 40
; RV32-NEXT:    sw a7, 40(sp)
; RV32-NEXT:    sw t0, 44(sp)
; RV32-NEXT:    sw a6, 48(sp)
; RV32-NEXT:    sw t1, 52(sp)
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    lw a3, 16(a2)
; RV32-NEXT:    lw a4, 4(a2)
; RV32-NEXT:    lw a5, 8(a2)
; RV32-NEXT:    lw a6, 12(a2)
; RV32-NEXT:    lw a2, 0(a2)
; RV32-NEXT:    lw a7, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    srl a7, a7, s5
; RV32-NEXT:    sll a0, ra, a0
; RV32-NEXT:    andi t0, a1, 31
; RV32-NEXT:    slli a3, a3, 1
; RV32-NEXT:    xori t0, t0, 31
; RV32-NEXT:    srl t1, a6, a1
; RV32-NEXT:    sll a3, a3, t0
; RV32-NEXT:    or a7, a0, a7
; RV32-NEXT:    or s11, t1, a3
; RV32-NEXT:    li t1, 0
; RV32-NEXT:    srl a0, a5, a1
; RV32-NEXT:    slli a6, a6, 1
; RV32-NEXT:    sll a3, a6, t0
; RV32-NEXT:    slli a5, a5, 1
; RV32-NEXT:    srl a6, a4, a1
; RV32-NEXT:    sll a5, a5, t0
; RV32-NEXT:    or a0, a0, a3
; RV32-NEXT:    or s5, a6, a5
; RV32-NEXT:    andi ra, a7, 1
; RV32-NEXT:    or a3, s4, s6
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    snez a3, a3
; RV32-NEXT:    slli a4, a4, 1
; RV32-NEXT:    lw a5, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    add a3, a5, a3
; RV32-NEXT:    sll a4, a4, t0
; RV32-NEXT:    addi a3, a3, 1
; RV32-NEXT:    or s6, a2, a4
; RV32-NEXT:    andi a3, a3, 1
; RV32-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    lw a5, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    seqz a2, a5
; RV32-NEXT:    lw a4, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    or a3, a5, a4
; RV32-NEXT:    sub a4, a4, a2
; RV32-NEXT:    seqz a2, a3
; RV32-NEXT:    lw a3, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    sub a6, a3, a2
; RV32-NEXT:    sw a6, 32(sp) # 4-byte Folded Spill
; RV32-NEXT:    sltu a2, a3, a2
; RV32-NEXT:    lw a3, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    sub s1, a3, a2
; RV32-NEXT:    addi a5, a5, -1
; RV32-NEXT:    sw a5, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw a4, 8(sp) # 4-byte Folded Spill
; RV32-NEXT:    j .LBB3_45
; RV32-NEXT:  .LBB3_44: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    mv s7, t1
; RV32-NEXT:    lw a7, 28(sp) # 4-byte Folded Reload
; RV32-NEXT:    and a7, a2, a7
; RV32-NEXT:    xor ra, s4, s11
; RV32-NEXT:    xor t0, a0, a7
; RV32-NEXT:    or t0, t0, ra
; RV32-NEXT:    srli t0, s0, 31
; RV32-NEXT:    slli ra, t5, 1
; RV32-NEXT:    srli t1, s2, 31
; RV32-NEXT:    slli s0, s0, 1
; RV32-NEXT:    or t0, ra, t0
; RV32-NEXT:    or t1, s0, t1
; RV32-NEXT:    srli s0, t6, 31
; RV32-NEXT:    slli s2, s2, 1
; RV32-NEXT:    or s0, s2, s0
; RV32-NEXT:    slli t6, t6, 1
; RV32-NEXT:    srli ra, t5, 31
; RV32-NEXT:    or t6, s7, t6
; RV32-NEXT:    or s2, s8, s0
; RV32-NEXT:    or s0, s9, t1
; RV32-NEXT:    or t5, s10, t0
; RV32-NEXT:    or t0, s3, ra
; RV32-NEXT:    andi ra, t0, 1
; RV32-NEXT:    andi t1, a2, 1
; RV32-NEXT:    sltu a2, a0, a7
; RV32-NEXT:    sub a7, a0, a7
; RV32-NEXT:    sub a0, a7, a3
; RV32-NEXT:    sub t0, s4, s11
; RV32-NEXT:    sltu a3, a7, a3
; RV32-NEXT:    sub a2, t0, a2
; RV32-NEXT:    sub s11, a2, a3
; RV32-NEXT:    sub a2, s5, a4
; RV32-NEXT:    lw a7, 36(sp) # 4-byte Folded Reload
; RV32-NEXT:    or a3, a1, a7
; RV32-NEXT:    or a4, t2, t3
; RV32-NEXT:    sub s5, a2, a6
; RV32-NEXT:    or a3, a3, a4
; RV32-NEXT:    sub s6, s6, a5
; RV32-NEXT:    snez a2, a3
; RV32-NEXT:    add a2, t4, a2
; RV32-NEXT:    or a3, a1, t2
; RV32-NEXT:    addi a2, a2, 1
; RV32-NEXT:    seqz a3, a3
; RV32-NEXT:    seqz a4, a1
; RV32-NEXT:    sltu a5, a7, a3
; RV32-NEXT:    sub t4, a7, a3
; RV32-NEXT:    sub t2, t2, a4
; RV32-NEXT:    sub t3, t3, a5
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    or a3, t2, t3
; RV32-NEXT:    or a4, a1, t4
; RV32-NEXT:    andi s3, a2, 1
; RV32-NEXT:    or a3, a4, a3
; RV32-NEXT:    or a2, a3, s3
; RV32-NEXT:    li s7, 0
; RV32-NEXT:    li s8, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li s10, 0
; RV32-NEXT:    lw a4, 8(sp) # 4-byte Folded Reload
; RV32-NEXT:    beqz a2, .LBB3_56
; RV32-NEXT:  .LBB3_45: # %udiv-do-while
; RV32-NEXT:    # =>This Inner Loop Header: Depth=1
; RV32-NEXT:    srli a2, a0, 31
; RV32-NEXT:    slli a3, s11, 1
; RV32-NEXT:    or s4, a3, a2
; RV32-NEXT:    srli a2, s5, 31
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    or a0, a0, a2
; RV32-NEXT:    sw t4, 36(sp) # 4-byte Folded Spill
; RV32-NEXT:    beq s1, s4, .LBB3_47
; RV32-NEXT:  # %bb.46: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    sltu a2, s1, s4
; RV32-NEXT:    j .LBB3_48
; RV32-NEXT:  .LBB3_47: # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    lw a2, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    sltu a2, a2, a0
; RV32-NEXT:  .LBB3_48: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    mv t4, s3
; RV32-NEXT:    srli a3, s6, 31
; RV32-NEXT:    slli s5, s5, 1
; RV32-NEXT:    or s5, s5, a3
; RV32-NEXT:    slli s6, s6, 1
; RV32-NEXT:    andi a3, ra, 1
; RV32-NEXT:    or s6, s6, a3
; RV32-NEXT:    mv s3, s10
; RV32-NEXT:    beq a4, s5, .LBB3_50
; RV32-NEXT:  # %bb.49: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    sltu a3, a4, s5
; RV32-NEXT:    j .LBB3_51
; RV32-NEXT:  .LBB3_50: # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    lw a3, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    sltu a3, a3, s6
; RV32-NEXT:  .LBB3_51: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    mv s10, s9
; RV32-NEXT:    lw a4, 32(sp) # 4-byte Folded Reload
; RV32-NEXT:    xor a4, a4, a0
; RV32-NEXT:    xor a5, s1, s4
; RV32-NEXT:    or a4, a4, a5
; RV32-NEXT:    beqz a4, .LBB3_53
; RV32-NEXT:  # %bb.52: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    mv a3, a2
; RV32-NEXT:  .LBB3_53: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    mv s9, s8
; RV32-NEXT:    srli a2, s11, 31
; RV32-NEXT:    lw a4, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    sub a2, a4, a2
; RV32-NEXT:    sub a2, a2, a3
; RV32-NEXT:    slli a2, a2, 31
; RV32-NEXT:    srai a2, a2, 31
; RV32-NEXT:    lw a3, 20(sp) # 4-byte Folded Reload
; RV32-NEXT:    and s11, a2, a3
; RV32-NEXT:    mv s8, s7
; RV32-NEXT:    lw a5, 24(sp) # 4-byte Folded Reload
; RV32-NEXT:    and a5, a2, a5
; RV32-NEXT:    lw a4, 16(sp) # 4-byte Folded Reload
; RV32-NEXT:    and a4, a2, a4
; RV32-NEXT:    sltu a6, s6, a5
; RV32-NEXT:    mv a3, a6
; RV32-NEXT:    beq s5, a4, .LBB3_44
; RV32-NEXT:  # %bb.54: # %udiv-do-while
; RV32-NEXT:    # in Loop: Header=BB3_45 Depth=1
; RV32-NEXT:    sltu a3, s5, a4
; RV32-NEXT:    j .LBB3_44
; RV32-NEXT:  .LBB3_55:
; RV32-NEXT:    li t1, 0
; RV32-NEXT:  .LBB3_56: # %udiv-loop-exit
; RV32-NEXT:    srli a0, t6, 31
; RV32-NEXT:    slli a2, s2, 1
; RV32-NEXT:    srli a1, s2, 31
; RV32-NEXT:    slli a3, s0, 1
; RV32-NEXT:    or a2, a2, a0
; RV32-NEXT:    or a1, a3, a1
; RV32-NEXT:    srli s0, s0, 31
; RV32-NEXT:    slli a3, t5, 1
; RV32-NEXT:    or a3, a3, s0
; RV32-NEXT:    slli a5, t6, 1
; RV32-NEXT:    srli a4, t5, 31
; RV32-NEXT:    or a5, t1, a5
; RV32-NEXT:    lw s9, 0(sp) # 4-byte Folded Reload
; RV32-NEXT:  .LBB3_57: # %udiv-end
; RV32-NEXT:    sw a5, 0(s9)
; RV32-NEXT:    sw a2, 4(s9)
; RV32-NEXT:    sw a1, 8(s9)
; RV32-NEXT:    sw a3, 12(s9)
; RV32-NEXT:    sb a4, 16(s9)
; RV32-NEXT:    lw ra, 220(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 216(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 212(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 208(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 204(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 200(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 196(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 192(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 180(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 176(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 172(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 224
; RV32-NEXT:    ret
;
; RV64-LABEL: udiv_i129:
; RV64:       # %bb.0: # %_udiv-special-cases
; RV64-NEXT:    ld a3, 8(a2)
; RV64-NEXT:    ld t1, 16(a2)
; RV64-NEXT:    lui a4, 349525
; RV64-NEXT:    ld a2, 0(a2)
; RV64-NEXT:    addi a6, a4, 1365
; RV64-NEXT:    lui a4, 209715
; RV64-NEXT:    slli a5, a6, 32
; RV64-NEXT:    srli a7, a3, 1
; RV64-NEXT:    slli t0, t1, 63
; RV64-NEXT:    add a6, a6, a5
; RV64-NEXT:    or t3, t0, a7
; RV64-NEXT:    addi a7, a4, 819
; RV64-NEXT:    lui a4, 61681
; RV64-NEXT:    slli a5, a7, 32
; RV64-NEXT:    addi t0, a4, -241
; RV64-NEXT:    add a7, a7, a5
; RV64-NEXT:    slli a4, t0, 32
; RV64-NEXT:    slli a5, a3, 63
; RV64-NEXT:    srli t2, a2, 1
; RV64-NEXT:    add t0, t0, a4
; RV64-NEXT:    or t5, t2, a5
; RV64-NEXT:    bnez t3, .LBB3_2
; RV64-NEXT:  # %bb.1: # %_udiv-special-cases
; RV64-NEXT:    srli a4, t5, 1
; RV64-NEXT:    or a4, t5, a4
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 8
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 16
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 32
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    not a4, a4
; RV64-NEXT:    srli a5, a4, 1
; RV64-NEXT:    and a5, a5, a6
; RV64-NEXT:    sub a4, a4, a5
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    and a4, a4, a7
; RV64-NEXT:    and a5, a5, a7
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    and a4, a4, t0
; RV64-NEXT:    slli a5, a4, 8
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 16
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 32
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli a4, a4, 56
; RV64-NEXT:    addi t2, a4, 64
; RV64-NEXT:    j .LBB3_3
; RV64-NEXT:  .LBB3_2:
; RV64-NEXT:    srli a4, t3, 1
; RV64-NEXT:    or a4, t3, a4
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 8
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 16
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 32
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    not a4, a4
; RV64-NEXT:    srli a5, a4, 1
; RV64-NEXT:    and a5, a5, a6
; RV64-NEXT:    sub a4, a4, a5
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    and a4, a4, a7
; RV64-NEXT:    and a5, a5, a7
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    and a4, a4, t0
; RV64-NEXT:    slli a5, a4, 8
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 16
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 32
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli t2, a4, 56
; RV64-NEXT:  .LBB3_3: # %_udiv-special-cases
; RV64-NEXT:    addi sp, sp, -192
; RV64-NEXT:    sd s0, 184(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s1, 176(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s2, 168(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s3, 160(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s4, 152(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s5, 144(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s6, 136(sp) # 8-byte Folded Spill
; RV64-NEXT:    slli a4, a2, 63
; RV64-NEXT:    li t4, 128
; RV64-NEXT:    bnez a4, .LBB3_5
; RV64-NEXT:  # %bb.4: # %_udiv-special-cases
; RV64-NEXT:    li s0, 128
; RV64-NEXT:    j .LBB3_6
; RV64-NEXT:  .LBB3_5:
; RV64-NEXT:    srli a5, a4, 1
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 8
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 16
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    srli a5, a4, 32
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    not a4, a4
; RV64-NEXT:    srli a5, a4, 1
; RV64-NEXT:    and a5, a5, a6
; RV64-NEXT:    sub a4, a4, a5
; RV64-NEXT:    srli a5, a4, 2
; RV64-NEXT:    and a4, a4, a7
; RV64-NEXT:    and a5, a5, a7
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli a5, a4, 4
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    and a4, a4, t0
; RV64-NEXT:    slli a5, a4, 8
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 16
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    slli a5, a4, 32
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    srli s0, a4, 56
; RV64-NEXT:  .LBB3_6: # %_udiv-special-cases
; RV64-NEXT:    ld a5, 0(a1)
; RV64-NEXT:    ld a4, 8(a1)
; RV64-NEXT:    ld a1, 16(a1)
; RV64-NEXT:    or t3, t5, t3
; RV64-NEXT:    addi s1, s0, 128
; RV64-NEXT:    bnez t3, .LBB3_8
; RV64-NEXT:  # %bb.7: # %_udiv-special-cases
; RV64-NEXT:    mv t2, s1
; RV64-NEXT:  .LBB3_8: # %_udiv-special-cases
; RV64-NEXT:    srli t5, a4, 1
; RV64-NEXT:    slli t6, a1, 63
; RV64-NEXT:    or t5, t6, t5
; RV64-NEXT:    slli t6, a4, 63
; RV64-NEXT:    srli s2, a5, 1
; RV64-NEXT:    snez s3, t3
; RV64-NEXT:    or t6, s2, t6
; RV64-NEXT:    bnez t5, .LBB3_10
; RV64-NEXT:  # %bb.9: # %_udiv-special-cases
; RV64-NEXT:    srli t3, t6, 1
; RV64-NEXT:    or t3, t6, t3
; RV64-NEXT:    srli s2, t3, 2
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 4
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 8
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 16
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 32
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    not t3, t3
; RV64-NEXT:    srli s2, t3, 1
; RV64-NEXT:    and s2, s2, a6
; RV64-NEXT:    sub t3, t3, s2
; RV64-NEXT:    srli s2, t3, 2
; RV64-NEXT:    and t3, t3, a7
; RV64-NEXT:    and s2, s2, a7
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    srli s2, t3, 4
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    and t3, t3, t0
; RV64-NEXT:    slli s2, t3, 8
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    slli s2, t3, 16
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    slli s2, t3, 32
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    srli t3, t3, 56
; RV64-NEXT:    addi s2, t3, 64
; RV64-NEXT:    j .LBB3_11
; RV64-NEXT:  .LBB3_10:
; RV64-NEXT:    srli t3, t5, 1
; RV64-NEXT:    or t3, t5, t3
; RV64-NEXT:    srli s2, t3, 2
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 4
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 8
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 16
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    srli s2, t3, 32
; RV64-NEXT:    or t3, t3, s2
; RV64-NEXT:    not t3, t3
; RV64-NEXT:    srli s2, t3, 1
; RV64-NEXT:    and s2, s2, a6
; RV64-NEXT:    sub t3, t3, s2
; RV64-NEXT:    srli s2, t3, 2
; RV64-NEXT:    and t3, t3, a7
; RV64-NEXT:    and s2, s2, a7
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    srli s2, t3, 4
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    and t3, t3, t0
; RV64-NEXT:    slli s2, t3, 8
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    slli s2, t3, 16
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    slli s2, t3, 32
; RV64-NEXT:    add t3, t3, s2
; RV64-NEXT:    srli s2, t3, 56
; RV64-NEXT:  .LBB3_11: # %_udiv-special-cases
; RV64-NEXT:    andi t3, a1, 1
; RV64-NEXT:    andi a1, t1, 1
; RV64-NEXT:    or t1, a2, a3
; RV64-NEXT:    sltu s0, s1, s0
; RV64-NEXT:    slli s4, a5, 63
; RV64-NEXT:    or s1, a5, a4
; RV64-NEXT:    addi s3, s3, -1
; RV64-NEXT:    beqz s4, .LBB3_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    srli t4, s4, 1
; RV64-NEXT:    or t4, s4, t4
; RV64-NEXT:    srli s4, t4, 2
; RV64-NEXT:    or t4, t4, s4
; RV64-NEXT:    srli s4, t4, 4
; RV64-NEXT:    or t4, t4, s4
; RV64-NEXT:    srli s4, t4, 8
; RV64-NEXT:    or t4, t4, s4
; RV64-NEXT:    srli s4, t4, 16
; RV64-NEXT:    or t4, t4, s4
; RV64-NEXT:    srli s4, t4, 32
; RV64-NEXT:    or t4, t4, s4
; RV64-NEXT:    not t4, t4
; RV64-NEXT:    srli s4, t4, 1
; RV64-NEXT:    and a6, s4, a6
; RV64-NEXT:    sub a6, t4, a6
; RV64-NEXT:    srli t4, a6, 2
; RV64-NEXT:    and a6, a6, a7
; RV64-NEXT:    and a7, t4, a7
; RV64-NEXT:    add a6, a6, a7
; RV64-NEXT:    srli a7, a6, 4
; RV64-NEXT:    add a6, a6, a7
; RV64-NEXT:    and a6, a6, t0
; RV64-NEXT:    slli a7, a6, 8
; RV64-NEXT:    add a6, a6, a7
; RV64-NEXT:    slli a7, a6, 16
; RV64-NEXT:    add a6, a6, a7
; RV64-NEXT:    slli a7, a6, 32
; RV64-NEXT:    add a6, a6, a7
; RV64-NEXT:    srli t4, a6, 56
; RV64-NEXT:  .LBB3_13: # %_udiv-special-cases
; RV64-NEXT:    or t0, t1, a1
; RV64-NEXT:    or a7, s1, t3
; RV64-NEXT:    or t5, t6, t5
; RV64-NEXT:    and a6, s3, s0
; RV64-NEXT:    addi t6, t4, 128
; RV64-NEXT:    bnez t5, .LBB3_15
; RV64-NEXT:  # %bb.14: # %_udiv-special-cases
; RV64-NEXT:    mv s2, t6
; RV64-NEXT:  .LBB3_15: # %_udiv-special-cases
; RV64-NEXT:    seqz t0, t0
; RV64-NEXT:    snez t5, t5
; RV64-NEXT:    sltu t4, t6, t4
; RV64-NEXT:    addi t5, t5, -1
; RV64-NEXT:    and t6, t5, t4
; RV64-NEXT:    sltu s0, t2, s2
; RV64-NEXT:    seqz a7, a7
; RV64-NEXT:    mv t4, s0
; RV64-NEXT:    beq a6, t6, .LBB3_17
; RV64-NEXT:  # %bb.16: # %_udiv-special-cases
; RV64-NEXT:    sltu t4, a6, t6
; RV64-NEXT:  .LBB3_17: # %_udiv-special-cases
; RV64-NEXT:    or t5, t0, a7
; RV64-NEXT:    andi t4, t4, 1
; RV64-NEXT:    sub a6, a6, t6
; RV64-NEXT:    sub a7, a6, s0
; RV64-NEXT:    sub t0, t2, s2
; RV64-NEXT:    beqz t4, .LBB3_19
; RV64-NEXT:  # %bb.18: # %_udiv-special-cases
; RV64-NEXT:    mv a6, t4
; RV64-NEXT:    j .LBB3_20
; RV64-NEXT:  .LBB3_19:
; RV64-NEXT:    sltiu a6, t0, 129
; RV64-NEXT:    xori a6, a6, 1
; RV64-NEXT:    snez t2, a7
; RV64-NEXT:    or a6, a6, t2
; RV64-NEXT:  .LBB3_20: # %_udiv-special-cases
; RV64-NEXT:    or t6, t5, a6
; RV64-NEXT:    addi t5, t6, -1
; RV64-NEXT:    and a6, t3, t5
; RV64-NEXT:    and t2, t5, a4
; RV64-NEXT:    and t5, t5, a5
; RV64-NEXT:    bnez t6, .LBB3_29
; RV64-NEXT:  # %bb.21: # %_udiv-special-cases
; RV64-NEXT:    xori t6, t0, 128
; RV64-NEXT:    or s0, t4, a7
; RV64-NEXT:    or t6, t6, s0
; RV64-NEXT:    beqz t6, .LBB3_29
; RV64-NEXT:  # %bb.22: # %udiv-bb1
; RV64-NEXT:    addi a6, t0, 1
; RV64-NEXT:    li t2, 128
; RV64-NEXT:    seqz t5, a6
; RV64-NEXT:    sub t2, t2, t0
; RV64-NEXT:    add a7, a7, t5
; RV64-NEXT:    sd zero, 64(sp)
; RV64-NEXT:    sd zero, 72(sp)
; RV64-NEXT:    sd zero, 80(sp)
; RV64-NEXT:    sd zero, 88(sp)
; RV64-NEXT:    srli t5, t2, 3
; RV64-NEXT:    andi t5, t5, 24
; RV64-NEXT:    addi t6, sp, 96
; RV64-NEXT:    sd a5, 96(sp)
; RV64-NEXT:    sd a4, 104(sp)
; RV64-NEXT:    sd t3, 112(sp)
; RV64-NEXT:    sub s4, t6, t5
; RV64-NEXT:    or t5, a6, a7
; RV64-NEXT:    ld t6, 0(s4)
; RV64-NEXT:    seqz s0, t5
; RV64-NEXT:    add t4, t4, s0
; RV64-NEXT:    ld s2, 8(s4)
; RV64-NEXT:    andi t4, t4, 1
; RV64-NEXT:    andi t2, t2, 63
; RV64-NEXT:    xori s1, t2, 63
; RV64-NEXT:    srli t2, t6, 1
; RV64-NEXT:    srl t2, t2, s1
; RV64-NEXT:    neg s3, t0
; RV64-NEXT:    sll t0, s2, s3
; RV64-NEXT:    or t5, t5, t4
; RV64-NEXT:    or t0, t0, t2
; RV64-NEXT:    sll t2, t6, s3
; RV64-NEXT:    beqz t5, .LBB3_28
; RV64-NEXT:  # %bb.23: # %udiv-preheader
; RV64-NEXT:    li t5, 0
; RV64-NEXT:    li t6, 0
; RV64-NEXT:    li s0, 0
; RV64-NEXT:    ld s4, 16(s4)
; RV64-NEXT:    srli s5, a6, 3
; RV64-NEXT:    sd zero, 32(sp)
; RV64-NEXT:    sd zero, 40(sp)
; RV64-NEXT:    sd zero, 48(sp)
; RV64-NEXT:    sd zero, 56(sp)
; RV64-NEXT:    andi s5, s5, 24
; RV64-NEXT:    mv s6, sp
; RV64-NEXT:    sd a5, 0(sp)
; RV64-NEXT:    sd a4, 8(sp)
; RV64-NEXT:    sd t3, 16(sp)
; RV64-NEXT:    sd zero, 24(sp)
; RV64-NEXT:    add s5, s6, s5
; RV64-NEXT:    ld a4, 8(s5)
; RV64-NEXT:    srli a5, s2, 1
; RV64-NEXT:    ld t3, 16(s5)
; RV64-NEXT:    srl a5, a5, s1
; RV64-NEXT:    sll s1, s4, s3
; RV64-NEXT:    ld s2, 0(s5)
; RV64-NEXT:    or a5, s1, a5
; RV64-NEXT:    srl s1, a4, a6
; RV64-NEXT:    andi s3, a6, 63
; RV64-NEXT:    slli t3, t3, 1
; RV64-NEXT:    xori s3, s3, 63
; RV64-NEXT:    sll t3, t3, s3
; RV64-NEXT:    slli a4, a4, 1
; RV64-NEXT:    or t3, s1, t3
; RV64-NEXT:    sll a4, a4, s3
; RV64-NEXT:    andi s1, a5, 1
; RV64-NEXT:    srl a5, s2, a6
; RV64-NEXT:    or s2, a5, a4
; RV64-NEXT:    snez a4, t1
; RV64-NEXT:    seqz a5, a2
; RV64-NEXT:    add a4, a1, a4
; RV64-NEXT:    sub a1, a3, a5
; RV64-NEXT:    addi a4, a4, 1
; RV64-NEXT:    andi a4, a4, 1
; RV64-NEXT:    addi a5, a2, -1
; RV64-NEXT:    j .LBB3_26
; RV64-NEXT:  .LBB3_24: # %udiv-do-while
; RV64-NEXT:    # in Loop: Header=BB3_26 Depth=1
; RV64-NEXT:    sltu s1, a1, t1
; RV64-NEXT:  .LBB3_25: # %udiv-do-while
; RV64-NEXT:    # in Loop: Header=BB3_26 Depth=1
; RV64-NEXT:    srli t3, t3, 63
; RV64-NEXT:    sub t3, a4, t3
; RV64-NEXT:    sub t3, t3, s1
; RV64-NEXT:    slli t3, t3, 63
; RV64-NEXT:    srai t3, t3, 63
; RV64-NEXT:    and s3, t3, a3
; RV64-NEXT:    srli s1, t2, 63
; RV64-NEXT:    slli s4, t0, 1
; RV64-NEXT:    slli t2, t2, 1
; RV64-NEXT:    or s1, s4, s1
; RV64-NEXT:    srli s4, t0, 63
; RV64-NEXT:    or t2, t5, t2
; RV64-NEXT:    or t0, t6, s1
; RV64-NEXT:    or s1, s0, s4
; RV64-NEXT:    andi s1, s1, 1
; RV64-NEXT:    and t6, t3, a2
; RV64-NEXT:    sltu s0, s2, t6
; RV64-NEXT:    sub t1, t1, s3
; RV64-NEXT:    andi t5, t3, 1
; RV64-NEXT:    sub t3, t1, s0
; RV64-NEXT:    sub s2, s2, t6
; RV64-NEXT:    or t1, a6, a7
; RV64-NEXT:    seqz t6, a6
; RV64-NEXT:    snez t1, t1
; RV64-NEXT:    sub a7, a7, t6
; RV64-NEXT:    add t1, t4, t1
; RV64-NEXT:    addi t1, t1, 1
; RV64-NEXT:    addi a6, a6, -1
; RV64-NEXT:    andi t4, t1, 1
; RV64-NEXT:    or t1, a6, a7
; RV64-NEXT:    or t1, t1, t4
; RV64-NEXT:    li t6, 0
; RV64-NEXT:    li s0, 0
; RV64-NEXT:    beqz t1, .LBB3_28
; RV64-NEXT:  .LBB3_26: # %udiv-do-while
; RV64-NEXT:    # =>This Inner Loop Header: Depth=1
; RV64-NEXT:    srli t1, s2, 63
; RV64-NEXT:    slli s3, t3, 1
; RV64-NEXT:    or t1, s3, t1
; RV64-NEXT:    slli s2, s2, 1
; RV64-NEXT:    andi s1, s1, 1
; RV64-NEXT:    or s2, s2, s1
; RV64-NEXT:    bne a1, t1, .LBB3_24
; RV64-NEXT:  # %bb.27: # in Loop: Header=BB3_26 Depth=1
; RV64-NEXT:    sltu s1, a5, s2
; RV64-NEXT:    j .LBB3_25
; RV64-NEXT:  .LBB3_28: # %udiv-loop-exit
; RV64-NEXT:    srli a1, t2, 63
; RV64-NEXT:    slli a2, t0, 1
; RV64-NEXT:    slli a3, t2, 1
; RV64-NEXT:    or t2, a2, a1
; RV64-NEXT:    srli a6, t0, 63
; RV64-NEXT:    or t5, t5, a3
; RV64-NEXT:  .LBB3_29: # %udiv-end
; RV64-NEXT:    sd t5, 0(a0)
; RV64-NEXT:    sd t2, 8(a0)
; RV64-NEXT:    sb a6, 16(a0)
; RV64-NEXT:    ld s0, 184(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s1, 176(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s2, 168(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s3, 160(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s4, 152(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s5, 144(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s6, 136(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 192
; RV64-NEXT:    ret
  %res = udiv i129 %x, %y
  ret i129 %res
}
