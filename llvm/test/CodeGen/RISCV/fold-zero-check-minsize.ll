; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+m,+v -enable-machine-outliner=never < %s | FileCheck %s

; Scaler tests
define i1 @fold_or_eq_zero_i16(i16 zeroext %a, i16 zeroext %b) minsize {
; CHECK-LABEL: fold_or_eq_zero_i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    mul a0, a0, a1
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0
  %cmp2 = icmp eq i16 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @fold_and_ne_zero_i16(i16 zeroext %a, i16 zeroext %b) minsize {
; CHECK-LABEL: fold_and_ne_zero_i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    mul a0, a0, a1
; CHECK-NEXT:    snez a0, a0
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp ne i16 %a, 0
  %cmp2 = icmp ne i16 %b, 0
  %and = and i1 %cmp1, %cmp2
  ret i1 %and
}

; Vector tests
define <8 x i1> @fold_vector_or_eq_zero_v8i16(<8 x i16> %a, <8 x i16> %b) minsize {
; CHECK-LABEL: fold_vector_or_eq_zero_v8i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    li a0, 255
; CHECK-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vand.vx v9, v9, a0
; CHECK-NEXT:    vmul.vv v8, v8, v9
; CHECK-NEXT:    vmseq.vi v0, v8, 0
; CHECK-NEXT:    ret
entry:
  %mask_splat = shufflevector <8 x i16> insertelement (<8 x i16> poison, i16 255, i64 0), <8 x i16> poison, <8 x i32> zeroinitializer
  %a.masked = and <8 x i16> %a, %mask_splat
  %b.masked = and <8 x i16> %b, %mask_splat
  %cmp1 = icmp eq <8 x i16> %a.masked, zeroinitializer
  %cmp2 = icmp eq <8 x i16> %b.masked, zeroinitializer
  %or = or <8 x i1> %cmp1, %cmp2
  ret <8 x i1> %or
}

define <4 x i1> @fold_vector_and_ne_zero_v4i32(<4 x i32> %a, <4 x i32> %b) minsize {
; CHECK-LABEL: fold_vector_and_ne_zero_v4i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lui a0, 16
; CHECK-NEXT:    addi a0, a0, -1
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vand.vx v9, v9, a0
; CHECK-NEXT:    vmul.vv v8, v8, v9
; CHECK-NEXT:    vmsne.vi v0, v8, 0
; CHECK-NEXT:    ret
entry:
  %mask_splat = shufflevector <4 x i32> insertelement (<4 x i32> poison, i32 65535, i64 0), <4 x i32> poison, <4 x i32> zeroinitializer
  %a.masked = and <4 x i32> %a, %mask_splat
  %b.masked = and <4 x i32> %b, %mask_splat
  %cmp1 = icmp ne <4 x i32> %a.masked, zeroinitializer
  %cmp2 = icmp ne <4 x i32> %b.masked, zeroinitializer
  %and = and <4 x i1> %cmp1, %cmp2
  ret <4 x i1> %and
}

; Negative tests
define i1 @negative_fold_multi_use(i16 zeroext %a, i16 zeroext %b, ptr %out) minsize {
; CHECK-LABEL: negative_fold_multi_use:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a3, a0
; CHECK-NEXT:    seqz a0, a1
; CHECK-NEXT:    or a0, a3, a0
; CHECK-NEXT:    sb a3, 0(a2)
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0
  store i1 %cmp1, ptr %out
  %cmp2 = icmp eq i16 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_potential_overflow_i64(i64 %a, i64 %b) minsize {
; CHECK-LABEL: negative_fold_potential_overflow_i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    seqz a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i64 %a, 0
  %cmp2 = icmp eq i64 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_no_minsize(i16 zeroext %a, i16 zeroext %b) {
; CHECK-LABEL: negative_fold_no_minsize:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    seqz a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0
  %cmp2 = icmp eq i16 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_mismatched_predicates(i16 zeroext %a, i16 zeroext %b) minsize {
; CHECK-LABEL: negative_fold_mismatched_predicates:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    snez a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0
  %cmp2 = icmp ne i16 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_lhs_nonzero_constant(i16 zeroext %a, i16 zeroext %b) minsize {
; CHECK-LABEL: negative_fold_lhs_nonzero_constant:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi a0, a0, -1
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    seqz a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 1;
  %cmp2 = icmp eq i16 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_rhs_nonzero_constant(i16 zeroext %a, i16 zeroext %b) minsize {
; CHECK-LABEL: negative_fold_rhs_nonzero_constant:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    seqz a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0;
  %cmp2 = icmp eq i16 %b, 1
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @negative_fold_type_mismatch(i16 zeroext %a, i32 signext %b) minsize {
; CHECK-LABEL: negative_fold_type_mismatch:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    seqz a1, a1
; CHECK-NEXT:    or a0, a0, a1
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq i16 %a, 0
  %cmp2 = icmp eq i32 %b, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define i1 @fold_nsw_unsafe_proof(i8 zeroext %a, i8 zeroext %b) minsize {
; CHECK-LABEL: fold_nsw_unsafe_proof:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    andi a0, a0, 15
; CHECK-NEXT:    andi a1, a1, 15
; CHECK-NEXT:    mul a0, a0, a1
; CHECK-NEXT:    seqz a0, a0
; CHECK-NEXT:    ret
entry:
  %a.masked = and i8 %a, 15
  %b.masked = and i8 %b, 15
  %cmp1 = icmp eq i8 %a.masked, 0
  %cmp2 = icmp eq i8 %b.masked, 0
  %or = or i1 %cmp1, %cmp2
  ret i1 %or
}

define <8 x i1> @negative_fold_vector_potential_overflow(<8 x i16> %a, <8 x i16> %b) minsize {
; CHECK-LABEL: negative_fold_vector_potential_overflow:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; CHECK-NEXT:    vmseq.vi v8, v8, 0
; CHECK-NEXT:    vmseq.vi v9, v9, 0
; CHECK-NEXT:    vmor.mm v0, v8, v9
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq <8 x i16> %a, zeroinitializer
  %cmp2 = icmp eq <8 x i16> %b, zeroinitializer
  %or = or <8 x i1> %cmp1, %cmp2
  ret <8 x i1> %or
}

define <8 x i1> @negative_fold_vector_multi_use(<8 x i16> %a, <8 x i16> %b, ptr %out) minsize {
; CHECK-LABEL: negative_fold_vector_multi_use:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; CHECK-NEXT:    vmseq.vi v8, v8, 0
; CHECK-NEXT:    vmseq.vi v9, v9, 0
; CHECK-NEXT:    vmor.mm v0, v8, v9
; CHECK-NEXT:    vsm.v v8, (a0)
; CHECK-NEXT:    ret
entry:
  %cmp1 = icmp eq <8 x i16> %a, zeroinitializer
  store <8 x i1> %cmp1, ptr %out  ;
  %cmp2 = icmp eq <8 x i16> %b, zeroinitializer
  %or = or <8 x i1> %cmp1, %cmp2
  ret <8 x i1> %or
}
