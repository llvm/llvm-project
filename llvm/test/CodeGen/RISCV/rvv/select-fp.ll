; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+d,+experimental-zfh,+experimental-v -target-abi=ilp32d \
; RUN:     -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV32
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-zfh,+experimental-v -target-abi=lp64d \
; RUN:     -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV64

define <vscale x 1 x half> @select_nxv1f16(i1 zeroext %c, <vscale x 1 x half> %a, <vscale x 1 x half> %b) {
; CHECK-LABEL: select_nxv1f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB0_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, mf4, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 1 x half> %a, <vscale x 1 x half> %b
  ret <vscale x 1 x half> %v
}

define <vscale x 1 x half> @selectcc_nxv1f16(half %a, half %b, <vscale x 1 x half> %c, <vscale x 1 x half> %d) {
; CHECK-LABEL: selectcc_nxv1f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB1_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB1_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, mf4, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a0
; CHECK-NEXT:    vmv.v.x v26, a0
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 1 x half> %c, <vscale x 1 x half> %d
  ret <vscale x 1 x half> %v
}

define <vscale x 2 x half> @select_nxv2f16(i1 zeroext %c, <vscale x 2 x half> %a, <vscale x 2 x half> %b) {
; CHECK-LABEL: select_nxv2f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB2_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB2_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 2 x half> %a, <vscale x 2 x half> %b
  ret <vscale x 2 x half> %v
}

define <vscale x 2 x half> @selectcc_nxv2f16(half %a, half %b, <vscale x 2 x half> %c, <vscale x 2 x half> %d) {
; CHECK-LABEL: selectcc_nxv2f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB3_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB3_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a0
; CHECK-NEXT:    vmv.v.x v26, a0
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 2 x half> %c, <vscale x 2 x half> %d
  ret <vscale x 2 x half> %v
}

define <vscale x 4 x half> @select_nxv4f16(i1 zeroext %c, <vscale x 4 x half> %a, <vscale x 4 x half> %b) {
; CHECK-LABEL: select_nxv4f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB4_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB4_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, m1, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 4 x half> %a, <vscale x 4 x half> %b
  ret <vscale x 4 x half> %v
}

define <vscale x 4 x half> @selectcc_nxv4f16(half %a, half %b, <vscale x 4 x half> %c, <vscale x 4 x half> %d) {
; CHECK-LABEL: selectcc_nxv4f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB5_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB5_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, m1, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a0
; CHECK-NEXT:    vmv.v.x v26, a0
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 4 x half> %c, <vscale x 4 x half> %d
  ret <vscale x 4 x half> %v
}

define <vscale x 8 x half> @select_nxv8f16(i1 zeroext %c, <vscale x 8 x half> %a, <vscale x 8 x half> %b) {
; CHECK-LABEL: select_nxv8f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB6_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB6_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, m2, ta, mu
; CHECK-NEXT:    vand.vx v26, v8, a1
; CHECK-NEXT:    vmv.v.x v28, a1
; CHECK-NEXT:    vxor.vi v28, v28, -1
; CHECK-NEXT:    vand.vv v28, v10, v28
; CHECK-NEXT:    vor.vv v8, v26, v28
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 8 x half> %a, <vscale x 8 x half> %b
  ret <vscale x 8 x half> %v
}

define <vscale x 8 x half> @selectcc_nxv8f16(half %a, half %b, <vscale x 8 x half> %c, <vscale x 8 x half> %d) {
; CHECK-LABEL: selectcc_nxv8f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB7_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB7_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, m2, ta, mu
; CHECK-NEXT:    vand.vx v26, v8, a0
; CHECK-NEXT:    vmv.v.x v28, a0
; CHECK-NEXT:    vxor.vi v28, v28, -1
; CHECK-NEXT:    vand.vv v28, v10, v28
; CHECK-NEXT:    vor.vv v8, v26, v28
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 8 x half> %c, <vscale x 8 x half> %d
  ret <vscale x 8 x half> %v
}

define <vscale x 16 x half> @select_nxv16f16(i1 zeroext %c, <vscale x 16 x half> %a, <vscale x 16 x half> %b) {
; CHECK-LABEL: select_nxv16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB8_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB8_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, m4, ta, mu
; CHECK-NEXT:    vand.vx v28, v8, a1
; CHECK-NEXT:    vmv.v.x v8, a1
; CHECK-NEXT:    vxor.vi v8, v8, -1
; CHECK-NEXT:    vand.vv v8, v12, v8
; CHECK-NEXT:    vor.vv v8, v28, v8
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 16 x half> %a, <vscale x 16 x half> %b
  ret <vscale x 16 x half> %v
}

define <vscale x 16 x half> @selectcc_nxv16f16(half %a, half %b, <vscale x 16 x half> %c, <vscale x 16 x half> %d) {
; CHECK-LABEL: selectcc_nxv16f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB9_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB9_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, m4, ta, mu
; CHECK-NEXT:    vand.vx v28, v8, a0
; CHECK-NEXT:    vmv.v.x v8, a0
; CHECK-NEXT:    vxor.vi v8, v8, -1
; CHECK-NEXT:    vand.vv v8, v12, v8
; CHECK-NEXT:    vor.vv v8, v28, v8
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 16 x half> %c, <vscale x 16 x half> %d
  ret <vscale x 16 x half> %v
}

define <vscale x 32 x half> @select_nxv32f16(i1 zeroext %c, <vscale x 32 x half> %a, <vscale x 32 x half> %b) {
; CHECK-LABEL: select_nxv32f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB10_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB10_2:
; CHECK-NEXT:    vsetvli a0, zero, e16, m8, ta, mu
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vmv.v.x v24, a1
; CHECK-NEXT:    vxor.vi v24, v24, -1
; CHECK-NEXT:    vand.vv v16, v16, v24
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 32 x half> %a, <vscale x 32 x half> %b
  ret <vscale x 32 x half> %v
}

define <vscale x 32 x half> @selectcc_nxv32f16(half %a, half %b, <vscale x 32 x half> %c, <vscale x 32 x half> %d) {
; CHECK-LABEL: selectcc_nxv32f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB11_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB11_2:
; CHECK-NEXT:    vsetvli a1, zero, e16, m8, ta, mu
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmv.v.x v24, a0
; CHECK-NEXT:    vxor.vi v24, v24, -1
; CHECK-NEXT:    vand.vv v16, v16, v24
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <vscale x 32 x half> %c, <vscale x 32 x half> %d
  ret <vscale x 32 x half> %v
}

define <vscale x 1 x float> @select_nxv1f32(i1 zeroext %c, <vscale x 1 x float> %a, <vscale x 1 x float> %b) {
; CHECK-LABEL: select_nxv1f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB12_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB12_2:
; CHECK-NEXT:    vsetvli a0, zero, e32, mf2, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 1 x float> %a, <vscale x 1 x float> %b
  ret <vscale x 1 x float> %v
}

define <vscale x 1 x float> @selectcc_nxv1f32(float %a, float %b, <vscale x 1 x float> %c, <vscale x 1 x float> %d) {
; CHECK-LABEL: selectcc_nxv1f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB13_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB13_2:
; CHECK-NEXT:    vsetvli a1, zero, e32, mf2, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a0
; CHECK-NEXT:    vmv.v.x v26, a0
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <vscale x 1 x float> %c, <vscale x 1 x float> %d
  ret <vscale x 1 x float> %v
}

define <vscale x 2 x float> @select_nxv2f32(i1 zeroext %c, <vscale x 2 x float> %a, <vscale x 2 x float> %b) {
; CHECK-LABEL: select_nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB14_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB14_2:
; CHECK-NEXT:    vsetvli a0, zero, e32, m1, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a1
; CHECK-NEXT:    vmv.v.x v26, a1
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 2 x float> %a, <vscale x 2 x float> %b
  ret <vscale x 2 x float> %v
}

define <vscale x 2 x float> @selectcc_nxv2f32(float %a, float %b, <vscale x 2 x float> %c, <vscale x 2 x float> %d) {
; CHECK-LABEL: selectcc_nxv2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB15_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB15_2:
; CHECK-NEXT:    vsetvli a1, zero, e32, m1, ta, mu
; CHECK-NEXT:    vand.vx v25, v8, a0
; CHECK-NEXT:    vmv.v.x v26, a0
; CHECK-NEXT:    vxor.vi v26, v26, -1
; CHECK-NEXT:    vand.vv v26, v9, v26
; CHECK-NEXT:    vor.vv v8, v25, v26
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <vscale x 2 x float> %c, <vscale x 2 x float> %d
  ret <vscale x 2 x float> %v
}

define <vscale x 4 x float> @select_nxv4f32(i1 zeroext %c, <vscale x 4 x float> %a, <vscale x 4 x float> %b) {
; CHECK-LABEL: select_nxv4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB16_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB16_2:
; CHECK-NEXT:    vsetvli a0, zero, e32, m2, ta, mu
; CHECK-NEXT:    vand.vx v26, v8, a1
; CHECK-NEXT:    vmv.v.x v28, a1
; CHECK-NEXT:    vxor.vi v28, v28, -1
; CHECK-NEXT:    vand.vv v28, v10, v28
; CHECK-NEXT:    vor.vv v8, v26, v28
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 4 x float> %a, <vscale x 4 x float> %b
  ret <vscale x 4 x float> %v
}

define <vscale x 4 x float> @selectcc_nxv4f32(float %a, float %b, <vscale x 4 x float> %c, <vscale x 4 x float> %d) {
; CHECK-LABEL: selectcc_nxv4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB17_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB17_2:
; CHECK-NEXT:    vsetvli a1, zero, e32, m2, ta, mu
; CHECK-NEXT:    vand.vx v26, v8, a0
; CHECK-NEXT:    vmv.v.x v28, a0
; CHECK-NEXT:    vxor.vi v28, v28, -1
; CHECK-NEXT:    vand.vv v28, v10, v28
; CHECK-NEXT:    vor.vv v8, v26, v28
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <vscale x 4 x float> %c, <vscale x 4 x float> %d
  ret <vscale x 4 x float> %v
}

define <vscale x 8 x float> @select_nxv8f32(i1 zeroext %c, <vscale x 8 x float> %a, <vscale x 8 x float> %b) {
; CHECK-LABEL: select_nxv8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB18_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB18_2:
; CHECK-NEXT:    vsetvli a0, zero, e32, m4, ta, mu
; CHECK-NEXT:    vand.vx v28, v8, a1
; CHECK-NEXT:    vmv.v.x v8, a1
; CHECK-NEXT:    vxor.vi v8, v8, -1
; CHECK-NEXT:    vand.vv v8, v12, v8
; CHECK-NEXT:    vor.vv v8, v28, v8
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 8 x float> %a, <vscale x 8 x float> %b
  ret <vscale x 8 x float> %v
}

define <vscale x 8 x float> @selectcc_nxv8f32(float %a, float %b, <vscale x 8 x float> %c, <vscale x 8 x float> %d) {
; CHECK-LABEL: selectcc_nxv8f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB19_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB19_2:
; CHECK-NEXT:    vsetvli a1, zero, e32, m4, ta, mu
; CHECK-NEXT:    vand.vx v28, v8, a0
; CHECK-NEXT:    vmv.v.x v8, a0
; CHECK-NEXT:    vxor.vi v8, v8, -1
; CHECK-NEXT:    vand.vv v8, v12, v8
; CHECK-NEXT:    vor.vv v8, v28, v8
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <vscale x 8 x float> %c, <vscale x 8 x float> %d
  ret <vscale x 8 x float> %v
}

define <vscale x 16 x float> @select_nxv16f32(i1 zeroext %c, <vscale x 16 x float> %a, <vscale x 16 x float> %b) {
; CHECK-LABEL: select_nxv16f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi a1, zero, -1
; CHECK-NEXT:    bnez a0, .LBB20_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a1, zero
; CHECK-NEXT:  .LBB20_2:
; CHECK-NEXT:    vsetvli a0, zero, e32, m8, ta, mu
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vmv.v.x v24, a1
; CHECK-NEXT:    vxor.vi v24, v24, -1
; CHECK-NEXT:    vand.vv v16, v16, v24
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    ret
  %v = select i1 %c, <vscale x 16 x float> %a, <vscale x 16 x float> %b
  ret <vscale x 16 x float> %v
}

define <vscale x 16 x float> @selectcc_nxv16f32(float %a, float %b, <vscale x 16 x float> %c, <vscale x 16 x float> %d) {
; CHECK-LABEL: selectcc_nxv16f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a1, fa0, fa1
; CHECK-NEXT:    addi a0, zero, -1
; CHECK-NEXT:    bnez a1, .LBB21_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    mv a0, zero
; CHECK-NEXT:  .LBB21_2:
; CHECK-NEXT:    vsetvli a1, zero, e32, m8, ta, mu
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmv.v.x v24, a0
; CHECK-NEXT:    vxor.vi v24, v24, -1
; CHECK-NEXT:    vand.vv v16, v16, v24
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <vscale x 16 x float> %c, <vscale x 16 x float> %d
  ret <vscale x 16 x float> %v
}

define <vscale x 1 x double> @select_nxv1f64(i1 zeroext %c, <vscale x 1 x double> %a, <vscale x 1 x double> %b) {
; RV32-LABEL: select_nxv1f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    addi a1, zero, -1
; RV32-NEXT:    bnez a0, .LBB22_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a1, zero
; RV32-NEXT:  .LBB22_2:
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v25, (a0), zero
; RV32-NEXT:    vand.vv v26, v8, v25
; RV32-NEXT:    vxor.vi v25, v25, -1
; RV32-NEXT:    vand.vv v25, v9, v25
; RV32-NEXT:    vor.vv v8, v26, v25
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: select_nxv1f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi a1, zero, -1
; RV64-NEXT:    bnez a0, .LBB22_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a1, zero
; RV64-NEXT:  .LBB22_2:
; RV64-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; RV64-NEXT:    vand.vx v25, v8, a1
; RV64-NEXT:    vmv.v.x v26, a1
; RV64-NEXT:    vxor.vi v26, v26, -1
; RV64-NEXT:    vand.vv v26, v9, v26
; RV64-NEXT:    vor.vv v8, v25, v26
; RV64-NEXT:    ret
  %v = select i1 %c, <vscale x 1 x double> %a, <vscale x 1 x double> %b
  ret <vscale x 1 x double> %v
}

define <vscale x 1 x double> @selectcc_nxv1f64(double %a, double %b, <vscale x 1 x double> %c, <vscale x 1 x double> %d) {
; RV32-LABEL: selectcc_nxv1f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    feq.d a1, fa0, fa1
; RV32-NEXT:    addi a0, zero, -1
; RV32-NEXT:    bnez a1, .LBB23_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a0, zero
; RV32-NEXT:  .LBB23_2:
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v25, (a0), zero
; RV32-NEXT:    vand.vv v26, v8, v25
; RV32-NEXT:    vxor.vi v25, v25, -1
; RV32-NEXT:    vand.vv v25, v9, v25
; RV32-NEXT:    vor.vv v8, v26, v25
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_nxv1f64:
; RV64:       # %bb.0:
; RV64-NEXT:    feq.d a1, fa0, fa1
; RV64-NEXT:    addi a0, zero, -1
; RV64-NEXT:    bnez a1, .LBB23_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a0, zero
; RV64-NEXT:  .LBB23_2:
; RV64-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; RV64-NEXT:    vand.vx v25, v8, a0
; RV64-NEXT:    vmv.v.x v26, a0
; RV64-NEXT:    vxor.vi v26, v26, -1
; RV64-NEXT:    vand.vv v26, v9, v26
; RV64-NEXT:    vor.vv v8, v25, v26
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <vscale x 1 x double> %c, <vscale x 1 x double> %d
  ret <vscale x 1 x double> %v
}

define <vscale x 2 x double> @select_nxv2f64(i1 zeroext %c, <vscale x 2 x double> %a, <vscale x 2 x double> %b) {
; RV32-LABEL: select_nxv2f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    addi a1, zero, -1
; RV32-NEXT:    bnez a0, .LBB24_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a1, zero
; RV32-NEXT:  .LBB24_2:
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v26, (a0), zero
; RV32-NEXT:    vand.vv v28, v8, v26
; RV32-NEXT:    vxor.vi v26, v26, -1
; RV32-NEXT:    vand.vv v26, v10, v26
; RV32-NEXT:    vor.vv v8, v28, v26
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: select_nxv2f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi a1, zero, -1
; RV64-NEXT:    bnez a0, .LBB24_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a1, zero
; RV64-NEXT:  .LBB24_2:
; RV64-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; RV64-NEXT:    vand.vx v26, v8, a1
; RV64-NEXT:    vmv.v.x v28, a1
; RV64-NEXT:    vxor.vi v28, v28, -1
; RV64-NEXT:    vand.vv v28, v10, v28
; RV64-NEXT:    vor.vv v8, v26, v28
; RV64-NEXT:    ret
  %v = select i1 %c, <vscale x 2 x double> %a, <vscale x 2 x double> %b
  ret <vscale x 2 x double> %v
}

define <vscale x 2 x double> @selectcc_nxv2f64(double %a, double %b, <vscale x 2 x double> %c, <vscale x 2 x double> %d) {
; RV32-LABEL: selectcc_nxv2f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    feq.d a1, fa0, fa1
; RV32-NEXT:    addi a0, zero, -1
; RV32-NEXT:    bnez a1, .LBB25_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a0, zero
; RV32-NEXT:  .LBB25_2:
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v26, (a0), zero
; RV32-NEXT:    vand.vv v28, v8, v26
; RV32-NEXT:    vxor.vi v26, v26, -1
; RV32-NEXT:    vand.vv v26, v10, v26
; RV32-NEXT:    vor.vv v8, v28, v26
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_nxv2f64:
; RV64:       # %bb.0:
; RV64-NEXT:    feq.d a1, fa0, fa1
; RV64-NEXT:    addi a0, zero, -1
; RV64-NEXT:    bnez a1, .LBB25_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a0, zero
; RV64-NEXT:  .LBB25_2:
; RV64-NEXT:    vsetvli a1, zero, e64, m2, ta, mu
; RV64-NEXT:    vand.vx v26, v8, a0
; RV64-NEXT:    vmv.v.x v28, a0
; RV64-NEXT:    vxor.vi v28, v28, -1
; RV64-NEXT:    vand.vv v28, v10, v28
; RV64-NEXT:    vor.vv v8, v26, v28
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <vscale x 2 x double> %c, <vscale x 2 x double> %d
  ret <vscale x 2 x double> %v
}

define <vscale x 4 x double> @select_nxv4f64(i1 zeroext %c, <vscale x 4 x double> %a, <vscale x 4 x double> %b) {
; RV32-LABEL: select_nxv4f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    addi a1, zero, -1
; RV32-NEXT:    bnez a0, .LBB26_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a1, zero
; RV32-NEXT:  .LBB26_2:
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v28, (a0), zero
; RV32-NEXT:    vand.vv v8, v8, v28
; RV32-NEXT:    vxor.vi v28, v28, -1
; RV32-NEXT:    vand.vv v28, v12, v28
; RV32-NEXT:    vor.vv v8, v8, v28
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: select_nxv4f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi a1, zero, -1
; RV64-NEXT:    bnez a0, .LBB26_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a1, zero
; RV64-NEXT:  .LBB26_2:
; RV64-NEXT:    vsetvli a0, zero, e64, m4, ta, mu
; RV64-NEXT:    vand.vx v28, v8, a1
; RV64-NEXT:    vmv.v.x v8, a1
; RV64-NEXT:    vxor.vi v8, v8, -1
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vor.vv v8, v28, v8
; RV64-NEXT:    ret
  %v = select i1 %c, <vscale x 4 x double> %a, <vscale x 4 x double> %b
  ret <vscale x 4 x double> %v
}

define <vscale x 4 x double> @selectcc_nxv4f64(double %a, double %b, <vscale x 4 x double> %c, <vscale x 4 x double> %d) {
; RV32-LABEL: selectcc_nxv4f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    feq.d a1, fa0, fa1
; RV32-NEXT:    addi a0, zero, -1
; RV32-NEXT:    bnez a1, .LBB27_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a0, zero
; RV32-NEXT:  .LBB27_2:
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v28, (a0), zero
; RV32-NEXT:    vand.vv v8, v8, v28
; RV32-NEXT:    vxor.vi v28, v28, -1
; RV32-NEXT:    vand.vv v28, v12, v28
; RV32-NEXT:    vor.vv v8, v8, v28
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_nxv4f64:
; RV64:       # %bb.0:
; RV64-NEXT:    feq.d a1, fa0, fa1
; RV64-NEXT:    addi a0, zero, -1
; RV64-NEXT:    bnez a1, .LBB27_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a0, zero
; RV64-NEXT:  .LBB27_2:
; RV64-NEXT:    vsetvli a1, zero, e64, m4, ta, mu
; RV64-NEXT:    vand.vx v28, v8, a0
; RV64-NEXT:    vmv.v.x v8, a0
; RV64-NEXT:    vxor.vi v8, v8, -1
; RV64-NEXT:    vand.vv v8, v12, v8
; RV64-NEXT:    vor.vv v8, v28, v8
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <vscale x 4 x double> %c, <vscale x 4 x double> %d
  ret <vscale x 4 x double> %v
}

define <vscale x 8 x double> @select_nxv8f64(i1 zeroext %c, <vscale x 8 x double> %a, <vscale x 8 x double> %b) {
; RV32-LABEL: select_nxv8f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    addi a1, zero, -1
; RV32-NEXT:    bnez a0, .LBB28_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a1, zero
; RV32-NEXT:  .LBB28_2:
; RV32-NEXT:    sw a1, 12(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v24, (a0), zero
; RV32-NEXT:    vand.vv v8, v8, v24
; RV32-NEXT:    vxor.vi v24, v24, -1
; RV32-NEXT:    vand.vv v16, v16, v24
; RV32-NEXT:    vor.vv v8, v8, v16
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: select_nxv8f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi a1, zero, -1
; RV64-NEXT:    bnez a0, .LBB28_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a1, zero
; RV64-NEXT:  .LBB28_2:
; RV64-NEXT:    vsetvli a0, zero, e64, m8, ta, mu
; RV64-NEXT:    vand.vx v8, v8, a1
; RV64-NEXT:    vmv.v.x v24, a1
; RV64-NEXT:    vxor.vi v24, v24, -1
; RV64-NEXT:    vand.vv v16, v16, v24
; RV64-NEXT:    vor.vv v8, v8, v16
; RV64-NEXT:    ret
  %v = select i1 %c, <vscale x 8 x double> %a, <vscale x 8 x double> %b
  ret <vscale x 8 x double> %v
}

define <vscale x 8 x double> @selectcc_nxv8f64(double %a, double %b, <vscale x 8 x double> %c, <vscale x 8 x double> %d) {
; RV32-LABEL: selectcc_nxv8f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    feq.d a1, fa0, fa1
; RV32-NEXT:    addi a0, zero, -1
; RV32-NEXT:    bnez a1, .LBB29_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a0, zero
; RV32-NEXT:  .LBB29_2:
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    vsetvli a0, zero, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vlse64.v v24, (a0), zero
; RV32-NEXT:    vand.vv v8, v8, v24
; RV32-NEXT:    vxor.vi v24, v24, -1
; RV32-NEXT:    vand.vv v16, v16, v24
; RV32-NEXT:    vor.vv v8, v8, v16
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_nxv8f64:
; RV64:       # %bb.0:
; RV64-NEXT:    feq.d a1, fa0, fa1
; RV64-NEXT:    addi a0, zero, -1
; RV64-NEXT:    bnez a1, .LBB29_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a0, zero
; RV64-NEXT:  .LBB29_2:
; RV64-NEXT:    vsetvli a1, zero, e64, m8, ta, mu
; RV64-NEXT:    vand.vx v8, v8, a0
; RV64-NEXT:    vmv.v.x v24, a0
; RV64-NEXT:    vxor.vi v24, v24, -1
; RV64-NEXT:    vand.vv v16, v16, v24
; RV64-NEXT:    vor.vv v8, v8, v16
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <vscale x 8 x double> %c, <vscale x 8 x double> %d
  ret <vscale x 8 x double> %v
}
