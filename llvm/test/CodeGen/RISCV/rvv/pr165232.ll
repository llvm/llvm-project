; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc < %s -mtriple=riscv64 -mattr=+v | FileCheck %s

target datalayout = "e-m:e-p:64:64-i64:64-i128:128-n32:64-S128"
target triple = "riscv64-unknown-linux-gnu"

define i1 @main(ptr %var_117, ptr %arrayinit.element3045, ptr %arrayinit.element3047, ptr %arrayinit.element3049, ptr %arrayinit.element3051, ptr %arrayinit.element3053, ptr %arrayinit.element3055, ptr %arrayinit.element3057, ptr %arrayinit.element3059, ptr %arrayinit.element3061, ptr %arrayinit.element3063, ptr %arrayinit.element3065, ptr %arrayinit.element3067, i64 %var_94_i.07698, target("riscv.vector.tuple", <vscale x 16 x i8>, 2) %0, target("riscv.vector.tuple", <vscale x 16 x i8>, 4) %1) {
; CHECK-LABEL: main:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    csrr t0, vlenb
; CHECK-NEXT:    slli t1, t0, 4
; CHECK-NEXT:    add t0, t1, t0
; CHECK-NEXT:    sub sp, sp, t0
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x10, 0x22, 0x11, 0x11, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 16 + 17 * vlenb
; CHECK-NEXT:    sd a0, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t0, a0, 3
; CHECK-NEXT:    add a0, t0, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs4r.v v12, (a0) # vscale x 32-byte Folded Spill
; CHECK-NEXT:    csrr t0, vlenb
; CHECK-NEXT:    slli t0, t0, 2
; CHECK-NEXT:    add a0, a0, t0
; CHECK-NEXT:    vs4r.v v16, (a0) # vscale x 32-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t0, a0, 2
; CHECK-NEXT:    add a0, t0, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs4r.v v8, (a0) # vscale x 32-byte Folded Spill
; CHECK-NEXT:    vsetvli t0, zero, e8, m1, ta, ma
; CHECK-NEXT:    vmv.v.i v13, 0
; CHECK-NEXT:    # kill: def $v14 killed $v13 killed $vtype
; CHECK-NEXT:    vmv.v.i v15, 0
; CHECK-NEXT:    vmv.v.i v16, 0
; CHECK-NEXT:    vmv.v.i v17, 0
; CHECK-NEXT:    vsetvli t0, zero, e8, m2, ta, ma
; CHECK-NEXT:    vmv.v.i v20, 0
; CHECK-NEXT:    vmv1r.v v18, v13
; CHECK-NEXT:    vmv.v.i v22, 0
; CHECK-NEXT:    vmv1r.v v19, v13
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t0, a0, 4
; CHECK-NEXT:    add a0, t0, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t0, 56(a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t1, a0, 4
; CHECK-NEXT:    add a0, t1, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t1, 48(a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t2, a0, 4
; CHECK-NEXT:    add a0, t2, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t2, 40(a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t3, a0, 4
; CHECK-NEXT:    add a0, t3, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t3, 32(a0)
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t4, a0, 4
; CHECK-NEXT:    add a0, t4, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t4, 16(a0)
; CHECK-NEXT:    vmv.v.i v24, 0
; CHECK-NEXT:    vmv1r.v v9, v13
; CHECK-NEXT:    vmv1r.v v10, v13
; CHECK-NEXT:    vsetivli zero, 0, e32, m1, ta, ma
; CHECK-NEXT:    vmv.v.i v8, 0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli t5, a0, 4
; CHECK-NEXT:    add a0, t5, a0
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    ld t5, 24(a0)
; CHECK-NEXT:    vmv1r.v v11, v13
; CHECK-NEXT:    vmv1r.v v12, v13
; CHECK-NEXT:    csrr t6, vlenb
; CHECK-NEXT:    add t6, sp, t6
; CHECK-NEXT:    addi t6, t6, 16
; CHECK-NEXT:    vs1r.v v9, (t6) # vscale x 8-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    add t6, t6, a0
; CHECK-NEXT:    vs2r.v v10, (t6) # vscale x 16-byte Folded Spill
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add t6, t6, a0
; CHECK-NEXT:    ld a0, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    vs1r.v v12, (t6) # vscale x 8-byte Folded Spill
; CHECK-NEXT:    vmclr.m v9
; CHECK-NEXT:    addi t6, sp, 16
; CHECK-NEXT:    vs1r.v v9, (t6) # vscale x 8-byte Folded Spill
; CHECK-NEXT:    vmv2r.v v26, v20
; CHECK-NEXT:    li t6, 1023
; CHECK-NEXT:    slli t6, t6, 52
; CHECK-NEXT:    sd zero, 0(a0)
; CHECK-NEXT:    sd t6, 0(t5)
; CHECK-NEXT:  .LBB0_1: # %for.body
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; CHECK-NEXT:    vmv.v.i v10, 0
; CHECK-NEXT:    vmv.v.i v30, 0
; CHECK-NEXT:    vmv1r.v v12, v8
; CHECK-NEXT:    vsetvli zero, zero, e64, m2, tu, ma
; CHECK-NEXT:    vle64.v v30, (a4)
; CHECK-NEXT:    vmv1r.v v28, v8
; CHECK-NEXT:    vle32.v v12, (t4)
; CHECK-NEXT:    vmv1r.v v29, v8
; CHECK-NEXT:    vle32.v v28, (t1)
; CHECK-NEXT:    vmv1r.v v9, v8
; CHECK-NEXT:    vle32.v v29, (t2)
; CHECK-NEXT:    vmv1r.v v10, v8
; CHECK-NEXT:    vle32.v v9, (t3)
; CHECK-NEXT:    vmv1r.v v9, v8
; CHECK-NEXT:    vle32.v v10, (a6)
; CHECK-NEXT:    vmv1r.v v10, v8
; CHECK-NEXT:    vle32.v v9, (a7)
; CHECK-NEXT:    vmv1r.v v11, v8
; CHECK-NEXT:    vle32.v v10, (a5)
; CHECK-NEXT:    vmv1r.v v10, v8
; CHECK-NEXT:    vle32.v v11, (a2)
; CHECK-NEXT:    vle32.v v10, (a3)
; CHECK-NEXT:    csrr t5, vlenb
; CHECK-NEXT:    add t5, sp, t5
; CHECK-NEXT:    addi t5, t5, 16
; CHECK-NEXT:    vl1r.v v1, (t5) # vscale x 8-byte Folded Reload
; CHECK-NEXT:    csrr t6, vlenb
; CHECK-NEXT:    add t5, t5, t6
; CHECK-NEXT:    vl2r.v v2, (t5) # vscale x 16-byte Folded Reload
; CHECK-NEXT:    slli t6, t6, 1
; CHECK-NEXT:    add t5, t5, t6
; CHECK-NEXT:    vl1r.v v4, (t5) # vscale x 8-byte Folded Reload
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; CHECK-NEXT:    vsseg4e32.v v1, (zero)
; CHECK-NEXT:    vmv4r.v v0, v12
; CHECK-NEXT:    vmv4r.v v4, v16
; CHECK-NEXT:    vmv1r.v v2, v9
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    vsseg8e32.v v0, (a1)
; CHECK-NEXT:    addi t5, sp, 16
; CHECK-NEXT:    vl1r.v v12, (t5) # vscale x 8-byte Folded Reload
; CHECK-NEXT:    vmv1r.v v9, v12
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    csrr t5, vlenb
; CHECK-NEXT:    slli t6, t5, 2
; CHECK-NEXT:    add t5, t6, t5
; CHECK-NEXT:    add t5, sp, t5
; CHECK-NEXT:    addi t5, t5, 16
; CHECK-NEXT:    vl2r.v v2, (t5) # vscale x 16-byte Folded Reload
; CHECK-NEXT:    csrr t6, vlenb
; CHECK-NEXT:    slli t6, t6, 1
; CHECK-NEXT:    add t5, t5, t6
; CHECK-NEXT:    vl2r.v v4, (t5) # vscale x 16-byte Folded Reload
; CHECK-NEXT:    vsetvli zero, t0, e64, m2, ta, ma
; CHECK-NEXT:    vsseg2e64.v v2, (zero)
; CHECK-NEXT:    vsetivli zero, 0, e64, m2, ta, mu
; CHECK-NEXT:    vmv.v.i v28, 0
; CHECK-NEXT:    vmflt.vv v9, v30, v28, v0.t
; CHECK-NEXT:    vmv1r.v v0, v9
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, tu, mu
; CHECK-NEXT:    vssub.vv v10, v8, v11, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; CHECK-NEXT:    vsseg4e64.v v20, (zero), v0.t
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; CHECK-NEXT:    vsseg8e32.v v10, (a0)
; CHECK-NEXT:    csrr t5, vlenb
; CHECK-NEXT:    slli t6, t5, 3
; CHECK-NEXT:    add t5, t6, t5
; CHECK-NEXT:    add t5, sp, t5
; CHECK-NEXT:    addi t5, t5, 16
; CHECK-NEXT:    vl8r.v v0, (t5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; CHECK-NEXT:    vsseg4e64.v v0, (zero)
; CHECK-NEXT:    j .LBB0_1
entry:
  store double 0.000000e+00, ptr %var_117, align 8
  store double 1.000000e+00, ptr %arrayinit.element3061, align 8
  br label %for.body

for.body:                                         ; preds = %for.body, %entry
  %2 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3059, i64 0)
  %3 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3067, i64 0)
  %4 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3065, i64 0)
  %5 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3063, i64 0)
  %6 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3055, i64 0)
  %7 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3057, i64 0)
  %8 = call <vscale x 2 x float> @llvm.riscv.vle.nxv2f32.p0.i64(<vscale x 2 x float> zeroinitializer, ptr %arrayinit.element3053, i64 0)
  %9 = call <vscale x 2 x double> @llvm.riscv.vle.nxv2f64.p0.i64(<vscale x 2 x double> zeroinitializer, ptr %arrayinit.element3051, i64 0)
  %10 = tail call <vscale x 2 x i32> @llvm.riscv.vle.nxv2i32.p0.i64(<vscale x 2 x i32> zeroinitializer, ptr %arrayinit.element3047, i64 0)
  %11 = tail call <vscale x 2 x i32> @llvm.riscv.vle.nxv2i32.p0.i64(<vscale x 2 x i32> zeroinitializer, ptr %arrayinit.element3049, i64 0)
  call void @llvm.riscv.vsseg4.triscv.vector.tuple_nxv8i8_4t.p0.i64(target("riscv.vector.tuple", <vscale x 8 x i8>, 4) zeroinitializer, ptr null, i64 0, i64 5)
  %12 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) zeroinitializer, <vscale x 2 x float> %8, i32 0)
  %13 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %12, <vscale x 2 x float> %7, i32 2)
  %14 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %13, <vscale x 2 x float> %6, i32 0)
  %15 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %14, <vscale x 2 x float> %5, i32 0)
  %16 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %15, <vscale x 2 x float> %4, i32 0)
  %17 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %16, <vscale x 2 x float> %3, i32 0)
  %18 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2f32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %17, <vscale x 2 x float> %2, i32 0)
  call void @llvm.riscv.vsseg8.triscv.vector.tuple_nxv8i8_8t.p0.i64(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %18, ptr %arrayinit.element3045, i64 0, i64 5)
  %19 = tail call <vscale x 2 x i1> @llvm.riscv.vmfgt.mask.nxv2f64.nxv2f64.i64(<vscale x 2 x i1> zeroinitializer, <vscale x 2 x double> zeroinitializer, <vscale x 2 x double> %9, <vscale x 2 x i1> zeroinitializer, i64 0)
  %20 = tail call <vscale x 2 x i32> @llvm.riscv.vssub.mask.nxv2i32.nxv2i32.i64(<vscale x 2 x i32> %11, <vscale x 2 x i32> zeroinitializer, <vscale x 2 x i32> %10, <vscale x 2 x i1> %19, i64 0, i64 0)
  call void @llvm.riscv.vsseg2.triscv.vector.tuple_nxv16i8_2t.p0.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 2) %0, ptr null, i64 %var_94_i.07698, i64 6)
  call void @llvm.riscv.vsseg4.mask.triscv.vector.tuple_nxv16i8_4t.p0.nxv2i1.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 4) zeroinitializer, ptr null, <vscale x 2 x i1> zeroinitializer, i64 0, i64 6)
  %21 = tail call target("riscv.vector.tuple", <vscale x 8 x i8>, 8) @llvm.riscv.tuple.insert.triscv.vector.tuple_nxv8i8_8t.nxv2i32(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) poison, <vscale x 2 x i32> %20, i32 0)
  call void @llvm.riscv.vsseg8.triscv.vector.tuple_nxv8i8_8t.p0.i64(target("riscv.vector.tuple", <vscale x 8 x i8>, 8) %21, ptr %var_117, i64 0, i64 5)
  call void @llvm.riscv.vsseg4.triscv.vector.tuple_nxv16i8_4t.p0.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 4) %1, ptr null, i64 0, i64 6)
  br label %for.body
}
