; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=riscv64-linux-gnu --mattr=+v,+zvfh < %s | FileCheck %s --check-prefix=ZVFH
; RUN: llc --mtriple=riscv64-linux-gnu --mattr=+v,+zvfhmin,+zfh < %s | FileCheck %s --check-prefix=ZVFHMIN

define <2 x double> @max_v2f64(<2 x double> %a, <2 x double> %b) {
; ZVFH-LABEL: max_v2f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v2f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x double> @llvm.maximumnum.v2f64(<2 x double> %a, <2 x double> %b)
  ret <2 x double> %c
}

define <3 x double> @max_v3f64(<3 x double> %a, <3 x double> %b) {
; ZVFH-LABEL: max_v3f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v3f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <3 x double> @llvm.maximumnum.v3f64(<3 x double> %a, <3 x double> %b)
  ret <3 x double> %c
}

define <4 x double> @max_v4f64(<4 x double> %a, <4 x double> %b) {
; ZVFH-LABEL: max_v4f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v4f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x double> @llvm.maximumnum.v4f64(<4 x double> %a, <4 x double> %b)
  ret <4 x double> %c
}

define <2 x float> @max_v2f32(<2 x float> %a, <2 x float> %b) {
; ZVFH-LABEL: max_v2f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v2f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x float> @llvm.maximumnum.v2f32(<2 x float> %a, <2 x float> %b)
  ret <2 x float> %c
}

define <3 x float> @max_v3f32(<3 x float> %a, <3 x float> %b) {
; ZVFH-LABEL: max_v3f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v3f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <3 x float> @llvm.maximumnum.v3f32(<3 x float> %a, <3 x float> %b)
  ret <3 x float> %c
}

define <4 x float> @max_v4f32(<4 x float> %a, <4 x float> %b) {
; ZVFH-LABEL: max_v4f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v4f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x float> @llvm.maximumnum.v4f32(<4 x float> %a, <4 x float> %b)
  ret <4 x float> %c
}

define <5 x float> @max_v5f32(<5 x float> %a, <5 x float> %b) {
; ZVFH-LABEL: max_v5f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v5f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <5 x float> @llvm.maximumnum.v5f32(<5 x float> %a, <5 x float> %b)
  ret <5 x float> %c
}

define <8 x float> @max_v8f32(<8 x float> %a, <8 x float> %b) {
; ZVFH-LABEL: max_v8f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v8f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <8 x float> @llvm.maximumnum.v8f32(<8 x float> %a, <8 x float> %b)
  ret <8 x float> %c
}

define <2 x half> @max_v2f16(<2 x half> %a, <2 x half> %b) {
; ZVFH-LABEL: max_v2f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e16, mf4, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v2f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e16, mf4, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v9, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v9, v9, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, mf4, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x half> @llvm.maximumnum.v2f16(<2 x half> %a, <2 x half> %b)
  ret <2 x half> %c
}

define <4 x half> @max_v4f16(<4 x half> %a, <4 x half> %b) {
; ZVFH-LABEL: max_v4f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v4f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v9, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v9, v9, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x half> @llvm.maximumnum.v4f16(<4 x half> %a, <4 x half> %b)
  ret <4 x half> %c
}

define <8 x half> @max_v8f16(<8 x half> %a, <8 x half> %b) {
; ZVFH-LABEL: max_v8f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v8f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v10, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <8 x half> @llvm.maximumnum.v8f16(<8 x half> %a, <8 x half> %b)
  ret <8 x half> %c
}

define <9 x half> @max_v9f16(<9 x half> %a, <9 x half> %b) {
; ZVFH-LABEL: max_v9f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v9f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v12, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v12
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <9 x half> @llvm.maximumnum.v9f16(<9 x half> %a, <9 x half> %b)
  ret <9 x half> %c
}

define <16 x half> @max_v16f16(<16 x half> %a, <16 x half> %b) {
; ZVFH-LABEL: max_v16f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: max_v16f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v12, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v12
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <16 x half> @llvm.maximumnum.v16f16(<16 x half> %a, <16 x half> %b)
  ret <16 x half> %c
}

define <2 x double> @min_v2f64(<2 x double> %a, <2 x double> %b) {
; ZVFH-LABEL: min_v2f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v2f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e64, m1, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x double> @llvm.minimumnum.v2f64(<2 x double> %a, <2 x double> %b)
  ret <2 x double> %c
}

define <3 x double> @min_v3f64(<3 x double> %a, <3 x double> %b) {
; ZVFH-LABEL: min_v3f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v3f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <3 x double> @llvm.minimumnum.v3f64(<3 x double> %a, <3 x double> %b)
  ret <3 x double> %c
}

define <4 x double> @min_v4f64(<4 x double> %a, <4 x double> %b) {
; ZVFH-LABEL: min_v4f64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v4f64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e64, m2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x double> @llvm.minimumnum.v4f64(<4 x double> %a, <4 x double> %b)
  ret <4 x double> %c
}

define <2 x float> @min_v2f32(<2 x float> %a, <2 x float> %b) {
; ZVFH-LABEL: min_v2f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v2f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x float> @llvm.minimumnum.v2f32(<2 x float> %a, <2 x float> %b)
  ret <2 x float> %c
}

define <3 x float> @min_v3f32(<3 x float> %a, <3 x float> %b) {
; ZVFH-LABEL: min_v3f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v3f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <3 x float> @llvm.minimumnum.v3f32(<3 x float> %a, <3 x float> %b)
  ret <3 x float> %c
}

define <4 x float> @min_v4f32(<4 x float> %a, <4 x float> %b) {
; ZVFH-LABEL: min_v4f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v4f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x float> @llvm.minimumnum.v4f32(<4 x float> %a, <4 x float> %b)
  ret <4 x float> %c
}

define <5 x float> @min_v5f32(<5 x float> %a, <5 x float> %b) {
; ZVFH-LABEL: min_v5f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v5f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <5 x float> @llvm.minimumnum.v5f32(<5 x float> %a, <5 x float> %b)
  ret <5 x float> %c
}

define <8 x float> @min_v8f32(<8 x float> %a, <8 x float> %b) {
; ZVFH-LABEL: min_v8f32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v8f32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <8 x float> @llvm.minimumnum.v8f32(<8 x float> %a, <8 x float> %b)
  ret <8 x float> %c
}

define <2 x half> @min_v2f16(<2 x half> %a, <2 x half> %b) {
; ZVFH-LABEL: min_v2f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 2, e16, mf4, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v2f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 2, e16, mf4, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v9, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v9, v9, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, mf4, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <2 x half> @llvm.minimumnum.v2f16(<2 x half> %a, <2 x half> %b)
  ret <2 x half> %c
}

define <4 x half> @min_v4f16(<4 x half> %a, <4 x half> %b) {
; ZVFH-LABEL: min_v4f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v4f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 4, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v9, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v9, v9, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, mf2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v9
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <4 x half> @llvm.minimumnum.v4f16(<4 x half> %a, <4 x half> %b)
  ret <4 x half> %c
}

define <8 x half> @min_v8f16(<8 x half> %a, <8 x half> %b) {
; ZVFH-LABEL: min_v8f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v8f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v10, v12, v10
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v10
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <8 x half> @llvm.minimumnum.v8f16(<8 x half> %a, <8 x half> %b)
  ret <8 x half> %c
}

define <9 x half> @min_v9f16(<9 x half> %a, <9 x half> %b) {
; ZVFH-LABEL: min_v9f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v9f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v12, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v12
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <9 x half> @llvm.minimumnum.v9f16(<9 x half> %a, <9 x half> %b)
  ret <9 x half> %c
}

define <16 x half> @min_v16f16(<16 x half> %a, <16 x half> %b) {
; ZVFH-LABEL: min_v16f16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: min_v16f16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    vsetivli zero, 16, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v10
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v16, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v12, v16, v12
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v8, v12
; ZVFHMIN-NEXT:    ret
entry:
  %c = call <16 x half> @llvm.minimumnum.v16f16(<16 x half> %a, <16 x half> %b)
  ret <16 x half> %c
}

;; vscale
define void @fmin32(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmin32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a5, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    srli a3, a5, 1
; ZVFH-NEXT:    neg a6, a3
; ZVFH-NEXT:    and a4, a6, a4
; ZVFH-NEXT:    slli a5, a5, 1
; ZVFH-NEXT:    vsetvli a6, zero, e32, m2, ta, ma
; ZVFH-NEXT:  .LBB32_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re32.v v8, (a0)
; ZVFH-NEXT:    vl2re32.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB32_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmin32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a5, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    srli a3, a5, 1
; ZVFHMIN-NEXT:    neg a6, a3
; ZVFHMIN-NEXT:    and a4, a6, a4
; ZVFHMIN-NEXT:    slli a5, a5, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB32_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re32.v v8, (a0)
; ZVFHMIN-NEXT:    vl2re32.v v10, (a1)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    vs2r.v v8, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB32_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 4
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 4
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x float], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw float, ptr %13, i32 0
  %wide.load = load <vscale x 4 x float>, ptr %14, align 4
  %15 = getelementptr inbounds nuw [4096 x float], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw float, ptr %15, i32 0
  %wide.load5 = load <vscale x 4 x float>, ptr %16, align 4
  %17 = call <vscale x 4 x float> @llvm.minimumnum.nxv4f32(<vscale x 4 x float> %wide.load, <vscale x 4 x float> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x float], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw float, ptr %18, i32 0
  store <vscale x 4 x float> %17, ptr %19, align 4
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}

define void @fmax32(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmax32:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a5, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    srli a3, a5, 1
; ZVFH-NEXT:    neg a6, a3
; ZVFH-NEXT:    and a4, a6, a4
; ZVFH-NEXT:    slli a5, a5, 1
; ZVFH-NEXT:    vsetvli a6, zero, e32, m2, ta, ma
; ZVFH-NEXT:  .LBB33_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re32.v v8, (a0)
; ZVFH-NEXT:    vl2re32.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB33_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmax32:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a5, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    srli a3, a5, 1
; ZVFHMIN-NEXT:    neg a6, a3
; ZVFHMIN-NEXT:    and a4, a6, a4
; ZVFHMIN-NEXT:    slli a5, a5, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e32, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB33_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re32.v v8, (a0)
; ZVFHMIN-NEXT:    vl2re32.v v10, (a1)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    vs2r.v v8, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB33_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 4
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 4
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x float], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw float, ptr %13, i32 0
  %wide.load = load <vscale x 4 x float>, ptr %14, align 4
  %15 = getelementptr inbounds nuw [4096 x float], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw float, ptr %15, i32 0
  %wide.load5 = load <vscale x 4 x float>, ptr %16, align 4
  %17 = call <vscale x 4 x float> @llvm.maximumnum.nxv4f32(<vscale x 4 x float> %wide.load, <vscale x 4 x float> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x float], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw float, ptr %18, i32 0
  store <vscale x 4 x float> %17, ptr %19, align 4
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}

define void @fmin64(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmin64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a5, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    srli a3, a5, 2
; ZVFH-NEXT:    neg a6, a3
; ZVFH-NEXT:    and a4, a6, a4
; ZVFH-NEXT:    slli a5, a5, 1
; ZVFH-NEXT:    vsetvli a6, zero, e64, m2, ta, ma
; ZVFH-NEXT:  .LBB34_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re64.v v8, (a0)
; ZVFH-NEXT:    vl2re64.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB34_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmin64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a5, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    srli a3, a5, 2
; ZVFHMIN-NEXT:    neg a6, a3
; ZVFHMIN-NEXT:    and a4, a6, a4
; ZVFHMIN-NEXT:    slli a5, a5, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e64, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB34_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re64.v v8, (a0)
; ZVFHMIN-NEXT:    vl2re64.v v10, (a1)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfmin.vv v8, v8, v10
; ZVFHMIN-NEXT:    vs2r.v v8, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB34_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 2
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 2
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x double], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw double, ptr %13, i32 0
  %wide.load = load <vscale x 2 x double>, ptr %14, align 8
  %15 = getelementptr inbounds nuw [4096 x double], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw double, ptr %15, i32 0
  %wide.load5 = load <vscale x 2 x double>, ptr %16, align 8
  %17 = call <vscale x 2 x double> @llvm.minimumnum.nxv2f64(<vscale x 2 x double> %wide.load, <vscale x 2 x double> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x double], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw double, ptr %18, i32 0
  store <vscale x 2 x double> %17, ptr %19, align 8
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}

define void @fmax64(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmax64:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a5, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    srli a3, a5, 2
; ZVFH-NEXT:    neg a6, a3
; ZVFH-NEXT:    and a4, a6, a4
; ZVFH-NEXT:    slli a5, a5, 1
; ZVFH-NEXT:    vsetvli a6, zero, e64, m2, ta, ma
; ZVFH-NEXT:  .LBB35_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re64.v v8, (a0)
; ZVFH-NEXT:    vl2re64.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB35_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmax64:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a5, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    srli a3, a5, 2
; ZVFHMIN-NEXT:    neg a6, a3
; ZVFHMIN-NEXT:    and a4, a6, a4
; ZVFHMIN-NEXT:    slli a5, a5, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e64, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB35_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re64.v v8, (a0)
; ZVFHMIN-NEXT:    vl2re64.v v10, (a1)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfmax.vv v8, v8, v10
; ZVFHMIN-NEXT:    vs2r.v v8, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB35_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 2
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 2
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x double], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw double, ptr %13, i32 0
  %wide.load = load <vscale x 2 x double>, ptr %14, align 8
  %15 = getelementptr inbounds nuw [4096 x double], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw double, ptr %15, i32 0
  %wide.load5 = load <vscale x 2 x double>, ptr %16, align 8
  %17 = call <vscale x 2 x double> @llvm.maximumnum.nxv2f64(<vscale x 2 x double> %wide.load, <vscale x 2 x double> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x double], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw double, ptr %18, i32 0
  store <vscale x 2 x double> %17, ptr %19, align 8
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}

define void @fmin16(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmin16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a3, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    neg a5, a3
; ZVFH-NEXT:    and a4, a5, a4
; ZVFH-NEXT:    slli a5, a3, 1
; ZVFH-NEXT:    vsetvli a6, zero, e16, m2, ta, ma
; ZVFH-NEXT:  .LBB36_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re16.v v8, (a0)
; ZVFH-NEXT:    vl2re16.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmin.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB36_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmin16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a3, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    neg a5, a3
; ZVFHMIN-NEXT:    and a4, a5, a4
; ZVFHMIN-NEXT:    slli a5, a3, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB36_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re16.v v12, (a1)
; ZVFHMIN-NEXT:    vl2re16.v v16, (a0)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v12
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v16
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmin.vv v8, v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vs2r.v v12, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB36_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 8
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 8
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x half], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw half, ptr %13, i32 0
  %wide.load = load <vscale x 8 x half>, ptr %14, align 2
  %15 = getelementptr inbounds nuw [4096 x half], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw half, ptr %15, i32 0
  %wide.load5 = load <vscale x 8 x half>, ptr %16, align 2
  %17 = call <vscale x 8 x half> @llvm.minimumnum.nxv8f16(<vscale x 8 x half> %wide.load, <vscale x 8 x half> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x half], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw half, ptr %18, i32 0
  store <vscale x 8 x half> %17, ptr %19, align 2
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}

define void @fmax16(ptr noundef readonly captures(none) %input1, ptr noundef readonly captures(none) %input2, ptr noundef writeonly captures(none) %output) {
; ZVFH-LABEL: fmax16:
; ZVFH:       # %bb.0: # %entry
; ZVFH-NEXT:    csrr a3, vlenb
; ZVFH-NEXT:    lui a4, 1
; ZVFH-NEXT:    neg a5, a3
; ZVFH-NEXT:    and a4, a5, a4
; ZVFH-NEXT:    slli a5, a3, 1
; ZVFH-NEXT:    vsetvli a6, zero, e16, m2, ta, ma
; ZVFH-NEXT:  .LBB37_1: # %vector.body
; ZVFH-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFH-NEXT:    vl2re16.v v8, (a0)
; ZVFH-NEXT:    vl2re16.v v10, (a1)
; ZVFH-NEXT:    sub a4, a4, a3
; ZVFH-NEXT:    add a1, a1, a5
; ZVFH-NEXT:    vfmax.vv v8, v8, v10
; ZVFH-NEXT:    vs2r.v v8, (a2)
; ZVFH-NEXT:    add a2, a2, a5
; ZVFH-NEXT:    add a0, a0, a5
; ZVFH-NEXT:    bnez a4, .LBB37_1
; ZVFH-NEXT:  # %bb.2: # %exit
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fmax16:
; ZVFHMIN:       # %bb.0: # %entry
; ZVFHMIN-NEXT:    csrr a3, vlenb
; ZVFHMIN-NEXT:    lui a4, 1
; ZVFHMIN-NEXT:    neg a5, a3
; ZVFHMIN-NEXT:    and a4, a5, a4
; ZVFHMIN-NEXT:    slli a5, a3, 1
; ZVFHMIN-NEXT:    vsetvli a6, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:  .LBB37_1: # %vector.body
; ZVFHMIN-NEXT:    # =>This Inner Loop Header: Depth=1
; ZVFHMIN-NEXT:    vl2re16.v v12, (a1)
; ZVFHMIN-NEXT:    vl2re16.v v16, (a0)
; ZVFHMIN-NEXT:    sub a4, a4, a3
; ZVFHMIN-NEXT:    add a1, a1, a5
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v8, v12
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v16
; ZVFHMIN-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; ZVFHMIN-NEXT:    vfmax.vv v8, v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; ZVFHMIN-NEXT:    vfncvt.f.f.w v12, v8
; ZVFHMIN-NEXT:    vs2r.v v12, (a2)
; ZVFHMIN-NEXT:    add a2, a2, a5
; ZVFHMIN-NEXT:    add a0, a0, a5
; ZVFHMIN-NEXT:    bnez a4, .LBB37_1
; ZVFHMIN-NEXT:  # %bb.2: # %exit
; ZVFHMIN-NEXT:    ret
entry:
  %input23 = ptrtoint ptr %input2 to i64
  %input12 = ptrtoint ptr %input1 to i64
  %output1 = ptrtoint ptr %output to i64
  br label %vector.ph

vector.ph:
  %9 = call i64 @llvm.vscale.i64()
  %10 = mul i64 %9, 8
  %n.mod.vf = urem i64 4096, %10
  %n.vec = sub i64 4096, %n.mod.vf
  %11 = call i64 @llvm.vscale.i64()
  %12 = mul i64 %11, 8
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %13 = getelementptr inbounds nuw [4096 x half], ptr %input1, i64 0, i64 %index
  %14 = getelementptr inbounds nuw half, ptr %13, i32 0
  %wide.load = load <vscale x 8 x half>, ptr %14, align 2
  %15 = getelementptr inbounds nuw [4096 x half], ptr %input2, i64 0, i64 %index
  %16 = getelementptr inbounds nuw half, ptr %15, i32 0
  %wide.load5 = load <vscale x 8 x half>, ptr %16, align 2
  %17 = call <vscale x 8 x half> @llvm.maximumnum.nxv8f16(<vscale x 8 x half> %wide.load, <vscale x 8 x half> %wide.load5)
  %18 = getelementptr inbounds nuw [4096 x half], ptr %output, i64 0, i64 %index
  %19 = getelementptr inbounds nuw half, ptr %18, i32 0
  store <vscale x 8 x half> %17, ptr %19, align 2
  %index.next = add nuw i64 %index, %12
  %20 = icmp eq i64 %index.next, %n.vec
  br i1 %20, label %exit, label %vector.body

exit:                                             ; preds = %middle.block, %for.body
  ret void
}
