; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=riscv64 -O3 -mcpu=sifive-p670 %s -o - | FileCheck %s

; Most of other tests for this part are in `test/CodeGen/RISCV/rvv/machine-combiner-fma.mir`

define double @legalized_vfmacc_vv(ptr %distmat1, ptr %distmat2, double %acc) nounwind {
; CHECK-LABEL: legalized_vfmacc_vv:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    sub sp, sp, a2
; CHECK-NEXT:    addi a2, a0, 256
; CHECK-NEXT:    vsetivli zero, 16, e64, m8, ta, ma
; CHECK-NEXT:    addi a3, sp, 16
; CHECK-NEXT:    vle64.v v8, (a2)
; CHECK-NEXT:    addi a2, a0, 384
; CHECK-NEXT:    vle64.v v16, (a2)
; CHECK-NEXT:    addi a2, a0, 128
; CHECK-NEXT:    vle64.v v24, (a2)
; CHECK-NEXT:    addi a2, a1, 128
; CHECK-NEXT:    vle64.v v0, (a2)
; CHECK-NEXT:    addi a2, a1, 384
; CHECK-NEXT:    vfsub.vv v24, v24, v0
; CHECK-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vle64.v v0, (a2)
; CHECK-NEXT:    addi a2, a1, 256
; CHECK-NEXT:    vfsub.vv v16, v16, v0
; CHECK-NEXT:    vfmul.vv v16, v16, v16
; CHECK-NEXT:    vle64.v v0, (a2)
; CHECK-NEXT:    vfsub.vv v8, v8, v0
; CHECK-NEXT:    vfmul.vv v8, v8, v8
; CHECK-NEXT:    vle64.v v0, (a0)
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vle64.v v24, (a1)
; CHECK-NEXT:    vfsub.vv v24, v0, v24
; CHECK-NEXT:    vfmacc.vv v8, v24, v24
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    vfmacc.vv v16, v0, v0
; CHECK-NEXT:    vfadd.vv v8, v8, v16
; CHECK-NEXT:    vfmv.s.f v16, fa0
; CHECK-NEXT:    vfredusum.vs v8, v8, v16
; CHECK-NEXT:    vfmv.f.s fa0, v8
; CHECK-NEXT:    sh3add sp, a0, sp
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
entry:
  %0 = load <64 x double>, ptr %distmat1, align 8
  %1 = load <64 x double>, ptr %distmat2, align 8
  %2 = fsub fast <64 x double> %0, %1
  %3 = fmul fast <64 x double> %2, %2
  %op.rdx = tail call fast double @llvm.vector.reduce.fadd.v64f64(double %acc, <64 x double> %3)
  ret double %op.rdx
}
