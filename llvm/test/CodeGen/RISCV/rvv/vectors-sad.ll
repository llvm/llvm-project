; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 2
; RUN: llc < %s -mtriple=riscv64 -mattr=+m,+v | FileCheck %s
; RUN: llc < %s -mtriple=riscv64 -mattr=+m,+v,+experimental-zvabd | FileCheck %s --check-prefix=ZVABD

define signext i32 @PIXEL_SAD_C(ptr %pix1, i32 %i_stride_pix1, ptr %pix2, i32 %i_stride_pix2, i32 %i_width, i32 %i_height){
; CHECK-LABEL: PIXEL_SAD_C:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    sext.w a6, a5
; CHECK-NEXT:    blez a6, .LBB0_12
; CHECK-NEXT:  # %bb.1: # %for.cond1.preheader.lr.ph
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    sd s0, 24(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s1, 16(sp) # 8-byte Folded Spill
; CHECK-NEXT:    sd s2, 8(sp) # 8-byte Folded Spill
; CHECK-NEXT:    .cfi_offset s0, -8
; CHECK-NEXT:    .cfi_offset s1, -16
; CHECK-NEXT:    .cfi_offset s2, -24
; CHECK-NEXT:    li a7, 0
; CHECK-NEXT:    li t0, 0
; CHECK-NEXT:    li a5, 0
; CHECK-NEXT:    sext.w a1, a1
; CHECK-NEXT:    sext.w a3, a3
; CHECK-NEXT:    slli t1, a4, 32
; CHECK-NEXT:    csrr t2, vlenb
; CHECK-NEXT:    sext.w a4, a4
; CHECK-NEXT:    vsetvli t3, zero, e32, m1, ta, ma
; CHECK-NEXT:    vmv.v.i v12, 0
; CHECK-NEXT:    vsetvli t3, zero, e32, m2, ta, ma
; CHECK-NEXT:    vmv.v.i v8, 0
; CHECK-NEXT:    srli t1, t1, 32
; CHECK-NEXT:    srli t2, t2, 1
; CHECK-NEXT:    neg t3, t2
; CHECK-NEXT:    and t3, t3, t1
; CHECK-NEXT:    vmv.s.x v13, zero
; CHECK-NEXT:    mv t4, a2
; CHECK-NEXT:    j .LBB0_3
; CHECK-NEXT:  .LBB0_2: # %for.cond.cleanup3
; CHECK-NEXT:    # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add t4, t4, a3
; CHECK-NEXT:    addiw t0, t0, 1
; CHECK-NEXT:    addi a7, a7, 1
; CHECK-NEXT:    beq t0, a6, .LBB0_11
; CHECK-NEXT:  .LBB0_3: # %for.cond1.preheader
; CHECK-NEXT:    # =>This Loop Header: Depth=1
; CHECK-NEXT:    # Child Loop BB0_7 Depth 2
; CHECK-NEXT:    # Child Loop BB0_10 Depth 2
; CHECK-NEXT:    blez a4, .LBB0_2
; CHECK-NEXT:  # %bb.4: # %for.body4.preheader
; CHECK-NEXT:    # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    bgeu t1, t2, .LBB0_6
; CHECK-NEXT:  # %bb.5: # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    li s0, 0
; CHECK-NEXT:    j .LBB0_9
; CHECK-NEXT:  .LBB0_6: # %vector.ph
; CHECK-NEXT:    # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    vmv1r.v v14, v12
; CHECK-NEXT:    vsetvli zero, zero, e32, m2, tu, ma
; CHECK-NEXT:    vmv.s.x v14, a5
; CHECK-NEXT:    vmv2r.v v10, v8
; CHECK-NEXT:    vmv1r.v v10, v14
; CHECK-NEXT:    mv a5, a0
; CHECK-NEXT:    mv t5, t4
; CHECK-NEXT:    mv t6, t3
; CHECK-NEXT:  .LBB0_7: # %vector.body
; CHECK-NEXT:    # Parent Loop BB0_3 Depth=1
; CHECK-NEXT:    # => This Inner Loop Header: Depth=2
; CHECK-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; CHECK-NEXT:    vle8.v v14, (a5)
; CHECK-NEXT:    vle8.v v15, (t5)
; CHECK-NEXT:    sub t6, t6, t2
; CHECK-NEXT:    add t5, t5, t2
; CHECK-NEXT:    vminu.vv v16, v14, v15
; CHECK-NEXT:    vmaxu.vv v14, v14, v15
; CHECK-NEXT:    vsub.vv v14, v14, v16
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; CHECK-NEXT:    vzext.vf2 v15, v14
; CHECK-NEXT:    vwaddu.wv v10, v10, v15
; CHECK-NEXT:    add a5, a5, t2
; CHECK-NEXT:    bnez t6, .LBB0_7
; CHECK-NEXT:  # %bb.8: # %middle.block
; CHECK-NEXT:    # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; CHECK-NEXT:    vredsum.vs v10, v10, v13
; CHECK-NEXT:    vmv.x.s a5, v10
; CHECK-NEXT:    mv s0, t3
; CHECK-NEXT:    beq t3, t1, .LBB0_2
; CHECK-NEXT:  .LBB0_9: # %for.body4.preheader31
; CHECK-NEXT:    # in Loop: Header=BB0_3 Depth=1
; CHECK-NEXT:    mul t6, a3, a7
; CHECK-NEXT:    add t5, t4, s0
; CHECK-NEXT:    add t6, t1, t6
; CHECK-NEXT:    add t6, a2, t6
; CHECK-NEXT:    add s0, a0, s0
; CHECK-NEXT:  .LBB0_10: # %for.body4
; CHECK-NEXT:    # Parent Loop BB0_3 Depth=1
; CHECK-NEXT:    # => This Inner Loop Header: Depth=2
; CHECK-NEXT:    lbu s1, 0(s0)
; CHECK-NEXT:    lbu s2, 0(t5)
; CHECK-NEXT:    addi t5, t5, 1
; CHECK-NEXT:    sub s1, s1, s2
; CHECK-NEXT:    srai s2, s1, 63
; CHECK-NEXT:    xor s1, s1, s2
; CHECK-NEXT:    sub a5, s2, a5
; CHECK-NEXT:    subw a5, s1, a5
; CHECK-NEXT:    addi s0, s0, 1
; CHECK-NEXT:    bne t5, t6, .LBB0_10
; CHECK-NEXT:    j .LBB0_2
; CHECK-NEXT:  .LBB0_11:
; CHECK-NEXT:    ld s0, 24(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s1, 16(sp) # 8-byte Folded Reload
; CHECK-NEXT:    ld s2, 8(sp) # 8-byte Folded Reload
; CHECK-NEXT:    .cfi_restore s0
; CHECK-NEXT:    .cfi_restore s1
; CHECK-NEXT:    .cfi_restore s2
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    mv a0, a5
; CHECK-NEXT:    ret
; CHECK-NEXT:  .LBB0_12:
; CHECK-NEXT:    li a0, 0
; CHECK-NEXT:    ret
;
; ZVABD-LABEL: PIXEL_SAD_C:
; ZVABD:       # %bb.0: # %entry
; ZVABD-NEXT:    sext.w a6, a5
; ZVABD-NEXT:    blez a6, .LBB0_12
; ZVABD-NEXT:  # %bb.1: # %for.cond1.preheader.lr.ph
; ZVABD-NEXT:    addi sp, sp, -32
; ZVABD-NEXT:    .cfi_def_cfa_offset 32
; ZVABD-NEXT:    sd s0, 24(sp) # 8-byte Folded Spill
; ZVABD-NEXT:    sd s1, 16(sp) # 8-byte Folded Spill
; ZVABD-NEXT:    sd s2, 8(sp) # 8-byte Folded Spill
; ZVABD-NEXT:    .cfi_offset s0, -8
; ZVABD-NEXT:    .cfi_offset s1, -16
; ZVABD-NEXT:    .cfi_offset s2, -24
; ZVABD-NEXT:    li a7, 0
; ZVABD-NEXT:    li t0, 0
; ZVABD-NEXT:    li a5, 0
; ZVABD-NEXT:    sext.w a1, a1
; ZVABD-NEXT:    sext.w a3, a3
; ZVABD-NEXT:    slli t1, a4, 32
; ZVABD-NEXT:    csrr t2, vlenb
; ZVABD-NEXT:    sext.w a4, a4
; ZVABD-NEXT:    vsetvli t3, zero, e32, m1, ta, ma
; ZVABD-NEXT:    vmv.v.i v12, 0
; ZVABD-NEXT:    vsetvli t3, zero, e32, m2, ta, ma
; ZVABD-NEXT:    vmv.v.i v8, 0
; ZVABD-NEXT:    srli t1, t1, 32
; ZVABD-NEXT:    srli t2, t2, 1
; ZVABD-NEXT:    neg t3, t2
; ZVABD-NEXT:    and t3, t3, t1
; ZVABD-NEXT:    vmv.s.x v13, zero
; ZVABD-NEXT:    mv t4, a2
; ZVABD-NEXT:    j .LBB0_3
; ZVABD-NEXT:  .LBB0_2: # %for.cond.cleanup3
; ZVABD-NEXT:    # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    add a0, a0, a1
; ZVABD-NEXT:    add t4, t4, a3
; ZVABD-NEXT:    addiw t0, t0, 1
; ZVABD-NEXT:    addi a7, a7, 1
; ZVABD-NEXT:    beq t0, a6, .LBB0_11
; ZVABD-NEXT:  .LBB0_3: # %for.cond1.preheader
; ZVABD-NEXT:    # =>This Loop Header: Depth=1
; ZVABD-NEXT:    # Child Loop BB0_7 Depth 2
; ZVABD-NEXT:    # Child Loop BB0_10 Depth 2
; ZVABD-NEXT:    blez a4, .LBB0_2
; ZVABD-NEXT:  # %bb.4: # %for.body4.preheader
; ZVABD-NEXT:    # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    bgeu t1, t2, .LBB0_6
; ZVABD-NEXT:  # %bb.5: # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    li s0, 0
; ZVABD-NEXT:    j .LBB0_9
; ZVABD-NEXT:  .LBB0_6: # %vector.ph
; ZVABD-NEXT:    # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    vmv1r.v v14, v12
; ZVABD-NEXT:    vsetvli zero, zero, e32, m2, tu, ma
; ZVABD-NEXT:    vmv.s.x v14, a5
; ZVABD-NEXT:    vmv2r.v v10, v8
; ZVABD-NEXT:    vmv1r.v v10, v14
; ZVABD-NEXT:    mv a5, a0
; ZVABD-NEXT:    mv t5, t4
; ZVABD-NEXT:    mv t6, t3
; ZVABD-NEXT:  .LBB0_7: # %vector.body
; ZVABD-NEXT:    # Parent Loop BB0_3 Depth=1
; ZVABD-NEXT:    # => This Inner Loop Header: Depth=2
; ZVABD-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; ZVABD-NEXT:    vle8.v v14, (a5)
; ZVABD-NEXT:    vle8.v v15, (t5)
; ZVABD-NEXT:    sub t6, t6, t2
; ZVABD-NEXT:    add t5, t5, t2
; ZVABD-NEXT:    vabdu.vv v14, v14, v15
; ZVABD-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; ZVABD-NEXT:    vzext.vf2 v15, v14
; ZVABD-NEXT:    vwaddu.wv v10, v10, v15
; ZVABD-NEXT:    add a5, a5, t2
; ZVABD-NEXT:    bnez t6, .LBB0_7
; ZVABD-NEXT:  # %bb.8: # %middle.block
; ZVABD-NEXT:    # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; ZVABD-NEXT:    vredsum.vs v10, v10, v13
; ZVABD-NEXT:    vmv.x.s a5, v10
; ZVABD-NEXT:    mv s0, t3
; ZVABD-NEXT:    beq t3, t1, .LBB0_2
; ZVABD-NEXT:  .LBB0_9: # %for.body4.preheader31
; ZVABD-NEXT:    # in Loop: Header=BB0_3 Depth=1
; ZVABD-NEXT:    mul t6, a3, a7
; ZVABD-NEXT:    add t5, t4, s0
; ZVABD-NEXT:    add t6, t1, t6
; ZVABD-NEXT:    add t6, a2, t6
; ZVABD-NEXT:    add s0, a0, s0
; ZVABD-NEXT:  .LBB0_10: # %for.body4
; ZVABD-NEXT:    # Parent Loop BB0_3 Depth=1
; ZVABD-NEXT:    # => This Inner Loop Header: Depth=2
; ZVABD-NEXT:    lbu s1, 0(s0)
; ZVABD-NEXT:    lbu s2, 0(t5)
; ZVABD-NEXT:    addi t5, t5, 1
; ZVABD-NEXT:    sub s1, s1, s2
; ZVABD-NEXT:    srai s2, s1, 63
; ZVABD-NEXT:    xor s1, s1, s2
; ZVABD-NEXT:    sub a5, s2, a5
; ZVABD-NEXT:    subw a5, s1, a5
; ZVABD-NEXT:    addi s0, s0, 1
; ZVABD-NEXT:    bne t5, t6, .LBB0_10
; ZVABD-NEXT:    j .LBB0_2
; ZVABD-NEXT:  .LBB0_11:
; ZVABD-NEXT:    ld s0, 24(sp) # 8-byte Folded Reload
; ZVABD-NEXT:    ld s1, 16(sp) # 8-byte Folded Reload
; ZVABD-NEXT:    ld s2, 8(sp) # 8-byte Folded Reload
; ZVABD-NEXT:    .cfi_restore s0
; ZVABD-NEXT:    .cfi_restore s1
; ZVABD-NEXT:    .cfi_restore s2
; ZVABD-NEXT:    addi sp, sp, 32
; ZVABD-NEXT:    .cfi_def_cfa_offset 0
; ZVABD-NEXT:    mv a0, a5
; ZVABD-NEXT:    ret
; ZVABD-NEXT:  .LBB0_12:
; ZVABD-NEXT:    li a0, 0
; ZVABD-NEXT:    ret
entry:
  %cmp23 = icmp sgt i32 %i_height, 0
  br i1 %cmp23, label %for.cond1.preheader.lr.ph, label %for.cond.cleanup

for.cond1.preheader.lr.ph:                        ; preds = %entry
  %cmp220 = icmp sgt i32 %i_width, 0
  %idx.ext = sext i32 %i_stride_pix1 to i64
  %idx.ext8 = sext i32 %i_stride_pix2 to i64
  %wide.trip.count = zext i32 %i_width to i64
  br label %for.cond1.preheader

for.cond1.preheader:                              ; preds = %for.cond1.preheader.lr.ph, %for.cond.cleanup3
  %y.027 = phi i32 [ 0, %for.cond1.preheader.lr.ph ], [ %inc11, %for.cond.cleanup3 ]
  %i_sum.026 = phi i32 [ 0, %for.cond1.preheader.lr.ph ], [ %i_sum.1.lcssa, %for.cond.cleanup3 ]
  %pix1.addr.025 = phi ptr [ %pix1, %for.cond1.preheader.lr.ph ], [ %add.ptr, %for.cond.cleanup3 ]
  %pix2.addr.024 = phi ptr [ %pix2, %for.cond1.preheader.lr.ph ], [ %add.ptr9, %for.cond.cleanup3 ]
  br i1 %cmp220, label %for.body4.preheader, label %for.cond.cleanup3

for.body4.preheader:                              ; preds = %for.cond1.preheader
  %0 = tail call i64 @llvm.vscale.i64()
  %1 = shl nuw nsw i64 %0, 2
  %min.iters.check = icmp samesign ugt i64 %1, %wide.trip.count
  br i1 %min.iters.check, label %for.body4.preheader31, label %vector.ph

vector.ph:                                        ; preds = %for.body4.preheader
  %2 = tail call i64 @llvm.vscale.i64()
  %.neg = mul nuw nsw i64 %2, 2147483644
  %n.vec = and i64 %.neg, %wide.trip.count
  %3 = tail call i64 @llvm.vscale.i64()
  %4 = shl nuw nsw i64 %3, 2
  %5 = insertelement <vscale x 4 x i32> zeroinitializer, i32 %i_sum.026, i64 0
  br label %vector.body

vector.body:                                      ; preds = %vector.body, %vector.ph
  %index = phi i64 [ 0, %vector.ph ], [ %index.next, %vector.body ]
  %vec.phi = phi <vscale x 4 x i32> [ %5, %vector.ph ], [ %12, %vector.body ]
  %6 = getelementptr inbounds nuw i8, ptr %pix1.addr.025, i64 %index
  %wide.load = load <vscale x 4 x i8>, ptr %6, align 1
  %7 = zext <vscale x 4 x i8> %wide.load to <vscale x 4 x i32>
  %8 = getelementptr inbounds nuw i8, ptr %pix2.addr.024, i64 %index
  %wide.load30 = load <vscale x 4 x i8>, ptr %8, align 1
  %9 = zext <vscale x 4 x i8> %wide.load30 to <vscale x 4 x i32>
  %10 = sub nsw <vscale x 4 x i32> %7, %9
  %11 = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> %10, i1 true)
  %12 = add <vscale x 4 x i32> %11, %vec.phi
  %index.next = add nuw i64 %index, %4
  %13 = icmp eq i64 %index.next, %n.vec
  br i1 %13, label %middle.block, label %vector.body

middle.block:                                     ; preds = %vector.body
  %14 = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> %12)
  %cmp.n = icmp eq i64 %n.vec, %wide.trip.count
  br i1 %cmp.n, label %for.cond.cleanup3, label %for.body4.preheader31

for.body4.preheader31:                            ; preds = %for.body4.preheader, %middle.block
  %indvars.iv.ph = phi i64 [ 0, %for.body4.preheader ], [ %n.vec, %middle.block ]
  %i_sum.121.ph = phi i32 [ %i_sum.026, %for.body4.preheader ], [ %14, %middle.block ]
  br label %for.body4

for.cond.cleanup:                                 ; preds = %for.cond.cleanup3, %entry
  %i_sum.0.lcssa = phi i32 [ 0, %entry ], [ %i_sum.1.lcssa, %for.cond.cleanup3 ]
  ret i32 %i_sum.0.lcssa

for.cond.cleanup3:                                ; preds = %for.body4, %middle.block, %for.cond1.preheader
  %i_sum.1.lcssa = phi i32 [ %i_sum.026, %for.cond1.preheader ], [ %14, %middle.block ], [ %add, %for.body4 ]
  %add.ptr = getelementptr inbounds i8, ptr %pix1.addr.025, i64 %idx.ext
  %add.ptr9 = getelementptr inbounds i8, ptr %pix2.addr.024, i64 %idx.ext8
  %inc11 = add nuw nsw i32 %y.027, 1
  %exitcond29.not = icmp eq i32 %inc11, %i_height
  br i1 %exitcond29.not, label %for.cond.cleanup, label %for.cond1.preheader

for.body4:                                        ; preds = %for.body4.preheader31, %for.body4
  %indvars.iv = phi i64 [ %indvars.iv.next, %for.body4 ], [ %indvars.iv.ph, %for.body4.preheader31 ]
  %i_sum.121 = phi i32 [ %add, %for.body4 ], [ %i_sum.121.ph, %for.body4.preheader31 ]
  %arrayidx = getelementptr inbounds nuw i8, ptr %pix1.addr.025, i64 %indvars.iv
  %15 = load i8, ptr %arrayidx, align 1
  %conv = zext i8 %15 to i32
  %arrayidx6 = getelementptr inbounds nuw i8, ptr %pix2.addr.024, i64 %indvars.iv
  %16 = load i8, ptr %arrayidx6, align 1
  %conv7 = zext i8 %16 to i32
  %sub = sub nsw i32 %conv, %conv7
  %17 = tail call i32 @llvm.abs.i32(i32 %sub, i1 true)
  %add = add nsw i32 %17, %i_sum.121
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond.not = icmp eq i64 %indvars.iv.next, %wide.trip.count
  br i1 %exitcond.not, label %for.cond.cleanup3, label %for.body4
}
