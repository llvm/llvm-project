; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -verify-machineinstrs -mattr=+v < %s | FileCheck %s --check-prefixes=CHECK,RV32,RV32V
; RUN: llc -mtriple=riscv64 -verify-machineinstrs -mattr=+v < %s | FileCheck %s --check-prefixes=CHECK,RV64,RV64V
; RUN: llc -mtriple=riscv32 -verify-machineinstrs -mattr=+v,+zvbc < %s | FileCheck %s --check-prefixes=CHECK,RV32,RV32ZVBC
; RUN: llc -mtriple=riscv64 -verify-machineinstrs -mattr=+v,+zvbc < %s | FileCheck %s --check-prefixes=CHECK,RV64,RV64ZVBC

define <vscale x 1 x i8> @clmulh_nxv1i8_vv(<vscale x 1 x i8> %va, <vscale x 1 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv1i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v9
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vand.vi v8, v10, 2
; CHECK-NEXT:    vand.vi v11, v10, 1
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v12, v10, 4
; CHECK-NEXT:    vand.vi v13, v10, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v11, v8
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v10, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v9, v9, v10
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    vsetvli zero, zero, e8, mf8, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 1 x i8> %va to <vscale x 1 x i16>
  %vb.ext = zext <vscale x 1 x i8> %vb to <vscale x 1 x i16>
  %clmul = call <vscale x 1 x i16> @llvm.clmul.nxv1i16(<vscale x 1 x i16> %va.ext, <vscale x 1 x i16> %vb.ext)
  %res.ext = lshr <vscale x 1 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 1 x i16> %res.ext to <vscale x 1 x i8>
  ret <vscale x 1 x i8> %res
}

define <vscale x 1 x i8> @clmulh_nxv1i8_vx(<vscale x 1 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv1i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8, mf8, ta, ma
; CHECK-NEXT:    vmv.v.x v9, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v8, v9
; CHECK-NEXT:    vand.vi v9, v8, 2
; CHECK-NEXT:    vand.vi v11, v8, 1
; CHECK-NEXT:    vmul.vv v9, v10, v9
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vand.vi v12, v8, 4
; CHECK-NEXT:    vand.vi v13, v8, 8
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vxor.vv v9, v11, v9
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v13, v10, v13
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v9, v9, v13
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    vmul.vv v11, v10, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    vxor.vv v8, v9, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, mf8, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 1 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 1 x i8> %elt.head, <vscale x 1 x i8> poison, <vscale x 1 x i32> zeroinitializer
  %va.ext = zext <vscale x 1 x i8> %va to <vscale x 1 x i16>
  %vb.ext = zext <vscale x 1 x i8> %vb to <vscale x 1 x i16>
  %clmul = call <vscale x 1 x i16> @llvm.clmul.nxv1i16(<vscale x 1 x i16> %va.ext, <vscale x 1 x i16> %vb.ext)
  %res.ext = lshr <vscale x 1 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 1 x i16> %res.ext to <vscale x 1 x i8>
  ret <vscale x 1 x i8> %res
}

define <vscale x 2 x i8> @clmulh_nxv2i8_vv(<vscale x 2 x i8> %va, <vscale x 2 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv2i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, mf2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v9
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vand.vi v8, v10, 2
; CHECK-NEXT:    vand.vi v11, v10, 1
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v12, v10, 4
; CHECK-NEXT:    vand.vi v13, v10, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v11, v8
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v10, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v9, v9, v10
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 2 x i8> %va to <vscale x 2 x i16>
  %vb.ext = zext <vscale x 2 x i8> %vb to <vscale x 2 x i16>
  %clmul = call <vscale x 2 x i16> @llvm.clmul.nxv2i16(<vscale x 2 x i16> %va.ext, <vscale x 2 x i16> %vb.ext)
  %res.ext = lshr <vscale x 2 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 2 x i16> %res.ext to <vscale x 2 x i8>
  ret <vscale x 2 x i8> %res
}

define <vscale x 2 x i8> @clmulh_nxv2i8_vx(<vscale x 2 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv2i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8, mf4, ta, ma
; CHECK-NEXT:    vmv.v.x v9, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v8, v9
; CHECK-NEXT:    vand.vi v9, v8, 2
; CHECK-NEXT:    vand.vi v11, v8, 1
; CHECK-NEXT:    vmul.vv v9, v10, v9
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vand.vi v12, v8, 4
; CHECK-NEXT:    vand.vi v13, v8, 8
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vxor.vv v9, v11, v9
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v13, v10, v13
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v9, v9, v13
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    vmul.vv v11, v10, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    vxor.vv v8, v9, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 2 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 2 x i8> %elt.head, <vscale x 2 x i8> poison, <vscale x 2 x i32> zeroinitializer
  %va.ext = zext <vscale x 2 x i8> %va to <vscale x 2 x i16>
  %vb.ext = zext <vscale x 2 x i8> %vb to <vscale x 2 x i16>
  %clmul = call <vscale x 2 x i16> @llvm.clmul.nxv2i16(<vscale x 2 x i16> %va.ext, <vscale x 2 x i16> %vb.ext)
  %res.ext = lshr <vscale x 2 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 2 x i16> %res.ext to <vscale x 2 x i8>
  ret <vscale x 2 x i8> %res
}

define <vscale x 4 x i8> @clmulh_nxv4i8_vv(<vscale x 4 x i8> %va, <vscale x 4 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv4i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, m1, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v9
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vand.vi v8, v10, 2
; CHECK-NEXT:    vand.vi v11, v10, 1
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v12, v10, 4
; CHECK-NEXT:    vand.vi v13, v10, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v11, v8
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    vand.vx v10, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v9, v9, v10
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 4 x i8> %va to <vscale x 4 x i16>
  %vb.ext = zext <vscale x 4 x i8> %vb to <vscale x 4 x i16>
  %clmul = call <vscale x 4 x i16> @llvm.clmul.nxv4i16(<vscale x 4 x i16> %va.ext, <vscale x 4 x i16> %vb.ext)
  %res.ext = lshr <vscale x 4 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 4 x i16> %res.ext to <vscale x 4 x i8>
  ret <vscale x 4 x i8> %res
}

define <vscale x 4 x i8> @clmulh_nxv4i8_vx(<vscale x 4 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv4i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8, mf2, ta, ma
; CHECK-NEXT:    vmv.v.x v9, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v8, v9
; CHECK-NEXT:    vand.vi v9, v8, 2
; CHECK-NEXT:    vand.vi v11, v8, 1
; CHECK-NEXT:    vmul.vv v9, v10, v9
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vand.vi v12, v8, 4
; CHECK-NEXT:    vand.vi v13, v8, 8
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vxor.vv v9, v11, v9
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v13, v10, v13
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v11, v10, v11
; CHECK-NEXT:    vxor.vv v9, v9, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v10, v13
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vxor.vv v9, v9, v12
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vxor.vv v9, v9, v11
; CHECK-NEXT:    vxor.vv v8, v9, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 8
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 4 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 4 x i8> %elt.head, <vscale x 4 x i8> poison, <vscale x 4 x i32> zeroinitializer
  %va.ext = zext <vscale x 4 x i8> %va to <vscale x 4 x i16>
  %vb.ext = zext <vscale x 4 x i8> %vb to <vscale x 4 x i16>
  %clmul = call <vscale x 4 x i16> @llvm.clmul.nxv4i16(<vscale x 4 x i16> %va.ext, <vscale x 4 x i16> %vb.ext)
  %res.ext = lshr <vscale x 4 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 4 x i16> %res.ext to <vscale x 4 x i8>
  ret <vscale x 4 x i8> %res
}

define <vscale x 8 x i8> @clmulh_nxv8i8_vv(<vscale x 8 x i8> %va, <vscale x 8 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv8i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, m2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v12, v9
; CHECK-NEXT:    vand.vi v8, v12, 2
; CHECK-NEXT:    vand.vi v14, v12, 1
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vxor.vv v8, v14, v8
; CHECK-NEXT:    vand.vi v14, v12, 4
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vand.vi v16, v12, 8
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v12, v12, a0
; CHECK-NEXT:    vmul.vv v10, v10, v12
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v10, v8, v10
; CHECK-NEXT:    vsetvli zero, zero, e8, m1, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v10, 8
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 8 x i8> %va to <vscale x 8 x i16>
  %vb.ext = zext <vscale x 8 x i8> %vb to <vscale x 8 x i16>
  %clmul = call <vscale x 8 x i16> @llvm.clmul.nxv8i16(<vscale x 8 x i16> %va.ext, <vscale x 8 x i16> %vb.ext)
  %res.ext = lshr <vscale x 8 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 8 x i16> %res.ext to <vscale x 8 x i8>
  ret <vscale x 8 x i8> %res
}

define <vscale x 8 x i8> @clmulh_nxv8i8_vx(<vscale x 8 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv8i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8, m1, ta, ma
; CHECK-NEXT:    vmv.v.x v12, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v8, v12
; CHECK-NEXT:    vand.vi v12, v8, 2
; CHECK-NEXT:    vand.vi v14, v8, 1
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vxor.vv v12, v14, v12
; CHECK-NEXT:    vand.vi v14, v8, 4
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vand.vi v16, v8, 8
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vxor.vv v10, v12, v14
; CHECK-NEXT:    vxor.vv v10, v10, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, m1, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v10, 8
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 8 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 8 x i8> %elt.head, <vscale x 8 x i8> poison, <vscale x 8 x i32> zeroinitializer
  %va.ext = zext <vscale x 8 x i8> %va to <vscale x 8 x i16>
  %vb.ext = zext <vscale x 8 x i8> %vb to <vscale x 8 x i16>
  %clmul = call <vscale x 8 x i16> @llvm.clmul.nxv8i16(<vscale x 8 x i16> %va.ext, <vscale x 8 x i16> %vb.ext)
  %res.ext = lshr <vscale x 8 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 8 x i16> %res.ext to <vscale x 8 x i8>
  ret <vscale x 8 x i8> %res
}

define <vscale x 16 x i8> @clmulh_nxv16i8_vv(<vscale x 16 x i8> %va, <vscale x 16 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv16i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, m4, ta, ma
; CHECK-NEXT:    vzext.vf2 v12, v8
; CHECK-NEXT:    vzext.vf2 v16, v10
; CHECK-NEXT:    vand.vi v8, v16, 2
; CHECK-NEXT:    vand.vi v20, v16, 1
; CHECK-NEXT:    vmul.vv v8, v12, v8
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vxor.vv v8, v20, v8
; CHECK-NEXT:    vand.vi v20, v16, 4
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vmul.vv v12, v12, v16
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v12, v8, v12
; CHECK-NEXT:    vsetvli zero, zero, e8, m2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v12, 8
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 16 x i8> %va to <vscale x 16 x i16>
  %vb.ext = zext <vscale x 16 x i8> %vb to <vscale x 16 x i16>
  %clmul = call <vscale x 16 x i16> @llvm.clmul.nxv16i16(<vscale x 16 x i16> %va.ext, <vscale x 16 x i16> %vb.ext)
  %res.ext = lshr <vscale x 16 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 16 x i16> %res.ext to <vscale x 16 x i8>
  ret <vscale x 16 x i8> %res
}

define <vscale x 16 x i8> @clmulh_nxv16i8_vx(<vscale x 16 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv16i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e8, m2, ta, ma
; CHECK-NEXT:    vmv.v.x v16, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, m4, ta, ma
; CHECK-NEXT:    vzext.vf2 v12, v8
; CHECK-NEXT:    vzext.vf2 v8, v16
; CHECK-NEXT:    vand.vi v16, v8, 2
; CHECK-NEXT:    vand.vi v20, v8, 1
; CHECK-NEXT:    vmul.vv v16, v12, v16
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vxor.vv v16, v20, v16
; CHECK-NEXT:    vand.vi v20, v8, 4
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vand.vi v24, v8, 8
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v12, v8
; CHECK-NEXT:    vxor.vv v12, v16, v20
; CHECK-NEXT:    vxor.vv v12, v12, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, m2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v12, 8
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 16 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 16 x i8> %elt.head, <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer
  %va.ext = zext <vscale x 16 x i8> %va to <vscale x 16 x i16>
  %vb.ext = zext <vscale x 16 x i8> %vb to <vscale x 16 x i16>
  %clmul = call <vscale x 16 x i16> @llvm.clmul.nxv16i16(<vscale x 16 x i16> %va.ext, <vscale x 16 x i16> %vb.ext)
  %res.ext = lshr <vscale x 16 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 16 x i16> %res.ext to <vscale x 16 x i8>
  ret <vscale x 16 x i8> %res
}

define <vscale x 32 x i8> @clmulh_nxv32i8_vv(<vscale x 32 x i8> %va, <vscale x 32 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv32i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    sub sp, sp, a0
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e16, m8, ta, ma
; CHECK-NEXT:    vzext.vf2 v16, v8
; CHECK-NEXT:    vzext.vf2 v24, v12
; CHECK-NEXT:    vand.vi v8, v24, 2
; CHECK-NEXT:    vand.vi v0, v24, 1
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v24, 4
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v8, v24, 8
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vmul.vv v16, v16, v24
; CHECK-NEXT:    vxor.vv v8, v8, v0
; CHECK-NEXT:    vxor.vv v16, v8, v16
; CHECK-NEXT:    vsetvli zero, zero, e8, m4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v16, 8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 32 x i8> %va to <vscale x 32 x i16>
  %vb.ext = zext <vscale x 32 x i8> %vb to <vscale x 32 x i16>
  %clmul = call <vscale x 32 x i16> @llvm.clmul.nxv32i16(<vscale x 32 x i16> %va.ext, <vscale x 32 x i16> %vb.ext)
  %res.ext = lshr <vscale x 32 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 32 x i16> %res.ext to <vscale x 32 x i8>
  ret <vscale x 32 x i8> %res
}

define <vscale x 32 x i8> @clmulh_nxv32i8_vx(<vscale x 32 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv32i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    mv a2, a1
; CHECK-NEXT:    slli a1, a1, 1
; CHECK-NEXT:    add a1, a1, a2
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    vsetvli a1, zero, e8, m4, ta, ma
; CHECK-NEXT:    vmv.v.x v24, a0
; CHECK-NEXT:    vsetvli zero, zero, e16, m8, ta, ma
; CHECK-NEXT:    vzext.vf2 v16, v8
; CHECK-NEXT:    vzext.vf2 v8, v24
; CHECK-NEXT:    vand.vi v24, v8, 2
; CHECK-NEXT:    vand.vi v0, v8, 1
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v8, 4
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v8, 8
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v16, v16, v8
; CHECK-NEXT:    vsetvli zero, zero, e8, m4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v16, 8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 32 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 32 x i8> %elt.head, <vscale x 32 x i8> poison, <vscale x 32 x i32> zeroinitializer
  %va.ext = zext <vscale x 32 x i8> %va to <vscale x 32 x i16>
  %vb.ext = zext <vscale x 32 x i8> %vb to <vscale x 32 x i16>
  %clmul = call <vscale x 32 x i16> @llvm.clmul.nxv32i16(<vscale x 32 x i16> %va.ext, <vscale x 32 x i16> %vb.ext)
  %res.ext = lshr <vscale x 32 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 32 x i16> %res.ext to <vscale x 32 x i8>
  ret <vscale x 32 x i8> %res
}

define <vscale x 64 x i8> @clmulh_nxv64i8_vv(<vscale x 64 x i8> %va, <vscale x 64 x i8> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv64i8_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    sub sp, sp, a0
; CHECK-NEXT:    li a0, 51
; CHECK-NEXT:    vsetvli a1, zero, e8, m8, ta, ma
; CHECK-NEXT:    vsll.vi v24, v8, 4
; CHECK-NEXT:    vsrl.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v8, v24
; CHECK-NEXT:    vsrl.vi v24, v8, 2
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 1
; CHECK-NEXT:    li a1, 85
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsll.vi v24, v16, 4
; CHECK-NEXT:    vsrl.vi v16, v16, 4
; CHECK-NEXT:    vor.vv v16, v16, v24
; CHECK-NEXT:    vsrl.vi v24, v16, 2
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vsll.vi v16, v16, 2
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 1
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vadd.vv v16, v16, v16
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vand.vi v24, v16, 2
; CHECK-NEXT:    vand.vi v0, v16, 1
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v16, 4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a2, 16
; CHECK-NEXT:    vand.vx v0, v16, a2
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a2, 32
; CHECK-NEXT:    vand.vx v24, v16, a2
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    li a2, 64
; CHECK-NEXT:    vand.vx v0, v16, a2
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    li a2, 128
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vmul.vv v8, v8, v16
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v8, v16, v8
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v16, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsll.vi v16, v16, 4
; CHECK-NEXT:    vsrl.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    vsrl.vi v16, v8, 2
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v8, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 64 x i8> %va to <vscale x 64 x i16>
  %vb.ext = zext <vscale x 64 x i8> %vb to <vscale x 64 x i16>
  %clmul = call <vscale x 64 x i16> @llvm.clmul.nxv64i16(<vscale x 64 x i16> %va.ext, <vscale x 64 x i16> %vb.ext)
  %res.ext = lshr <vscale x 64 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 64 x i16> %res.ext to <vscale x 64 x i8>
  ret <vscale x 64 x i8> %res
}

define <vscale x 64 x i8> @clmulh_nxv64i8_vx(<vscale x 64 x i8> %va, i8 %b) nounwind {
; CHECK-LABEL: clmulh_nxv64i8_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    mv a2, a1
; CHECK-NEXT:    slli a1, a1, 1
; CHECK-NEXT:    add a1, a1, a2
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    vsetvli a1, zero, e8, m8, ta, ma
; CHECK-NEXT:    vmv.v.x v16, a0
; CHECK-NEXT:    vsll.vi v24, v8, 4
; CHECK-NEXT:    vsrl.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v8, v24
; CHECK-NEXT:    vsrl.vi v24, v8, 2
; CHECK-NEXT:    li a0, 51
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 1
; CHECK-NEXT:    li a1, 85
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsll.vi v24, v16, 4
; CHECK-NEXT:    vsrl.vi v16, v16, 4
; CHECK-NEXT:    vor.vv v16, v16, v24
; CHECK-NEXT:    vsrl.vi v24, v16, 2
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vsll.vi v16, v16, 2
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 1
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vadd.vv v16, v16, v16
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vand.vi v24, v16, 2
; CHECK-NEXT:    vand.vi v0, v16, 1
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v16, 4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a2, 16
; CHECK-NEXT:    vand.vx v0, v16, a2
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a2, 32
; CHECK-NEXT:    vand.vx v24, v16, a2
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 3
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a2, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    li a2, 64
; CHECK-NEXT:    vand.vx v0, v16, a2
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    li a2, 128
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vmul.vv v8, v8, v16
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v8, v16, v8
; CHECK-NEXT:    csrr a2, vlenb
; CHECK-NEXT:    slli a2, a2, 4
; CHECK-NEXT:    add a2, sp, a2
; CHECK-NEXT:    addi a2, a2, 16
; CHECK-NEXT:    vl8r.v v16, (a2) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsll.vi v16, v16, 4
; CHECK-NEXT:    vsrl.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v8, v16
; CHECK-NEXT:    vsrl.vi v16, v8, 2
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v8, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 64 x i8> poison, i8 %b, i32 0
  %vb = shufflevector <vscale x 64 x i8> %elt.head, <vscale x 64 x i8> poison, <vscale x 64 x i32> zeroinitializer
  %va.ext = zext <vscale x 64 x i8> %va to <vscale x 64 x i16>
  %vb.ext = zext <vscale x 64 x i8> %vb to <vscale x 64 x i16>
  %clmul = call <vscale x 64 x i16> @llvm.clmul.nxv64i16(<vscale x 64 x i16> %va.ext, <vscale x 64 x i16> %vb.ext)
  %res.ext = lshr <vscale x 64 x i16> %clmul, splat(i16 8)
  %res = trunc <vscale x 64 x i16> %res.ext to <vscale x 64 x i8>
  ret <vscale x 64 x i8> %res
}

define <vscale x 1 x i16> @clmulh_nxv1i16_vv(<vscale x 1 x i16> %va, <vscale x 1 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv1i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e32, mf2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v9
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vand.vi v8, v10, 2
; CHECK-NEXT:    vand.vi v11, v10, 1
; CHECK-NEXT:    vand.vi v12, v10, 4
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v13, v10, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v11, v8
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v12
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v10, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    vmul.vv v9, v9, v10
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 1 x i16> %va to <vscale x 1 x i32>
  %vb.ext = zext <vscale x 1 x i16> %vb to <vscale x 1 x i32>
  %clmul = call <vscale x 1 x i32> @llvm.clmul.nxv1i32(<vscale x 1 x i32> %va.ext, <vscale x 1 x i32> %vb.ext)
  %res.ext = lshr <vscale x 1 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 1 x i32> %res.ext to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %res
}

define <vscale x 1 x i16> @clmulh_nxv1i16_vx(<vscale x 1 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv1i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16, mf4, ta, ma
; CHECK-NEXT:    vmv.v.x v10, a0
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vzext.vf2 v8, v10
; CHECK-NEXT:    vand.vi v10, v8, 2
; CHECK-NEXT:    vand.vi v11, v8, 1
; CHECK-NEXT:    vand.vi v12, v8, 4
; CHECK-NEXT:    vmul.vv v10, v9, v10
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v13, v8, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v11, v10
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v10, v13
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v10, v13
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v10, v13
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vmul.vv v11, v9, v12
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v13
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vxor.vv v9, v10, v11
; CHECK-NEXT:    vxor.vv v8, v9, v8
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 1 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 1 x i16> %elt.head, <vscale x 1 x i16> poison, <vscale x 1 x i32> zeroinitializer
  %va.ext = zext <vscale x 1 x i16> %va to <vscale x 1 x i32>
  %vb.ext = zext <vscale x 1 x i16> %vb to <vscale x 1 x i32>
  %clmul = call <vscale x 1 x i32> @llvm.clmul.nxv1i32(<vscale x 1 x i32> %va.ext, <vscale x 1 x i32> %vb.ext)
  %res.ext = lshr <vscale x 1 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 1 x i32> %res.ext to <vscale x 1 x i16>
  ret <vscale x 1 x i16> %res
}

define <vscale x 2 x i16> @clmulh_nxv2i16_vv(<vscale x 2 x i16> %va, <vscale x 2 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv2i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e32, m1, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v9
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vand.vi v8, v10, 2
; CHECK-NEXT:    vand.vi v11, v10, 1
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v12, v10, 4
; CHECK-NEXT:    vand.vi v13, v10, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v11, v8
; CHECK-NEXT:    vand.vx v11, v10, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v12, v10, a0
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vxor.vv v8, v8, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v13, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v10, v10, a0
; CHECK-NEXT:    vxor.vv v8, v8, v12
; CHECK-NEXT:    vmul.vv v9, v9, v10
; CHECK-NEXT:    vxor.vv v8, v8, v11
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 2 x i16> %va to <vscale x 2 x i32>
  %vb.ext = zext <vscale x 2 x i16> %vb to <vscale x 2 x i32>
  %clmul = call <vscale x 2 x i32> @llvm.clmul.nxv2i32(<vscale x 2 x i32> %va.ext, <vscale x 2 x i32> %vb.ext)
  %res.ext = lshr <vscale x 2 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 2 x i32> %res.ext to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %res
}

define <vscale x 2 x i16> @clmulh_nxv2i16_vx(<vscale x 2 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv2i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16, mf2, ta, ma
; CHECK-NEXT:    vmv.v.x v10, a0
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; CHECK-NEXT:    vzext.vf2 v9, v8
; CHECK-NEXT:    vzext.vf2 v8, v10
; CHECK-NEXT:    vand.vi v10, v8, 2
; CHECK-NEXT:    vand.vi v11, v8, 1
; CHECK-NEXT:    vmul.vv v10, v9, v10
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vand.vi v12, v8, 4
; CHECK-NEXT:    vand.vi v13, v8, 8
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v11, v10
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v11, v8, a0
; CHECK-NEXT:    vmul.vv v13, v9, v13
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v12, v8, a0
; CHECK-NEXT:    vmul.vv v11, v9, v11
; CHECK-NEXT:    vxor.vv v10, v10, v13
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vmul.vv v12, v9, v12
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vmul.vv v12, v9, v13
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v13, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v11
; CHECK-NEXT:    vmul.vv v11, v9, v13
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vxor.vv v10, v10, v12
; CHECK-NEXT:    vmul.vv v8, v9, v8
; CHECK-NEXT:    vxor.vv v9, v10, v11
; CHECK-NEXT:    vxor.vv v8, v9, v8
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v8, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 2 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 2 x i16> %elt.head, <vscale x 2 x i16> poison, <vscale x 2 x i32> zeroinitializer
  %va.ext = zext <vscale x 2 x i16> %va to <vscale x 2 x i32>
  %vb.ext = zext <vscale x 2 x i16> %vb to <vscale x 2 x i32>
  %clmul = call <vscale x 2 x i32> @llvm.clmul.nxv2i32(<vscale x 2 x i32> %va.ext, <vscale x 2 x i32> %vb.ext)
  %res.ext = lshr <vscale x 2 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 2 x i32> %res.ext to <vscale x 2 x i16>
  ret <vscale x 2 x i16> %res
}

define <vscale x 4 x i16> @clmulh_nxv4i16_vv(<vscale x 4 x i16> %va, <vscale x 4 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv4i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v12, v9
; CHECK-NEXT:    vand.vi v8, v12, 2
; CHECK-NEXT:    vand.vi v14, v12, 1
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vxor.vv v8, v14, v8
; CHECK-NEXT:    vand.vi v14, v12, 4
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vand.vi v16, v12, 8
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v16, v12, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v8, v8, v16
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v14, v12, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v12, v12, a0
; CHECK-NEXT:    vmul.vv v10, v10, v12
; CHECK-NEXT:    vxor.vv v8, v8, v14
; CHECK-NEXT:    vxor.vv v10, v8, v10
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v10, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 4 x i16> %va to <vscale x 4 x i32>
  %vb.ext = zext <vscale x 4 x i16> %vb to <vscale x 4 x i32>
  %clmul = call <vscale x 4 x i32> @llvm.clmul.nxv4i32(<vscale x 4 x i32> %va.ext, <vscale x 4 x i32> %vb.ext)
  %res.ext = lshr <vscale x 4 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 4 x i32> %res.ext to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %res
}

define <vscale x 4 x i16> @clmulh_nxv4i16_vx(<vscale x 4 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv4i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16, m1, ta, ma
; CHECK-NEXT:    vmv.v.x v12, a0
; CHECK-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; CHECK-NEXT:    vzext.vf2 v10, v8
; CHECK-NEXT:    vzext.vf2 v8, v12
; CHECK-NEXT:    vand.vi v12, v8, 2
; CHECK-NEXT:    vand.vi v14, v8, 1
; CHECK-NEXT:    vmul.vv v12, v10, v12
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vxor.vv v12, v14, v12
; CHECK-NEXT:    vand.vi v14, v8, 4
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    vand.vi v16, v8, 8
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v16, v8, a0
; CHECK-NEXT:    vmul.vv v16, v10, v16
; CHECK-NEXT:    vxor.vv v12, v12, v14
; CHECK-NEXT:    vxor.vv v12, v12, v16
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v14, v8, a0
; CHECK-NEXT:    vmul.vv v14, v10, v14
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v10, v8
; CHECK-NEXT:    vxor.vv v10, v12, v14
; CHECK-NEXT:    vxor.vv v10, v10, v8
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v10, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 4 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 4 x i16> %elt.head, <vscale x 4 x i16> poison, <vscale x 4 x i32> zeroinitializer
  %va.ext = zext <vscale x 4 x i16> %va to <vscale x 4 x i32>
  %vb.ext = zext <vscale x 4 x i16> %vb to <vscale x 4 x i32>
  %clmul = call <vscale x 4 x i32> @llvm.clmul.nxv4i32(<vscale x 4 x i32> %va.ext, <vscale x 4 x i32> %vb.ext)
  %res.ext = lshr <vscale x 4 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 4 x i32> %res.ext to <vscale x 4 x i16>
  ret <vscale x 4 x i16> %res
}

define <vscale x 8 x i16> @clmulh_nxv8i16_vv(<vscale x 8 x i16> %va, <vscale x 8 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv8i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; CHECK-NEXT:    vzext.vf2 v12, v8
; CHECK-NEXT:    vzext.vf2 v16, v10
; CHECK-NEXT:    vand.vi v8, v16, 2
; CHECK-NEXT:    vand.vi v20, v16, 1
; CHECK-NEXT:    vmul.vv v8, v12, v8
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vxor.vv v8, v20, v8
; CHECK-NEXT:    vand.vi v20, v16, 4
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v24, v16, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v8, v8, v24
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v20, v16, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vmul.vv v12, v12, v16
; CHECK-NEXT:    vxor.vv v8, v8, v20
; CHECK-NEXT:    vxor.vv v12, v8, v12
; CHECK-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v12, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 8 x i16> %va to <vscale x 8 x i32>
  %vb.ext = zext <vscale x 8 x i16> %vb to <vscale x 8 x i32>
  %clmul = call <vscale x 8 x i32> @llvm.clmul.nxv8i32(<vscale x 8 x i32> %va.ext, <vscale x 8 x i32> %vb.ext)
  %res.ext = lshr <vscale x 8 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 8 x i32> %res.ext to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %res
}

define <vscale x 8 x i16> @clmulh_nxv8i16_vx(<vscale x 8 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv8i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli a1, zero, e16, m2, ta, ma
; CHECK-NEXT:    vmv.v.x v16, a0
; CHECK-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; CHECK-NEXT:    vzext.vf2 v12, v8
; CHECK-NEXT:    vzext.vf2 v8, v16
; CHECK-NEXT:    vand.vi v16, v8, 2
; CHECK-NEXT:    vand.vi v20, v8, 1
; CHECK-NEXT:    vmul.vv v16, v12, v16
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vxor.vv v16, v20, v16
; CHECK-NEXT:    vand.vi v20, v8, 4
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    vand.vi v24, v8, 8
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v12, v24
; CHECK-NEXT:    vxor.vv v16, v16, v20
; CHECK-NEXT:    vxor.vv v16, v16, v24
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v20, v8, a0
; CHECK-NEXT:    vmul.vv v20, v12, v20
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v12, v8
; CHECK-NEXT:    vxor.vv v12, v16, v20
; CHECK-NEXT:    vxor.vv v12, v12, v8
; CHECK-NEXT:    vsetvli zero, zero, e16, m2, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v12, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 8 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 8 x i16> %elt.head, <vscale x 8 x i16> poison, <vscale x 8 x i32> zeroinitializer
  %va.ext = zext <vscale x 8 x i16> %va to <vscale x 8 x i32>
  %vb.ext = zext <vscale x 8 x i16> %vb to <vscale x 8 x i32>
  %clmul = call <vscale x 8 x i32> @llvm.clmul.nxv8i32(<vscale x 8 x i32> %va.ext, <vscale x 8 x i32> %vb.ext)
  %res.ext = lshr <vscale x 8 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 8 x i32> %res.ext to <vscale x 8 x i16>
  ret <vscale x 8 x i16> %res
}

define <vscale x 16 x i16> @clmulh_nxv16i16_vv(<vscale x 16 x i16> %va, <vscale x 16 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv16i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    sub sp, sp, a0
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vsetvli a1, zero, e32, m8, ta, ma
; CHECK-NEXT:    vzext.vf2 v16, v8
; CHECK-NEXT:    vzext.vf2 v24, v12
; CHECK-NEXT:    vand.vi v8, v24, 2
; CHECK-NEXT:    vand.vi v0, v24, 1
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v24, 4
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v8, v24, 8
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v8, v24, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v8
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v8, v0, v8
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v0, v24, a0
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vmul.vv v16, v16, v24
; CHECK-NEXT:    vxor.vv v8, v8, v0
; CHECK-NEXT:    vxor.vv v16, v8, v16
; CHECK-NEXT:    vsetvli zero, zero, e16, m4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v16, 16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 16 x i16> %va to <vscale x 16 x i32>
  %vb.ext = zext <vscale x 16 x i16> %vb to <vscale x 16 x i32>
  %clmul = call <vscale x 16 x i32> @llvm.clmul.nxv16i32(<vscale x 16 x i32> %va.ext, <vscale x 16 x i32> %vb.ext)
  %res.ext = lshr <vscale x 16 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 16 x i32> %res.ext to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %res
}

define <vscale x 16 x i16> @clmulh_nxv16i16_vx(<vscale x 16 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv16i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    mv a2, a1
; CHECK-NEXT:    slli a1, a1, 1
; CHECK-NEXT:    add a1, a1, a2
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    vsetvli a1, zero, e16, m4, ta, ma
; CHECK-NEXT:    vmv.v.x v24, a0
; CHECK-NEXT:    vsetvli zero, zero, e32, m8, ta, ma
; CHECK-NEXT:    vzext.vf2 v16, v8
; CHECK-NEXT:    vzext.vf2 v8, v24
; CHECK-NEXT:    vand.vi v24, v8, 2
; CHECK-NEXT:    vand.vi v0, v8, 1
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v8, 4
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v8, 8
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 16
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 32
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 64
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 128
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 256
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 512
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 1024
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a0, 1
; CHECK-NEXT:    slli a0, a0, 11
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a0, 1
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v0
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a0, 2
; CHECK-NEXT:    vand.vx v24, v8, a0
; CHECK-NEXT:    vmul.vv v24, v16, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    lui a0, 4
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vmul.vv v0, v16, v0
; CHECK-NEXT:    lui a0, 8
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vmul.vv v8, v16, v8
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v16, v16, v8
; CHECK-NEXT:    vsetvli zero, zero, e16, m4, ta, ma
; CHECK-NEXT:    vnsrl.wi v8, v16, 16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 16 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 16 x i16> %elt.head, <vscale x 16 x i16> poison, <vscale x 16 x i32> zeroinitializer
  %va.ext = zext <vscale x 16 x i16> %va to <vscale x 16 x i32>
  %vb.ext = zext <vscale x 16 x i16> %vb to <vscale x 16 x i32>
  %clmul = call <vscale x 16 x i32> @llvm.clmul.nxv16i32(<vscale x 16 x i32> %va.ext, <vscale x 16 x i32> %vb.ext)
  %res.ext = lshr <vscale x 16 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 16 x i32> %res.ext to <vscale x 16 x i16>
  ret <vscale x 16 x i16> %res
}

define <vscale x 32 x i16> @clmulh_nxv32i16_vv(<vscale x 32 x i16> %va, <vscale x 32 x i16> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv32i16_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 5
; CHECK-NEXT:    sub sp, sp, a0
; CHECK-NEXT:    lui a3, 1
; CHECK-NEXT:    addi a0, a3, -241
; CHECK-NEXT:    vsetvli a1, zero, e16, m8, ta, ma
; CHECK-NEXT:    vsrl.vi v24, v8, 8
; CHECK-NEXT:    vsll.vi v8, v8, 8
; CHECK-NEXT:    vor.vv v8, v8, v24
; CHECK-NEXT:    vsrl.vi v24, v8, 4
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 2
; CHECK-NEXT:    lui a1, 3
; CHECK-NEXT:    addi a1, a1, 819
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 1
; CHECK-NEXT:    lui a2, 5
; CHECK-NEXT:    addi a2, a2, 1365
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v16, 8
; CHECK-NEXT:    vsll.vi v16, v16, 8
; CHECK-NEXT:    vor.vv v16, v16, v24
; CHECK-NEXT:    vsrl.vi v24, v16, 4
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vsll.vi v16, v16, 4
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 2
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vsll.vi v16, v16, 2
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 1
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vadd.vv v16, v16, v16
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vand.vi v24, v16, 2
; CHECK-NEXT:    vand.vi v0, v16, 1
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v16, 4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 16
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 32
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 64
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 128
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 256
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 512
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 1024
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 1
; CHECK-NEXT:    slli a4, a4, 11
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vx v0, v16, a3
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a3, 2
; CHECK-NEXT:    vand.vx v24, v16, a3
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a3, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 4
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v0, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v24, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a3, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    lui a3, 4
; CHECK-NEXT:    vand.vx v0, v16, a3
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    lui a3, 8
; CHECK-NEXT:    vand.vx v16, v16, a3
; CHECK-NEXT:    vmul.vv v8, v8, v16
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v8, v16, v8
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    mv a4, a3
; CHECK-NEXT:    slli a3, a3, 1
; CHECK-NEXT:    add a3, a3, a4
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v16, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsll.vi v16, v16, 8
; CHECK-NEXT:    vsrl.vi v8, v8, 8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 4
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 2
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 1
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v8, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 5
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 32 x i16> %va to <vscale x 32 x i32>
  %vb.ext = zext <vscale x 32 x i16> %vb to <vscale x 32 x i32>
  %clmul = call <vscale x 32 x i32> @llvm.clmul.nxv32i32(<vscale x 32 x i32> %va.ext, <vscale x 32 x i32> %vb.ext)
  %res.ext = lshr <vscale x 32 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 32 x i32> %res.ext to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %res
}

define <vscale x 32 x i16> @clmulh_nxv32i16_vx(<vscale x 32 x i16> %va, i16 %b) nounwind {
; CHECK-LABEL: clmulh_nxv32i16_vx:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 5
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    vsetvli a1, zero, e16, m8, ta, ma
; CHECK-NEXT:    vmv.v.x v16, a0
; CHECK-NEXT:    vsrl.vi v24, v8, 8
; CHECK-NEXT:    vsll.vi v8, v8, 8
; CHECK-NEXT:    vor.vv v8, v8, v24
; CHECK-NEXT:    vsrl.vi v24, v8, 4
; CHECK-NEXT:    lui a3, 1
; CHECK-NEXT:    addi a0, a3, -241
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 2
; CHECK-NEXT:    lui a1, 3
; CHECK-NEXT:    addi a1, a1, 819
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 1
; CHECK-NEXT:    lui a2, 5
; CHECK-NEXT:    addi a2, a2, 1365
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v16, 8
; CHECK-NEXT:    vsll.vi v16, v16, 8
; CHECK-NEXT:    vor.vv v16, v16, v24
; CHECK-NEXT:    vsrl.vi v24, v16, 4
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vsll.vi v16, v16, 4
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 2
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vsll.vi v16, v16, 2
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 1
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vadd.vv v16, v16, v16
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vand.vi v24, v16, 2
; CHECK-NEXT:    vand.vi v0, v16, 1
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v16, 4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 16
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 32
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 64
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 128
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 256
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 512
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 1024
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a4, 1
; CHECK-NEXT:    slli a4, a4, 11
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vx v0, v16, a3
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a3, 2
; CHECK-NEXT:    vand.vx v24, v16, a3
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a3, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 4
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v0, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v24, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a3, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    lui a3, 4
; CHECK-NEXT:    vand.vx v0, v16, a3
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    lui a3, 8
; CHECK-NEXT:    vand.vx v16, v16, a3
; CHECK-NEXT:    vmul.vv v8, v8, v16
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v8, v16, v8
; CHECK-NEXT:    csrr a3, vlenb
; CHECK-NEXT:    slli a3, a3, 3
; CHECK-NEXT:    mv a4, a3
; CHECK-NEXT:    slli a3, a3, 1
; CHECK-NEXT:    add a3, a3, a4
; CHECK-NEXT:    add a3, sp, a3
; CHECK-NEXT:    addi a3, a3, 16
; CHECK-NEXT:    vl8r.v v16, (a3) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsll.vi v16, v16, 8
; CHECK-NEXT:    vsrl.vi v8, v8, 8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 4
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vand.vx v8, v8, a0
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 2
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 1
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v8, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 5
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %elt.head = insertelement <vscale x 32 x i16> poison, i16 %b, i32 0
  %vb = shufflevector <vscale x 32 x i16> %elt.head, <vscale x 32 x i16> poison, <vscale x 32 x i32> zeroinitializer
  %va.ext = zext <vscale x 32 x i16> %va to <vscale x 32 x i32>
  %vb.ext = zext <vscale x 32 x i16> %vb to <vscale x 32 x i32>
  %clmul = call <vscale x 32 x i32> @llvm.clmul.nxv32i32(<vscale x 32 x i32> %va.ext, <vscale x 32 x i32> %vb.ext)
  %res.ext = lshr <vscale x 32 x i32> %clmul, splat(i32 16)
  %res = trunc <vscale x 32 x i32> %res.ext to <vscale x 32 x i16>
  ret <vscale x 32 x i16> %res
}

define <vscale x 1 x i32> @clmulh_nxv1i32_vv(<vscale x 1 x i32> %va, <vscale x 1 x i32> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv1i32_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -288
; RV32V-NEXT:    sw s0, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 272(sp) # 4-byte Folded Spill
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsetvli a0, zero, e64, m1, ta, ma
; RV32V-NEXT:    vzext.vf2 v10, v9
; RV32V-NEXT:    vzext.vf2 v9, v8
; RV32V-NEXT:    vand.vi v8, v10, 1
; RV32V-NEXT:    vand.vi v11, v10, 2
; RV32V-NEXT:    vmul.vv v8, v9, v8
; RV32V-NEXT:    vand.vi v12, v10, 4
; RV32V-NEXT:    vand.vi v13, v10, 8
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vxor.vi v8, v8, 0
; RV32V-NEXT:    vand.vx v14, v10, a1
; RV32V-NEXT:    vmul.vv v12, v9, v12
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v11, v10, a0
; RV32V-NEXT:    vmul.vv v13, v9, v13
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    li a3, 64
; RV32V-NEXT:    vmul.vv v12, v9, v14
; RV32V-NEXT:    vand.vx v14, v10, a3
; RV32V-NEXT:    vxor.vv v8, v8, v13
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    li a4, 128
; RV32V-NEXT:    vand.vx v12, v10, a4
; RV32V-NEXT:    vmul.vv v13, v9, v14
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    li a5, 256
; RV32V-NEXT:    vand.vx v11, v10, a5
; RV32V-NEXT:    vmul.vv v12, v9, v12
; RV32V-NEXT:    vxor.vv v8, v8, v13
; RV32V-NEXT:    li a6, 512
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vand.vx v13, v10, a6
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    li a7, 1024
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a7
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    li s2, 1
; RV32V-NEXT:    slli t0, s2, 11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t0
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    lui t1, 1
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t1
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    lui t3, 2
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t3
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t2
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    lui t4, 8
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    lui t6, 16
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t6
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, t5
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    lui s0, 64
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, s0
; RV32V-NEXT:    lui a2, 524288
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw a2, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s2, 260(sp)
; RV32V-NEXT:    vand.vx v13, v10, s1
; RV32V-NEXT:    li s2, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s2, 252(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    li s3, 4
; RV32V-NEXT:    lui s2, 256
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s3, 244(sp)
; RV32V-NEXT:    vand.vx v13, v10, s2
; RV32V-NEXT:    li s3, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s3, 236(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    lui a1, 512
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a1
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a4, 204(sp)
; RV32V-NEXT:    lui a3, 1024
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a3
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a5, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a6, 188(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a7, 180(sp)
; RV32V-NEXT:    lui a4, 2048
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a4
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t1, 164(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t3, 156(sp)
; RV32V-NEXT:    lui a5, 4096
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a5
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t4, 140(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t6, 132(sp)
; RV32V-NEXT:    lui a6, 8192
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a6
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s0, 116(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s1, 108(sp)
; RV32V-NEXT:    lui a7, 16384
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a7
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s2, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw a1, 92(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw a3, 84(sp)
; RV32V-NEXT:    lui a1, 32768
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a1
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw a4, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a5, 68(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a6, 60(sp)
; RV32V-NEXT:    lui a3, 65536
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a3
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a7, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a1, 44(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a3, 36(sp)
; RV32V-NEXT:    lui a1, 131072
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a1
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw a2, 12(sp)
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v10, a1
; RV32V-NEXT:    addi a1, sp, 264
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v10, v14
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vlse64.v v13, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vlse64.v v12, (a1), zero
; RV32V-NEXT:    vand.vv v13, v10, v13
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vlse64.v v11, (a1), zero
; RV32V-NEXT:    vmul.vv v13, v9, v13
; RV32V-NEXT:    vand.vv v12, v10, v12
; RV32V-NEXT:    vand.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v12
; RV32V-NEXT:    vxor.vv v8, v8, v13
; RV32V-NEXT:    vmul.vv v9, v9, v10
; RV32V-NEXT:    vxor.vv v8, v8, v11
; RV32V-NEXT:    vxor.vv v8, v8, v9
; RV32V-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v8, a0
; RV32V-NEXT:    lw s0, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 272(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 288
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i32_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV64V-NEXT:    vzext.vf2 v10, v9
; RV64V-NEXT:    vzext.vf2 v9, v8
; RV64V-NEXT:    vand.vi v8, v10, 2
; RV64V-NEXT:    vand.vi v11, v10, 1
; RV64V-NEXT:    vmul.vv v8, v9, v8
; RV64V-NEXT:    vmul.vv v11, v9, v11
; RV64V-NEXT:    vand.vi v12, v10, 4
; RV64V-NEXT:    vand.vi v13, v10, 8
; RV64V-NEXT:    vmul.vv v12, v9, v12
; RV64V-NEXT:    vxor.vv v8, v11, v8
; RV64V-NEXT:    vand.vx v11, v10, a0
; RV64V-NEXT:    vmul.vv v13, v9, v13
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v12, v10, a0
; RV64V-NEXT:    vmul.vv v11, v9, v11
; RV64V-NEXT:    vxor.vv v8, v8, v13
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v13, v10, a1
; RV64V-NEXT:    vmul.vv v12, v9, v12
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    vand.vx v13, v10, a1
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v13, v10, a1
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v13, v10, a1
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v13, v10, a1
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v13, v10, a2
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v10, v10, a1
; RV64V-NEXT:    vxor.vv v8, v8, v12
; RV64V-NEXT:    vmul.vv v9, v9, v10
; RV64V-NEXT:    vxor.vv v8, v8, v11
; RV64V-NEXT:    vxor.vv v8, v8, v9
; RV64V-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v8, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i32_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v10, v9
; RV32ZVBC-NEXT:    vzext.vf2 v9, v8
; RV32ZVBC-NEXT:    vclmul.vv v8, v9, v10
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v8, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i32_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v10, v9
; RV64ZVBC-NEXT:    vzext.vf2 v9, v8
; RV64ZVBC-NEXT:    vclmul.vv v8, v9, v10
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 1 x i32> %va to <vscale x 1 x i64>
  %vb.ext = zext <vscale x 1 x i32> %vb to <vscale x 1 x i64>
  %clmul = call <vscale x 1 x i64> @llvm.clmul.nxv1i64(<vscale x 1 x i64> %va.ext, <vscale x 1 x i64> %vb.ext)
  %res.ext = lshr <vscale x 1 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 1 x i64> %res.ext to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %res
}

define <vscale x 1 x i32> @clmulh_nxv1i32_vx(<vscale x 1 x i32> %va, i32 %b) nounwind {
; RV32V-LABEL: clmulh_nxv1i32_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -288
; RV32V-NEXT:    sw s0, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 272(sp) # 4-byte Folded Spill
; RV32V-NEXT:    vsetvli a1, zero, e32, mf2, ta, ma
; RV32V-NEXT:    vmv.v.x v10, a0
; RV32V-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; RV32V-NEXT:    vzext.vf2 v9, v8
; RV32V-NEXT:    vzext.vf2 v8, v10
; RV32V-NEXT:    vand.vi v10, v8, 1
; RV32V-NEXT:    vand.vi v11, v8, 2
; RV32V-NEXT:    vmul.vv v10, v9, v10
; RV32V-NEXT:    vand.vi v12, v8, 4
; RV32V-NEXT:    vand.vi v13, v8, 8
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vxor.vi v10, v10, 0
; RV32V-NEXT:    li a2, 16
; RV32V-NEXT:    vand.vx v14, v8, a2
; RV32V-NEXT:    vmul.vv v12, v9, v12
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v11, v8, a0
; RV32V-NEXT:    vmul.vv v13, v9, v13
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    li a3, 64
; RV32V-NEXT:    vmul.vv v12, v9, v14
; RV32V-NEXT:    vand.vx v14, v8, a3
; RV32V-NEXT:    vxor.vv v10, v10, v13
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    li a4, 128
; RV32V-NEXT:    vand.vx v12, v8, a4
; RV32V-NEXT:    vmul.vv v13, v9, v14
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    li a5, 256
; RV32V-NEXT:    vand.vx v11, v8, a5
; RV32V-NEXT:    vmul.vv v12, v9, v12
; RV32V-NEXT:    vxor.vv v10, v10, v13
; RV32V-NEXT:    li a6, 512
; RV32V-NEXT:    vmul.vv v11, v9, v11
; RV32V-NEXT:    vand.vx v13, v8, a6
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    li a7, 1024
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a7
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    li s2, 1
; RV32V-NEXT:    slli t0, s2, 11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t0
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    lui t1, 1
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t1
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    lui t3, 2
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t3
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t2
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    lui t4, 8
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t4
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    lui t6, 16
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t6
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, t5
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    lui s0, 64
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, s0
; RV32V-NEXT:    lui a1, 524288
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw a1, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s2, 260(sp)
; RV32V-NEXT:    vand.vx v13, v8, s1
; RV32V-NEXT:    li s2, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s2, 252(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    li s3, 4
; RV32V-NEXT:    lui s2, 256
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s3, 244(sp)
; RV32V-NEXT:    vand.vx v13, v8, s2
; RV32V-NEXT:    li s3, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s3, 236(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a2, 228(sp)
; RV32V-NEXT:    lui a2, 512
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a2
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a4, 204(sp)
; RV32V-NEXT:    lui a3, 1024
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a3
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a5, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a6, 188(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a7, 180(sp)
; RV32V-NEXT:    lui a4, 2048
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a4
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t1, 164(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t3, 156(sp)
; RV32V-NEXT:    lui a5, 4096
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a5
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t4, 140(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t6, 132(sp)
; RV32V-NEXT:    lui a6, 8192
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a6
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s0, 116(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s1, 108(sp)
; RV32V-NEXT:    lui a7, 16384
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a7
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s2, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw a2, 92(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw a3, 84(sp)
; RV32V-NEXT:    lui a2, 32768
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a2
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw a4, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a5, 68(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a6, 60(sp)
; RV32V-NEXT:    lui a3, 65536
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a3
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a7, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a2, 44(sp)
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a3, 36(sp)
; RV32V-NEXT:    lui a2, 131072
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a2
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a2, 28(sp)
; RV32V-NEXT:    lui a2, 262144
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a2, 20(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw a1, 12(sp)
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vx v13, v8, a2
; RV32V-NEXT:    addi a1, sp, 264
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    vmul.vv v12, v9, v13
; RV32V-NEXT:    vand.vv v13, v8, v14
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vmul.vv v11, v9, v13
; RV32V-NEXT:    vlse64.v v13, (a1), zero
; RV32V-NEXT:    vxor.vv v10, v10, v12
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vlse64.v v12, (a1), zero
; RV32V-NEXT:    vand.vv v13, v8, v13
; RV32V-NEXT:    vxor.vv v10, v10, v11
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vlse64.v v11, (a1), zero
; RV32V-NEXT:    vmul.vv v13, v9, v13
; RV32V-NEXT:    vand.vv v12, v8, v12
; RV32V-NEXT:    vand.vv v8, v8, v11
; RV32V-NEXT:    vmul.vv v11, v9, v12
; RV32V-NEXT:    vxor.vv v10, v10, v13
; RV32V-NEXT:    vmul.vv v8, v9, v8
; RV32V-NEXT:    vxor.vv v9, v10, v11
; RV32V-NEXT:    vxor.vv v8, v9, v8
; RV32V-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v8, a0
; RV32V-NEXT:    lw s0, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 272(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 288
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i32_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    vsetvli a1, zero, e32, mf2, ta, ma
; RV64V-NEXT:    vmv.v.x v10, a0
; RV64V-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; RV64V-NEXT:    vzext.vf2 v9, v8
; RV64V-NEXT:    vzext.vf2 v8, v10
; RV64V-NEXT:    vand.vi v10, v8, 2
; RV64V-NEXT:    vand.vi v11, v8, 1
; RV64V-NEXT:    vmul.vv v10, v9, v10
; RV64V-NEXT:    vmul.vv v11, v9, v11
; RV64V-NEXT:    vand.vi v12, v8, 4
; RV64V-NEXT:    vand.vi v13, v8, 8
; RV64V-NEXT:    vmul.vv v12, v9, v12
; RV64V-NEXT:    vxor.vv v10, v11, v10
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vand.vx v11, v8, a0
; RV64V-NEXT:    vmul.vv v13, v9, v13
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v12, v8, a0
; RV64V-NEXT:    vmul.vv v11, v9, v11
; RV64V-NEXT:    vxor.vv v10, v10, v13
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v13, v8, a1
; RV64V-NEXT:    vmul.vv v12, v9, v12
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    vand.vx v13, v8, a1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v13, v8, a1
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v13, v8, a1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v13, v8, a1
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v9, v13
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v13, v8, a2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v9, v13
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v8, v9, v8
; RV64V-NEXT:    vxor.vv v9, v10, v11
; RV64V-NEXT:    vxor.vv v8, v9, v8
; RV64V-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v8, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i32_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a1, zero, e32, mf2, ta, ma
; RV32ZVBC-NEXT:    vmv.v.x v9, a0
; RV32ZVBC-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v10, v8
; RV32ZVBC-NEXT:    vzext.vf2 v8, v9
; RV32ZVBC-NEXT:    vclmul.vv v8, v10, v8
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v8, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i32_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e32, mf2, ta, ma
; RV64ZVBC-NEXT:    vmv.v.x v9, a0
; RV64ZVBC-NEXT:    vsetvli zero, zero, e64, m1, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v10, v8
; RV64ZVBC-NEXT:    vzext.vf2 v8, v9
; RV64ZVBC-NEXT:    vclmul.vv v8, v10, v8
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, mf2, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 1 x i32> poison, i32 %b, i64 0
  %vb = shufflevector <vscale x 1 x i32> %elt.head, <vscale x 1 x i32> poison, <vscale x 1 x i32> zeroinitializer
  %va.ext = zext <vscale x 1 x i32> %va to <vscale x 1 x i64>
  %vb.ext = zext <vscale x 1 x i32> %vb to <vscale x 1 x i64>
  %clmul = call <vscale x 1 x i64> @llvm.clmul.nxv1i64(<vscale x 1 x i64> %va.ext, <vscale x 1 x i64> %vb.ext)
  %res.ext = lshr <vscale x 1 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 1 x i64> %res.ext to <vscale x 1 x i32>
  ret <vscale x 1 x i32> %res
}

define <vscale x 2 x i32> @clmulh_nxv2i32_vv(<vscale x 2 x i32> %va, <vscale x 2 x i32> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv2i32_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -320
; RV32V-NEXT:    sw s0, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsetvli a0, zero, e64, m2, ta, ma
; RV32V-NEXT:    vzext.vf2 v10, v8
; RV32V-NEXT:    vzext.vf2 v12, v9
; RV32V-NEXT:    vand.vi v8, v12, 2
; RV32V-NEXT:    vand.vi v14, v12, 1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vmul.vv v8, v10, v8
; RV32V-NEXT:    vxor.vi v14, v14, 0
; RV32V-NEXT:    vxor.vv v8, v14, v8
; RV32V-NEXT:    vand.vi v14, v12, 4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vand.vi v16, v12, 8
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vand.vx v14, v12, a1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v16, v12, a0
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    li a2, 64
; RV32V-NEXT:    vand.vx v14, v12, a2
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a3, 128
; RV32V-NEXT:    vand.vx v16, v12, a3
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v14, v12, a4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v16, v12, a5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v14, v12, a6
; RV32V-NEXT:    li s8, 1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    slli a7, s8, 11
; RV32V-NEXT:    vand.vx v16, v12, a7
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v14, v12, t0
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v16, v12, t1
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v14, v12, t2
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v16, v12, t3
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui t4, 16
; RV32V-NEXT:    vand.vx v14, v12, t4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vand.vx v16, v12, t5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    vand.vx v14, v12, t6
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s0, 128
; RV32V-NEXT:    vand.vx v16, v12, s0
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui s3, 256
; RV32V-NEXT:    vand.vx v14, v12, s3
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s2, 512
; RV32V-NEXT:    vand.vx v16, v12, s2
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui s4, 1024
; RV32V-NEXT:    vand.vx v14, v12, s4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v16, v12, s5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v14, v12, s6
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v16, v12, s7
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    lui s1, 524288
; RV32V-NEXT:    sw s1, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    vand.vx v14, v12, s9
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s8, 260(sp)
; RV32V-NEXT:    li s8, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s8, 252(sp)
; RV32V-NEXT:    li s8, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s8, 244(sp)
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li s10, 8
; RV32V-NEXT:    lui s8, 32768
; RV32V-NEXT:    vand.vx v16, v12, s8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s10, 236(sp)
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a2, 212(sp)
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a3, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a5, 188(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a6, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw a7, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t0, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t1, 156(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t3, 140(sp)
; RV32V-NEXT:    lui a1, 65536
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t4, 132(sp)
; RV32V-NEXT:    vand.vx v14, v12, a1
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s0, 108(sp)
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s3, 100(sp)
; RV32V-NEXT:    lui a2, 131072
; RV32V-NEXT:    vand.vx v16, v12, a2
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s2, 92(sp)
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s4, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s5, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s6, 68(sp)
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s7, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s8, 44(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a1, 36(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a2, 28(sp)
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw s1, 12(sp)
; RV32V-NEXT:    addi a2, sp, 264
; RV32V-NEXT:    vlse64.v v14, (a2), zero
; RV32V-NEXT:    vand.vx v16, v12, a1
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v12, v14
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v12, v18
; RV32V-NEXT:    vand.vv v12, v12, v14
; RV32V-NEXT:    vmul.vv v14, v10, v16
; RV32V-NEXT:    vmul.vv v10, v10, v12
; RV32V-NEXT:    vxor.vv v8, v8, v14
; RV32V-NEXT:    vxor.vv v10, v8, v10
; RV32V-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v10, a0
; RV32V-NEXT:    lw s0, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 320
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv2i32_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV64V-NEXT:    vzext.vf2 v10, v8
; RV64V-NEXT:    vzext.vf2 v12, v9
; RV64V-NEXT:    vand.vi v8, v12, 2
; RV64V-NEXT:    vand.vi v14, v12, 1
; RV64V-NEXT:    vmul.vv v8, v10, v8
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    vxor.vv v8, v14, v8
; RV64V-NEXT:    vand.vi v14, v12, 4
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    vand.vi v16, v12, 8
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    vand.vx v14, v12, a0
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v16, v12, a0
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v14, v12, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v16, v12, a1
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v14, v12, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v16, v12, a1
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v14, v12, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v16, v12, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v8, v8, v16
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v14, v12, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v12, v12, a1
; RV64V-NEXT:    vmul.vv v10, v10, v12
; RV64V-NEXT:    vxor.vv v8, v8, v14
; RV64V-NEXT:    vxor.vv v10, v8, v10
; RV64V-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v10, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv2i32_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v10, v9
; RV32ZVBC-NEXT:    vzext.vf2 v12, v8
; RV32ZVBC-NEXT:    vclmul.vv v10, v12, v10
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v10, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv2i32_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v10, v9
; RV64ZVBC-NEXT:    vzext.vf2 v12, v8
; RV64ZVBC-NEXT:    vclmul.vv v10, v12, v10
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v10, a0
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 2 x i32> %va to <vscale x 2 x i64>
  %vb.ext = zext <vscale x 2 x i32> %vb to <vscale x 2 x i64>
  %clmul = call <vscale x 2 x i64> @llvm.clmul.nxv2i64(<vscale x 2 x i64> %va.ext, <vscale x 2 x i64> %vb.ext)
  %res.ext = lshr <vscale x 2 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 2 x i64> %res.ext to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %res
}

define <vscale x 2 x i32> @clmulh_nxv2i32_vx(<vscale x 2 x i32> %va, i32 %b) nounwind {
; RV32V-LABEL: clmulh_nxv2i32_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -320
; RV32V-NEXT:    sw s0, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    vsetvli a1, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a0
; RV32V-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32V-NEXT:    vzext.vf2 v10, v8
; RV32V-NEXT:    vzext.vf2 v8, v12
; RV32V-NEXT:    vand.vi v12, v8, 2
; RV32V-NEXT:    vand.vi v14, v8, 1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vmul.vv v12, v10, v12
; RV32V-NEXT:    vxor.vi v14, v14, 0
; RV32V-NEXT:    vxor.vv v12, v14, v12
; RV32V-NEXT:    vand.vi v14, v8, 4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vand.vi v16, v8, 8
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vand.vx v14, v8, a1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v16, v8, a0
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    li a2, 64
; RV32V-NEXT:    vand.vx v14, v8, a2
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a3, 128
; RV32V-NEXT:    vand.vx v16, v8, a3
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v14, v8, a4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v16, v8, a5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v14, v8, a6
; RV32V-NEXT:    li s8, 1
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    slli a7, s8, 11
; RV32V-NEXT:    vand.vx v16, v8, a7
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v14, v8, t0
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v16, v8, t1
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v14, v8, t2
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v16, v8, t3
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui t4, 16
; RV32V-NEXT:    vand.vx v14, v8, t4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vand.vx v16, v8, t5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    vand.vx v14, v8, t6
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s0, 128
; RV32V-NEXT:    vand.vx v16, v8, s0
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui s3, 256
; RV32V-NEXT:    vand.vx v14, v8, s3
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s2, 512
; RV32V-NEXT:    vand.vx v16, v8, s2
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui s4, 1024
; RV32V-NEXT:    vand.vx v14, v8, s4
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v16, v8, s5
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v14, v8, s6
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v16, v8, s7
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    lui s1, 524288
; RV32V-NEXT:    sw s1, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    vand.vx v14, v8, s9
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s8, 260(sp)
; RV32V-NEXT:    li s8, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s8, 252(sp)
; RV32V-NEXT:    li s8, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s8, 244(sp)
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    li s10, 8
; RV32V-NEXT:    lui s8, 32768
; RV32V-NEXT:    vand.vx v16, v8, s8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s10, 236(sp)
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a2, 212(sp)
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a3, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a5, 188(sp)
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a6, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw a7, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t0, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t1, 156(sp)
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t3, 140(sp)
; RV32V-NEXT:    lui a1, 65536
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t4, 132(sp)
; RV32V-NEXT:    vand.vx v14, v8, a1
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s0, 108(sp)
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s3, 100(sp)
; RV32V-NEXT:    lui a2, 131072
; RV32V-NEXT:    vand.vx v16, v8, a2
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s2, 92(sp)
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s4, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s5, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s6, 68(sp)
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s7, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s8, 44(sp)
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a1, 36(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a2, 28(sp)
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw s1, 12(sp)
; RV32V-NEXT:    addi a2, sp, 264
; RV32V-NEXT:    vlse64.v v14, (a2), zero
; RV32V-NEXT:    vand.vx v16, v8, a1
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vmul.vv v16, v10, v16
; RV32V-NEXT:    vand.vv v14, v8, v14
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vmul.vv v14, v10, v14
; RV32V-NEXT:    vlse64.v v18, (a1), zero
; RV32V-NEXT:    vxor.vv v12, v12, v16
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vxor.vv v12, v12, v14
; RV32V-NEXT:    vlse64.v v14, (a1), zero
; RV32V-NEXT:    vand.vv v16, v8, v18
; RV32V-NEXT:    vand.vv v8, v8, v14
; RV32V-NEXT:    vmul.vv v14, v10, v16
; RV32V-NEXT:    vmul.vv v8, v10, v8
; RV32V-NEXT:    vxor.vv v10, v12, v14
; RV32V-NEXT:    vxor.vv v10, v10, v8
; RV32V-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v10, a0
; RV32V-NEXT:    lw s0, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 320
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv2i32_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    vsetvli a1, zero, e32, m1, ta, ma
; RV64V-NEXT:    vmv.v.x v12, a0
; RV64V-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64V-NEXT:    vzext.vf2 v10, v8
; RV64V-NEXT:    vzext.vf2 v8, v12
; RV64V-NEXT:    vand.vi v12, v8, 2
; RV64V-NEXT:    vand.vi v14, v8, 1
; RV64V-NEXT:    vmul.vv v12, v10, v12
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    vxor.vv v12, v14, v12
; RV64V-NEXT:    vand.vi v14, v8, 4
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    vand.vi v16, v8, 8
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vand.vx v14, v8, a0
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v16, v8, a0
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v14, v8, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v16, v8, a1
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v14, v8, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v16, v8, a1
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v14, v8, a1
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v16, v8, a2
; RV64V-NEXT:    vmul.vv v16, v10, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vmul.vv v14, v10, v14
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vmul.vv v8, v10, v8
; RV64V-NEXT:    vxor.vv v10, v12, v14
; RV64V-NEXT:    vxor.vv v10, v10, v8
; RV64V-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v10, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv2i32_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a1, zero, e32, m1, ta, ma
; RV32ZVBC-NEXT:    vmv.v.x v12, a0
; RV32ZVBC-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v10, v8
; RV32ZVBC-NEXT:    vzext.vf2 v8, v12
; RV32ZVBC-NEXT:    vclmul.vv v10, v10, v8
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v10, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv2i32_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e32, m1, ta, ma
; RV64ZVBC-NEXT:    vmv.v.x v12, a0
; RV64ZVBC-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v10, v8
; RV64ZVBC-NEXT:    vzext.vf2 v8, v12
; RV64ZVBC-NEXT:    vclmul.vv v10, v10, v8
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m1, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v10, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 2 x i32> poison, i32 %b, i64 0
  %vb = shufflevector <vscale x 2 x i32> %elt.head, <vscale x 2 x i32> poison, <vscale x 2 x i32> zeroinitializer
  %va.ext = zext <vscale x 2 x i32> %va to <vscale x 2 x i64>
  %vb.ext = zext <vscale x 2 x i32> %vb to <vscale x 2 x i64>
  %clmul = call <vscale x 2 x i64> @llvm.clmul.nxv2i64(<vscale x 2 x i64> %va.ext, <vscale x 2 x i64> %vb.ext)
  %res.ext = lshr <vscale x 2 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 2 x i64> %res.ext to <vscale x 2 x i32>
  ret <vscale x 2 x i32> %res
}

define <vscale x 4 x i32> @clmulh_nxv4i32_vv(<vscale x 4 x i32> %va, <vscale x 4 x i32> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv4i32_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -320
; RV32V-NEXT:    sw s0, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 272(sp) # 4-byte Folded Spill
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsetvli a0, zero, e64, m4, ta, ma
; RV32V-NEXT:    vzext.vf2 v12, v8
; RV32V-NEXT:    vzext.vf2 v16, v10
; RV32V-NEXT:    vand.vi v8, v16, 2
; RV32V-NEXT:    vand.vi v20, v16, 1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vmul.vv v8, v12, v8
; RV32V-NEXT:    vxor.vi v20, v20, 0
; RV32V-NEXT:    vxor.vv v8, v20, v8
; RV32V-NEXT:    vand.vi v20, v16, 4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vand.vi v24, v16, 8
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vand.vx v20, v16, a1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v24, v16, a0
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    li a3, 64
; RV32V-NEXT:    vand.vx v20, v16, a3
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a2, 128
; RV32V-NEXT:    vand.vx v24, v16, a2
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v20, v16, a4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v24, v16, a5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v20, v16, a6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li s4, 1
; RV32V-NEXT:    slli a7, s4, 11
; RV32V-NEXT:    vand.vx v24, v16, a7
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v20, v16, t0
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v24, v16, t1
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v20, v16, t2
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v24, v16, t3
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui t4, 16
; RV32V-NEXT:    vand.vx v20, v16, t4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vand.vx v24, v16, t5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    vand.vx v20, v16, t6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s0, 128
; RV32V-NEXT:    vand.vx v24, v16, s0
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui s1, 256
; RV32V-NEXT:    vand.vx v20, v16, s1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s2, 512
; RV32V-NEXT:    vand.vx v24, v16, s2
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui s3, 1024
; RV32V-NEXT:    vand.vx v20, v16, s3
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v24, v16, s5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v20, v16, s6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v24, v16, s7
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    vand.vx v20, v16, s9
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s10, 32768
; RV32V-NEXT:    vand.vx v24, v16, s10
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui s11, 65536
; RV32V-NEXT:    lui s8, 524288
; RV32V-NEXT:    vand.vx v20, v16, s11
; RV32V-NEXT:    sw s8, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s4, 260(sp)
; RV32V-NEXT:    li s4, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s4, 252(sp)
; RV32V-NEXT:    li s4, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s4, 244(sp)
; RV32V-NEXT:    li s4, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s4, 236(sp)
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    lui a1, 131072
; RV32V-NEXT:    vand.vx v24, v16, a1
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a2, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a5, 188(sp)
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a6, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw a7, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t0, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t1, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t3, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t4, 132(sp)
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s0, 108(sp)
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s1, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s2, 92(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s3, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s5, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s6, 68(sp)
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s7, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s10, 44(sp)
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s11, 36(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw s8, 12(sp)
; RV32V-NEXT:    addi a2, sp, 264
; RV32V-NEXT:    vlse64.v v20, (a2), zero
; RV32V-NEXT:    vand.vx v24, v16, a1
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v16, v20
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v8, v8, v24
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v16, v28
; RV32V-NEXT:    vand.vv v16, v16, v20
; RV32V-NEXT:    vmul.vv v20, v12, v24
; RV32V-NEXT:    vmul.vv v12, v12, v16
; RV32V-NEXT:    vxor.vv v8, v8, v20
; RV32V-NEXT:    vxor.vv v12, v8, v12
; RV32V-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v12, a0
; RV32V-NEXT:    lw s0, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 272(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 320
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv4i32_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV64V-NEXT:    vzext.vf2 v12, v8
; RV64V-NEXT:    vzext.vf2 v16, v10
; RV64V-NEXT:    vand.vi v8, v16, 2
; RV64V-NEXT:    vand.vi v20, v16, 1
; RV64V-NEXT:    vmul.vv v8, v12, v8
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    vxor.vv v8, v20, v8
; RV64V-NEXT:    vand.vi v20, v16, 4
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    vand.vi v24, v16, 8
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    vand.vx v20, v16, a0
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v24, v16, a0
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v20, v16, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v24, v16, a1
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v20, v16, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v24, v16, a1
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v20, v16, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v24, v16, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v8, v8, v24
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v20, v16, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v16, v16, a1
; RV64V-NEXT:    vmul.vv v12, v12, v16
; RV64V-NEXT:    vxor.vv v8, v8, v20
; RV64V-NEXT:    vxor.vv v12, v8, v12
; RV64V-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v12, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv4i32_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v12, v10
; RV32ZVBC-NEXT:    vzext.vf2 v16, v8
; RV32ZVBC-NEXT:    vclmul.vv v12, v16, v12
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v12, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv4i32_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v12, v10
; RV64ZVBC-NEXT:    vzext.vf2 v16, v8
; RV64ZVBC-NEXT:    vclmul.vv v12, v16, v12
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v12, a0
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 4 x i32> %va to <vscale x 4 x i64>
  %vb.ext = zext <vscale x 4 x i32> %vb to <vscale x 4 x i64>
  %clmul = call <vscale x 4 x i64> @llvm.clmul.nxv4i64(<vscale x 4 x i64> %va.ext, <vscale x 4 x i64> %vb.ext)
  %res.ext = lshr <vscale x 4 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 4 x i64> %res.ext to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %res
}

define <vscale x 4 x i32> @clmulh_nxv4i32_vx(<vscale x 4 x i32> %va, i32 %b) nounwind {
; RV32V-LABEL: clmulh_nxv4i32_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -320
; RV32V-NEXT:    sw s0, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 280(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 276(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 272(sp) # 4-byte Folded Spill
; RV32V-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32V-NEXT:    vmv.v.x v16, a0
; RV32V-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; RV32V-NEXT:    vzext.vf2 v12, v8
; RV32V-NEXT:    vzext.vf2 v8, v16
; RV32V-NEXT:    vand.vi v16, v8, 2
; RV32V-NEXT:    vand.vi v20, v8, 1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vmul.vv v16, v12, v16
; RV32V-NEXT:    vxor.vi v20, v20, 0
; RV32V-NEXT:    vxor.vv v16, v20, v16
; RV32V-NEXT:    vand.vi v20, v8, 4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vand.vi v24, v8, 8
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vand.vx v20, v8, a1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v24, v8, a0
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    li a3, 64
; RV32V-NEXT:    vand.vx v20, v8, a3
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a2, 128
; RV32V-NEXT:    vand.vx v24, v8, a2
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v20, v8, a4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v24, v8, a5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v20, v8, a6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    li s4, 1
; RV32V-NEXT:    slli a7, s4, 11
; RV32V-NEXT:    vand.vx v24, v8, a7
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v20, v8, t0
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v24, v8, t1
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v20, v8, t2
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v24, v8, t3
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui t4, 16
; RV32V-NEXT:    vand.vx v20, v8, t4
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui t5, 32
; RV32V-NEXT:    vand.vx v24, v8, t5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    vand.vx v20, v8, t6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s0, 128
; RV32V-NEXT:    vand.vx v24, v8, s0
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui s1, 256
; RV32V-NEXT:    vand.vx v20, v8, s1
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s2, 512
; RV32V-NEXT:    vand.vx v24, v8, s2
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui s3, 1024
; RV32V-NEXT:    vand.vx v20, v8, s3
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v24, v8, s5
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v20, v8, s6
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v24, v8, s7
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    vand.vx v20, v8, s9
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    lui s10, 32768
; RV32V-NEXT:    vand.vx v24, v8, s10
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui s11, 65536
; RV32V-NEXT:    lui s8, 524288
; RV32V-NEXT:    vand.vx v20, v8, s11
; RV32V-NEXT:    sw s8, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s4, 260(sp)
; RV32V-NEXT:    li s4, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw s4, 252(sp)
; RV32V-NEXT:    li s4, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw s4, 244(sp)
; RV32V-NEXT:    li s4, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw s4, 236(sp)
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    lui a1, 131072
; RV32V-NEXT:    vand.vx v24, v8, a1
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a2, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a5, 188(sp)
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a6, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw a7, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t0, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t1, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t2, 148(sp)
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t3, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t4, 132(sp)
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw t5, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s0, 108(sp)
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s1, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s2, 92(sp)
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s3, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s5, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s6, 68(sp)
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s7, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s10, 44(sp)
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s11, 36(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    sw s8, 12(sp)
; RV32V-NEXT:    addi a2, sp, 264
; RV32V-NEXT:    vlse64.v v20, (a2), zero
; RV32V-NEXT:    vand.vx v24, v8, a1
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vmul.vv v24, v12, v24
; RV32V-NEXT:    vand.vv v20, v8, v20
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    vmul.vv v20, v12, v20
; RV32V-NEXT:    vlse64.v v28, (a1), zero
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    addi a1, sp, 8
; RV32V-NEXT:    vxor.vv v16, v16, v20
; RV32V-NEXT:    vlse64.v v20, (a1), zero
; RV32V-NEXT:    vand.vv v24, v8, v28
; RV32V-NEXT:    vand.vv v8, v8, v20
; RV32V-NEXT:    vmul.vv v20, v12, v24
; RV32V-NEXT:    vmul.vv v8, v12, v8
; RV32V-NEXT:    vxor.vv v12, v16, v20
; RV32V-NEXT:    vxor.vv v12, v12, v8
; RV32V-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v12, a0
; RV32V-NEXT:    lw s0, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 280(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 276(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 272(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 320
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv4i32_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV64V-NEXT:    vmv.v.x v16, a0
; RV64V-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; RV64V-NEXT:    vzext.vf2 v12, v8
; RV64V-NEXT:    vzext.vf2 v8, v16
; RV64V-NEXT:    vand.vi v16, v8, 2
; RV64V-NEXT:    vand.vi v20, v8, 1
; RV64V-NEXT:    vmul.vv v16, v12, v16
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    vxor.vv v16, v20, v16
; RV64V-NEXT:    vand.vi v20, v8, 4
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    vand.vi v24, v8, 8
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vand.vx v20, v8, a0
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v24, v8, a0
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v20, v8, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v24, v8, a1
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v20, v8, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v24, v8, a1
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v20, v8, a1
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v12, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v20, v8, a2
; RV64V-NEXT:    vmul.vv v20, v12, v20
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vmul.vv v8, v12, v8
; RV64V-NEXT:    vxor.vv v12, v16, v20
; RV64V-NEXT:    vxor.vv v12, v12, v8
; RV64V-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v12, a0
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv4i32_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32ZVBC-NEXT:    vmv.v.x v16, a0
; RV32ZVBC-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v12, v8
; RV32ZVBC-NEXT:    vzext.vf2 v8, v16
; RV32ZVBC-NEXT:    vclmul.vv v12, v12, v8
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v12, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv4i32_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV64ZVBC-NEXT:    vmv.v.x v16, a0
; RV64ZVBC-NEXT:    vsetvli zero, zero, e64, m4, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v12, v8
; RV64ZVBC-NEXT:    vzext.vf2 v8, v16
; RV64ZVBC-NEXT:    vclmul.vv v12, v12, v8
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m2, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v12, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 4 x i32> poison, i32 %b, i64 0
  %vb = shufflevector <vscale x 4 x i32> %elt.head, <vscale x 4 x i32> poison, <vscale x 4 x i32> zeroinitializer
  %va.ext = zext <vscale x 4 x i32> %va to <vscale x 4 x i64>
  %vb.ext = zext <vscale x 4 x i32> %vb to <vscale x 4 x i64>
  %clmul = call <vscale x 4 x i64> @llvm.clmul.nxv4i64(<vscale x 4 x i64> %va.ext, <vscale x 4 x i64> %vb.ext)
  %res.ext = lshr <vscale x 4 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 4 x i64> %res.ext to <vscale x 4 x i32>
  ret <vscale x 4 x i32> %res
}

define <vscale x 8 x i32> @clmulh_nxv8i32_vv(<vscale x 8 x i32> %va, <vscale x 8 x i32> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv8i32_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -352
; RV32V-NEXT:    sw ra, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    sub sp, sp, a0
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vzext.vf2 v16, v8
; RV32V-NEXT:    vzext.vf2 v24, v12
; RV32V-NEXT:    vand.vi v8, v24, 2
; RV32V-NEXT:    vand.vi v0, v24, 1
; RV32V-NEXT:    vmul.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    vxor.vi v0, v0, 0
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a2, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v0, v24, 4
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v8, v24, 8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a2, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a2, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v24, a1
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    sw a1, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v8, v24, a0
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a2, 64
; RV32V-NEXT:    vand.vx v0, v24, a2
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a3, 128
; RV32V-NEXT:    vand.vx v8, v24, a3
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v0, v24, a4
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v8, v24, a5
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v0, v24, a6
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li t4, 1
; RV32V-NEXT:    slli a7, t4, 11
; RV32V-NEXT:    vand.vx v8, v24, a7
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v0, v24, t0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v8, v24, t1
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v0, v24, t2
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v8, v24, t3
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t5, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t5
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t5, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t5
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t5, 16
; RV32V-NEXT:    vand.vx v0, v24, t5
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t6, 32
; RV32V-NEXT:    vand.vx v8, v24, t6
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s0, 64
; RV32V-NEXT:    vand.vx v0, v24, s0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    vand.vx v8, v24, s1
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s2, 256
; RV32V-NEXT:    vand.vx v0, v24, s2
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s3, 512
; RV32V-NEXT:    vand.vx v8, v24, s3
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s4, 1024
; RV32V-NEXT:    vand.vx v0, v24, s4
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v8, v24, s5
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v0, v24, s6
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v8, v24, s7
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s8, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s8
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s8, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s8
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s8, 16384
; RV32V-NEXT:    vand.vx v0, v24, s8
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s9, 32768
; RV32V-NEXT:    vand.vx v8, v24, s9
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s10, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s10
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s10, 65536
; RV32V-NEXT:    vand.vx v8, v24, s10
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    lui s11, 131072
; RV32V-NEXT:    vand.vx v0, v24, s11
; RV32V-NEXT:    vmul.vv v0, v16, v0
; RV32V-NEXT:    csrr ra, vlenb
; RV32V-NEXT:    slli ra, ra, 3
; RV32V-NEXT:    mv a1, ra
; RV32V-NEXT:    slli ra, ra, 1
; RV32V-NEXT:    add ra, ra, a1
; RV32V-NEXT:    lw a1, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add ra, sp, ra
; RV32V-NEXT:    addi ra, ra, 288
; RV32V-NEXT:    vs8r.v v0, (ra) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr ra, vlenb
; RV32V-NEXT:    slli ra, ra, 4
; RV32V-NEXT:    add ra, sp, ra
; RV32V-NEXT:    addi ra, ra, 288
; RV32V-NEXT:    vl8r.v v0, (ra) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v8
; RV32V-NEXT:    lui ra, 524288
; RV32V-NEXT:    sw ra, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw zero, 264(sp)
; RV32V-NEXT:    sw t4, 268(sp)
; RV32V-NEXT:    li t4, 2
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw t4, 260(sp)
; RV32V-NEXT:    li t4, 4
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw t4, 252(sp)
; RV32V-NEXT:    li t4, 8
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw t4, 244(sp)
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a1, 236(sp)
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a0, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a2, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a4, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a5, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a6, 188(sp)
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a7, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t1, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t2, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t3, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t5, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t6, 132(sp)
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s1, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s2, 108(sp)
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s3, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s4, 92(sp)
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s5, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s6, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s7, 68(sp)
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v0, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s8, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s10, 44(sp)
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s11, 36(sp)
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw ra, 20(sp)
; RV32V-NEXT:    addi a2, sp, 272
; RV32V-NEXT:    vlse64.v v0, (a2), zero
; RV32V-NEXT:    vand.vx v8, v24, a1
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 264
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v24, v0
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vmul.vv v8, v16, v0
; RV32V-NEXT:    addi a2, sp, 288
; RV32V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v8, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    addi a2, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v8, v0
; RV32V-NEXT:    csrr a2, vlenb
; RV32V-NEXT:    slli a2, a2, 3
; RV32V-NEXT:    mv a3, a2
; RV32V-NEXT:    slli a2, a2, 1
; RV32V-NEXT:    add a2, a2, a3
; RV32V-NEXT:    add a2, sp, a2
; RV32V-NEXT:    addi a2, a2, 288
; RV32V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v24, v8
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    vmul.vv v16, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v8, v24, v8
; RV32V-NEXT:    vxor.vv v16, v8, v16
; RV32V-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v16, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 352
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv8i32_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    addi sp, sp, -16
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    sub sp, sp, a0
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV64V-NEXT:    vzext.vf2 v16, v8
; RV64V-NEXT:    vzext.vf2 v24, v12
; RV64V-NEXT:    vand.vi v8, v24, 2
; RV64V-NEXT:    vand.vi v0, v24, 1
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    vmul.vv v0, v16, v0
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v0, v24, 4
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v8, v24, 8
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vx v0, v24, a0
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v8, v24, a0
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v0, v24, a1
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v8, v24, a1
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v0, v24, a1
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v8, v24, a1
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v0, v24, a1
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v8, v24, a2
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v8, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v8
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v8, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v8, v0, v8
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v0, v24, a2
; RV64V-NEXT:    vmul.vv v0, v16, v0
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v24, v24, a1
; RV64V-NEXT:    vmul.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v8, v8, v0
; RV64V-NEXT:    vxor.vv v16, v8, v16
; RV64V-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v16, a0
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    add sp, sp, a0
; RV64V-NEXT:    addi sp, sp, 16
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv8i32_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v16, v12
; RV32ZVBC-NEXT:    vzext.vf2 v24, v8
; RV32ZVBC-NEXT:    vclmul.vv v16, v24, v16
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v16, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv8i32_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v16, v12
; RV64ZVBC-NEXT:    vzext.vf2 v24, v8
; RV64ZVBC-NEXT:    vclmul.vv v16, v24, v16
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v16, a0
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 8 x i32> %va to <vscale x 8 x i64>
  %vb.ext = zext <vscale x 8 x i32> %vb to <vscale x 8 x i64>
  %clmul = call <vscale x 8 x i64> @llvm.clmul.nxv8i64(<vscale x 8 x i64> %va.ext, <vscale x 8 x i64> %vb.ext)
  %res.ext = lshr <vscale x 8 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 8 x i64> %res.ext to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %res
}

define <vscale x 8 x i32> @clmulh_nxv8i32_vx(<vscale x 8 x i32> %va, i32 %b) nounwind {
; RV32V-LABEL: clmulh_nxv8i32_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -352
; RV32V-NEXT:    sw ra, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 5
; RV32V-NEXT:    sub sp, sp, a1
; RV32V-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    vsetvli zero, zero, e64, m8, ta, ma
; RV32V-NEXT:    vzext.vf2 v16, v8
; RV32V-NEXT:    vzext.vf2 v8, v24
; RV32V-NEXT:    vand.vi v24, v8, 2
; RV32V-NEXT:    vand.vi v0, v8, 1
; RV32V-NEXT:    vmul.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    vxor.vi v0, v0, 0
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v0, v8, 4
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v24, v8, 8
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vand.vx v0, v8, a1
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    sw a1, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a0, 32
; RV32V-NEXT:    vand.vx v24, v8, a0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a2, 64
; RV32V-NEXT:    vand.vx v0, v8, a2
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a3, 128
; RV32V-NEXT:    vand.vx v24, v8, a3
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    vand.vx v0, v8, a4
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a5, 512
; RV32V-NEXT:    vand.vx v24, v8, a5
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li a6, 1024
; RV32V-NEXT:    vand.vx v0, v8, a6
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    li t4, 1
; RV32V-NEXT:    slli a7, t4, 11
; RV32V-NEXT:    vand.vx v24, v8, a7
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t0, 1
; RV32V-NEXT:    vand.vx v0, v8, t0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t1, 2
; RV32V-NEXT:    vand.vx v24, v8, t1
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t2, 4
; RV32V-NEXT:    vand.vx v0, v8, t2
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t3, 8
; RV32V-NEXT:    vand.vx v24, v8, t3
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t5, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t5
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv t5, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, t5
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t5, 16
; RV32V-NEXT:    vand.vx v0, v8, t5
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui t6, 32
; RV32V-NEXT:    vand.vx v24, v8, t6
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s0, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s0
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s0, 64
; RV32V-NEXT:    vand.vx v0, v8, s0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    vand.vx v24, v8, s1
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s2, 256
; RV32V-NEXT:    vand.vx v0, v8, s2
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s3, 512
; RV32V-NEXT:    vand.vx v24, v8, s3
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s4, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s4, 1024
; RV32V-NEXT:    vand.vx v0, v8, s4
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s5, 2048
; RV32V-NEXT:    vand.vx v24, v8, s5
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s6, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s6, 4096
; RV32V-NEXT:    vand.vx v0, v8, s6
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s7, 8192
; RV32V-NEXT:    vand.vx v24, v8, s7
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s8, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s8
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s8, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s8
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s8, 16384
; RV32V-NEXT:    vand.vx v0, v8, s8
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s9, 32768
; RV32V-NEXT:    vand.vx v24, v8, s9
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv s10, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, s10
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui s10, 65536
; RV32V-NEXT:    vand.vx v24, v8, s10
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    lui s11, 131072
; RV32V-NEXT:    vand.vx v0, v8, s11
; RV32V-NEXT:    vmul.vv v0, v16, v0
; RV32V-NEXT:    csrr ra, vlenb
; RV32V-NEXT:    slli ra, ra, 3
; RV32V-NEXT:    mv a1, ra
; RV32V-NEXT:    slli ra, ra, 1
; RV32V-NEXT:    add ra, ra, a1
; RV32V-NEXT:    lw a1, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add ra, sp, ra
; RV32V-NEXT:    addi ra, ra, 288
; RV32V-NEXT:    vs8r.v v0, (ra) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr ra, vlenb
; RV32V-NEXT:    slli ra, ra, 4
; RV32V-NEXT:    add ra, sp, ra
; RV32V-NEXT:    addi ra, ra, 288
; RV32V-NEXT:    vl8r.v v0, (ra) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    lui ra, 524288
; RV32V-NEXT:    sw ra, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw zero, 264(sp)
; RV32V-NEXT:    sw t4, 268(sp)
; RV32V-NEXT:    li t4, 2
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw t4, 260(sp)
; RV32V-NEXT:    li t4, 4
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw t4, 252(sp)
; RV32V-NEXT:    li t4, 8
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw t4, 244(sp)
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a1, 236(sp)
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a0, 228(sp)
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a2, 220(sp)
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a3, 212(sp)
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a4, 204(sp)
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a5, 196(sp)
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw a6, 188(sp)
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw a7, 180(sp)
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t1, 164(sp)
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t2, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t3, 148(sp)
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t5, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw t6, 132(sp)
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s1, 116(sp)
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s2, 108(sp)
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s3, 100(sp)
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s4, 92(sp)
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s5, 84(sp)
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw s6, 76(sp)
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s7, 68(sp)
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s8, 60(sp)
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s10, 44(sp)
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s11, 36(sp)
; RV32V-NEXT:    lui a1, 262144
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw ra, 20(sp)
; RV32V-NEXT:    addi a2, sp, 272
; RV32V-NEXT:    vlse64.v v0, (a2), zero
; RV32V-NEXT:    vand.vx v24, v8, a1
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 264
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 256
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 248
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 240
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 232
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 224
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 216
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 208
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 200
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 192
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 184
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 176
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 168
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 160
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 152
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 144
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 136
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 128
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 120
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 112
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 104
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 96
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 88
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 80
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 72
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 64
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 56
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 48
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 40
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a1, sp, 32
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    addi a1, sp, 24
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a2, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v24, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a1, sp, 16
; RV32V-NEXT:    addi a2, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a2, vlenb
; RV32V-NEXT:    slli a2, a2, 3
; RV32V-NEXT:    mv a3, a2
; RV32V-NEXT:    slli a2, a2, 1
; RV32V-NEXT:    add a2, a2, a3
; RV32V-NEXT:    add a2, sp, a2
; RV32V-NEXT:    addi a2, a2, 288
; RV32V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v0, (a1), zero
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v8, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 3
; RV32V-NEXT:    mv a2, a1
; RV32V-NEXT:    slli a1, a1, 1
; RV32V-NEXT:    add a1, a1, a2
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 288
; RV32V-NEXT:    vl8r.v v16, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v16, v16, v8
; RV32V-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV32V-NEXT:    vnsrl.wx v8, v16, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 352
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv8i32_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    addi sp, sp, -16
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    mv a2, a1
; RV64V-NEXT:    slli a1, a1, 1
; RV64V-NEXT:    add a1, a1, a2
; RV64V-NEXT:    sub sp, sp, a1
; RV64V-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV64V-NEXT:    vmv.v.x v24, a0
; RV64V-NEXT:    vsetvli zero, zero, e64, m8, ta, ma
; RV64V-NEXT:    vzext.vf2 v16, v8
; RV64V-NEXT:    vzext.vf2 v8, v24
; RV64V-NEXT:    vand.vi v24, v8, 2
; RV64V-NEXT:    vand.vi v0, v8, 1
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    vmul.vv v0, v16, v0
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v0, v8, 4
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v24, v8, 8
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a0, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a0, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a0, 16
; RV64V-NEXT:    vand.vx v0, v8, a0
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a0, 32
; RV64V-NEXT:    vand.vx v24, v8, a0
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 64
; RV64V-NEXT:    vand.vx v0, v8, a1
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 128
; RV64V-NEXT:    vand.vx v24, v8, a1
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 256
; RV64V-NEXT:    vand.vx v0, v8, a1
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 512
; RV64V-NEXT:    vand.vx v24, v8, a1
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 4
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 1024
; RV64V-NEXT:    vand.vx v0, v8, a1
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 3
; RV64V-NEXT:    add a1, sp, a1
; RV64V-NEXT:    addi a1, a1, 16
; RV64V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li a1, 1
; RV64V-NEXT:    slli a2, a1, 11
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 1
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 2
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 4
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 8
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 16
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 32
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 64
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 128
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 256
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 512
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 1024
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 2048
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 4096
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 8192
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 16384
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 32768
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 65536
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v0
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui a2, 131072
; RV64V-NEXT:    vand.vx v24, v8, a2
; RV64V-NEXT:    vmul.vv v24, v16, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 4
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v0, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a2, vlenb
; RV64V-NEXT:    slli a2, a2, 3
; RV64V-NEXT:    add a2, sp, a2
; RV64V-NEXT:    addi a2, a2, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a2, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    lui a2, 262144
; RV64V-NEXT:    vand.vx v0, v8, a2
; RV64V-NEXT:    vmul.vv v0, v16, v0
; RV64V-NEXT:    slli a1, a1, 31
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vmul.vv v8, v16, v8
; RV64V-NEXT:    vxor.vv v16, v24, v0
; RV64V-NEXT:    vxor.vv v16, v16, v8
; RV64V-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV64V-NEXT:    vnsrl.wx v8, v16, a0
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    add sp, sp, a0
; RV64V-NEXT:    addi sp, sp, 16
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv8i32_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV32ZVBC-NEXT:    vmv.v.x v24, a0
; RV32ZVBC-NEXT:    vsetvli zero, zero, e64, m8, ta, ma
; RV32ZVBC-NEXT:    vzext.vf2 v16, v8
; RV32ZVBC-NEXT:    vzext.vf2 v8, v24
; RV32ZVBC-NEXT:    vclmul.vv v16, v16, v8
; RV32ZVBC-NEXT:    li a0, 32
; RV32ZVBC-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV32ZVBC-NEXT:    vnsrl.wx v8, v16, a0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv8i32_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV64ZVBC-NEXT:    vmv.v.x v24, a0
; RV64ZVBC-NEXT:    vsetvli zero, zero, e64, m8, ta, ma
; RV64ZVBC-NEXT:    vzext.vf2 v16, v8
; RV64ZVBC-NEXT:    vzext.vf2 v8, v24
; RV64ZVBC-NEXT:    vclmul.vv v16, v16, v8
; RV64ZVBC-NEXT:    li a0, 32
; RV64ZVBC-NEXT:    vsetvli zero, zero, e32, m4, ta, ma
; RV64ZVBC-NEXT:    vnsrl.wx v8, v16, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 8 x i32> poison, i32 %b, i64 0
  %vb = shufflevector <vscale x 8 x i32> %elt.head, <vscale x 8 x i32> poison, <vscale x 8 x i32> zeroinitializer
  %va.ext = zext <vscale x 8 x i32> %va to <vscale x 8 x i64>
  %vb.ext = zext <vscale x 8 x i32> %vb to <vscale x 8 x i64>
  %clmul = call <vscale x 8 x i64> @llvm.clmul.nxv8i64(<vscale x 8 x i64> %va.ext, <vscale x 8 x i64> %vb.ext)
  %res.ext = lshr <vscale x 8 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 8 x i64> %res.ext to <vscale x 8 x i32>
  ret <vscale x 8 x i32> %res
}

define <vscale x 16 x i32> @clmulh_nxv16i32_vv(<vscale x 16 x i32> %va, <vscale x 16 x i32> %vb) nounwind {
; CHECK-LABEL: clmulh_nxv16i32_vv:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    sub sp, sp, a0
; CHECK-NEXT:    lui a4, 16
; CHECK-NEXT:    addi a0, a4, -256
; CHECK-NEXT:    vsetvli a1, zero, e32, m8, ta, ma
; CHECK-NEXT:    vsrl.vi v24, v8, 8
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vsrl.vi v0, v8, 24
; CHECK-NEXT:    vor.vv v24, v24, v0
; CHECK-NEXT:    vand.vx v0, v8, a0
; CHECK-NEXT:    vsll.vi v0, v0, 8
; CHECK-NEXT:    vsll.vi v8, v8, 24
; CHECK-NEXT:    vor.vv v8, v8, v0
; CHECK-NEXT:    vor.vv v8, v8, v24
; CHECK-NEXT:    vsrl.vi v24, v8, 4
; CHECK-NEXT:    lui a1, 61681
; CHECK-NEXT:    addi a1, a1, -241
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 2
; CHECK-NEXT:    lui a2, 209715
; CHECK-NEXT:    addi a2, a2, 819
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v8, 1
; CHECK-NEXT:    lui a3, 349525
; CHECK-NEXT:    addi a3, a3, 1365
; CHECK-NEXT:    vand.vx v24, v24, a3
; CHECK-NEXT:    vand.vx v8, v8, a3
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v24, v8
; CHECK-NEXT:    vsrl.vi v24, v16, 8
; CHECK-NEXT:    vand.vx v24, v24, a0
; CHECK-NEXT:    vsrl.vi v0, v16, 24
; CHECK-NEXT:    vor.vv v24, v24, v0
; CHECK-NEXT:    vand.vx v0, v16, a0
; CHECK-NEXT:    vsll.vi v0, v0, 8
; CHECK-NEXT:    vsll.vi v16, v16, 24
; CHECK-NEXT:    vor.vv v16, v16, v0
; CHECK-NEXT:    vor.vv v16, v16, v24
; CHECK-NEXT:    vsrl.vi v24, v16, 4
; CHECK-NEXT:    vand.vx v24, v24, a1
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vsll.vi v16, v16, 4
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 2
; CHECK-NEXT:    vand.vx v24, v24, a2
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vsll.vi v16, v16, 2
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vsrl.vi v24, v16, 1
; CHECK-NEXT:    vand.vx v24, v24, a3
; CHECK-NEXT:    vand.vx v16, v16, a3
; CHECK-NEXT:    vadd.vv v16, v16, v16
; CHECK-NEXT:    vor.vv v16, v24, v16
; CHECK-NEXT:    vand.vi v24, v16, 2
; CHECK-NEXT:    vand.vi v0, v16, 1
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v0, v16, 4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vi v24, v16, 8
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 16
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 32
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 64
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 128
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 256
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 512
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    mv a6, a5
; CHECK-NEXT:    slli a5, a5, 1
; CHECK-NEXT:    add a5, a5, a6
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 1024
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    li a5, 1
; CHECK-NEXT:    slli a5, a5, 11
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v24, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a5, 1
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a5, 2
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a5, 4
; CHECK-NEXT:    vand.vx v0, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a5, 8
; CHECK-NEXT:    vand.vx v24, v16, a5
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 4
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 3
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a5, vlenb
; CHECK-NEXT:    slli a5, a5, 5
; CHECK-NEXT:    add a5, sp, a5
; CHECK-NEXT:    addi a5, a5, 16
; CHECK-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 32
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 64
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 128
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 256
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 512
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 1024
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 2048
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsll.vi v24, v24, 24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vand.vx v0, v0, a0
; CHECK-NEXT:    vsll.vi v0, v0, 8
; CHECK-NEXT:    vor.vv v24, v24, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    mv a5, a4
; CHECK-NEXT:    slli a4, a4, 1
; CHECK-NEXT:    add a4, a4, a5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 4096
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 8192
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 16384
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 32768
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 65536
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v0
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    lui a4, 131072
; CHECK-NEXT:    vand.vx v24, v16, a4
; CHECK-NEXT:    vmul.vv v24, v8, v24
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 4
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 3
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v0, v0, v24
; CHECK-NEXT:    addi a4, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vxor.vv v24, v0, v24
; CHECK-NEXT:    lui a4, 262144
; CHECK-NEXT:    vand.vx v0, v16, a4
; CHECK-NEXT:    vmul.vv v0, v8, v0
; CHECK-NEXT:    lui a4, 524288
; CHECK-NEXT:    vand.vx v16, v16, a4
; CHECK-NEXT:    vmul.vv v8, v8, v16
; CHECK-NEXT:    vxor.vv v16, v24, v0
; CHECK-NEXT:    vxor.vv v8, v16, v8
; CHECK-NEXT:    csrr a4, vlenb
; CHECK-NEXT:    slli a4, a4, 5
; CHECK-NEXT:    add a4, sp, a4
; CHECK-NEXT:    addi a4, a4, 16
; CHECK-NEXT:    vl8r.v v16, (a4) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vsrl.vi v16, v16, 8
; CHECK-NEXT:    vand.vx v16, v16, a0
; CHECK-NEXT:    vsrl.vi v8, v8, 24
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a4, a0
; CHECK-NEXT:    slli a0, a0, 1
; CHECK-NEXT:    add a0, a0, a4
; CHECK-NEXT:    add a0, sp, a0
; CHECK-NEXT:    addi a0, a0, 16
; CHECK-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 4
; CHECK-NEXT:    vand.vx v16, v16, a1
; CHECK-NEXT:    vand.vx v8, v8, a1
; CHECK-NEXT:    vsll.vi v8, v8, 4
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 2
; CHECK-NEXT:    vand.vx v16, v16, a2
; CHECK-NEXT:    vand.vx v8, v8, a2
; CHECK-NEXT:    vsll.vi v8, v8, 2
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v16, v8, 1
; CHECK-NEXT:    vand.vx v16, v16, a3
; CHECK-NEXT:    vand.vx v8, v8, a3
; CHECK-NEXT:    vadd.vv v8, v8, v8
; CHECK-NEXT:    vor.vv v8, v16, v8
; CHECK-NEXT:    vsrl.vi v8, v8, 1
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 3
; CHECK-NEXT:    mv a1, a0
; CHECK-NEXT:    slli a0, a0, 2
; CHECK-NEXT:    add a0, a0, a1
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %va.ext = zext <vscale x 16 x i32> %va to <vscale x 16 x i64>
  %vb.ext = zext <vscale x 16 x i32> %vb to <vscale x 16 x i64>
  %clmul = call <vscale x 16 x i64> @llvm.clmul.nxv16i64(<vscale x 16 x i64> %va.ext, <vscale x 16 x i64> %vb.ext)
  %res.ext = lshr <vscale x 16 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 16 x i64> %res.ext to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %res
}

define <vscale x 16 x i32> @clmulh_nxv16i32_vx(<vscale x 16 x i32> %va, i32 %b) nounwind {
; RV32-LABEL: clmulh_nxv16i32_vx:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    csrr a1, vlenb
; RV32-NEXT:    slli a1, a1, 4
; RV32-NEXT:    sub sp, sp, a1
; RV32-NEXT:    lui a5, 16
; RV32-NEXT:    addi a1, a5, -256
; RV32-NEXT:    vsetvli a2, zero, e32, m8, ta, ma
; RV32-NEXT:    vsrl.vi v16, v8, 8
; RV32-NEXT:    vand.vx v16, v16, a1
; RV32-NEXT:    vsrl.vi v24, v8, 24
; RV32-NEXT:    vor.vv v16, v16, v24
; RV32-NEXT:    vand.vx v24, v8, a1
; RV32-NEXT:    vsll.vi v24, v24, 8
; RV32-NEXT:    vsll.vi v8, v8, 24
; RV32-NEXT:    vor.vv v8, v8, v24
; RV32-NEXT:    vor.vv v8, v8, v16
; RV32-NEXT:    vsrl.vi v16, v8, 4
; RV32-NEXT:    lui a2, 61681
; RV32-NEXT:    addi a2, a2, -241
; RV32-NEXT:    vand.vx v16, v16, a2
; RV32-NEXT:    vand.vx v8, v8, a2
; RV32-NEXT:    vsll.vi v8, v8, 4
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v16, v8, 2
; RV32-NEXT:    lui a3, 209715
; RV32-NEXT:    addi a3, a3, 819
; RV32-NEXT:    vand.vx v16, v16, a3
; RV32-NEXT:    vand.vx v8, v8, a3
; RV32-NEXT:    vsll.vi v8, v8, 2
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v16, v8, 1
; RV32-NEXT:    lui a4, 349525
; RV32-NEXT:    addi a4, a4, 1365
; RV32-NEXT:    vand.vx v16, v16, a4
; RV32-NEXT:    vand.vx v8, v8, a4
; RV32-NEXT:    vadd.vv v8, v8, v8
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    srli a6, a0, 8
; RV32-NEXT:    and a6, a6, a1
; RV32-NEXT:    srli a7, a0, 24
; RV32-NEXT:    and t0, a0, a1
; RV32-NEXT:    slli t0, t0, 8
; RV32-NEXT:    slli a0, a0, 24
; RV32-NEXT:    or a6, a6, a7
; RV32-NEXT:    or a0, a0, t0
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    srli a6, a0, 4
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    and a6, a6, a2
; RV32-NEXT:    slli a0, a0, 4
; RV32-NEXT:    or a0, a6, a0
; RV32-NEXT:    srli a6, a0, 2
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a6, a6, a3
; RV32-NEXT:    slli a0, a0, 2
; RV32-NEXT:    or a0, a6, a0
; RV32-NEXT:    srli a6, a0, 1
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    and a6, a6, a4
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    or a0, a6, a0
; RV32-NEXT:    andi a6, a0, 2
; RV32-NEXT:    vmul.vx v16, v8, a6
; RV32-NEXT:    andi a6, a0, 1
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    andi a6, a0, 4
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v24, v16
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    andi a6, a0, 8
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    andi a6, a0, 16
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    andi a6, a0, 32
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    andi a6, a0, 64
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    andi a6, a0, 128
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    andi a6, a0, 256
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    csrr a6, vlenb
; RV32-NEXT:    slli a6, a6, 3
; RV32-NEXT:    add a6, sp, a6
; RV32-NEXT:    addi a6, a6, 16
; RV32-NEXT:    vs8r.v v16, (a6) # vscale x 64-byte Folded Spill
; RV32-NEXT:    vxor.vv v24, v16, v0
; RV32-NEXT:    andi a6, a0, 512
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    andi a6, a0, 1024
; RV32-NEXT:    vmul.vx v16, v8, a6
; RV32-NEXT:    vxor.vv v24, v24, v0
; RV32-NEXT:    vxor.vv v24, v24, v16
; RV32-NEXT:    li a6, 1
; RV32-NEXT:    slli a6, a6, 11
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    vmul.vx v16, v8, a6
; RV32-NEXT:    lui a6, 1
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v24, v16
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a6, 2
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    lui a6, 4
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    vmul.vx v0, v8, a6
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a6, 8
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    vmul.vx v24, v8, a6
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a5, 32
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    lui a5, 64
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a5, 128
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    lui a5, 256
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a5, 512
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    lui a5, 1024
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a5, 2048
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    csrr a5, vlenb
; RV32-NEXT:    slli a5, a5, 3
; RV32-NEXT:    add a5, sp, a5
; RV32-NEXT:    addi a5, a5, 16
; RV32-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV32-NEXT:    vsll.vi v0, v0, 24
; RV32-NEXT:    vxor.vv v24, v16, v24
; RV32-NEXT:    vand.vx v16, v24, a1
; RV32-NEXT:    addi a5, sp, 16
; RV32-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV32-NEXT:    vsll.vi v16, v16, 8
; RV32-NEXT:    vor.vv v16, v0, v16
; RV32-NEXT:    csrr a5, vlenb
; RV32-NEXT:    slli a5, a5, 3
; RV32-NEXT:    add a5, sp, a5
; RV32-NEXT:    addi a5, a5, 16
; RV32-NEXT:    vs8r.v v16, (a5) # vscale x 64-byte Folded Spill
; RV32-NEXT:    lui a5, 4096
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    lui a5, 8192
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v16, v8, a5
; RV32-NEXT:    vxor.vv v0, v24, v0
; RV32-NEXT:    vxor.vv v24, v0, v16
; RV32-NEXT:    lui a5, 16384
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    lui a5, 32768
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v16, v8, a5
; RV32-NEXT:    vxor.vv v24, v24, v0
; RV32-NEXT:    vxor.vv v16, v24, v16
; RV32-NEXT:    lui a5, 65536
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    lui a5, 131072
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v0, v8, a5
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v16, v16, v0
; RV32-NEXT:    lui a5, 262144
; RV32-NEXT:    and a5, a0, a5
; RV32-NEXT:    vmul.vx v24, v8, a5
; RV32-NEXT:    lui a5, 524288
; RV32-NEXT:    and a0, a0, a5
; RV32-NEXT:    vmul.vx v8, v8, a0
; RV32-NEXT:    vxor.vv v16, v16, v24
; RV32-NEXT:    vxor.vv v8, v16, v8
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32-NEXT:    vsrl.vi v16, v16, 8
; RV32-NEXT:    vand.vx v16, v16, a1
; RV32-NEXT:    vsrl.vi v8, v8, 24
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 3
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 16
; RV32-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v16, v8, 4
; RV32-NEXT:    vand.vx v16, v16, a2
; RV32-NEXT:    vand.vx v8, v8, a2
; RV32-NEXT:    vsll.vi v8, v8, 4
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v16, v8, 2
; RV32-NEXT:    vand.vx v16, v16, a3
; RV32-NEXT:    vand.vx v8, v8, a3
; RV32-NEXT:    vsll.vi v8, v8, 2
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v16, v8, 1
; RV32-NEXT:    vand.vx v16, v16, a4
; RV32-NEXT:    vand.vx v8, v8, a4
; RV32-NEXT:    vadd.vv v8, v8, v8
; RV32-NEXT:    vor.vv v8, v16, v8
; RV32-NEXT:    vsrl.vi v8, v8, 1
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 4
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: clmulh_nxv16i32_vx:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    csrr a1, vlenb
; RV64-NEXT:    slli a1, a1, 3
; RV64-NEXT:    mv a2, a1
; RV64-NEXT:    slli a1, a1, 2
; RV64-NEXT:    add a1, a1, a2
; RV64-NEXT:    sub sp, sp, a1
; RV64-NEXT:    vsetvli a1, zero, e32, m8, ta, ma
; RV64-NEXT:    vmv.v.x v16, a0
; RV64-NEXT:    vsrl.vi v24, v8, 8
; RV64-NEXT:    lui a4, 16
; RV64-NEXT:    addi a0, a4, -256
; RV64-NEXT:    vand.vx v24, v24, a0
; RV64-NEXT:    vsrl.vi v0, v8, 24
; RV64-NEXT:    vor.vv v24, v24, v0
; RV64-NEXT:    vand.vx v0, v8, a0
; RV64-NEXT:    vsll.vi v0, v0, 8
; RV64-NEXT:    vsll.vi v8, v8, 24
; RV64-NEXT:    vor.vv v8, v8, v0
; RV64-NEXT:    vor.vv v8, v8, v24
; RV64-NEXT:    vsrl.vi v24, v8, 4
; RV64-NEXT:    lui a1, 61681
; RV64-NEXT:    addi a1, a1, -241
; RV64-NEXT:    vand.vx v24, v24, a1
; RV64-NEXT:    vand.vx v8, v8, a1
; RV64-NEXT:    vsll.vi v8, v8, 4
; RV64-NEXT:    vor.vv v8, v24, v8
; RV64-NEXT:    vsrl.vi v24, v8, 2
; RV64-NEXT:    lui a2, 209715
; RV64-NEXT:    addi a2, a2, 819
; RV64-NEXT:    vand.vx v24, v24, a2
; RV64-NEXT:    vand.vx v8, v8, a2
; RV64-NEXT:    vsll.vi v8, v8, 2
; RV64-NEXT:    vor.vv v8, v24, v8
; RV64-NEXT:    vsrl.vi v24, v8, 1
; RV64-NEXT:    lui a3, 349525
; RV64-NEXT:    addi a3, a3, 1365
; RV64-NEXT:    vand.vx v24, v24, a3
; RV64-NEXT:    vand.vx v8, v8, a3
; RV64-NEXT:    vadd.vv v8, v8, v8
; RV64-NEXT:    vor.vv v8, v24, v8
; RV64-NEXT:    vsrl.vi v24, v16, 8
; RV64-NEXT:    vand.vx v24, v24, a0
; RV64-NEXT:    vsrl.vi v0, v16, 24
; RV64-NEXT:    vor.vv v24, v24, v0
; RV64-NEXT:    vand.vx v0, v16, a0
; RV64-NEXT:    vsll.vi v0, v0, 8
; RV64-NEXT:    vsll.vi v16, v16, 24
; RV64-NEXT:    vor.vv v16, v16, v0
; RV64-NEXT:    vor.vv v16, v16, v24
; RV64-NEXT:    vsrl.vi v24, v16, 4
; RV64-NEXT:    vand.vx v24, v24, a1
; RV64-NEXT:    vand.vx v16, v16, a1
; RV64-NEXT:    vsll.vi v16, v16, 4
; RV64-NEXT:    vor.vv v16, v24, v16
; RV64-NEXT:    vsrl.vi v24, v16, 2
; RV64-NEXT:    vand.vx v24, v24, a2
; RV64-NEXT:    vand.vx v16, v16, a2
; RV64-NEXT:    vsll.vi v16, v16, 2
; RV64-NEXT:    vor.vv v16, v24, v16
; RV64-NEXT:    vsrl.vi v24, v16, 1
; RV64-NEXT:    vand.vx v24, v24, a3
; RV64-NEXT:    vand.vx v16, v16, a3
; RV64-NEXT:    vadd.vv v16, v16, v16
; RV64-NEXT:    vor.vv v16, v24, v16
; RV64-NEXT:    vand.vi v24, v16, 2
; RV64-NEXT:    vand.vi v0, v16, 1
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    vmul.vv v0, v8, v0
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    vand.vi v0, v16, 4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    vand.vi v24, v16, 8
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 16
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 32
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 64
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 128
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 256
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 512
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    mv a6, a5
; RV64-NEXT:    slli a5, a5, 1
; RV64-NEXT:    add a5, a5, a6
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 1024
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    li a5, 1
; RV64-NEXT:    slli a5, a5, 11
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v24, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v24, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a5, 1
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a5, 2
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a5, 4
; RV64-NEXT:    vand.vx v0, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a5, 8
; RV64-NEXT:    vand.vx v24, v16, a5
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v0, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 4
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 3
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vl8r.v v24, (a5) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a5, vlenb
; RV64-NEXT:    slli a5, a5, 5
; RV64-NEXT:    add a5, sp, a5
; RV64-NEXT:    addi a5, a5, 16
; RV64-NEXT:    vs8r.v v24, (a5) # vscale x 64-byte Folded Spill
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 32
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 64
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 128
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 256
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 512
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 1024
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 2048
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    mv a5, a4
; RV64-NEXT:    slli a4, a4, 1
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vsll.vi v24, v24, 24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vand.vx v0, v0, a0
; RV64-NEXT:    vsll.vi v0, v0, 8
; RV64-NEXT:    vor.vv v24, v24, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    mv a5, a4
; RV64-NEXT:    slli a4, a4, 1
; RV64-NEXT:    add a4, a4, a5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 4096
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 8192
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 16384
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 32768
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    addi a4, sp, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    addi a4, sp, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 65536
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v0
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    lui a4, 131072
; RV64-NEXT:    vand.vx v24, v16, a4
; RV64-NEXT:    vmul.vv v24, v8, v24
; RV64-NEXT:    addi a4, sp, 16
; RV64-NEXT:    vs8r.v v24, (a4) # vscale x 64-byte Folded Spill
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 4
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v0, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 3
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v0, v0, v24
; RV64-NEXT:    addi a4, sp, 16
; RV64-NEXT:    vl8r.v v24, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vxor.vv v24, v0, v24
; RV64-NEXT:    lui a4, 262144
; RV64-NEXT:    vand.vx v0, v16, a4
; RV64-NEXT:    vmul.vv v0, v8, v0
; RV64-NEXT:    lui a4, 524288
; RV64-NEXT:    vand.vx v16, v16, a4
; RV64-NEXT:    vmul.vv v8, v8, v16
; RV64-NEXT:    vxor.vv v16, v24, v0
; RV64-NEXT:    vxor.vv v8, v16, v8
; RV64-NEXT:    csrr a4, vlenb
; RV64-NEXT:    slli a4, a4, 5
; RV64-NEXT:    add a4, sp, a4
; RV64-NEXT:    addi a4, a4, 16
; RV64-NEXT:    vl8r.v v16, (a4) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vsrl.vi v16, v16, 8
; RV64-NEXT:    vand.vx v16, v16, a0
; RV64-NEXT:    vsrl.vi v8, v8, 24
; RV64-NEXT:    vor.vv v8, v16, v8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 3
; RV64-NEXT:    mv a4, a0
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, a0, a4
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64-NEXT:    vor.vv v8, v16, v8
; RV64-NEXT:    vsrl.vi v16, v8, 4
; RV64-NEXT:    vand.vx v16, v16, a1
; RV64-NEXT:    vand.vx v8, v8, a1
; RV64-NEXT:    vsll.vi v8, v8, 4
; RV64-NEXT:    vor.vv v8, v16, v8
; RV64-NEXT:    vsrl.vi v16, v8, 2
; RV64-NEXT:    vand.vx v16, v16, a2
; RV64-NEXT:    vand.vx v8, v8, a2
; RV64-NEXT:    vsll.vi v8, v8, 2
; RV64-NEXT:    vor.vv v8, v16, v8
; RV64-NEXT:    vsrl.vi v16, v8, 1
; RV64-NEXT:    vand.vx v16, v16, a3
; RV64-NEXT:    vand.vx v8, v8, a3
; RV64-NEXT:    vadd.vv v8, v8, v8
; RV64-NEXT:    vor.vv v8, v16, v8
; RV64-NEXT:    vsrl.vi v8, v8, 1
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 3
; RV64-NEXT:    mv a1, a0
; RV64-NEXT:    slli a0, a0, 2
; RV64-NEXT:    add a0, a0, a1
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    ret
  %elt.head = insertelement <vscale x 16 x i32> poison, i32 %b, i64 0
  %vb = shufflevector <vscale x 16 x i32> %elt.head, <vscale x 16 x i32> poison, <vscale x 16 x i32> zeroinitializer
  %va.ext = zext <vscale x 16 x i32> %va to <vscale x 16 x i64>
  %vb.ext = zext <vscale x 16 x i32> %vb to <vscale x 16 x i64>
  %clmul = call <vscale x 16 x i64> @llvm.clmul.nxv16i64(<vscale x 16 x i64> %va.ext, <vscale x 16 x i64> %vb.ext)
  %res.ext = lshr <vscale x 16 x i64> %clmul, splat(i64 32)
  %res = trunc <vscale x 16 x i64> %res.ext to <vscale x 16 x i32>
  ret <vscale x 16 x i32> %res
}

define <vscale x 1 x i64> @clmulh_nxv1i64_vv(<vscale x 1 x i64> %va, <vscale x 1 x i64> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv1i64_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li s6, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s6, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li s11, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw s11, 228(sp)
; RV32V-NEXT:    li ra, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw ra, 220(sp)
; RV32V-NEXT:    li a6, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a6, 212(sp)
; RV32V-NEXT:    li a5, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a5, 204(sp)
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    li s7, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s7, 188(sp)
; RV32V-NEXT:    li s8, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s8, 180(sp)
; RV32V-NEXT:    slli s6, s6, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s6, 172(sp)
; RV32V-NEXT:    lui s5, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s5, 164(sp)
; RV32V-NEXT:    lui s3, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s3, 156(sp)
; RV32V-NEXT:    lui s4, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw s4, 148(sp)
; RV32V-NEXT:    lui s2, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s2, 140(sp)
; RV32V-NEXT:    lui s1, 16
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s1, 132(sp)
; RV32V-NEXT:    lui s0, 32
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    lui t5, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw t5, 108(sp)
; RV32V-NEXT:    lui t4, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw t4, 100(sp)
; RV32V-NEXT:    lui t3, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t3, 92(sp)
; RV32V-NEXT:    lui t2, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t2, 84(sp)
; RV32V-NEXT:    lui t1, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t1, 76(sp)
; RV32V-NEXT:    lui a0, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a0, 68(sp)
; RV32V-NEXT:    lui a0, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a0, 60(sp)
; RV32V-NEXT:    lui a0, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a0, 52(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    lui a2, 65536
; RV32V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vx v10, v9, a0
; RV32V-NEXT:    li a1, 56
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a2, 36(sp)
; RV32V-NEXT:    vsrl.vx v11, v9, a1
; RV32V-NEXT:    lui s10, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s10, 28(sp)
; RV32V-NEXT:    addi a2, s1, -256
; RV32V-NEXT:    vand.vx v12, v10, a2
; RV32V-NEXT:    lui s9, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s9, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    vsrl.vi v13, v9, 24
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    addi a3, sp, 272
; RV32V-NEXT:    vlse64.v v10, (a3), zero
; RV32V-NEXT:    vsrl.vi v14, v9, 8
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vv v14, v14, v10
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v12, v14, v13
; RV32V-NEXT:    vand.vx v13, v9, a2
; RV32V-NEXT:    vsll.vx v14, v9, a1
; RV32V-NEXT:    vsll.vx v13, v13, a0
; RV32V-NEXT:    vand.vx v15, v9, a3
; RV32V-NEXT:    vand.vv v9, v9, v10
; RV32V-NEXT:    vsll.vi v15, v15, 24
; RV32V-NEXT:    vsll.vi v9, v9, 8
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v9, v15, v9
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v9, v13, v9
; RV32V-NEXT:    vor.vv v11, v9, v11
; RV32V-NEXT:    lui a7, 61681
; RV32V-NEXT:    addi a7, a7, -241
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v9, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v12, v11, 4
; RV32V-NEXT:    vand.vv v11, v11, v9
; RV32V-NEXT:    vand.vv v12, v12, v9
; RV32V-NEXT:    vsll.vi v11, v11, 4
; RV32V-NEXT:    vor.vv v12, v12, v11
; RV32V-NEXT:    lui a7, 209715
; RV32V-NEXT:    addi a7, a7, 819
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v11, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v13, v12, 2
; RV32V-NEXT:    vand.vv v12, v12, v11
; RV32V-NEXT:    vand.vv v13, v13, v11
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v13, v13, v12
; RV32V-NEXT:    lui a7, 349525
; RV32V-NEXT:    addi a7, a7, 1365
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v14, v13, 1
; RV32V-NEXT:    vand.vv v13, v13, v12
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vadd.vv v13, v13, v13
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vand.vx v16, v13, s11
; RV32V-NEXT:    vand.vx v18, v13, ra
; RV32V-NEXT:    vand.vx v15, v13, a6
; RV32V-NEXT:    vand.vx v17, v13, a5
; RV32V-NEXT:    vand.vx v14, v13, a4
; RV32V-NEXT:    vsrl.vx v19, v8, a0
; RV32V-NEXT:    vsrl.vx v20, v8, a1
; RV32V-NEXT:    vand.vx v19, v19, a2
; RV32V-NEXT:    vsrl.vi v21, v8, 24
; RV32V-NEXT:    vsrl.vi v22, v8, 8
; RV32V-NEXT:    vand.vx v21, v21, a3
; RV32V-NEXT:    vand.vv v22, v22, v10
; RV32V-NEXT:    vor.vv v19, v19, v20
; RV32V-NEXT:    vor.vv v20, v22, v21
; RV32V-NEXT:    vand.vx v21, v8, a2
; RV32V-NEXT:    vsll.vx v22, v8, a1
; RV32V-NEXT:    vsll.vx v21, v21, a0
; RV32V-NEXT:    vand.vx v23, v8, a3
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vsll.vi v23, v23, 24
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v21, v22, v21
; RV32V-NEXT:    vor.vv v8, v23, v8
; RV32V-NEXT:    vor.vv v19, v20, v19
; RV32V-NEXT:    vor.vv v8, v21, v8
; RV32V-NEXT:    vand.vx v20, v13, s7
; RV32V-NEXT:    vor.vv v8, v8, v19
; RV32V-NEXT:    vand.vx v19, v13, s8
; RV32V-NEXT:    vsrl.vi v21, v8, 4
; RV32V-NEXT:    vand.vv v8, v8, v9
; RV32V-NEXT:    vand.vv v21, v21, v9
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vand.vx v22, v13, s6
; RV32V-NEXT:    vor.vv v8, v21, v8
; RV32V-NEXT:    vand.vx v21, v13, s5
; RV32V-NEXT:    vsrl.vi v23, v8, 2
; RV32V-NEXT:    vand.vv v8, v8, v11
; RV32V-NEXT:    vand.vv v23, v23, v11
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vand.vx v24, v13, s3
; RV32V-NEXT:    vor.vv v8, v23, v8
; RV32V-NEXT:    vand.vx v23, v13, s4
; RV32V-NEXT:    vsrl.vi v25, v8, 1
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vand.vv v25, v25, v12
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vand.vx v26, v13, s2
; RV32V-NEXT:    vor.vv v8, v25, v8
; RV32V-NEXT:    vand.vi v25, v13, 1
; RV32V-NEXT:    vand.vi v27, v13, 2
; RV32V-NEXT:    vmul.vv v25, v8, v25
; RV32V-NEXT:    vand.vx v28, v13, s1
; RV32V-NEXT:    vmul.vv v27, v8, v27
; RV32V-NEXT:    vand.vi v29, v13, 4
; RV32V-NEXT:    vxor.vi v25, v25, 0
; RV32V-NEXT:    vmul.vv v29, v8, v29
; RV32V-NEXT:    vand.vi v30, v13, 8
; RV32V-NEXT:    vxor.vv v25, v25, v27
; RV32V-NEXT:    vmul.vv v27, v8, v30
; RV32V-NEXT:    vand.vx v30, v13, s0
; RV32V-NEXT:    vxor.vv v25, v25, v29
; RV32V-NEXT:    vmul.vv v16, v8, v16
; RV32V-NEXT:    vxor.vv v25, v25, v27
; RV32V-NEXT:    vmul.vv v18, v8, v18
; RV32V-NEXT:    vand.vx v27, v13, t6
; RV32V-NEXT:    vxor.vv v16, v25, v16
; RV32V-NEXT:    vmul.vv v15, v8, v15
; RV32V-NEXT:    vxor.vv v16, v16, v18
; RV32V-NEXT:    vmul.vv v17, v8, v17
; RV32V-NEXT:    vand.vx v18, v13, t5
; RV32V-NEXT:    vxor.vv v15, v16, v15
; RV32V-NEXT:    vmul.vv v14, v8, v14
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v16, v8, v20
; RV32V-NEXT:    vand.vx v17, v13, t4
; RV32V-NEXT:    vxor.vv v14, v15, v14
; RV32V-NEXT:    vmul.vv v15, v8, v19
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v22
; RV32V-NEXT:    vand.vx v19, v13, t3
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v21
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v24
; RV32V-NEXT:    vand.vx v20, v13, t2
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v23
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v26
; RV32V-NEXT:    vand.vx v21, v13, t1
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v28
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v30
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    vand.vx v22, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v27
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v18
; RV32V-NEXT:    lui a4, 8192
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v19
; RV32V-NEXT:    lui a4, 16384
; RV32V-NEXT:    vand.vx v17, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v20
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v21
; RV32V-NEXT:    lui a4, 32768
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v22
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v18
; RV32V-NEXT:    lui a4, 65536
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v19
; RV32V-NEXT:    vand.vx v17, v13, s10
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v18
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vx v17, v13, s9
; RV32V-NEXT:    addi a4, sp, 264
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 256
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 248
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 240
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 232
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 224
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 216
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 208
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 200
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 192
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 184
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 176
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 168
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 160
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 152
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 144
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 136
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 128
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 120
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 112
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 104
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 96
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 88
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 80
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 72
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 64
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 56
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 48
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 40
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 32
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 24
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 16
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 8
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v13, v13, v18
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v8, v8, v13
; RV32V-NEXT:    vxor.vv v13, v14, v15
; RV32V-NEXT:    vxor.vv v8, v13, v8
; RV32V-NEXT:    vsrl.vi v13, v8, 24
; RV32V-NEXT:    vsrl.vx v14, v8, a0
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vx v15, v8, a3
; RV32V-NEXT:    vand.vx v14, v14, a2
; RV32V-NEXT:    vand.vx v16, v8, a2
; RV32V-NEXT:    vsrl.vx v17, v8, a1
; RV32V-NEXT:    vsll.vx v18, v8, a1
; RV32V-NEXT:    vsll.vx v16, v16, a0
; RV32V-NEXT:    vor.vv v14, v14, v17
; RV32V-NEXT:    vsrl.vi v17, v8, 8
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vand.vv v10, v17, v10
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vsll.vi v15, v15, 24
; RV32V-NEXT:    vor.vv v10, v10, v13
; RV32V-NEXT:    vor.vv v8, v15, v8
; RV32V-NEXT:    vor.vv v13, v18, v16
; RV32V-NEXT:    vor.vv v10, v10, v14
; RV32V-NEXT:    vor.vv v8, v13, v8
; RV32V-NEXT:    vor.vv v8, v8, v10
; RV32V-NEXT:    vsrl.vi v10, v8, 4
; RV32V-NEXT:    vand.vv v8, v8, v9
; RV32V-NEXT:    vand.vv v9, v10, v9
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v9, v8, 2
; RV32V-NEXT:    vand.vv v8, v8, v11
; RV32V-NEXT:    vand.vv v9, v9, v11
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v9, v8, 1
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vand.vv v9, v9, v12
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i64_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 56
; RV64V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV64V-NEXT:    vsrl.vx v10, v8, a0
; RV64V-NEXT:    li a1, 40
; RV64V-NEXT:    vsrl.vx v11, v8, a1
; RV64V-NEXT:    lui t1, 16
; RV64V-NEXT:    vsrl.vi v12, v8, 24
; RV64V-NEXT:    addi a2, t1, -256
; RV64V-NEXT:    vand.vx v11, v11, a2
; RV64V-NEXT:    vsrl.vi v13, v8, 8
; RV64V-NEXT:    lui a3, 4080
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    li a4, 255
; RV64V-NEXT:    slli a4, a4, 24
; RV64V-NEXT:    vand.vx v13, v13, a4
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vor.vv v11, v13, v12
; RV64V-NEXT:    vand.vx v12, v8, a3
; RV64V-NEXT:    vand.vx v13, v8, a4
; RV64V-NEXT:    vsll.vi v12, v12, 24
; RV64V-NEXT:    vsll.vi v13, v13, 8
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vsll.vx v8, v8, a0
; RV64V-NEXT:    vsll.vx v14, v14, a1
; RV64V-NEXT:    vor.vv v12, v12, v13
; RV64V-NEXT:    vor.vv v8, v8, v14
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vor.vv v8, v8, v12
; RV64V-NEXT:    vor.vv v8, v8, v10
; RV64V-NEXT:    lui a5, 61681
; RV64V-NEXT:    addi a5, a5, -241
; RV64V-NEXT:    vsrl.vi v10, v8, 4
; RV64V-NEXT:    slli a6, a5, 32
; RV64V-NEXT:    add a5, a5, a6
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vand.vx v10, v10, a5
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 2
; RV64V-NEXT:    lui a6, 209715
; RV64V-NEXT:    addi a6, a6, 819
; RV64V-NEXT:    slli a7, a6, 32
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    vand.vx v10, v10, a6
; RV64V-NEXT:    vsrl.vx v11, v9, a1
; RV64V-NEXT:    vsrl.vx v12, v9, a0
; RV64V-NEXT:    vand.vx v11, v11, a2
; RV64V-NEXT:    vsrl.vi v13, v9, 24
; RV64V-NEXT:    vsrl.vi v14, v9, 8
; RV64V-NEXT:    vand.vx v13, v13, a3
; RV64V-NEXT:    vand.vx v14, v14, a4
; RV64V-NEXT:    vor.vv v11, v11, v12
; RV64V-NEXT:    vor.vv v12, v14, v13
; RV64V-NEXT:    vand.vx v13, v9, a3
; RV64V-NEXT:    vand.vx v14, v9, a4
; RV64V-NEXT:    vsll.vi v13, v13, 24
; RV64V-NEXT:    vsll.vi v14, v14, 8
; RV64V-NEXT:    vand.vx v15, v9, a2
; RV64V-NEXT:    vsll.vx v9, v9, a0
; RV64V-NEXT:    vsll.vx v15, v15, a1
; RV64V-NEXT:    vor.vv v13, v13, v14
; RV64V-NEXT:    vor.vv v9, v9, v15
; RV64V-NEXT:    vor.vv v11, v12, v11
; RV64V-NEXT:    vor.vv v9, v9, v13
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vor.vv v9, v9, v11
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vsrl.vi v11, v9, 4
; RV64V-NEXT:    vand.vx v9, v9, a5
; RV64V-NEXT:    vand.vx v11, v11, a5
; RV64V-NEXT:    vsll.vi v9, v9, 4
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vsrl.vi v10, v8, 1
; RV64V-NEXT:    vsrl.vi v11, v9, 2
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    lui a7, 349525
; RV64V-NEXT:    vand.vx v11, v11, a6
; RV64V-NEXT:    addi a7, a7, 1365
; RV64V-NEXT:    slli t0, a7, 32
; RV64V-NEXT:    vsll.vi v9, v9, 2
; RV64V-NEXT:    add a7, a7, t0
; RV64V-NEXT:    vand.vx v10, v10, a7
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vand.vx v8, v8, a7
; RV64V-NEXT:    vsrl.vi v11, v9, 1
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vand.vx v11, v11, a7
; RV64V-NEXT:    vadd.vv v9, v9, v9
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vand.vi v10, v9, 2
; RV64V-NEXT:    vand.vi v11, v9, 1
; RV64V-NEXT:    vmul.vv v10, v8, v10
; RV64V-NEXT:    vmul.vv v11, v8, v11
; RV64V-NEXT:    vand.vi v12, v9, 4
; RV64V-NEXT:    vmul.vv v12, v8, v12
; RV64V-NEXT:    vand.vi v13, v9, 8
; RV64V-NEXT:    vxor.vv v10, v11, v10
; RV64V-NEXT:    vmul.vv v11, v8, v13
; RV64V-NEXT:    li t0, 16
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v8, v13
; RV64V-NEXT:    li t0, 32
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v8, v13
; RV64V-NEXT:    li t0, 64
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v8, v13
; RV64V-NEXT:    li t0, 128
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v8, v13
; RV64V-NEXT:    li t0, 256
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v8, v13
; RV64V-NEXT:    li t0, 512
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    vmul.vv v11, v8, v13
; RV64V-NEXT:    li t0, 1024
; RV64V-NEXT:    vand.vx v13, v9, t0
; RV64V-NEXT:    vxor.vv v12, v10, v12
; RV64V-NEXT:    li t0, 1
; RV64V-NEXT:    vmul.vv v13, v8, v13
; RV64V-NEXT:    slli t2, t0, 11
; RV64V-NEXT:    vand.vx v14, v9, t2
; RV64V-NEXT:    vxor.vv v11, v12, v11
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t2, 1
; RV64V-NEXT:    vand.vx v14, v9, t2
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t2, 2
; RV64V-NEXT:    vand.vx v14, v9, t2
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t2, 4
; RV64V-NEXT:    vand.vx v14, v9, t2
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t2, 8
; RV64V-NEXT:    vand.vx v14, v9, t2
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 32
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 64
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 128
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 256
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 512
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 1024
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 2048
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vsll.vx v10, v10, a0
; RV64V-NEXT:    vand.vx v12, v11, a2
; RV64V-NEXT:    lui t1, 4096
; RV64V-NEXT:    vand.vx v13, v9, t1
; RV64V-NEXT:    vsll.vx v12, v12, a1
; RV64V-NEXT:    vmul.vv v13, v8, v13
; RV64V-NEXT:    lui t1, 8192
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vor.vv v10, v10, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 16384
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 32768
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 65536
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    lui t1, 131072
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    lui t1, 262144
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 31
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 33
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 34
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 35
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 36
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 37
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 38
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 39
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 40
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v8, v14
; RV64V-NEXT:    slli t1, t0, 41
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v8, v14
; RV64V-NEXT:    slli t1, t0, 42
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vxor.vv v13, v11, v13
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    slli t1, t0, 43
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v13, v12
; RV64V-NEXT:    vmul.vv v13, v8, v15
; RV64V-NEXT:    slli t1, t0, 44
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v8, v15
; RV64V-NEXT:    slli t1, t0, 45
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v8, v15
; RV64V-NEXT:    slli t1, t0, 46
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v8, v15
; RV64V-NEXT:    slli t1, t0, 47
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v8, v15
; RV64V-NEXT:    slli t1, t0, 48
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v8, v15
; RV64V-NEXT:    slli t1, t0, 49
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v8, v15
; RV64V-NEXT:    slli t1, t0, 50
; RV64V-NEXT:    vand.vx v15, v9, t1
; RV64V-NEXT:    vxor.vv v14, v12, v14
; RV64V-NEXT:    vmul.vv v15, v8, v15
; RV64V-NEXT:    slli t1, t0, 51
; RV64V-NEXT:    vand.vx v16, v9, t1
; RV64V-NEXT:    vxor.vv v13, v14, v13
; RV64V-NEXT:    vmul.vv v14, v8, v16
; RV64V-NEXT:    slli t1, t0, 52
; RV64V-NEXT:    vand.vx v16, v9, t1
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vmul.vv v15, v8, v16
; RV64V-NEXT:    slli t1, t0, 53
; RV64V-NEXT:    vand.vx v16, v9, t1
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vmul.vv v14, v8, v16
; RV64V-NEXT:    slli t1, t0, 54
; RV64V-NEXT:    vand.vx v16, v9, t1
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vmul.vv v15, v8, v16
; RV64V-NEXT:    slli t1, t0, 55
; RV64V-NEXT:    vand.vx v16, v9, t1
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vmul.vv v14, v8, v16
; RV64V-NEXT:    vand.vx v16, v11, a3
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vsll.vi v15, v16, 24
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    slli t1, t0, 56
; RV64V-NEXT:    vand.vx v14, v9, t1
; RV64V-NEXT:    vand.vx v16, v13, a4
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    slli t1, t0, 57
; RV64V-NEXT:    vand.vx v17, v9, t1
; RV64V-NEXT:    vsll.vi v16, v16, 8
; RV64V-NEXT:    vmul.vv v17, v8, v17
; RV64V-NEXT:    slli t1, t0, 58
; RV64V-NEXT:    vand.vx v18, v9, t1
; RV64V-NEXT:    vxor.vv v14, v13, v14
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    slli t1, t0, 59
; RV64V-NEXT:    vand.vx v19, v9, t1
; RV64V-NEXT:    vxor.vv v14, v14, v17
; RV64V-NEXT:    vmul.vv v17, v8, v19
; RV64V-NEXT:    slli t1, t0, 60
; RV64V-NEXT:    vand.vx v19, v9, t1
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    vmul.vv v18, v8, v19
; RV64V-NEXT:    slli t1, t0, 61
; RV64V-NEXT:    vand.vx v19, v9, t1
; RV64V-NEXT:    vxor.vv v14, v14, v17
; RV64V-NEXT:    vmul.vv v17, v8, v19
; RV64V-NEXT:    vor.vv v15, v15, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    vsrl.vi v11, v11, 8
; RV64V-NEXT:    vxor.vv v14, v14, v17
; RV64V-NEXT:    slli t0, t0, 62
; RV64V-NEXT:    vand.vx v16, v9, t0
; RV64V-NEXT:    vsrl.vi v12, v12, 24
; RV64V-NEXT:    li t0, -1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t0, t0, 63
; RV64V-NEXT:    vand.vx v9, v9, t0
; RV64V-NEXT:    vand.vx v11, v11, a4
; RV64V-NEXT:    vmul.vv v8, v8, v9
; RV64V-NEXT:    vand.vx v9, v12, a3
; RV64V-NEXT:    vxor.vv v12, v14, v16
; RV64V-NEXT:    vsrl.vx v13, v13, a1
; RV64V-NEXT:    vxor.vv v8, v12, v8
; RV64V-NEXT:    vand.vx v12, v13, a2
; RV64V-NEXT:    vsrl.vx v8, v8, a0
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vor.vv v10, v10, v15
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 4
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vand.vx v9, v9, a5
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 2
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 1
; RV64V-NEXT:    vand.vx v8, v8, a7
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i64_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a0, zero, e64, m1, ta, ma
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v9
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i64_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a0, zero, e64, m1, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vv v8, v8, v9
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 1 x i64> %va to <vscale x 1 x i128>
  %vb.ext = zext <vscale x 1 x i64> %vb to <vscale x 1 x i128>
  %clmul = call <vscale x 1 x i128> @llvm.clmul.nxv1i128(<vscale x 1 x i128> %va.ext, <vscale x 1 x i128> %vb.ext)
  %res.ext = lshr <vscale x 1 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 1 x i128> %res.ext to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %res
}

define <vscale x 1 x i64> @clmulh_nxv1i64_vx(<vscale x 1 x i64> %va, i64 %b) nounwind {
; RV32V-LABEL: clmulh_nxv1i64_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw a0, 0(sp)
; RV32V-NEXT:    sw a1, 4(sp)
; RV32V-NEXT:    mv a0, sp
; RV32V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32V-NEXT:    vlse64.v v10, (a0), zero
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li s6, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s6, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li s11, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw s11, 228(sp)
; RV32V-NEXT:    li ra, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw ra, 220(sp)
; RV32V-NEXT:    li a6, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a6, 212(sp)
; RV32V-NEXT:    li a5, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a5, 204(sp)
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    li s7, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s7, 188(sp)
; RV32V-NEXT:    li s8, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s8, 180(sp)
; RV32V-NEXT:    slli s6, s6, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s6, 172(sp)
; RV32V-NEXT:    lui s5, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s5, 164(sp)
; RV32V-NEXT:    lui s3, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s3, 156(sp)
; RV32V-NEXT:    lui s4, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw s4, 148(sp)
; RV32V-NEXT:    lui s2, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s2, 140(sp)
; RV32V-NEXT:    lui s1, 16
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s1, 132(sp)
; RV32V-NEXT:    lui s0, 32
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    lui t5, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw t5, 108(sp)
; RV32V-NEXT:    lui t4, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw t4, 100(sp)
; RV32V-NEXT:    lui t3, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t3, 92(sp)
; RV32V-NEXT:    lui t2, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t2, 84(sp)
; RV32V-NEXT:    lui t1, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t1, 76(sp)
; RV32V-NEXT:    lui a0, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a0, 68(sp)
; RV32V-NEXT:    lui a0, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a0, 60(sp)
; RV32V-NEXT:    lui a0, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a0, 52(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    lui a2, 65536
; RV32V-NEXT:    vsrl.vx v9, v10, a0
; RV32V-NEXT:    li a1, 56
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a2, 36(sp)
; RV32V-NEXT:    vsrl.vx v11, v10, a1
; RV32V-NEXT:    lui s10, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s10, 28(sp)
; RV32V-NEXT:    addi a2, s1, -256
; RV32V-NEXT:    vand.vx v12, v9, a2
; RV32V-NEXT:    lui s9, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s9, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    vsrl.vi v13, v10, 24
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    addi a3, sp, 272
; RV32V-NEXT:    vlse64.v v9, (a3), zero
; RV32V-NEXT:    vsrl.vi v14, v10, 8
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vv v14, v14, v9
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v12, v14, v13
; RV32V-NEXT:    vand.vv v13, v10, v9
; RV32V-NEXT:    vand.vx v14, v10, a3
; RV32V-NEXT:    vsll.vi v13, v13, 8
; RV32V-NEXT:    vsll.vi v14, v14, 24
; RV32V-NEXT:    vand.vx v15, v10, a2
; RV32V-NEXT:    vsll.vx v10, v10, a1
; RV32V-NEXT:    vsll.vx v15, v15, a0
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v10, v10, v15
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v10, v10, v13
; RV32V-NEXT:    vor.vv v11, v10, v11
; RV32V-NEXT:    lui a7, 61681
; RV32V-NEXT:    addi a7, a7, -241
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v10, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v12, v11, 4
; RV32V-NEXT:    vand.vv v11, v11, v10
; RV32V-NEXT:    vand.vv v12, v12, v10
; RV32V-NEXT:    vsll.vi v11, v11, 4
; RV32V-NEXT:    vor.vv v12, v12, v11
; RV32V-NEXT:    lui a7, 209715
; RV32V-NEXT:    addi a7, a7, 819
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v11, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v13, v12, 2
; RV32V-NEXT:    vand.vv v12, v12, v11
; RV32V-NEXT:    vand.vv v13, v13, v11
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v13, v13, v12
; RV32V-NEXT:    lui a7, 349525
; RV32V-NEXT:    addi a7, a7, 1365
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v14, v13, 1
; RV32V-NEXT:    vand.vv v13, v13, v12
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vadd.vv v13, v13, v13
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vand.vx v16, v13, s11
; RV32V-NEXT:    vand.vx v18, v13, ra
; RV32V-NEXT:    vand.vx v15, v13, a6
; RV32V-NEXT:    vand.vx v17, v13, a5
; RV32V-NEXT:    vand.vx v14, v13, a4
; RV32V-NEXT:    vsrl.vx v19, v8, a0
; RV32V-NEXT:    vsrl.vx v20, v8, a1
; RV32V-NEXT:    vand.vx v19, v19, a2
; RV32V-NEXT:    vsrl.vi v21, v8, 24
; RV32V-NEXT:    vsrl.vi v22, v8, 8
; RV32V-NEXT:    vand.vx v21, v21, a3
; RV32V-NEXT:    vand.vv v22, v22, v9
; RV32V-NEXT:    vor.vv v19, v19, v20
; RV32V-NEXT:    vor.vv v20, v22, v21
; RV32V-NEXT:    vand.vx v21, v8, a2
; RV32V-NEXT:    vsll.vx v22, v8, a1
; RV32V-NEXT:    vsll.vx v21, v21, a0
; RV32V-NEXT:    vand.vx v23, v8, a3
; RV32V-NEXT:    vand.vv v8, v8, v9
; RV32V-NEXT:    vsll.vi v23, v23, 24
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v21, v22, v21
; RV32V-NEXT:    vor.vv v8, v23, v8
; RV32V-NEXT:    vor.vv v19, v20, v19
; RV32V-NEXT:    vor.vv v8, v21, v8
; RV32V-NEXT:    vand.vx v20, v13, s7
; RV32V-NEXT:    vor.vv v8, v8, v19
; RV32V-NEXT:    vand.vx v19, v13, s8
; RV32V-NEXT:    vsrl.vi v21, v8, 4
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vand.vv v21, v21, v10
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vand.vx v22, v13, s6
; RV32V-NEXT:    vor.vv v8, v21, v8
; RV32V-NEXT:    vand.vx v21, v13, s5
; RV32V-NEXT:    vsrl.vi v23, v8, 2
; RV32V-NEXT:    vand.vv v8, v8, v11
; RV32V-NEXT:    vand.vv v23, v23, v11
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vand.vx v24, v13, s3
; RV32V-NEXT:    vor.vv v8, v23, v8
; RV32V-NEXT:    vand.vx v23, v13, s4
; RV32V-NEXT:    vsrl.vi v25, v8, 1
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vand.vv v25, v25, v12
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vand.vx v26, v13, s2
; RV32V-NEXT:    vor.vv v8, v25, v8
; RV32V-NEXT:    vand.vi v25, v13, 1
; RV32V-NEXT:    vand.vi v27, v13, 2
; RV32V-NEXT:    vmul.vv v25, v8, v25
; RV32V-NEXT:    vand.vx v28, v13, s1
; RV32V-NEXT:    vmul.vv v27, v8, v27
; RV32V-NEXT:    vand.vi v29, v13, 4
; RV32V-NEXT:    vxor.vi v25, v25, 0
; RV32V-NEXT:    vmul.vv v29, v8, v29
; RV32V-NEXT:    vand.vi v30, v13, 8
; RV32V-NEXT:    vxor.vv v25, v25, v27
; RV32V-NEXT:    vmul.vv v27, v8, v30
; RV32V-NEXT:    vand.vx v30, v13, s0
; RV32V-NEXT:    vxor.vv v25, v25, v29
; RV32V-NEXT:    vmul.vv v16, v8, v16
; RV32V-NEXT:    vxor.vv v25, v25, v27
; RV32V-NEXT:    vmul.vv v18, v8, v18
; RV32V-NEXT:    vand.vx v27, v13, t6
; RV32V-NEXT:    vxor.vv v16, v25, v16
; RV32V-NEXT:    vmul.vv v15, v8, v15
; RV32V-NEXT:    vxor.vv v16, v16, v18
; RV32V-NEXT:    vmul.vv v17, v8, v17
; RV32V-NEXT:    vand.vx v18, v13, t5
; RV32V-NEXT:    vxor.vv v15, v16, v15
; RV32V-NEXT:    vmul.vv v14, v8, v14
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v16, v8, v20
; RV32V-NEXT:    vand.vx v17, v13, t4
; RV32V-NEXT:    vxor.vv v14, v15, v14
; RV32V-NEXT:    vmul.vv v15, v8, v19
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v22
; RV32V-NEXT:    vand.vx v19, v13, t3
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v21
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v24
; RV32V-NEXT:    vand.vx v20, v13, t2
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v23
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v26
; RV32V-NEXT:    vand.vx v21, v13, t1
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v28
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v30
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    vand.vx v22, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v27
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v18
; RV32V-NEXT:    lui a4, 8192
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v19
; RV32V-NEXT:    lui a4, 16384
; RV32V-NEXT:    vand.vx v17, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v20
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v21
; RV32V-NEXT:    lui a4, 32768
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v22
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v18
; RV32V-NEXT:    lui a4, 65536
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v19
; RV32V-NEXT:    vand.vx v17, v13, s10
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v18
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vx v17, v13, s9
; RV32V-NEXT:    addi a4, sp, 264
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 256
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 248
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 240
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 232
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 224
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 216
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 208
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 200
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 192
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 184
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 176
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 168
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 160
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 152
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 144
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 136
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 128
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 120
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 112
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 104
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 96
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 88
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 80
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 72
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 64
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 56
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 48
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 40
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 32
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 24
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 16
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v16, v8, v17
; RV32V-NEXT:    vand.vv v17, v13, v18
; RV32V-NEXT:    addi a4, sp, 8
; RV32V-NEXT:    vlse64.v v18, (a4), zero
; RV32V-NEXT:    vxor.vv v14, v14, v15
; RV32V-NEXT:    vmul.vv v15, v8, v17
; RV32V-NEXT:    vand.vv v13, v13, v18
; RV32V-NEXT:    vxor.vv v14, v14, v16
; RV32V-NEXT:    vmul.vv v8, v8, v13
; RV32V-NEXT:    vxor.vv v13, v14, v15
; RV32V-NEXT:    vxor.vv v8, v13, v8
; RV32V-NEXT:    vsrl.vi v13, v8, 24
; RV32V-NEXT:    vsrl.vx v14, v8, a0
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vx v15, v8, a3
; RV32V-NEXT:    vand.vx v14, v14, a2
; RV32V-NEXT:    vand.vx v16, v8, a2
; RV32V-NEXT:    vsrl.vx v17, v8, a1
; RV32V-NEXT:    vsll.vx v18, v8, a1
; RV32V-NEXT:    vsll.vx v16, v16, a0
; RV32V-NEXT:    vor.vv v14, v14, v17
; RV32V-NEXT:    vsrl.vi v17, v8, 8
; RV32V-NEXT:    vand.vv v8, v8, v9
; RV32V-NEXT:    vand.vv v9, v17, v9
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vsll.vi v15, v15, 24
; RV32V-NEXT:    vor.vv v9, v9, v13
; RV32V-NEXT:    vor.vv v8, v15, v8
; RV32V-NEXT:    vor.vv v13, v18, v16
; RV32V-NEXT:    vor.vv v9, v9, v14
; RV32V-NEXT:    vor.vv v8, v13, v8
; RV32V-NEXT:    vor.vv v8, v8, v9
; RV32V-NEXT:    vsrl.vi v9, v8, 4
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vand.vv v9, v9, v10
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v9, v8, 2
; RV32V-NEXT:    vand.vv v8, v8, v11
; RV32V-NEXT:    vand.vv v9, v9, v11
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v9, v8, 1
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vand.vv v9, v9, v12
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v9, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i64_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a1, 56
; RV64V-NEXT:    vsetvli a2, zero, e64, m1, ta, ma
; RV64V-NEXT:    vsrl.vx v9, v8, a1
; RV64V-NEXT:    li a2, 40
; RV64V-NEXT:    vsrl.vx v10, v8, a2
; RV64V-NEXT:    lui t2, 16
; RV64V-NEXT:    vsrl.vi v11, v8, 24
; RV64V-NEXT:    addi a3, t2, -256
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vsrl.vi v12, v8, 8
; RV64V-NEXT:    lui a5, 4080
; RV64V-NEXT:    vand.vx v11, v11, a5
; RV64V-NEXT:    li a4, 255
; RV64V-NEXT:    slli a4, a4, 24
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vor.vv v10, v12, v11
; RV64V-NEXT:    vand.vx v11, v8, a5
; RV64V-NEXT:    vand.vx v12, v8, a4
; RV64V-NEXT:    vsll.vi v11, v11, 24
; RV64V-NEXT:    vsll.vi v12, v12, 8
; RV64V-NEXT:    vand.vx v13, v8, a3
; RV64V-NEXT:    vsll.vx v8, v8, a1
; RV64V-NEXT:    vsll.vx v13, v13, a2
; RV64V-NEXT:    vor.vv v11, v11, v12
; RV64V-NEXT:    vor.vv v8, v8, v13
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vor.vv v8, v8, v11
; RV64V-NEXT:    vor.vv v8, v8, v9
; RV64V-NEXT:    lui a6, 61681
; RV64V-NEXT:    addi a6, a6, -241
; RV64V-NEXT:    vsrl.vi v9, v8, 4
; RV64V-NEXT:    slli a7, a6, 32
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    lui a7, 209715
; RV64V-NEXT:    addi a7, a7, 819
; RV64V-NEXT:    vsrl.vi v9, v8, 2
; RV64V-NEXT:    slli t0, a7, 32
; RV64V-NEXT:    add a7, a7, t0
; RV64V-NEXT:    vand.vx v8, v8, a7
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    lui t0, 349525
; RV64V-NEXT:    addi t0, t0, 1365
; RV64V-NEXT:    vsrl.vi v9, v8, 1
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    add t0, t0, t1
; RV64V-NEXT:    vand.vx v8, v8, t0
; RV64V-NEXT:    srli t1, a0, 24
; RV64V-NEXT:    and t1, t1, a5
; RV64V-NEXT:    srli t3, a0, 8
; RV64V-NEXT:    and t3, t3, a4
; RV64V-NEXT:    srli t4, a0, 40
; RV64V-NEXT:    and t4, t4, a3
; RV64V-NEXT:    srli t5, a0, 56
; RV64V-NEXT:    or t1, t3, t1
; RV64V-NEXT:    or t3, t4, t5
; RV64V-NEXT:    and t4, a0, a5
; RV64V-NEXT:    srliw t5, a0, 24
; RV64V-NEXT:    slli t4, t4, 24
; RV64V-NEXT:    slli t5, t5, 32
; RV64V-NEXT:    or t4, t4, t5
; RV64V-NEXT:    and t5, a0, a3
; RV64V-NEXT:    slli t5, t5, 40
; RV64V-NEXT:    slli a0, a0, 56
; RV64V-NEXT:    or a0, a0, t5
; RV64V-NEXT:    vand.vx v9, v9, t0
; RV64V-NEXT:    or t1, t1, t3
; RV64V-NEXT:    or a0, a0, t4
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    or a0, a0, t1
; RV64V-NEXT:    srli t1, a0, 4
; RV64V-NEXT:    and a0, a0, a6
; RV64V-NEXT:    and t1, t1, a6
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 2
; RV64V-NEXT:    and a0, a0, a7
; RV64V-NEXT:    and t1, t1, a7
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 1
; RV64V-NEXT:    and a0, a0, t0
; RV64V-NEXT:    and t1, t1, t0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    andi t1, a0, 2
; RV64V-NEXT:    vmul.vx v9, v8, t1
; RV64V-NEXT:    andi t1, a0, 1
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    andi t1, a0, 4
; RV64V-NEXT:    vmul.vx v11, v8, t1
; RV64V-NEXT:    vxor.vv v9, v10, v9
; RV64V-NEXT:    andi t1, a0, 8
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v11
; RV64V-NEXT:    andi t1, a0, 16
; RV64V-NEXT:    vmul.vx v11, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v10
; RV64V-NEXT:    andi t1, a0, 32
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v11
; RV64V-NEXT:    andi t1, a0, 64
; RV64V-NEXT:    vmul.vx v11, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v10
; RV64V-NEXT:    andi t1, a0, 128
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v11
; RV64V-NEXT:    andi t1, a0, 256
; RV64V-NEXT:    vmul.vx v11, v8, t1
; RV64V-NEXT:    vxor.vv v9, v9, v10
; RV64V-NEXT:    andi t1, a0, 512
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    vxor.vv v11, v9, v11
; RV64V-NEXT:    andi t1, a0, 1024
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    li t1, 1
; RV64V-NEXT:    vxor.vv v10, v11, v10
; RV64V-NEXT:    slli t3, t1, 11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v11, v8, t3
; RV64V-NEXT:    lui t3, 1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v12, v8, t3
; RV64V-NEXT:    lui t3, 2
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v11, v8, t3
; RV64V-NEXT:    lui t3, 4
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v12, v8, t3
; RV64V-NEXT:    lui t3, 8
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v11, v8, t3
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 32
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v11, v8, t2
; RV64V-NEXT:    lui t2, 64
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 128
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v11, v8, t2
; RV64V-NEXT:    lui t2, 256
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 512
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v11, v8, t2
; RV64V-NEXT:    lui t2, 1024
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 2048
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v11, v8, t2
; RV64V-NEXT:    lui t2, 4096
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 8192
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v11, v8, t2
; RV64V-NEXT:    lui t2, 16384
; RV64V-NEXT:    vxor.vv v12, v10, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    lui t2, 32768
; RV64V-NEXT:    vxor.vv v11, v12, v11
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 65536
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    lui t2, 131072
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    lui t2, 262144
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    srliw t2, a0, 31
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    slli t2, t2, 31
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 32
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 33
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 34
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 35
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 36
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 37
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 38
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 39
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 40
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 41
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v8, t2
; RV64V-NEXT:    slli t2, t1, 42
; RV64V-NEXT:    vxor.vv v13, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 43
; RV64V-NEXT:    vxor.vv v12, v13, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 44
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 45
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 46
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 47
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 48
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 49
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    slli t2, t1, 50
; RV64V-NEXT:    vxor.vv v14, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v8, t2
; RV64V-NEXT:    slli t2, t1, 51
; RV64V-NEXT:    vxor.vv v13, v14, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 52
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v8, t2
; RV64V-NEXT:    vsll.vx v9, v9, a1
; RV64V-NEXT:    slli t2, t1, 53
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    slli t2, t1, 54
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v8, t2
; RV64V-NEXT:    vsll.vx v10, v10, a2
; RV64V-NEXT:    slli t2, t1, 55
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    vor.vv v9, v9, v10
; RV64V-NEXT:    slli t2, t1, 56
; RV64V-NEXT:    vxor.vv v10, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v8, t2
; RV64V-NEXT:    vsrl.vi v15, v11, 8
; RV64V-NEXT:    slli t2, t1, 57
; RV64V-NEXT:    vxor.vv v10, v10, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    vand.vx v15, v15, a4
; RV64V-NEXT:    slli t2, t1, 58
; RV64V-NEXT:    vxor.vv v13, v10, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vsrl.vi v12, v12, 24
; RV64V-NEXT:    slli t2, t1, 59
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    vand.vx v12, v12, a5
; RV64V-NEXT:    slli t2, t1, 60
; RV64V-NEXT:    vxor.vv v13, v13, v16
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vor.vv v12, v15, v12
; RV64V-NEXT:    slli t2, t1, 61
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t1, t1, 62
; RV64V-NEXT:    vxor.vv v13, v13, v16
; RV64V-NEXT:    and t1, a0, t1
; RV64V-NEXT:    vmul.vx v15, v8, t1
; RV64V-NEXT:    vand.vx v11, v11, a5
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vsll.vi v11, v11, 24
; RV64V-NEXT:    srli a0, a0, 63
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    slli a0, a0, 63
; RV64V-NEXT:    vmul.vx v8, v8, a0
; RV64V-NEXT:    vand.vx v14, v10, a4
; RV64V-NEXT:    vsrl.vx v10, v10, a2
; RV64V-NEXT:    vsll.vi v14, v14, 8
; RV64V-NEXT:    vxor.vv v8, v13, v8
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vsrl.vx v8, v8, a1
; RV64V-NEXT:    vor.vv v11, v11, v14
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vor.vv v9, v9, v11
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 4
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 2
; RV64V-NEXT:    vand.vx v8, v8, a7
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v9, v8, 1
; RV64V-NEXT:    vand.vx v8, v8, t0
; RV64V-NEXT:    vand.vx v9, v9, t0
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v9, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i64_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    addi sp, sp, -16
; RV32ZVBC-NEXT:    sw a0, 8(sp)
; RV32ZVBC-NEXT:    sw a1, 12(sp)
; RV32ZVBC-NEXT:    addi a0, sp, 8
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32ZVBC-NEXT:    vlse64.v v9, (a0), zero
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v9
; RV32ZVBC-NEXT:    addi sp, sp, 16
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i64_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 1 x i64> poison, i64 %b, i128 0
  %vb = shufflevector <vscale x 1 x i64> %elt.head, <vscale x 1 x i64> poison, <vscale x 1 x i32> zeroinitializer
  %va.ext = zext <vscale x 1 x i64> %va to <vscale x 1 x i128>
  %vb.ext = zext <vscale x 1 x i64> %vb to <vscale x 1 x i128>
  %clmul = call <vscale x 1 x i128> @llvm.clmul.nxv1i128(<vscale x 1 x i128> %va.ext, <vscale x 1 x i128> %vb.ext)
  %res.ext = lshr <vscale x 1 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 1 x i128> %res.ext to <vscale x 1 x i64>
  ret <vscale x 1 x i64> %res
}

define <vscale x 2 x i64> @clmulh_nxv2i64_vv(<vscale x 2 x i64> %va, <vscale x 2 x i64> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv2i64_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li t0, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw t0, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li t1, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw t1, 228(sp)
; RV32V-NEXT:    li t2, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw t2, 220(sp)
; RV32V-NEXT:    li t3, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw t3, 212(sp)
; RV32V-NEXT:    li t4, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw t4, 204(sp)
; RV32V-NEXT:    li t5, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw t5, 196(sp)
; RV32V-NEXT:    li t6, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw t6, 188(sp)
; RV32V-NEXT:    li s0, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s0, 180(sp)
; RV32V-NEXT:    slli t0, t0, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    lui a0, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw a0, 164(sp)
; RV32V-NEXT:    lui a0, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw a0, 156(sp)
; RV32V-NEXT:    lui a0, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw a0, 148(sp)
; RV32V-NEXT:    lui a1, 8
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw a1, 140(sp)
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    lui s3, 16
; RV32V-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a0
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s3, 132(sp)
; RV32V-NEXT:    lui s4, 32
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s4, 124(sp)
; RV32V-NEXT:    vmv.v.x v14, a0
; RV32V-NEXT:    lui s5, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s5, 116(sp)
; RV32V-NEXT:    lui a0, 349525
; RV32V-NEXT:    lui s11, 128
; RV32V-NEXT:    addi a0, a0, 1365
; RV32V-NEXT:    vmv.v.x v16, a0
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s11, 108(sp)
; RV32V-NEXT:    lui ra, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw ra, 100(sp)
; RV32V-NEXT:    lui s8, 512
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vsetvli a0, zero, e64, m2, ta, ma
; RV32V-NEXT:    vsrl.vx v18, v10, a6
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s8, 92(sp)
; RV32V-NEXT:    lui a0, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw a0, 84(sp)
; RV32V-NEXT:    lui a5, 2048
; RV32V-NEXT:    li a1, 40
; RV32V-NEXT:    vsrl.vx v20, v10, a1
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw a5, 76(sp)
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a4, 68(sp)
; RV32V-NEXT:    lui s9, 8192
; RV32V-NEXT:    addi a2, s3, -256
; RV32V-NEXT:    vand.vx v20, v20, a2
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s9, 60(sp)
; RV32V-NEXT:    lui s10, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s10, 52(sp)
; RV32V-NEXT:    lui s6, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s6, 44(sp)
; RV32V-NEXT:    vor.vv v20, v20, v18
; RV32V-NEXT:    lui s7, 65536
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s7, 36(sp)
; RV32V-NEXT:    lui s2, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s2, 28(sp)
; RV32V-NEXT:    vsrl.vi v22, v10, 24
; RV32V-NEXT:    lui s1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s1, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    addi a7, sp, 272
; RV32V-NEXT:    vlse64.v v18, (a7), zero
; RV32V-NEXT:    vand.vx v22, v22, a3
; RV32V-NEXT:    vsrl.vi v24, v10, 8
; RV32V-NEXT:    vand.vv v24, v24, v18
; RV32V-NEXT:    vor.vv v22, v24, v22
; RV32V-NEXT:    vor.vv v20, v22, v20
; RV32V-NEXT:    vsll.vx v22, v10, a6
; RV32V-NEXT:    vand.vx v24, v10, a2
; RV32V-NEXT:    vsll.vx v24, v24, a1
; RV32V-NEXT:    vor.vv v22, v22, v24
; RV32V-NEXT:    vand.vx v24, v10, a3
; RV32V-NEXT:    vsll.vi v24, v24, 24
; RV32V-NEXT:    vand.vv v10, v10, v18
; RV32V-NEXT:    vsll.vi v10, v10, 8
; RV32V-NEXT:    vor.vv v10, v24, v10
; RV32V-NEXT:    vor.vv v10, v22, v10
; RV32V-NEXT:    vor.vv v10, v10, v20
; RV32V-NEXT:    vsrl.vi v20, v10, 4
; RV32V-NEXT:    vand.vv v20, v20, v12
; RV32V-NEXT:    vand.vv v10, v10, v12
; RV32V-NEXT:    vsll.vi v10, v10, 4
; RV32V-NEXT:    vor.vv v10, v20, v10
; RV32V-NEXT:    vsrl.vi v20, v10, 2
; RV32V-NEXT:    vand.vv v20, v20, v14
; RV32V-NEXT:    vand.vv v10, v10, v14
; RV32V-NEXT:    vsll.vi v10, v10, 2
; RV32V-NEXT:    vor.vv v10, v20, v10
; RV32V-NEXT:    vsrl.vi v20, v10, 1
; RV32V-NEXT:    vand.vv v20, v20, v16
; RV32V-NEXT:    vand.vv v10, v10, v16
; RV32V-NEXT:    vadd.vv v10, v10, v10
; RV32V-NEXT:    vor.vv v10, v20, v10
; RV32V-NEXT:    vand.vx v6, v10, t1
; RV32V-NEXT:    vand.vx v30, v10, t2
; RV32V-NEXT:    vand.vx v28, v10, t3
; RV32V-NEXT:    vand.vx v26, v10, t4
; RV32V-NEXT:    vand.vx v24, v10, t5
; RV32V-NEXT:    vand.vx v22, v10, t6
; RV32V-NEXT:    vand.vx v20, v10, s0
; RV32V-NEXT:    vsrl.vx v4, v8, a6
; RV32V-NEXT:    vsrl.vx v2, v8, a1
; RV32V-NEXT:    vand.vx v2, v2, a2
; RV32V-NEXT:    vor.vv v4, v2, v4
; RV32V-NEXT:    vsrl.vi v2, v8, 24
; RV32V-NEXT:    vand.vx v2, v2, a3
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    vand.vv v0, v0, v18
; RV32V-NEXT:    vor.vv v2, v0, v2
; RV32V-NEXT:    vor.vv v4, v2, v4
; RV32V-NEXT:    vsll.vx v2, v8, a6
; RV32V-NEXT:    vand.vx v0, v8, a2
; RV32V-NEXT:    vsll.vx v0, v0, a1
; RV32V-NEXT:    vor.vv v2, v2, v0
; RV32V-NEXT:    vand.vx v0, v8, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vand.vv v8, v8, v18
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vor.vv v8, v2, v8
; RV32V-NEXT:    vor.vv v8, v8, v4
; RV32V-NEXT:    vsrl.vi v4, v8, 4
; RV32V-NEXT:    vand.vv v4, v4, v12
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vsrl.vi v4, v8, 2
; RV32V-NEXT:    vand.vv v4, v4, v14
; RV32V-NEXT:    vand.vv v8, v8, v14
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vsrl.vi v4, v8, 1
; RV32V-NEXT:    vand.vv v4, v4, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vand.vi v4, v10, 1
; RV32V-NEXT:    vmul.vv v4, v8, v4
; RV32V-NEXT:    vand.vi v2, v10, 2
; RV32V-NEXT:    vxor.vi v4, v4, 0
; RV32V-NEXT:    vmul.vv v2, v8, v2
; RV32V-NEXT:    vand.vi v0, v10, 4
; RV32V-NEXT:    vmul.vv v0, v8, v0
; RV32V-NEXT:    vxor.vv v4, v4, v2
; RV32V-NEXT:    vxor.vv v2, v4, v0
; RV32V-NEXT:    vand.vi v4, v10, 8
; RV32V-NEXT:    vmul.vv v0, v8, v4
; RV32V-NEXT:    vand.vx v4, v10, t0
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v6
; RV32V-NEXT:    lui a7, 1
; RV32V-NEXT:    vand.vx v6, v10, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v30
; RV32V-NEXT:    lui a7, 2
; RV32V-NEXT:    vand.vx v30, v10, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v28
; RV32V-NEXT:    lui a7, 4
; RV32V-NEXT:    vand.vx v28, v10, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v26
; RV32V-NEXT:    lui a7, 8
; RV32V-NEXT:    vand.vx v26, v10, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v24
; RV32V-NEXT:    vand.vx v24, v10, s3
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v22
; RV32V-NEXT:    vand.vx v22, v10, s4
; RV32V-NEXT:    vxor.vv v0, v2, v0
; RV32V-NEXT:    vmul.vv v20, v8, v20
; RV32V-NEXT:    vand.vx v2, v10, s5
; RV32V-NEXT:    vxor.vv v20, v0, v20
; RV32V-NEXT:    vmul.vv v0, v8, v4
; RV32V-NEXT:    vand.vx v4, v10, s11
; RV32V-NEXT:    vxor.vv v0, v20, v0
; RV32V-NEXT:    vmul.vv v6, v8, v6
; RV32V-NEXT:    vand.vx v20, v10, ra
; RV32V-NEXT:    vxor.vv v6, v0, v6
; RV32V-NEXT:    vmul.vv v0, v8, v30
; RV32V-NEXT:    vand.vx v30, v10, s8
; RV32V-NEXT:    vxor.vv v6, v6, v0
; RV32V-NEXT:    vmul.vv v0, v8, v28
; RV32V-NEXT:    vand.vx v28, v10, a0
; RV32V-NEXT:    vxor.vv v6, v6, v0
; RV32V-NEXT:    vmul.vv v26, v8, v26
; RV32V-NEXT:    vand.vx v0, v10, a5
; RV32V-NEXT:    vxor.vv v26, v6, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vx v6, v10, a4
; RV32V-NEXT:    vxor.vv v24, v26, v24
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    vand.vx v26, v10, s9
; RV32V-NEXT:    vxor.vv v22, v24, v22
; RV32V-NEXT:    vmul.vv v24, v8, v2
; RV32V-NEXT:    vand.vx v2, v10, s10
; RV32V-NEXT:    vxor.vv v22, v22, v24
; RV32V-NEXT:    vmul.vv v24, v8, v4
; RV32V-NEXT:    vand.vx v4, v10, s6
; RV32V-NEXT:    vxor.vv v22, v22, v24
; RV32V-NEXT:    vand.vx v24, v10, s7
; RV32V-NEXT:    vmul.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v30, v8, v30
; RV32V-NEXT:    vxor.vv v20, v22, v20
; RV32V-NEXT:    vxor.vv v20, v20, v30
; RV32V-NEXT:    vmul.vv v22, v8, v28
; RV32V-NEXT:    vmul.vv v28, v8, v0
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v28
; RV32V-NEXT:    vmul.vv v22, v8, v6
; RV32V-NEXT:    vmul.vv v26, v8, v26
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v26
; RV32V-NEXT:    vmul.vv v22, v8, v2
; RV32V-NEXT:    vmul.vv v26, v8, v4
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v26
; RV32V-NEXT:    vmul.vv v22, v8, v24
; RV32V-NEXT:    vand.vx v24, v10, s2
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vx v24, v10, s1
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v10, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 16
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 8
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v10, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v10, v10, v22
; RV32V-NEXT:    vmul.vv v8, v8, v10
; RV32V-NEXT:    vxor.vv v10, v20, v24
; RV32V-NEXT:    vxor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v10, v8, 24
; RV32V-NEXT:    vand.vx v10, v10, a3
; RV32V-NEXT:    vand.vx v20, v8, a3
; RV32V-NEXT:    vsrl.vx v22, v8, a6
; RV32V-NEXT:    vsll.vx v24, v8, a6
; RV32V-NEXT:    vsrl.vx v26, v8, a1
; RV32V-NEXT:    vand.vx v26, v26, a2
; RV32V-NEXT:    vand.vx v28, v8, a2
; RV32V-NEXT:    vsll.vx v28, v28, a1
; RV32V-NEXT:    vor.vv v22, v26, v22
; RV32V-NEXT:    vsrl.vi v26, v8, 8
; RV32V-NEXT:    vand.vv v26, v26, v18
; RV32V-NEXT:    vor.vv v10, v26, v10
; RV32V-NEXT:    vor.vv v10, v10, v22
; RV32V-NEXT:    vand.vv v8, v8, v18
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vsll.vi v18, v20, 24
; RV32V-NEXT:    vor.vv v8, v18, v8
; RV32V-NEXT:    vor.vv v18, v24, v28
; RV32V-NEXT:    vor.vv v8, v18, v8
; RV32V-NEXT:    vor.vv v8, v8, v10
; RV32V-NEXT:    vsrl.vi v10, v8, 4
; RV32V-NEXT:    vand.vv v10, v10, v12
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v10, v8, 2
; RV32V-NEXT:    vand.vv v10, v10, v14
; RV32V-NEXT:    vand.vv v8, v8, v14
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v10, v8, 1
; RV32V-NEXT:    vand.vv v10, v10, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv2i64_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 56
; RV64V-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV64V-NEXT:    vsrl.vx v12, v8, a0
; RV64V-NEXT:    li a4, 40
; RV64V-NEXT:    vsrl.vx v14, v8, a4
; RV64V-NEXT:    lui t1, 16
; RV64V-NEXT:    addi a5, t1, -256
; RV64V-NEXT:    vand.vx v14, v14, a5
; RV64V-NEXT:    vor.vv v12, v14, v12
; RV64V-NEXT:    vsrl.vi v14, v8, 24
; RV64V-NEXT:    lui a6, 4080
; RV64V-NEXT:    vand.vx v14, v14, a6
; RV64V-NEXT:    vsrl.vi v16, v8, 8
; RV64V-NEXT:    li a7, 255
; RV64V-NEXT:    slli a7, a7, 24
; RV64V-NEXT:    vand.vx v16, v16, a7
; RV64V-NEXT:    vor.vv v14, v16, v14
; RV64V-NEXT:    vor.vv v12, v14, v12
; RV64V-NEXT:    vand.vx v14, v8, a6
; RV64V-NEXT:    vsll.vi v14, v14, 24
; RV64V-NEXT:    vand.vx v16, v8, a7
; RV64V-NEXT:    vsll.vi v16, v16, 8
; RV64V-NEXT:    vor.vv v14, v14, v16
; RV64V-NEXT:    vsll.vx v16, v8, a0
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vsll.vx v8, v8, a4
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vor.vv v8, v8, v14
; RV64V-NEXT:    vor.vv v8, v8, v12
; RV64V-NEXT:    vsrl.vi v12, v8, 4
; RV64V-NEXT:    lui a1, 61681
; RV64V-NEXT:    addi a1, a1, -241
; RV64V-NEXT:    slli a2, a1, 32
; RV64V-NEXT:    add a1, a1, a2
; RV64V-NEXT:    vand.vx v12, v12, a1
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 2
; RV64V-NEXT:    lui a2, 209715
; RV64V-NEXT:    addi a2, a2, 819
; RV64V-NEXT:    slli a3, a2, 32
; RV64V-NEXT:    add a2, a2, a3
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 1
; RV64V-NEXT:    lui a3, 349525
; RV64V-NEXT:    addi a3, a3, 1365
; RV64V-NEXT:    slli t0, a3, 32
; RV64V-NEXT:    add a3, a3, t0
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vx v12, v10, a0
; RV64V-NEXT:    vsrl.vx v14, v10, a4
; RV64V-NEXT:    vand.vx v14, v14, a5
; RV64V-NEXT:    vor.vv v12, v14, v12
; RV64V-NEXT:    vsrl.vi v14, v10, 24
; RV64V-NEXT:    vand.vx v14, v14, a6
; RV64V-NEXT:    vsrl.vi v16, v10, 8
; RV64V-NEXT:    vand.vx v16, v16, a7
; RV64V-NEXT:    vor.vv v14, v16, v14
; RV64V-NEXT:    vor.vv v12, v14, v12
; RV64V-NEXT:    vand.vx v14, v10, a6
; RV64V-NEXT:    vsll.vi v14, v14, 24
; RV64V-NEXT:    vand.vx v16, v10, a7
; RV64V-NEXT:    vsll.vi v16, v16, 8
; RV64V-NEXT:    vor.vv v14, v14, v16
; RV64V-NEXT:    vsll.vx v16, v10, a0
; RV64V-NEXT:    vand.vx v10, v10, a5
; RV64V-NEXT:    vsll.vx v10, v10, a4
; RV64V-NEXT:    vor.vv v10, v16, v10
; RV64V-NEXT:    vor.vv v10, v10, v14
; RV64V-NEXT:    vor.vv v10, v10, v12
; RV64V-NEXT:    vsrl.vi v12, v10, 4
; RV64V-NEXT:    vand.vx v12, v12, a1
; RV64V-NEXT:    vand.vx v10, v10, a1
; RV64V-NEXT:    vsll.vi v10, v10, 4
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vsrl.vi v12, v10, 2
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vand.vx v10, v10, a2
; RV64V-NEXT:    vsll.vi v10, v10, 2
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vsrl.vi v12, v10, 1
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vadd.vv v10, v10, v10
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vand.vi v12, v10, 2
; RV64V-NEXT:    vand.vi v14, v10, 1
; RV64V-NEXT:    vmul.vv v12, v8, v12
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    vxor.vv v12, v14, v12
; RV64V-NEXT:    vand.vi v14, v10, 4
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    vand.vi v16, v10, 8
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li t0, 16
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    li t0, 32
; RV64V-NEXT:    vand.vx v16, v10, t0
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    li t0, 64
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vmul.vv v14, v8, v14
; RV64V-NEXT:    li t0, 128
; RV64V-NEXT:    vand.vx v16, v10, t0
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v14, v12, v16
; RV64V-NEXT:    vsll.vx v12, v14, a0
; RV64V-NEXT:    li t0, 256
; RV64V-NEXT:    vand.vx v16, v10, t0
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    li t0, 512
; RV64V-NEXT:    vand.vx v18, v10, t0
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    li t0, 1024
; RV64V-NEXT:    vand.vx v16, v10, t0
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    li t0, 1
; RV64V-NEXT:    slli t2, t0, 11
; RV64V-NEXT:    vand.vx v18, v10, t2
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t2, 1
; RV64V-NEXT:    vand.vx v16, v10, t2
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t2, 2
; RV64V-NEXT:    vand.vx v18, v10, t2
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t2, 4
; RV64V-NEXT:    vand.vx v16, v10, t2
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t2, 8
; RV64V-NEXT:    vand.vx v18, v10, t2
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 32
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 64
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 128
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 256
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 512
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 1024
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 2048
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    vand.vx v16, v14, a5
; RV64V-NEXT:    vsll.vx v16, v16, a4
; RV64V-NEXT:    vor.vv v12, v12, v16
; RV64V-NEXT:    lui t1, 4096
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 8192
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 16384
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 32768
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 65536
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    lui t1, 131072
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    lui t1, 262144
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 31
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 33
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t1, t0, 34
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 35
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t1, t0, 36
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 37
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t1, t0, 38
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 39
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t1, t0, 40
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    slli t1, t0, 41
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    vxor.vv v16, v14, v16
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    slli t1, t0, 42
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    slli t1, t0, 43
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t1, t0, 44
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    slli t1, t0, 45
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t1, t0, 46
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    slli t1, t0, 47
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t1, t0, 48
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vmul.vv v18, v8, v18
; RV64V-NEXT:    slli t1, t0, 49
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vxor.vv v18, v16, v18
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    slli t1, t0, 50
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    slli t1, t0, 51
; RV64V-NEXT:    vand.vx v22, v10, t1
; RV64V-NEXT:    vmul.vv v22, v8, v22
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vxor.vv v18, v18, v22
; RV64V-NEXT:    slli t1, t0, 52
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    slli t1, t0, 53
; RV64V-NEXT:    vand.vx v22, v10, t1
; RV64V-NEXT:    vmul.vv v22, v8, v22
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vxor.vv v18, v18, v22
; RV64V-NEXT:    slli t1, t0, 54
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vand.vx v22, v14, a6
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    slli t1, t0, 55
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vsll.vi v22, v22, 24
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vand.vx v20, v18, a7
; RV64V-NEXT:    vsll.vi v20, v20, 8
; RV64V-NEXT:    vor.vv v20, v22, v20
; RV64V-NEXT:    vor.vv v12, v12, v20
; RV64V-NEXT:    vsrl.vi v14, v14, 8
; RV64V-NEXT:    vand.vx v14, v14, a7
; RV64V-NEXT:    vsrl.vi v16, v16, 24
; RV64V-NEXT:    vand.vx v16, v16, a6
; RV64V-NEXT:    vor.vv v14, v14, v16
; RV64V-NEXT:    vsrl.vx v16, v18, a4
; RV64V-NEXT:    vand.vx v16, v16, a5
; RV64V-NEXT:    slli a4, t0, 56
; RV64V-NEXT:    vand.vx v20, v10, a4
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    slli a4, t0, 57
; RV64V-NEXT:    vand.vx v22, v10, a4
; RV64V-NEXT:    vmul.vv v22, v8, v22
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vxor.vv v18, v18, v22
; RV64V-NEXT:    slli a4, t0, 58
; RV64V-NEXT:    vand.vx v20, v10, a4
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    slli a4, t0, 59
; RV64V-NEXT:    vand.vx v22, v10, a4
; RV64V-NEXT:    vmul.vv v22, v8, v22
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vxor.vv v18, v18, v22
; RV64V-NEXT:    slli a4, t0, 60
; RV64V-NEXT:    vand.vx v20, v10, a4
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    slli a4, t0, 61
; RV64V-NEXT:    vand.vx v22, v10, a4
; RV64V-NEXT:    vmul.vv v22, v8, v22
; RV64V-NEXT:    vxor.vv v18, v18, v20
; RV64V-NEXT:    vxor.vv v18, v18, v22
; RV64V-NEXT:    slli t0, t0, 62
; RV64V-NEXT:    vand.vx v20, v10, t0
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    li a4, -1
; RV64V-NEXT:    slli a4, a4, 63
; RV64V-NEXT:    vand.vx v10, v10, a4
; RV64V-NEXT:    vmul.vv v8, v8, v10
; RV64V-NEXT:    vxor.vv v10, v18, v20
; RV64V-NEXT:    vxor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vx v8, v8, a0
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vor.vv v8, v14, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 4
; RV64V-NEXT:    vand.vx v10, v10, a1
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 2
; RV64V-NEXT:    vand.vx v10, v10, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 1
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv2i64_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a0, zero, e64, m2, ta, ma
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v10
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv2i64_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a0, zero, e64, m2, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vv v8, v8, v10
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 2 x i64> %va to <vscale x 2 x i128>
  %vb.ext = zext <vscale x 2 x i64> %vb to <vscale x 2 x i128>
  %clmul = call <vscale x 2 x i128> @llvm.clmul.nxv2i128(<vscale x 2 x i128> %va.ext, <vscale x 2 x i128> %vb.ext)
  %res.ext = lshr <vscale x 2 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 2 x i128> %res.ext to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %res
}

define <vscale x 2 x i64> @clmulh_nxv2i64_vx(<vscale x 2 x i64> %va, i64 %b) nounwind {
; RV32V-LABEL: clmulh_nxv2i64_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw a0, 0(sp)
; RV32V-NEXT:    sw a1, 4(sp)
; RV32V-NEXT:    mv a0, sp
; RV32V-NEXT:    lui a1, 1044480
; RV32V-NEXT:    vsetvli a2, zero, e64, m2, ta, ma
; RV32V-NEXT:    vlse64.v v18, (a0), zero
; RV32V-NEXT:    sw a1, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li t0, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw t0, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li t1, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw t1, 228(sp)
; RV32V-NEXT:    li t2, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw t2, 220(sp)
; RV32V-NEXT:    li t3, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw t3, 212(sp)
; RV32V-NEXT:    li t4, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw t4, 204(sp)
; RV32V-NEXT:    li t5, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw t5, 196(sp)
; RV32V-NEXT:    li t6, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw t6, 188(sp)
; RV32V-NEXT:    li s0, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s0, 180(sp)
; RV32V-NEXT:    slli t0, t0, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t0, 172(sp)
; RV32V-NEXT:    lui a0, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw a0, 164(sp)
; RV32V-NEXT:    lui a0, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw a0, 156(sp)
; RV32V-NEXT:    lui a0, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw a0, 148(sp)
; RV32V-NEXT:    lui a1, 8
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw a1, 140(sp)
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    lui s3, 16
; RV32V-NEXT:    vsetvli a1, zero, e32, m2, ta, ma
; RV32V-NEXT:    vmv.v.x v10, a0
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s3, 132(sp)
; RV32V-NEXT:    lui s4, 32
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s4, 124(sp)
; RV32V-NEXT:    vmv.v.x v12, a0
; RV32V-NEXT:    lui s5, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s5, 116(sp)
; RV32V-NEXT:    lui a0, 349525
; RV32V-NEXT:    lui s11, 128
; RV32V-NEXT:    addi a0, a0, 1365
; RV32V-NEXT:    vmv.v.x v14, a0
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s11, 108(sp)
; RV32V-NEXT:    lui ra, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw ra, 100(sp)
; RV32V-NEXT:    lui s8, 512
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vsetvli a0, zero, e64, m2, ta, ma
; RV32V-NEXT:    vsrl.vx v16, v18, a6
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s8, 92(sp)
; RV32V-NEXT:    lui a0, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw a0, 84(sp)
; RV32V-NEXT:    lui a5, 2048
; RV32V-NEXT:    li a1, 40
; RV32V-NEXT:    vsrl.vx v20, v18, a1
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw a5, 76(sp)
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a4, 68(sp)
; RV32V-NEXT:    lui s9, 8192
; RV32V-NEXT:    addi a2, s3, -256
; RV32V-NEXT:    vand.vx v20, v20, a2
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s9, 60(sp)
; RV32V-NEXT:    lui s10, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s10, 52(sp)
; RV32V-NEXT:    lui s6, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s6, 44(sp)
; RV32V-NEXT:    vor.vv v20, v20, v16
; RV32V-NEXT:    lui s7, 65536
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s7, 36(sp)
; RV32V-NEXT:    lui s2, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s2, 28(sp)
; RV32V-NEXT:    vsrl.vi v22, v18, 24
; RV32V-NEXT:    lui s1, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s1, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    addi a7, sp, 272
; RV32V-NEXT:    vlse64.v v16, (a7), zero
; RV32V-NEXT:    vand.vx v22, v22, a3
; RV32V-NEXT:    vsrl.vi v24, v18, 8
; RV32V-NEXT:    vand.vv v24, v24, v16
; RV32V-NEXT:    vor.vv v22, v24, v22
; RV32V-NEXT:    vor.vv v20, v22, v20
; RV32V-NEXT:    vand.vv v22, v18, v16
; RV32V-NEXT:    vsll.vi v22, v22, 8
; RV32V-NEXT:    vand.vx v24, v18, a3
; RV32V-NEXT:    vsll.vi v24, v24, 24
; RV32V-NEXT:    vor.vv v22, v24, v22
; RV32V-NEXT:    vsll.vx v24, v18, a6
; RV32V-NEXT:    vand.vx v18, v18, a2
; RV32V-NEXT:    vsll.vx v18, v18, a1
; RV32V-NEXT:    vor.vv v18, v24, v18
; RV32V-NEXT:    vor.vv v18, v18, v22
; RV32V-NEXT:    vor.vv v18, v18, v20
; RV32V-NEXT:    vsrl.vi v20, v18, 4
; RV32V-NEXT:    vand.vv v20, v20, v10
; RV32V-NEXT:    vand.vv v18, v18, v10
; RV32V-NEXT:    vsll.vi v18, v18, 4
; RV32V-NEXT:    vor.vv v18, v20, v18
; RV32V-NEXT:    vsrl.vi v20, v18, 2
; RV32V-NEXT:    vand.vv v20, v20, v12
; RV32V-NEXT:    vand.vv v18, v18, v12
; RV32V-NEXT:    vsll.vi v18, v18, 2
; RV32V-NEXT:    vor.vv v18, v20, v18
; RV32V-NEXT:    vsrl.vi v20, v18, 1
; RV32V-NEXT:    vand.vv v20, v20, v14
; RV32V-NEXT:    vand.vv v18, v18, v14
; RV32V-NEXT:    vadd.vv v18, v18, v18
; RV32V-NEXT:    vor.vv v18, v20, v18
; RV32V-NEXT:    vand.vx v6, v18, t1
; RV32V-NEXT:    vand.vx v30, v18, t2
; RV32V-NEXT:    vand.vx v28, v18, t3
; RV32V-NEXT:    vand.vx v26, v18, t4
; RV32V-NEXT:    vand.vx v24, v18, t5
; RV32V-NEXT:    vand.vx v22, v18, t6
; RV32V-NEXT:    vand.vx v20, v18, s0
; RV32V-NEXT:    vsrl.vx v4, v8, a6
; RV32V-NEXT:    vsrl.vx v2, v8, a1
; RV32V-NEXT:    vand.vx v2, v2, a2
; RV32V-NEXT:    vor.vv v4, v2, v4
; RV32V-NEXT:    vsrl.vi v2, v8, 24
; RV32V-NEXT:    vand.vx v2, v2, a3
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    vand.vv v0, v0, v16
; RV32V-NEXT:    vor.vv v2, v0, v2
; RV32V-NEXT:    vor.vv v4, v2, v4
; RV32V-NEXT:    vsll.vx v2, v8, a6
; RV32V-NEXT:    vand.vx v0, v8, a2
; RV32V-NEXT:    vsll.vx v0, v0, a1
; RV32V-NEXT:    vor.vv v2, v2, v0
; RV32V-NEXT:    vand.vx v0, v8, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vor.vv v8, v2, v8
; RV32V-NEXT:    vor.vv v8, v8, v4
; RV32V-NEXT:    vsrl.vi v4, v8, 4
; RV32V-NEXT:    vand.vv v4, v4, v10
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vsrl.vi v4, v8, 2
; RV32V-NEXT:    vand.vv v4, v4, v12
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vsrl.vi v4, v8, 1
; RV32V-NEXT:    vand.vv v4, v4, v14
; RV32V-NEXT:    vand.vv v8, v8, v14
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v4, v8
; RV32V-NEXT:    vand.vi v4, v18, 1
; RV32V-NEXT:    vmul.vv v4, v8, v4
; RV32V-NEXT:    vand.vi v2, v18, 2
; RV32V-NEXT:    vxor.vi v4, v4, 0
; RV32V-NEXT:    vmul.vv v2, v8, v2
; RV32V-NEXT:    vand.vi v0, v18, 4
; RV32V-NEXT:    vmul.vv v0, v8, v0
; RV32V-NEXT:    vxor.vv v4, v4, v2
; RV32V-NEXT:    vxor.vv v2, v4, v0
; RV32V-NEXT:    vand.vi v4, v18, 8
; RV32V-NEXT:    vmul.vv v0, v8, v4
; RV32V-NEXT:    vand.vx v4, v18, t0
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v6
; RV32V-NEXT:    lui a7, 1
; RV32V-NEXT:    vand.vx v6, v18, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v30
; RV32V-NEXT:    lui a7, 2
; RV32V-NEXT:    vand.vx v30, v18, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v28
; RV32V-NEXT:    lui a7, 4
; RV32V-NEXT:    vand.vx v28, v18, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v26
; RV32V-NEXT:    lui a7, 8
; RV32V-NEXT:    vand.vx v26, v18, a7
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v24
; RV32V-NEXT:    vand.vx v24, v18, s3
; RV32V-NEXT:    vxor.vv v2, v2, v0
; RV32V-NEXT:    vmul.vv v0, v8, v22
; RV32V-NEXT:    vand.vx v22, v18, s4
; RV32V-NEXT:    vxor.vv v0, v2, v0
; RV32V-NEXT:    vmul.vv v20, v8, v20
; RV32V-NEXT:    vand.vx v2, v18, s5
; RV32V-NEXT:    vxor.vv v20, v0, v20
; RV32V-NEXT:    vmul.vv v0, v8, v4
; RV32V-NEXT:    vand.vx v4, v18, s11
; RV32V-NEXT:    vxor.vv v0, v20, v0
; RV32V-NEXT:    vmul.vv v6, v8, v6
; RV32V-NEXT:    vand.vx v20, v18, ra
; RV32V-NEXT:    vxor.vv v6, v0, v6
; RV32V-NEXT:    vmul.vv v0, v8, v30
; RV32V-NEXT:    vand.vx v30, v18, s8
; RV32V-NEXT:    vxor.vv v6, v6, v0
; RV32V-NEXT:    vmul.vv v0, v8, v28
; RV32V-NEXT:    vand.vx v28, v18, a0
; RV32V-NEXT:    vxor.vv v6, v6, v0
; RV32V-NEXT:    vmul.vv v26, v8, v26
; RV32V-NEXT:    vand.vx v0, v18, a5
; RV32V-NEXT:    vxor.vv v26, v6, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vx v6, v18, a4
; RV32V-NEXT:    vxor.vv v24, v26, v24
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    vand.vx v26, v18, s9
; RV32V-NEXT:    vxor.vv v22, v24, v22
; RV32V-NEXT:    vmul.vv v24, v8, v2
; RV32V-NEXT:    vand.vx v2, v18, s10
; RV32V-NEXT:    vxor.vv v22, v22, v24
; RV32V-NEXT:    vmul.vv v24, v8, v4
; RV32V-NEXT:    vand.vx v4, v18, s6
; RV32V-NEXT:    vxor.vv v22, v22, v24
; RV32V-NEXT:    vand.vx v24, v18, s7
; RV32V-NEXT:    vmul.vv v20, v8, v20
; RV32V-NEXT:    vmul.vv v30, v8, v30
; RV32V-NEXT:    vxor.vv v20, v22, v20
; RV32V-NEXT:    vxor.vv v20, v20, v30
; RV32V-NEXT:    vmul.vv v22, v8, v28
; RV32V-NEXT:    vmul.vv v28, v8, v0
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v28
; RV32V-NEXT:    vmul.vv v22, v8, v6
; RV32V-NEXT:    vmul.vv v26, v8, v26
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v26
; RV32V-NEXT:    vmul.vv v22, v8, v2
; RV32V-NEXT:    vmul.vv v26, v8, v4
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v26
; RV32V-NEXT:    vmul.vv v22, v8, v24
; RV32V-NEXT:    vand.vx v24, v18, s2
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vx v24, v18, s1
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v22, v18, v22
; RV32V-NEXT:    vmul.vv v22, v8, v22
; RV32V-NEXT:    addi a0, sp, 16
; RV32V-NEXT:    vlse64.v v26, (a0), zero
; RV32V-NEXT:    vxor.vv v20, v20, v24
; RV32V-NEXT:    vxor.vv v20, v20, v22
; RV32V-NEXT:    addi a0, sp, 8
; RV32V-NEXT:    vlse64.v v22, (a0), zero
; RV32V-NEXT:    vand.vv v24, v18, v26
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v18, v18, v22
; RV32V-NEXT:    vmul.vv v8, v8, v18
; RV32V-NEXT:    vxor.vv v18, v20, v24
; RV32V-NEXT:    vxor.vv v8, v18, v8
; RV32V-NEXT:    vsrl.vi v18, v8, 24
; RV32V-NEXT:    vand.vx v18, v18, a3
; RV32V-NEXT:    vand.vx v20, v8, a3
; RV32V-NEXT:    vsrl.vx v22, v8, a6
; RV32V-NEXT:    vsll.vx v24, v8, a6
; RV32V-NEXT:    vsrl.vx v26, v8, a1
; RV32V-NEXT:    vand.vx v26, v26, a2
; RV32V-NEXT:    vand.vx v28, v8, a2
; RV32V-NEXT:    vsll.vx v28, v28, a1
; RV32V-NEXT:    vor.vv v22, v26, v22
; RV32V-NEXT:    vsrl.vi v26, v8, 8
; RV32V-NEXT:    vand.vv v26, v26, v16
; RV32V-NEXT:    vor.vv v18, v26, v18
; RV32V-NEXT:    vor.vv v18, v18, v22
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vsll.vi v16, v20, 24
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vor.vv v16, v24, v28
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vor.vv v8, v8, v18
; RV32V-NEXT:    vsrl.vi v16, v8, 4
; RV32V-NEXT:    vand.vv v16, v16, v10
; RV32V-NEXT:    vand.vv v8, v8, v10
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v10, v8, 2
; RV32V-NEXT:    vand.vv v10, v10, v12
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v10, v8, 1
; RV32V-NEXT:    vand.vv v10, v10, v14
; RV32V-NEXT:    vand.vv v8, v8, v14
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v10, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv2i64_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a1, 56
; RV64V-NEXT:    vsetvli a2, zero, e64, m2, ta, ma
; RV64V-NEXT:    vsrl.vx v10, v8, a1
; RV64V-NEXT:    li a5, 40
; RV64V-NEXT:    vsrl.vx v12, v8, a5
; RV64V-NEXT:    lui t2, 16
; RV64V-NEXT:    addi a6, t2, -256
; RV64V-NEXT:    vand.vx v12, v12, a6
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vsrl.vi v12, v8, 24
; RV64V-NEXT:    lui a7, 4080
; RV64V-NEXT:    vand.vx v12, v12, a7
; RV64V-NEXT:    vsrl.vi v14, v8, 8
; RV64V-NEXT:    li t0, 255
; RV64V-NEXT:    slli t0, t0, 24
; RV64V-NEXT:    vand.vx v14, v14, t0
; RV64V-NEXT:    vor.vv v12, v14, v12
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vand.vx v12, v8, a7
; RV64V-NEXT:    vsll.vi v12, v12, 24
; RV64V-NEXT:    vand.vx v14, v8, t0
; RV64V-NEXT:    vsll.vi v14, v14, 8
; RV64V-NEXT:    vor.vv v12, v12, v14
; RV64V-NEXT:    vsll.vx v14, v8, a1
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vsll.vx v8, v8, a5
; RV64V-NEXT:    vor.vv v8, v14, v8
; RV64V-NEXT:    vor.vv v8, v8, v12
; RV64V-NEXT:    vor.vv v8, v8, v10
; RV64V-NEXT:    vsrl.vi v10, v8, 4
; RV64V-NEXT:    lui a2, 61681
; RV64V-NEXT:    addi a2, a2, -241
; RV64V-NEXT:    slli a3, a2, 32
; RV64V-NEXT:    add a2, a2, a3
; RV64V-NEXT:    vand.vx v10, v10, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 2
; RV64V-NEXT:    lui a3, 209715
; RV64V-NEXT:    addi a3, a3, 819
; RV64V-NEXT:    slli a4, a3, 32
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v10, v10, v8
; RV64V-NEXT:    vsrl.vi v8, v10, 1
; RV64V-NEXT:    lui a4, 349525
; RV64V-NEXT:    addi a4, a4, 1365
; RV64V-NEXT:    slli t1, a4, 32
; RV64V-NEXT:    add a4, a4, t1
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vand.vx v10, v10, a4
; RV64V-NEXT:    srli t1, a0, 24
; RV64V-NEXT:    srli t3, a0, 8
; RV64V-NEXT:    and t1, t1, a7
; RV64V-NEXT:    and t3, t3, t0
; RV64V-NEXT:    or t1, t3, t1
; RV64V-NEXT:    srli t3, a0, 40
; RV64V-NEXT:    and t3, t3, a6
; RV64V-NEXT:    srli t4, a0, 56
; RV64V-NEXT:    or t3, t3, t4
; RV64V-NEXT:    and t4, a0, a7
; RV64V-NEXT:    slli t4, t4, 24
; RV64V-NEXT:    srliw t5, a0, 24
; RV64V-NEXT:    slli t5, t5, 32
; RV64V-NEXT:    and t6, a0, a6
; RV64V-NEXT:    slli t6, t6, 40
; RV64V-NEXT:    slli a0, a0, 56
; RV64V-NEXT:    or t4, t4, t5
; RV64V-NEXT:    or a0, a0, t6
; RV64V-NEXT:    or t1, t1, t3
; RV64V-NEXT:    or a0, a0, t4
; RV64V-NEXT:    vadd.vv v10, v10, v10
; RV64V-NEXT:    or a0, a0, t1
; RV64V-NEXT:    srli t1, a0, 4
; RV64V-NEXT:    and a0, a0, a2
; RV64V-NEXT:    and t1, t1, a2
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 2
; RV64V-NEXT:    and a0, a0, a3
; RV64V-NEXT:    and t1, t1, a3
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    vor.vv v8, v8, v10
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 1
; RV64V-NEXT:    and a0, a0, a4
; RV64V-NEXT:    and t1, t1, a4
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    andi t1, a0, 2
; RV64V-NEXT:    vmul.vx v10, v8, t1
; RV64V-NEXT:    andi t1, a0, 1
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    vxor.vv v10, v12, v10
; RV64V-NEXT:    andi t1, a0, 4
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    andi t1, a0, 8
; RV64V-NEXT:    vmul.vx v14, v8, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vxor.vv v10, v10, v14
; RV64V-NEXT:    andi t1, a0, 16
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    andi t1, a0, 32
; RV64V-NEXT:    vmul.vx v14, v8, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vxor.vv v10, v10, v14
; RV64V-NEXT:    andi t1, a0, 64
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    andi t1, a0, 128
; RV64V-NEXT:    vmul.vx v14, v8, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    vxor.vv v12, v10, v14
; RV64V-NEXT:    vsll.vx v10, v12, a1
; RV64V-NEXT:    andi t1, a0, 256
; RV64V-NEXT:    vmul.vx v14, v8, t1
; RV64V-NEXT:    andi t1, a0, 512
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    andi t1, a0, 1024
; RV64V-NEXT:    vmul.vx v14, v8, t1
; RV64V-NEXT:    li t1, 1
; RV64V-NEXT:    slli t3, t1, 11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v16, v8, t3
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t3, 1
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v14, v8, t3
; RV64V-NEXT:    lui t3, 2
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v16, v8, t3
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t3, 4
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v14, v8, t3
; RV64V-NEXT:    lui t3, 8
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v16, v8, t3
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 64
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 128
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 256
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 512
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 1024
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 2048
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    vand.vx v14, v12, a6
; RV64V-NEXT:    vsll.vx v14, v14, a5
; RV64V-NEXT:    vor.vv v10, v10, v14
; RV64V-NEXT:    lui t2, 4096
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 8192
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 16384
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 32768
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 65536
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    lui t2, 131072
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    lui t2, 262144
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    srliw t2, a0, 31
; RV64V-NEXT:    slli t2, t2, 31
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    slli t2, t1, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 33
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    slli t2, t1, 34
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 35
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    slli t2, t1, 36
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 37
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    slli t2, t1, 38
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 39
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    slli t2, t1, 40
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v8, t2
; RV64V-NEXT:    slli t2, t1, 41
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v14, v12, v14
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    slli t2, t1, 42
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    slli t2, t1, 43
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t2, t1, 44
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    slli t2, t1, 45
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t2, t1, 46
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    slli t2, t1, 47
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vxor.vv v14, v14, v18
; RV64V-NEXT:    slli t2, t1, 48
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    slli t2, t1, 49
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vxor.vv v16, v14, v16
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    slli t2, t1, 50
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    slli t2, t1, 51
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t2, t1, 52
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    slli t2, t1, 53
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t2, t1, 54
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vand.vx v20, v12, a7
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    slli t2, t1, 55
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v18, v8, t2
; RV64V-NEXT:    vsll.vi v20, v20, 24
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vand.vx v18, v16, t0
; RV64V-NEXT:    vsll.vi v18, v18, 8
; RV64V-NEXT:    vor.vv v18, v20, v18
; RV64V-NEXT:    vor.vv v10, v10, v18
; RV64V-NEXT:    vsrl.vi v12, v12, 8
; RV64V-NEXT:    vand.vx v12, v12, t0
; RV64V-NEXT:    vsrl.vi v14, v14, 24
; RV64V-NEXT:    vand.vx v14, v14, a7
; RV64V-NEXT:    vor.vv v12, v12, v14
; RV64V-NEXT:    vsrl.vx v14, v16, a5
; RV64V-NEXT:    vand.vx v14, v14, a6
; RV64V-NEXT:    slli a5, t1, 56
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v18, v8, a5
; RV64V-NEXT:    slli a5, t1, 57
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v20, v8, a5
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli a5, t1, 58
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v18, v8, a5
; RV64V-NEXT:    slli a5, t1, 59
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v20, v8, a5
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli a5, t1, 60
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v18, v8, a5
; RV64V-NEXT:    slli a5, t1, 61
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v20, v8, a5
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    slli t1, t1, 62
; RV64V-NEXT:    and a5, a0, t1
; RV64V-NEXT:    vmul.vx v18, v8, a5
; RV64V-NEXT:    srli a0, a0, 63
; RV64V-NEXT:    slli a0, a0, 63
; RV64V-NEXT:    vmul.vx v8, v8, a0
; RV64V-NEXT:    vxor.vv v16, v16, v18
; RV64V-NEXT:    vxor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vx v8, v8, a1
; RV64V-NEXT:    vor.vv v8, v14, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 4
; RV64V-NEXT:    vand.vx v10, v10, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 2
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v10, v8, 1
; RV64V-NEXT:    vand.vx v10, v10, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v10, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv2i64_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    addi sp, sp, -16
; RV32ZVBC-NEXT:    sw a0, 8(sp)
; RV32ZVBC-NEXT:    sw a1, 12(sp)
; RV32ZVBC-NEXT:    addi a0, sp, 8
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV32ZVBC-NEXT:    vlse64.v v10, (a0), zero
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v10
; RV32ZVBC-NEXT:    addi sp, sp, 16
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv2i64_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m2, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 2 x i64> poison, i64 %b, i128 0
  %vb = shufflevector <vscale x 2 x i64> %elt.head, <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
  %va.ext = zext <vscale x 2 x i64> %va to <vscale x 2 x i128>
  %vb.ext = zext <vscale x 2 x i64> %vb to <vscale x 2 x i128>
  %clmul = call <vscale x 2 x i128> @llvm.clmul.nxv2i128(<vscale x 2 x i128> %va.ext, <vscale x 2 x i128> %vb.ext)
  %res.ext = lshr <vscale x 2 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 2 x i128> %res.ext to <vscale x 2 x i64>
  ret <vscale x 2 x i64> %res
}

define <vscale x 4 x i64> @clmulh_nxv4i64_vv(<vscale x 4 x i64> %va, <vscale x 4 x i64> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv4i64_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -352
; RV32V-NEXT:    sw ra, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    sub sp, sp, a0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs4r.v v12, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    li s1, 1
; RV32V-NEXT:    li a1, 2
; RV32V-NEXT:    li a5, 56
; RV32V-NEXT:    li a2, 4
; RV32V-NEXT:    vsetvli a4, zero, e64, m4, ta, ma
; RV32V-NEXT:    vsrl.vx v16, v8, a5
; RV32V-NEXT:    li t0, 56
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s1, 260(sp)
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a1, 252(sp)
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a2, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsrl.vx v20, v8, a0
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a1, 228(sp)
; RV32V-NEXT:    li t6, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw t6, 220(sp)
; RV32V-NEXT:    li a0, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a0, 212(sp)
; RV32V-NEXT:    li a0, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a0, 204(sp)
; RV32V-NEXT:    lui a4, 16
; RV32V-NEXT:    li t1, 256
; RV32V-NEXT:    addi a2, a4, -256
; RV32V-NEXT:    vand.vx v20, v20, a2
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw t1, 196(sp)
; RV32V-NEXT:    li s2, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s2, 188(sp)
; RV32V-NEXT:    li s0, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s0, 180(sp)
; RV32V-NEXT:    slli s1, s1, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s1, 172(sp)
; RV32V-NEXT:    lui t2, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t2, 164(sp)
; RV32V-NEXT:    lui t3, 2
; RV32V-NEXT:    vor.vv v28, v20, v16
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t3, 156(sp)
; RV32V-NEXT:    lui t4, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw t4, 148(sp)
; RV32V-NEXT:    lui t5, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw t5, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw a4, 132(sp)
; RV32V-NEXT:    lui a1, 32
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw a1, 124(sp)
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    lui s3, 64
; RV32V-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV32V-NEXT:    vmv.v.x v20, a0
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s3, 116(sp)
; RV32V-NEXT:    lui s7, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s7, 108(sp)
; RV32V-NEXT:    lui s4, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s4, 100(sp)
; RV32V-NEXT:    lui s5, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw s5, 92(sp)
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    lui s8, 1024
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s8, 84(sp)
; RV32V-NEXT:    lui a0, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw a0, 76(sp)
; RV32V-NEXT:    lui s11, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s11, 68(sp)
; RV32V-NEXT:    lui ra, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw ra, 60(sp)
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw s9, 52(sp)
; RV32V-NEXT:    lui s10, 32768
; RV32V-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV32V-NEXT:    vsrl.vi v4, v8, 24
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw s10, 44(sp)
; RV32V-NEXT:    lui a5, 65536
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a5, 36(sp)
; RV32V-NEXT:    lui a1, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a1, 28(sp)
; RV32V-NEXT:    lui s6, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s6, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    addi a6, sp, 272
; RV32V-NEXT:    vlse64.v v12, (a6), zero
; RV32V-NEXT:    vand.vx v4, v4, a3
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    vand.vv v0, v0, v12
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vor.vv v28, v4, v28
; RV32V-NEXT:    vsll.vx v4, v8, t0
; RV32V-NEXT:    vand.vx v0, v8, a2
; RV32V-NEXT:    vsll.vx v0, v0, a7
; RV32V-NEXT:    vor.vv v4, v4, v0
; RV32V-NEXT:    vand.vx v0, v8, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vand.vv v8, v8, v12
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v0, v0, v8
; RV32V-NEXT:    lui a6, 349525
; RV32V-NEXT:    addi a6, a6, 1365
; RV32V-NEXT:    vsetvli a7, zero, e32, m4, ta, ma
; RV32V-NEXT:    vmv.v.x v8, a6
; RV32V-NEXT:    vsetvli a6, zero, e64, m4, ta, ma
; RV32V-NEXT:    vor.vv v4, v4, v0
; RV32V-NEXT:    vor.vv v28, v4, v28
; RV32V-NEXT:    vsrl.vi v4, v28, 4
; RV32V-NEXT:    vand.vv v4, v4, v20
; RV32V-NEXT:    vand.vv v28, v28, v20
; RV32V-NEXT:    vsll.vi v28, v28, 4
; RV32V-NEXT:    vor.vv v28, v4, v28
; RV32V-NEXT:    vsrl.vi v4, v28, 2
; RV32V-NEXT:    vand.vv v4, v4, v24
; RV32V-NEXT:    vand.vv v28, v28, v24
; RV32V-NEXT:    vsll.vi v28, v28, 2
; RV32V-NEXT:    vor.vv v28, v4, v28
; RV32V-NEXT:    vsrl.vi v4, v28, 1
; RV32V-NEXT:    sw a0, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    mv a6, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs4r.v v8, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v4, v4, v8
; RV32V-NEXT:    vand.vv v28, v28, v8
; RV32V-NEXT:    vadd.vv v28, v28, v28
; RV32V-NEXT:    vor.vv v28, v4, v28
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl4r.v v8, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vsrl.vx v4, v8, a6
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    vsrl.vx v0, v8, a7
; RV32V-NEXT:    vand.vx v0, v0, a2
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vsrl.vi v0, v8, 24
; RV32V-NEXT:    vand.vx v0, v0, a3
; RV32V-NEXT:    vsrl.vi v16, v8, 8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs4r.v v12, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v16, v16, v12
; RV32V-NEXT:    vor.vv v16, v16, v0
; RV32V-NEXT:    vor.vv v16, v16, v4
; RV32V-NEXT:    vand.vx v4, v8, a2
; RV32V-NEXT:    vsll.vx v4, v4, a7
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    vsll.vx v0, v8, a6
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vand.vx v0, v8, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vand.vv v12, v8, v12
; RV32V-NEXT:    vsll.vi v12, v12, 8
; RV32V-NEXT:    vor.vv v12, v0, v12
; RV32V-NEXT:    vor.vv v12, v4, v12
; RV32V-NEXT:    vor.vv v12, v12, v16
; RV32V-NEXT:    vsrl.vi v16, v12, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs4r.v v20, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v16, v16, v20
; RV32V-NEXT:    vand.vv v12, v12, v20
; RV32V-NEXT:    vsll.vi v12, v12, 4
; RV32V-NEXT:    vor.vv v12, v16, v12
; RV32V-NEXT:    vsrl.vi v16, v12, 2
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v12, v12, v24
; RV32V-NEXT:    vmv4r.v v20, v24
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v12, v16, v12
; RV32V-NEXT:    vsrl.vi v16, v12, 1
; RV32V-NEXT:    csrr t0, vlenb
; RV32V-NEXT:    slli t0, t0, 2
; RV32V-NEXT:    mv a0, t0
; RV32V-NEXT:    slli t0, t0, 1
; RV32V-NEXT:    add t0, t0, a0
; RV32V-NEXT:    lw a0, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add t0, sp, t0
; RV32V-NEXT:    addi t0, t0, 288
; RV32V-NEXT:    vl4r.v v8, (t0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v8
; RV32V-NEXT:    vand.vv v12, v12, v8
; RV32V-NEXT:    vadd.vv v12, v12, v12
; RV32V-NEXT:    vor.vv v12, v16, v12
; RV32V-NEXT:    vand.vi v16, v12, 2
; RV32V-NEXT:    vand.vi v4, v12, 1
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    vxor.vi v4, v4, 0
; RV32V-NEXT:    vxor.vv v16, v4, v16
; RV32V-NEXT:    vand.vi v4, v12, 4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vi v0, v12, 8
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    li t0, 16
; RV32V-NEXT:    vand.vx v4, v12, t0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, t6
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    li t0, 64
; RV32V-NEXT:    vand.vx v4, v12, t0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    li t0, 128
; RV32V-NEXT:    vand.vx v0, v12, t0
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, t1
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, s2
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, s1
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, t2
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, t3
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, t4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, t5
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, a4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    lui a4, 32
; RV32V-NEXT:    vand.vx v0, v12, a4
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s3
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, s7
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, s5
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s8
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, a0
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s11
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, ra
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, s9
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, s10
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vand.vx v4, v12, a5
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vx v0, v12, a1
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v4, (a0), zero
; RV32V-NEXT:    vand.vx v0, v12, s6
; RV32V-NEXT:    vmul.vv v0, v28, v0
; RV32V-NEXT:    vand.vv v4, v12, v4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v8, (a0), zero
; RV32V-NEXT:    vxor.vv v16, v16, v0
; RV32V-NEXT:    vxor.vv v16, v16, v4
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v4, (a0), zero
; RV32V-NEXT:    vand.vv v8, v12, v8
; RV32V-NEXT:    vmul.vv v8, v28, v8
; RV32V-NEXT:    vand.vv v4, v12, v4
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v16, v8
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vmul.vv v4, v28, v4
; RV32V-NEXT:    vand.vv v16, v12, v16
; RV32V-NEXT:    addi a0, sp, 16
; RV32V-NEXT:    vmul.vv v16, v28, v16
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    addi a0, sp, 8
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    vand.vv v4, v12, v0
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vmul.vv v16, v28, v4
; RV32V-NEXT:    vmul.vv v12, v28, v12
; RV32V-NEXT:    vxor.vv v8, v8, v16
; RV32V-NEXT:    vxor.vv v12, v8, v12
; RV32V-NEXT:    vsrl.vx v8, v12, a6
; RV32V-NEXT:    vsrl.vx v16, v12, a7
; RV32V-NEXT:    vand.vx v16, v16, a2
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v12, 24
; RV32V-NEXT:    vand.vx v16, v16, a3
; RV32V-NEXT:    vsrl.vi v28, v12, 8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl4r.v v24, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v28, v28, v24
; RV32V-NEXT:    vor.vv v16, v28, v16
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vand.vx v16, v12, a3
; RV32V-NEXT:    vand.vv v24, v12, v24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vsll.vi v16, v16, 24
; RV32V-NEXT:    vor.vv v16, v16, v24
; RV32V-NEXT:    vsll.vx v24, v12, a6
; RV32V-NEXT:    vand.vx v12, v12, a2
; RV32V-NEXT:    vsll.vx v12, v12, a7
; RV32V-NEXT:    vor.vv v12, v24, v12
; RV32V-NEXT:    vor.vv v12, v12, v16
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl4r.v v16, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 2
; RV32V-NEXT:    vand.vv v12, v12, v20
; RV32V-NEXT:    vand.vv v8, v8, v20
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl4r.v v16, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 352
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv4i64_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 56
; RV64V-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV64V-NEXT:    vsrl.vx v16, v8, a0
; RV64V-NEXT:    li a4, 40
; RV64V-NEXT:    vsrl.vx v20, v8, a4
; RV64V-NEXT:    lui t1, 16
; RV64V-NEXT:    addi a5, t1, -256
; RV64V-NEXT:    vand.vx v20, v20, a5
; RV64V-NEXT:    vor.vv v16, v20, v16
; RV64V-NEXT:    vsrl.vi v20, v8, 24
; RV64V-NEXT:    lui a6, 4080
; RV64V-NEXT:    vand.vx v20, v20, a6
; RV64V-NEXT:    vsrl.vi v24, v8, 8
; RV64V-NEXT:    li a7, 255
; RV64V-NEXT:    slli a7, a7, 24
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vor.vv v20, v24, v20
; RV64V-NEXT:    vor.vv v16, v20, v16
; RV64V-NEXT:    vand.vx v20, v8, a6
; RV64V-NEXT:    vsll.vi v20, v20, 24
; RV64V-NEXT:    vand.vx v24, v8, a7
; RV64V-NEXT:    vsll.vi v24, v24, 8
; RV64V-NEXT:    vor.vv v20, v20, v24
; RV64V-NEXT:    vsll.vx v24, v8, a0
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vsll.vx v8, v8, a4
; RV64V-NEXT:    vor.vv v8, v24, v8
; RV64V-NEXT:    vor.vv v8, v8, v20
; RV64V-NEXT:    vor.vv v8, v8, v16
; RV64V-NEXT:    vsrl.vi v16, v8, 4
; RV64V-NEXT:    lui a1, 61681
; RV64V-NEXT:    addi a1, a1, -241
; RV64V-NEXT:    slli a2, a1, 32
; RV64V-NEXT:    add a1, a1, a2
; RV64V-NEXT:    vand.vx v16, v16, a1
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 2
; RV64V-NEXT:    lui a2, 209715
; RV64V-NEXT:    addi a2, a2, 819
; RV64V-NEXT:    slli a3, a2, 32
; RV64V-NEXT:    add a2, a2, a3
; RV64V-NEXT:    vand.vx v16, v16, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 1
; RV64V-NEXT:    lui a3, 349525
; RV64V-NEXT:    addi a3, a3, 1365
; RV64V-NEXT:    slli t0, a3, 32
; RV64V-NEXT:    add a3, a3, t0
; RV64V-NEXT:    vand.vx v16, v16, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vx v16, v12, a0
; RV64V-NEXT:    vsrl.vx v20, v12, a4
; RV64V-NEXT:    vand.vx v20, v20, a5
; RV64V-NEXT:    vor.vv v16, v20, v16
; RV64V-NEXT:    vsrl.vi v20, v12, 24
; RV64V-NEXT:    vand.vx v20, v20, a6
; RV64V-NEXT:    vsrl.vi v24, v12, 8
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vor.vv v20, v24, v20
; RV64V-NEXT:    vor.vv v16, v20, v16
; RV64V-NEXT:    vand.vx v20, v12, a6
; RV64V-NEXT:    vsll.vi v20, v20, 24
; RV64V-NEXT:    vand.vx v24, v12, a7
; RV64V-NEXT:    vsll.vi v24, v24, 8
; RV64V-NEXT:    vor.vv v20, v20, v24
; RV64V-NEXT:    vsll.vx v24, v12, a0
; RV64V-NEXT:    vand.vx v12, v12, a5
; RV64V-NEXT:    vsll.vx v12, v12, a4
; RV64V-NEXT:    vor.vv v12, v24, v12
; RV64V-NEXT:    vor.vv v12, v12, v20
; RV64V-NEXT:    vor.vv v12, v12, v16
; RV64V-NEXT:    vsrl.vi v16, v12, 4
; RV64V-NEXT:    vand.vx v16, v16, a1
; RV64V-NEXT:    vand.vx v12, v12, a1
; RV64V-NEXT:    vsll.vi v12, v12, 4
; RV64V-NEXT:    vor.vv v12, v16, v12
; RV64V-NEXT:    vsrl.vi v16, v12, 2
; RV64V-NEXT:    vand.vx v16, v16, a2
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vsll.vi v12, v12, 2
; RV64V-NEXT:    vor.vv v12, v16, v12
; RV64V-NEXT:    vsrl.vi v16, v12, 1
; RV64V-NEXT:    vand.vx v16, v16, a3
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vadd.vv v12, v12, v12
; RV64V-NEXT:    vor.vv v12, v16, v12
; RV64V-NEXT:    vand.vi v16, v12, 2
; RV64V-NEXT:    vand.vi v20, v12, 1
; RV64V-NEXT:    vmul.vv v16, v8, v16
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vxor.vv v16, v20, v16
; RV64V-NEXT:    vand.vi v20, v12, 4
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    vand.vi v24, v12, 8
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li t0, 16
; RV64V-NEXT:    vand.vx v20, v12, t0
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    li t0, 32
; RV64V-NEXT:    vand.vx v24, v12, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    li t0, 64
; RV64V-NEXT:    vand.vx v20, v12, t0
; RV64V-NEXT:    vmul.vv v20, v8, v20
; RV64V-NEXT:    li t0, 128
; RV64V-NEXT:    vand.vx v24, v12, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v20, v16, v24
; RV64V-NEXT:    vsll.vx v16, v20, a0
; RV64V-NEXT:    li t0, 256
; RV64V-NEXT:    vand.vx v24, v12, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    li t0, 512
; RV64V-NEXT:    vand.vx v28, v12, t0
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    li t0, 1024
; RV64V-NEXT:    vand.vx v24, v12, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    li t0, 1
; RV64V-NEXT:    slli t2, t0, 11
; RV64V-NEXT:    vand.vx v28, v12, t2
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t2, 1
; RV64V-NEXT:    vand.vx v24, v12, t2
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t2, 2
; RV64V-NEXT:    vand.vx v28, v12, t2
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t2, 4
; RV64V-NEXT:    vand.vx v24, v12, t2
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t2, 8
; RV64V-NEXT:    vand.vx v28, v12, t2
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 32
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 64
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 128
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 256
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 512
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 1024
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 2048
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    vand.vx v24, v20, a5
; RV64V-NEXT:    vsll.vx v24, v24, a4
; RV64V-NEXT:    vor.vv v16, v16, v24
; RV64V-NEXT:    lui t1, 4096
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 8192
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 16384
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 32768
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 65536
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    lui t1, 131072
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    lui t1, 262144
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 31
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 33
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t1, t0, 34
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 35
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t1, t0, 36
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 37
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t1, t0, 38
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 39
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t1, t0, 40
; RV64V-NEXT:    vand.vx v24, v12, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    slli t1, t0, 41
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    vxor.vv v24, v20, v24
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    slli t1, t0, 42
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    slli t1, t0, 43
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t1, t0, 44
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    slli t1, t0, 45
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t1, t0, 46
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    slli t1, t0, 47
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t1, t0, 48
; RV64V-NEXT:    vand.vx v28, v12, t1
; RV64V-NEXT:    vmul.vv v28, v8, v28
; RV64V-NEXT:    slli t1, t0, 49
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vxor.vv v28, v24, v28
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    slli t1, t0, 50
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    slli t1, t0, 51
; RV64V-NEXT:    vand.vx v0, v12, t1
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vxor.vv v28, v28, v0
; RV64V-NEXT:    slli t1, t0, 52
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    slli t1, t0, 53
; RV64V-NEXT:    vand.vx v0, v12, t1
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vxor.vv v28, v28, v0
; RV64V-NEXT:    slli t1, t0, 54
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vand.vx v0, v20, a6
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    slli t1, t0, 55
; RV64V-NEXT:    vand.vx v4, v12, t1
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    vsll.vi v0, v0, 24
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vand.vx v4, v28, a7
; RV64V-NEXT:    vsll.vi v4, v4, 8
; RV64V-NEXT:    vor.vv v4, v0, v4
; RV64V-NEXT:    vor.vv v16, v16, v4
; RV64V-NEXT:    vsrl.vi v20, v20, 8
; RV64V-NEXT:    vand.vx v20, v20, a7
; RV64V-NEXT:    vsrl.vi v24, v24, 24
; RV64V-NEXT:    vand.vx v24, v24, a6
; RV64V-NEXT:    vor.vv v20, v20, v24
; RV64V-NEXT:    vsrl.vx v24, v28, a4
; RV64V-NEXT:    vand.vx v24, v24, a5
; RV64V-NEXT:    slli a4, t0, 56
; RV64V-NEXT:    vand.vx v4, v12, a4
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    slli a4, t0, 57
; RV64V-NEXT:    vand.vx v0, v12, a4
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vxor.vv v28, v28, v0
; RV64V-NEXT:    slli a4, t0, 58
; RV64V-NEXT:    vand.vx v4, v12, a4
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    slli a4, t0, 59
; RV64V-NEXT:    vand.vx v0, v12, a4
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vxor.vv v28, v28, v0
; RV64V-NEXT:    slli a4, t0, 60
; RV64V-NEXT:    vand.vx v4, v12, a4
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    slli a4, t0, 61
; RV64V-NEXT:    vand.vx v0, v12, a4
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v28, v28, v4
; RV64V-NEXT:    vxor.vv v28, v28, v0
; RV64V-NEXT:    slli t0, t0, 62
; RV64V-NEXT:    vand.vx v4, v12, t0
; RV64V-NEXT:    vmul.vv v4, v8, v4
; RV64V-NEXT:    li a4, -1
; RV64V-NEXT:    slli a4, a4, 63
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vmul.vv v8, v8, v12
; RV64V-NEXT:    vxor.vv v12, v28, v4
; RV64V-NEXT:    vxor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vx v8, v8, a0
; RV64V-NEXT:    vor.vv v8, v24, v8
; RV64V-NEXT:    vor.vv v8, v20, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 4
; RV64V-NEXT:    vand.vx v12, v12, a1
; RV64V-NEXT:    vand.vx v8, v8, a1
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 2
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 1
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv4i64_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a0, zero, e64, m4, ta, ma
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v12
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv4i64_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a0, zero, e64, m4, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vv v8, v8, v12
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 4 x i64> %va to <vscale x 4 x i128>
  %vb.ext = zext <vscale x 4 x i64> %vb to <vscale x 4 x i128>
  %clmul = call <vscale x 4 x i128> @llvm.clmul.nxv4i128(<vscale x 4 x i128> %va.ext, <vscale x 4 x i128> %vb.ext)
  %res.ext = lshr <vscale x 4 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 4 x i128> %res.ext to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %res
}

define <vscale x 4 x i64> @clmulh_nxv4i64_vx(<vscale x 4 x i64> %va, i64 %b) nounwind {
; RV32V-LABEL: clmulh_nxv4i64_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -368
; RV32V-NEXT:    sw ra, 364(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 360(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 356(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 352(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a2, vlenb
; RV32V-NEXT:    slli a2, a2, 4
; RV32V-NEXT:    sub sp, sp, a2
; RV32V-NEXT:    addi a2, sp, 16
; RV32V-NEXT:    sw a0, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV32V-NEXT:    vlse64.v v28, (a2), zero
; RV32V-NEXT:    sw a0, 288(sp)
; RV32V-NEXT:    sw zero, 292(sp)
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    li a0, 56
; RV32V-NEXT:    vsrl.vx v12, v8, a0
; RV32V-NEXT:    li t0, 56
; RV32V-NEXT:    sw a3, 280(sp)
; RV32V-NEXT:    sw zero, 284(sp)
; RV32V-NEXT:    li s1, 1
; RV32V-NEXT:    sw zero, 272(sp)
; RV32V-NEXT:    sw s1, 276(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 264(sp)
; RV32V-NEXT:    sw a0, 268(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw a0, 260(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    vsrl.vx v16, v8, a0
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a1, 244(sp)
; RV32V-NEXT:    li t6, 32
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw t6, 236(sp)
; RV32V-NEXT:    li a0, 64
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a0, 228(sp)
; RV32V-NEXT:    li a0, 128
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a0, 220(sp)
; RV32V-NEXT:    lui a4, 16
; RV32V-NEXT:    li t1, 256
; RV32V-NEXT:    addi a2, a4, -256
; RV32V-NEXT:    vand.vx v16, v16, a2
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw t1, 212(sp)
; RV32V-NEXT:    li s2, 512
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw s2, 204(sp)
; RV32V-NEXT:    li s0, 1024
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw s0, 196(sp)
; RV32V-NEXT:    slli s1, s1, 11
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s1, 188(sp)
; RV32V-NEXT:    lui t2, 1
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw t2, 180(sp)
; RV32V-NEXT:    lui t3, 2
; RV32V-NEXT:    vor.vv v24, v16, v12
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw t3, 172(sp)
; RV32V-NEXT:    lui t4, 4
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw t4, 164(sp)
; RV32V-NEXT:    lui t5, 8
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw t5, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw a4, 148(sp)
; RV32V-NEXT:    lui a1, 32
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw a1, 140(sp)
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    lui s3, 64
; RV32V-NEXT:    vsetvli a1, zero, e32, m4, ta, ma
; RV32V-NEXT:    vmv.v.x v20, a0
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s3, 132(sp)
; RV32V-NEXT:    lui s6, 128
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s6, 124(sp)
; RV32V-NEXT:    lui s4, 256
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s4, 116(sp)
; RV32V-NEXT:    lui s5, 512
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s5, 108(sp)
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    lui s8, 1024
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    vmv.v.x v16, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs4r.v v16, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s8, 100(sp)
; RV32V-NEXT:    lui a0, 2048
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw a0, 92(sp)
; RV32V-NEXT:    lui s11, 4096
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw s11, 84(sp)
; RV32V-NEXT:    lui ra, 8192
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw ra, 76(sp)
; RV32V-NEXT:    lui s9, 16384
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw s9, 68(sp)
; RV32V-NEXT:    lui s10, 32768
; RV32V-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV32V-NEXT:    vsrl.vi v4, v8, 24
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw s10, 60(sp)
; RV32V-NEXT:    lui a5, 65536
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a5, 52(sp)
; RV32V-NEXT:    lui a1, 131072
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a1, 44(sp)
; RV32V-NEXT:    lui s7, 262144
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw s7, 36(sp)
; RV32V-NEXT:    sw a3, 28(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    addi a6, sp, 288
; RV32V-NEXT:    vlse64.v v16, (a6), zero
; RV32V-NEXT:    vand.vx v4, v4, a3
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    vand.vv v0, v0, v16
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vor.vv v24, v4, v24
; RV32V-NEXT:    vsll.vx v4, v8, t0
; RV32V-NEXT:    vand.vx v0, v8, a2
; RV32V-NEXT:    vsll.vx v0, v0, a7
; RV32V-NEXT:    vor.vv v4, v4, v0
; RV32V-NEXT:    vand.vx v0, v8, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v0, v0, v8
; RV32V-NEXT:    lui a6, 349525
; RV32V-NEXT:    addi a6, a6, 1365
; RV32V-NEXT:    vsetvli a7, zero, e32, m4, ta, ma
; RV32V-NEXT:    vmv.v.x v8, a6
; RV32V-NEXT:    vsetvli a6, zero, e64, m4, ta, ma
; RV32V-NEXT:    vor.vv v4, v4, v0
; RV32V-NEXT:    vor.vv v24, v4, v24
; RV32V-NEXT:    vsrl.vi v4, v24, 4
; RV32V-NEXT:    vand.vv v4, v4, v20
; RV32V-NEXT:    vand.vv v24, v24, v20
; RV32V-NEXT:    vsll.vi v24, v24, 4
; RV32V-NEXT:    vor.vv v24, v4, v24
; RV32V-NEXT:    vsrl.vi v4, v24, 2
; RV32V-NEXT:    sw a0, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    mv a6, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl4r.v v12, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v4, v4, v12
; RV32V-NEXT:    vand.vv v24, v24, v12
; RV32V-NEXT:    vsll.vi v24, v24, 2
; RV32V-NEXT:    vor.vv v24, v4, v24
; RV32V-NEXT:    vsrl.vi v4, v24, 1
; RV32V-NEXT:    vand.vv v4, v4, v8
; RV32V-NEXT:    vand.vv v24, v24, v8
; RV32V-NEXT:    vadd.vv v24, v24, v24
; RV32V-NEXT:    vor.vv v24, v4, v24
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vsrl.vx v4, v28, a6
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    vsrl.vx v0, v28, a7
; RV32V-NEXT:    vand.vx v0, v0, a2
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vsrl.vi v0, v28, 24
; RV32V-NEXT:    vand.vx v0, v0, a3
; RV32V-NEXT:    vsrl.vi v12, v28, 8
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs4r.v v16, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vor.vv v12, v12, v0
; RV32V-NEXT:    vor.vv v12, v12, v4
; RV32V-NEXT:    vand.vv v4, v28, v16
; RV32V-NEXT:    vsll.vi v4, v4, 8
; RV32V-NEXT:    vand.vx v0, v28, a3
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vor.vv v4, v0, v4
; RV32V-NEXT:    vsll.vx v0, v28, a6
; RV32V-NEXT:    li a6, 56
; RV32V-NEXT:    vand.vx v28, v28, a2
; RV32V-NEXT:    vsll.vx v28, v28, a7
; RV32V-NEXT:    li a7, 40
; RV32V-NEXT:    vor.vv v28, v0, v28
; RV32V-NEXT:    vor.vv v28, v28, v4
; RV32V-NEXT:    vor.vv v12, v28, v12
; RV32V-NEXT:    vsrl.vi v28, v12, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs4r.v v20, (a0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v28, v28, v20
; RV32V-NEXT:    vand.vv v12, v12, v20
; RV32V-NEXT:    vsll.vi v12, v12, 4
; RV32V-NEXT:    vor.vv v12, v28, v12
; RV32V-NEXT:    vsrl.vi v28, v12, 2
; RV32V-NEXT:    csrr t0, vlenb
; RV32V-NEXT:    slli t0, t0, 2
; RV32V-NEXT:    mv a0, t0
; RV32V-NEXT:    slli t0, t0, 1
; RV32V-NEXT:    add t0, t0, a0
; RV32V-NEXT:    lw a0, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add t0, sp, t0
; RV32V-NEXT:    addi t0, t0, 304
; RV32V-NEXT:    vl4r.v v20, (t0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v28, v28, v20
; RV32V-NEXT:    vand.vv v12, v12, v20
; RV32V-NEXT:    vmv4r.v v16, v20
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v12, v28, v12
; RV32V-NEXT:    vsrl.vi v28, v12, 1
; RV32V-NEXT:    csrr t0, vlenb
; RV32V-NEXT:    slli t0, t0, 2
; RV32V-NEXT:    add t0, sp, t0
; RV32V-NEXT:    addi t0, t0, 304
; RV32V-NEXT:    vs4r.v v8, (t0) # vscale x 32-byte Folded Spill
; RV32V-NEXT:    vand.vv v28, v28, v8
; RV32V-NEXT:    vand.vv v12, v12, v8
; RV32V-NEXT:    vadd.vv v12, v12, v12
; RV32V-NEXT:    vor.vv v28, v28, v12
; RV32V-NEXT:    vand.vi v12, v28, 2
; RV32V-NEXT:    vand.vi v4, v28, 1
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    vxor.vi v4, v4, 0
; RV32V-NEXT:    vxor.vv v12, v4, v12
; RV32V-NEXT:    vand.vi v4, v28, 4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vi v0, v28, 8
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    li t0, 16
; RV32V-NEXT:    vand.vx v4, v28, t0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, t6
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    li t0, 64
; RV32V-NEXT:    vand.vx v4, v28, t0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    li t0, 128
; RV32V-NEXT:    vand.vx v0, v28, t0
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, t1
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, s2
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, s1
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, t2
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, t3
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, t4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, t5
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, a4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    lui a4, 32
; RV32V-NEXT:    vand.vx v0, v28, a4
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s3
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, s6
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, s5
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s8
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, a0
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s11
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, ra
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, s9
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, s10
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vand.vx v4, v28, a5
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vx v0, v28, a1
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    addi a0, sp, 280
; RV32V-NEXT:    vlse64.v v4, (a0), zero
; RV32V-NEXT:    vand.vx v0, v28, s7
; RV32V-NEXT:    vmul.vv v0, v24, v0
; RV32V-NEXT:    vand.vv v4, v28, v4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    addi a0, sp, 272
; RV32V-NEXT:    vlse64.v v8, (a0), zero
; RV32V-NEXT:    vxor.vv v12, v12, v0
; RV32V-NEXT:    vxor.vv v12, v12, v4
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v4, (a0), zero
; RV32V-NEXT:    vand.vv v8, v28, v8
; RV32V-NEXT:    vmul.vv v8, v24, v8
; RV32V-NEXT:    vand.vv v4, v28, v4
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v12, v8
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vmul.vv v4, v24, v4
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    vxor.vv v8, v8, v4
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vxor.vv v8, v8, v12
; RV32V-NEXT:    vlse64.v v12, (a0), zero
; RV32V-NEXT:    vand.vv v4, v28, v0
; RV32V-NEXT:    vand.vv v12, v28, v12
; RV32V-NEXT:    vmul.vv v28, v24, v4
; RV32V-NEXT:    vmul.vv v12, v24, v12
; RV32V-NEXT:    vxor.vv v8, v8, v28
; RV32V-NEXT:    vxor.vv v24, v8, v12
; RV32V-NEXT:    vsrl.vx v8, v24, a6
; RV32V-NEXT:    vsrl.vx v12, v24, a7
; RV32V-NEXT:    vand.vx v12, v12, a2
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v24, 24
; RV32V-NEXT:    vand.vx v12, v12, a3
; RV32V-NEXT:    vsrl.vi v28, v24, 8
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl4r.v v20, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v28, v28, v20
; RV32V-NEXT:    vor.vv v12, v28, v12
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vand.vx v12, v24, a3
; RV32V-NEXT:    vand.vv v20, v24, v20
; RV32V-NEXT:    vsll.vi v20, v20, 8
; RV32V-NEXT:    vsll.vi v12, v12, 24
; RV32V-NEXT:    vor.vv v12, v12, v20
; RV32V-NEXT:    vsll.vx v20, v24, a6
; RV32V-NEXT:    vand.vx v24, v24, a2
; RV32V-NEXT:    vsll.vx v24, v24, a7
; RV32V-NEXT:    vor.vv v20, v20, v24
; RV32V-NEXT:    vor.vv v12, v20, v12
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl4r.v v20, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v12, v12, v20
; RV32V-NEXT:    vand.vv v8, v8, v20
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 2
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v12, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl4r.v v16, (a0) # vscale x 32-byte Folded Reload
; RV32V-NEXT:    vand.vv v12, v12, v16
; RV32V-NEXT:    vand.vv v8, v8, v16
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v12, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 364(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 360(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 356(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 352(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 368
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv4i64_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a1, 56
; RV64V-NEXT:    vsetvli a2, zero, e64, m4, ta, ma
; RV64V-NEXT:    vsrl.vx v12, v8, a1
; RV64V-NEXT:    li a5, 40
; RV64V-NEXT:    vsrl.vx v16, v8, a5
; RV64V-NEXT:    lui t2, 16
; RV64V-NEXT:    addi a6, t2, -256
; RV64V-NEXT:    vand.vx v16, v16, a6
; RV64V-NEXT:    vor.vv v12, v16, v12
; RV64V-NEXT:    vsrl.vi v16, v8, 24
; RV64V-NEXT:    lui a7, 4080
; RV64V-NEXT:    vand.vx v16, v16, a7
; RV64V-NEXT:    vsrl.vi v20, v8, 8
; RV64V-NEXT:    li t0, 255
; RV64V-NEXT:    slli t0, t0, 24
; RV64V-NEXT:    vand.vx v20, v20, t0
; RV64V-NEXT:    vor.vv v16, v20, v16
; RV64V-NEXT:    vor.vv v12, v16, v12
; RV64V-NEXT:    vand.vx v16, v8, a7
; RV64V-NEXT:    vsll.vi v16, v16, 24
; RV64V-NEXT:    vand.vx v20, v8, t0
; RV64V-NEXT:    vsll.vi v20, v20, 8
; RV64V-NEXT:    vor.vv v16, v16, v20
; RV64V-NEXT:    vsll.vx v20, v8, a1
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vsll.vx v8, v8, a5
; RV64V-NEXT:    vor.vv v8, v20, v8
; RV64V-NEXT:    vor.vv v8, v8, v16
; RV64V-NEXT:    vor.vv v8, v8, v12
; RV64V-NEXT:    vsrl.vi v12, v8, 4
; RV64V-NEXT:    lui a2, 61681
; RV64V-NEXT:    addi a2, a2, -241
; RV64V-NEXT:    slli a3, a2, 32
; RV64V-NEXT:    add a2, a2, a3
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 2
; RV64V-NEXT:    lui a3, 209715
; RV64V-NEXT:    addi a3, a3, 819
; RV64V-NEXT:    slli a4, a3, 32
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v12, v12, v8
; RV64V-NEXT:    vsrl.vi v8, v12, 1
; RV64V-NEXT:    lui a4, 349525
; RV64V-NEXT:    addi a4, a4, 1365
; RV64V-NEXT:    slli t1, a4, 32
; RV64V-NEXT:    add a4, a4, t1
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vadd.vv v12, v12, v12
; RV64V-NEXT:    srli t1, a0, 24
; RV64V-NEXT:    srli t3, a0, 8
; RV64V-NEXT:    and t1, t1, a7
; RV64V-NEXT:    and t3, t3, t0
; RV64V-NEXT:    or t1, t3, t1
; RV64V-NEXT:    srli t3, a0, 40
; RV64V-NEXT:    and t3, t3, a6
; RV64V-NEXT:    srli t4, a0, 56
; RV64V-NEXT:    or t3, t3, t4
; RV64V-NEXT:    and t4, a0, a7
; RV64V-NEXT:    slli t4, t4, 24
; RV64V-NEXT:    srliw t5, a0, 24
; RV64V-NEXT:    slli t5, t5, 32
; RV64V-NEXT:    and t6, a0, a6
; RV64V-NEXT:    slli t6, t6, 40
; RV64V-NEXT:    slli a0, a0, 56
; RV64V-NEXT:    or t4, t4, t5
; RV64V-NEXT:    or a0, a0, t6
; RV64V-NEXT:    or t1, t1, t3
; RV64V-NEXT:    or a0, a0, t4
; RV64V-NEXT:    or a0, a0, t1
; RV64V-NEXT:    srli t1, a0, 4
; RV64V-NEXT:    and a0, a0, a2
; RV64V-NEXT:    and t1, t1, a2
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    vor.vv v8, v8, v12
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 2
; RV64V-NEXT:    and a0, a0, a3
; RV64V-NEXT:    and t1, t1, a3
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 1
; RV64V-NEXT:    and a0, a0, a4
; RV64V-NEXT:    and t1, t1, a4
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    andi t1, a0, 2
; RV64V-NEXT:    vmul.vx v12, v8, t1
; RV64V-NEXT:    andi t1, a0, 1
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    vxor.vv v12, v16, v12
; RV64V-NEXT:    andi t1, a0, 4
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    andi t1, a0, 8
; RV64V-NEXT:    vmul.vx v20, v8, t1
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    vxor.vv v12, v12, v20
; RV64V-NEXT:    andi t1, a0, 16
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    andi t1, a0, 32
; RV64V-NEXT:    vmul.vx v20, v8, t1
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    vxor.vv v12, v12, v20
; RV64V-NEXT:    andi t1, a0, 64
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    andi t1, a0, 128
; RV64V-NEXT:    vmul.vx v20, v8, t1
; RV64V-NEXT:    vxor.vv v12, v12, v16
; RV64V-NEXT:    vxor.vv v16, v12, v20
; RV64V-NEXT:    vsll.vx v12, v16, a1
; RV64V-NEXT:    andi t1, a0, 256
; RV64V-NEXT:    vmul.vx v20, v8, t1
; RV64V-NEXT:    andi t1, a0, 512
; RV64V-NEXT:    vmul.vx v24, v8, t1
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    andi t1, a0, 1024
; RV64V-NEXT:    vmul.vx v20, v8, t1
; RV64V-NEXT:    li t1, 1
; RV64V-NEXT:    slli t3, t1, 11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v24, v8, t3
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t3, 1
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v20, v8, t3
; RV64V-NEXT:    lui t3, 2
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v24, v8, t3
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t3, 4
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v20, v8, t3
; RV64V-NEXT:    lui t3, 8
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v24, v8, t3
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 64
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 128
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 256
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 512
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 1024
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 2048
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vand.vx v20, v16, a6
; RV64V-NEXT:    vsll.vx v20, v20, a5
; RV64V-NEXT:    vor.vv v12, v12, v20
; RV64V-NEXT:    lui t2, 4096
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 8192
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 16384
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 32768
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 65536
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    lui t2, 131072
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    lui t2, 262144
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    srliw t2, a0, 31
; RV64V-NEXT:    slli t2, t2, 31
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    slli t2, t1, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    slli t2, t1, 33
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    slli t2, t1, 34
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    slli t2, t1, 35
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    slli t2, t1, 36
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    slli t2, t1, 37
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    slli t2, t1, 38
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    slli t2, t1, 39
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v20
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    slli t2, t1, 40
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v20, v8, t2
; RV64V-NEXT:    slli t2, t1, 41
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    vxor.vv v20, v16, v20
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    slli t2, t1, 42
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 43
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t2, t1, 44
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 45
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t2, t1, 46
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 47
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vxor.vv v20, v20, v24
; RV64V-NEXT:    vxor.vv v20, v20, v28
; RV64V-NEXT:    slli t2, t1, 48
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 49
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vxor.vv v24, v20, v24
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    slli t2, t1, 50
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    slli t2, t1, 51
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v4, v8, t2
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t2, t1, 52
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    slli t2, t1, 53
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v4, v8, t2
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t2, t1, 54
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vand.vx v4, v16, a7
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    slli t2, t1, 55
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v28, v8, t2
; RV64V-NEXT:    vsll.vi v4, v4, 24
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vand.vx v28, v24, t0
; RV64V-NEXT:    vsll.vi v28, v28, 8
; RV64V-NEXT:    vor.vv v28, v4, v28
; RV64V-NEXT:    vor.vv v12, v12, v28
; RV64V-NEXT:    vsrl.vi v16, v16, 8
; RV64V-NEXT:    vand.vx v16, v16, t0
; RV64V-NEXT:    vsrl.vi v20, v20, 24
; RV64V-NEXT:    vand.vx v20, v20, a7
; RV64V-NEXT:    vor.vv v16, v16, v20
; RV64V-NEXT:    vsrl.vx v20, v24, a5
; RV64V-NEXT:    vand.vx v20, v20, a6
; RV64V-NEXT:    slli a5, t1, 56
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v28, v8, a5
; RV64V-NEXT:    slli a5, t1, 57
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v4, v8, a5
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli a5, t1, 58
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v28, v8, a5
; RV64V-NEXT:    slli a5, t1, 59
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v4, v8, a5
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli a5, t1, 60
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v28, v8, a5
; RV64V-NEXT:    slli a5, t1, 61
; RV64V-NEXT:    and a5, a0, a5
; RV64V-NEXT:    vmul.vx v4, v8, a5
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v24, v24, v4
; RV64V-NEXT:    slli t1, t1, 62
; RV64V-NEXT:    and a5, a0, t1
; RV64V-NEXT:    vmul.vx v28, v8, a5
; RV64V-NEXT:    srli a0, a0, 63
; RV64V-NEXT:    slli a0, a0, 63
; RV64V-NEXT:    vmul.vx v8, v8, a0
; RV64V-NEXT:    vxor.vv v24, v24, v28
; RV64V-NEXT:    vxor.vv v8, v24, v8
; RV64V-NEXT:    vsrl.vx v8, v8, a1
; RV64V-NEXT:    vor.vv v8, v20, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 4
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 2
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v12, v8, 1
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v12, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv4i64_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    addi sp, sp, -16
; RV32ZVBC-NEXT:    sw a0, 8(sp)
; RV32ZVBC-NEXT:    sw a1, 12(sp)
; RV32ZVBC-NEXT:    addi a0, sp, 8
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV32ZVBC-NEXT:    vlse64.v v12, (a0), zero
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v12
; RV32ZVBC-NEXT:    addi sp, sp, 16
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv4i64_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m4, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 4 x i64> poison, i64 %b, i128 0
  %vb = shufflevector <vscale x 4 x i64> %elt.head, <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
  %va.ext = zext <vscale x 4 x i64> %va to <vscale x 4 x i128>
  %vb.ext = zext <vscale x 4 x i64> %vb to <vscale x 4 x i128>
  %clmul = call <vscale x 4 x i128> @llvm.clmul.nxv4i128(<vscale x 4 x i128> %va.ext, <vscale x 4 x i128> %vb.ext)
  %res.ext = lshr <vscale x 4 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 4 x i128> %res.ext to <vscale x 4 x i64>
  ret <vscale x 4 x i64> %res
}

define <vscale x 8 x i64> @clmulh_nxv8i64_vv(<vscale x 8 x i64> %va, <vscale x 8 x i64> %vb) nounwind {
; RV32V-LABEL: clmulh_nxv8i64_vv:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -352
; RV32V-NEXT:    sw ra, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    sub sp, sp, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vmv8r.v v16, v8
; RV32V-NEXT:    li a0, 56
; RV32V-NEXT:    vsrl.vx v24, v8, a0
; RV32V-NEXT:    li a7, 56
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    vsrl.vx v0, v8, a0
; RV32V-NEXT:    li a1, 40
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a4, 524288
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a4, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li s8, 1
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    lui a6, 16
; RV32V-NEXT:    addi a2, a6, -256
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s8, 260(sp)
; RV32V-NEXT:    vand.vx v0, v0, a2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li a3, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw a3, 228(sp)
; RV32V-NEXT:    li a5, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw a5, 220(sp)
; RV32V-NEXT:    li t0, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw t0, 212(sp)
; RV32V-NEXT:    li ra, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw ra, 204(sp)
; RV32V-NEXT:    li s11, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw s11, 196(sp)
; RV32V-NEXT:    li s10, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s10, 188(sp)
; RV32V-NEXT:    li s9, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s9, 180(sp)
; RV32V-NEXT:    slli s8, s8, 11
; RV32V-NEXT:    vor.vv v8, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv t1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add t1, t1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, t1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s8, 172(sp)
; RV32V-NEXT:    lui s7, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s7, 164(sp)
; RV32V-NEXT:    lui s6, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s6, 156(sp)
; RV32V-NEXT:    lui s5, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw s5, 148(sp)
; RV32V-NEXT:    lui s4, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s4, 140(sp)
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw a6, 132(sp)
; RV32V-NEXT:    lui s3, 32
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s3, 124(sp)
; RV32V-NEXT:    lui s2, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s2, 116(sp)
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw s1, 108(sp)
; RV32V-NEXT:    lui s0, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw s0, 100(sp)
; RV32V-NEXT:    lui t6, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t6, 92(sp)
; RV32V-NEXT:    lui t4, 1024
; RV32V-NEXT:    vsrl.vi v0, v16, 24
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t4, 84(sp)
; RV32V-NEXT:    lui t5, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t5, 76(sp)
; RV32V-NEXT:    lui t3, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw t3, 68(sp)
; RV32V-NEXT:    lui t2, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw t2, 60(sp)
; RV32V-NEXT:    lui t1, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw t1, 52(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    lui a0, 65536
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a0, 36(sp)
; RV32V-NEXT:    lui a0, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw a0, 28(sp)
; RV32V-NEXT:    lui a0, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw a0, 20(sp)
; RV32V-NEXT:    sw a4, 12(sp)
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    lui a4, 4080
; RV32V-NEXT:    addi a0, sp, 272
; RV32V-NEXT:    vand.vx v0, v0, a4
; RV32V-NEXT:    vlse64.v v8, (a0), zero
; RV32V-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v16, 8
; RV32V-NEXT:    vand.vv v24, v24, v8
; RV32V-NEXT:    vor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a3, a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a3, a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, a2
; RV32V-NEXT:    vsll.vx v0, v0, a1
; RV32V-NEXT:    vsll.vx v24, v16, a7
; RV32V-NEXT:    vor.vv v24, v24, v0
; RV32V-NEXT:    vand.vx v0, v16, a4
; RV32V-NEXT:    vmv8r.v v8, v16
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v8, v16
; RV32V-NEXT:    vsll.vi v16, v16, 8
; RV32V-NEXT:    vor.vv v16, v0, v16
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 4
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 2
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    lui a0, 349525
; RV32V-NEXT:    addi a0, a0, 1365
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vsrl.vi v16, v8, 1
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    li a1, 40
; RV32V-NEXT:    vsrl.vx v16, v8, a1
; RV32V-NEXT:    vand.vx v16, v16, a2
; RV32V-NEXT:    li a0, 56
; RV32V-NEXT:    vsrl.vx v24, v8, a0
; RV32V-NEXT:    vor.vv v16, v16, v24
; RV32V-NEXT:    csrr a3, vlenb
; RV32V-NEXT:    slli a3, a3, 4
; RV32V-NEXT:    add a3, sp, a3
; RV32V-NEXT:    addi a3, a3, 288
; RV32V-NEXT:    vs8r.v v16, (a3) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v8, 24
; RV32V-NEXT:    vand.vx v24, v24, a4
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    csrr a3, vlenb
; RV32V-NEXT:    slli a3, a3, 6
; RV32V-NEXT:    add a3, sp, a3
; RV32V-NEXT:    addi a3, a3, 288
; RV32V-NEXT:    vl8r.v v16, (a3) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v0, v0, v16
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    csrr a3, vlenb
; RV32V-NEXT:    slli a3, a3, 4
; RV32V-NEXT:    add a3, sp, a3
; RV32V-NEXT:    addi a3, a3, 288
; RV32V-NEXT:    vl8r.v v16, (a3) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    csrr a3, vlenb
; RV32V-NEXT:    slli a3, a3, 4
; RV32V-NEXT:    add a3, sp, a3
; RV32V-NEXT:    addi a3, a3, 288
; RV32V-NEXT:    vs8r.v v16, (a3) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v16, v8, a2
; RV32V-NEXT:    vsll.vx v16, v16, a1
; RV32V-NEXT:    li a1, 40
; RV32V-NEXT:    vsll.vx v0, v8, a0
; RV32V-NEXT:    li a7, 56
; RV32V-NEXT:    vor.vv v16, v0, v16
; RV32V-NEXT:    vand.vx v0, v8, a4
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    vor.vv v16, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v16, v16, v8
; RV32V-NEXT:    vsrl.vi v24, v16, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a3, a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v16, v16, v0
; RV32V-NEXT:    vsll.vi v16, v16, 4
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    vsrl.vi v24, v16, 2
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v16, v16, v0
; RV32V-NEXT:    vsll.vi v16, v16, 2
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    vsrl.vi v24, v16, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v16, v16, v0
; RV32V-NEXT:    vadd.vv v16, v16, v16
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    vand.vi v24, v16, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a3, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a3
; RV32V-NEXT:    lw a3, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vand.vi v0, v16, 2
; RV32V-NEXT:    vmul.vv v0, v8, v0
; RV32V-NEXT:    vxor.vi v24, v24, 0
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v0, v16, 4
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v24, v16, 8
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, a3
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, a5
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, t0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, ra
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s11
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s10
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s9
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s8
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s7
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s6
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s5
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s4
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, a6
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s3
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s2
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, s1
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, s0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, t6
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, t4
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, t5
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, t3
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v16, t2
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v16, t1
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    vand.vx v24, v16, a0
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 65536
; RV32V-NEXT:    vand.vx v0, v16, a0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 131072
; RV32V-NEXT:    vand.vx v24, v16, a0
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    lui a0, 262144
; RV32V-NEXT:    vand.vx v24, v16, a0
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v16, v0
; RV32V-NEXT:    addi a0, sp, 16
; RV32V-NEXT:    vmul.vv v24, v8, v0
; RV32V-NEXT:    addi a3, sp, 288
; RV32V-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 8
; RV32V-NEXT:    vl8r.v v0, (a3) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a3, vlenb
; RV32V-NEXT:    slli a3, a3, 5
; RV32V-NEXT:    add a3, sp, a3
; RV32V-NEXT:    addi a3, a3, 288
; RV32V-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v16, v24
; RV32V-NEXT:    vand.vv v16, v16, v0
; RV32V-NEXT:    vmul.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v8, v8, v16
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vx v16, v8, a1
; RV32V-NEXT:    vand.vx v16, v16, a2
; RV32V-NEXT:    vsrl.vx v24, v8, a7
; RV32V-NEXT:    vor.vv v16, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v8, 24
; RV32V-NEXT:    vand.vx v24, v24, a4
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v0, v0, v16
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vand.vx v0, v8, a4
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    vsll.vx v0, v8, a7
; RV32V-NEXT:    vand.vx v8, v8, a2
; RV32V-NEXT:    vsll.vx v8, v8, a1
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vor.vv v8, v8, v24
; RV32V-NEXT:    vor.vv v8, v8, v16
; RV32V-NEXT:    vsrl.vi v16, v8, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 2
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 288
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 352
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv8i64_vv:
; RV64V:       # %bb.0:
; RV64V-NEXT:    addi sp, sp, -16
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    sub sp, sp, a0
; RV64V-NEXT:    li a0, 56
; RV64V-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV64V-NEXT:    vsrl.vx v24, v8, a0
; RV64V-NEXT:    li a1, 40
; RV64V-NEXT:    vsrl.vx v0, v8, a1
; RV64V-NEXT:    lui t1, 16
; RV64V-NEXT:    addi a2, t1, -256
; RV64V-NEXT:    vand.vx v0, v0, a2
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    csrr a3, vlenb
; RV64V-NEXT:    slli a3, a3, 3
; RV64V-NEXT:    mv a4, a3
; RV64V-NEXT:    slli a3, a3, 2
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    add a3, sp, a3
; RV64V-NEXT:    addi a3, a3, 16
; RV64V-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vsrl.vi v0, v8, 24
; RV64V-NEXT:    lui a6, 4080
; RV64V-NEXT:    vand.vx v0, v0, a6
; RV64V-NEXT:    vsrl.vi v24, v8, 8
; RV64V-NEXT:    li a7, 255
; RV64V-NEXT:    slli a7, a7, 24
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr a3, vlenb
; RV64V-NEXT:    slli a3, a3, 3
; RV64V-NEXT:    mv a4, a3
; RV64V-NEXT:    slli a3, a3, 2
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    add a3, sp, a3
; RV64V-NEXT:    addi a3, a3, 16
; RV64V-NEXT:    vl8r.v v0, (a3) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr a3, vlenb
; RV64V-NEXT:    slli a3, a3, 3
; RV64V-NEXT:    mv a4, a3
; RV64V-NEXT:    slli a3, a3, 2
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    add a3, sp, a3
; RV64V-NEXT:    addi a3, a3, 16
; RV64V-NEXT:    vs8r.v v24, (a3) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vx v0, v8, a6
; RV64V-NEXT:    vsll.vi v0, v0, 24
; RV64V-NEXT:    vand.vx v24, v8, a7
; RV64V-NEXT:    vsll.vi v24, v24, 8
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    vsll.vx v0, v8, a0
; RV64V-NEXT:    vand.vx v8, v8, a2
; RV64V-NEXT:    vsll.vx v8, v8, a1
; RV64V-NEXT:    vor.vv v8, v0, v8
; RV64V-NEXT:    vor.vv v8, v8, v24
; RV64V-NEXT:    csrr a3, vlenb
; RV64V-NEXT:    slli a3, a3, 3
; RV64V-NEXT:    mv a4, a3
; RV64V-NEXT:    slli a3, a3, 2
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    add a3, sp, a3
; RV64V-NEXT:    addi a3, a3, 16
; RV64V-NEXT:    vl8r.v v24, (a3) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v8, v8, v24
; RV64V-NEXT:    vsrl.vi v24, v8, 4
; RV64V-NEXT:    lui a3, 61681
; RV64V-NEXT:    addi a3, a3, -241
; RV64V-NEXT:    slli a4, a3, 32
; RV64V-NEXT:    add a3, a3, a4
; RV64V-NEXT:    vand.vx v24, v24, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v24, v8
; RV64V-NEXT:    vsrl.vi v24, v8, 2
; RV64V-NEXT:    lui a4, 209715
; RV64V-NEXT:    addi a4, a4, 819
; RV64V-NEXT:    slli a5, a4, 32
; RV64V-NEXT:    add a4, a4, a5
; RV64V-NEXT:    vand.vx v24, v24, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v24, v8
; RV64V-NEXT:    vsrl.vi v24, v8, 1
; RV64V-NEXT:    lui a5, 349525
; RV64V-NEXT:    addi a5, a5, 1365
; RV64V-NEXT:    slli t0, a5, 32
; RV64V-NEXT:    add a5, a5, t0
; RV64V-NEXT:    vand.vx v24, v24, a5
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v24, v8
; RV64V-NEXT:    vsrl.vx v24, v16, a0
; RV64V-NEXT:    vsrl.vx v0, v16, a1
; RV64V-NEXT:    vand.vx v0, v0, a2
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vsrl.vi v0, v16, 24
; RV64V-NEXT:    vand.vx v0, v0, a6
; RV64V-NEXT:    vsrl.vi v24, v16, 8
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v0, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vx v0, v16, a6
; RV64V-NEXT:    vsll.vi v0, v0, 24
; RV64V-NEXT:    vand.vx v24, v16, a7
; RV64V-NEXT:    vsll.vi v24, v24, 8
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    vsll.vx v0, v16, a0
; RV64V-NEXT:    vand.vx v16, v16, a2
; RV64V-NEXT:    vsll.vx v16, v16, a1
; RV64V-NEXT:    vor.vv v16, v0, v16
; RV64V-NEXT:    vor.vv v16, v16, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v16, v16, v24
; RV64V-NEXT:    vsrl.vi v24, v16, 4
; RV64V-NEXT:    vand.vx v24, v24, a3
; RV64V-NEXT:    vand.vx v16, v16, a3
; RV64V-NEXT:    vsll.vi v16, v16, 4
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    vsrl.vi v24, v16, 2
; RV64V-NEXT:    vand.vx v24, v24, a4
; RV64V-NEXT:    vand.vx v16, v16, a4
; RV64V-NEXT:    vsll.vi v16, v16, 2
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    vsrl.vi v24, v16, 1
; RV64V-NEXT:    vand.vx v24, v24, a5
; RV64V-NEXT:    vand.vx v16, v16, a5
; RV64V-NEXT:    vadd.vv v16, v16, v16
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    vand.vi v24, v16, 2
; RV64V-NEXT:    vand.vi v0, v16, 1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v0, v16, 4
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vi v24, v16, 8
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v0, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 16
; RV64V-NEXT:    vand.vx v0, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 32
; RV64V-NEXT:    vand.vx v24, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v0, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 64
; RV64V-NEXT:    vand.vx v0, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 128
; RV64V-NEXT:    vand.vx v24, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v0, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 256
; RV64V-NEXT:    vand.vx v0, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 512
; RV64V-NEXT:    vand.vx v24, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 2
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v0, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 5
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 1024
; RV64V-NEXT:    vand.vx v24, v16, t0
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t0, vlenb
; RV64V-NEXT:    slli t0, t0, 3
; RV64V-NEXT:    mv t2, t0
; RV64V-NEXT:    slli t0, t0, 1
; RV64V-NEXT:    add t0, t0, t2
; RV64V-NEXT:    add t0, sp, t0
; RV64V-NEXT:    addi t0, t0, 16
; RV64V-NEXT:    vs8r.v v24, (t0) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    li t0, 1
; RV64V-NEXT:    slli t2, t0, 11
; RV64V-NEXT:    vand.vx v0, v16, t2
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t2, 1
; RV64V-NEXT:    vand.vx v0, v16, t2
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t2, 2
; RV64V-NEXT:    vand.vx v24, v16, t2
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t2, 4
; RV64V-NEXT:    vand.vx v0, v16, t2
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t2, 8
; RV64V-NEXT:    vand.vx v24, v16, t2
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 5
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 32
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 64
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 128
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 256
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 512
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 1024
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 2048
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v0, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsll.vx v24, v24, a0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vand.vx v24, v0, a2
; RV64V-NEXT:    vsll.vx v24, v24, a1
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 4096
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 8192
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 16384
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 32768
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 65536
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 131072
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t1, 262144
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 31
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 33
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 34
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 35
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 36
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 37
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 38
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 39
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 40
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 41
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 42
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 43
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 44
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 45
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 46
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 47
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 48
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 49
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 50
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 51
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 52
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 53
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 54
; RV64V-NEXT:    vand.vx v0, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t1, t0, 55
; RV64V-NEXT:    vand.vx v24, v16, t1
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi t1, sp, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vand.vx v24, v24, a6
; RV64V-NEXT:    vsll.vi v24, v24, 24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 5
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vand.vx v0, v0, a7
; RV64V-NEXT:    vsll.vi v0, v0, 8
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v0, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 4
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v24, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t2, t1
; RV64V-NEXT:    slli t1, t1, 2
; RV64V-NEXT:    add t1, t1, t2
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vl8r.v v24, (t1) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsrl.vi v24, v24, 8
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    csrr a7, vlenb
; RV64V-NEXT:    slli a7, a7, 3
; RV64V-NEXT:    mv t1, a7
; RV64V-NEXT:    slli a7, a7, 1
; RV64V-NEXT:    add a7, a7, t1
; RV64V-NEXT:    add a7, sp, a7
; RV64V-NEXT:    addi a7, a7, 16
; RV64V-NEXT:    vl8r.v v0, (a7) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsrl.vi v0, v0, 24
; RV64V-NEXT:    vand.vx v0, v0, a6
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 2
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 56
; RV64V-NEXT:    vand.vx v24, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 57
; RV64V-NEXT:    vand.vx v0, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 5
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v24, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v0, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v0, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 58
; RV64V-NEXT:    vand.vx v0, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 59
; RV64V-NEXT:    vand.vx v24, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    addi a6, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v0, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v24, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a6, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 60
; RV64V-NEXT:    vand.vx v0, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v0
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a6, t0, 61
; RV64V-NEXT:    vand.vx v24, v16, a6
; RV64V-NEXT:    vmul.vv v24, v8, v24
; RV64V-NEXT:    addi a6, sp, 16
; RV64V-NEXT:    vs8r.v v24, (a6) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    mv a7, a6
; RV64V-NEXT:    slli a6, a6, 1
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v0, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 3
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v24, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v0, v0, v24
; RV64V-NEXT:    addi a6, sp, 16
; RV64V-NEXT:    vl8r.v v24, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    slli t0, t0, 62
; RV64V-NEXT:    vand.vx v0, v16, t0
; RV64V-NEXT:    vmul.vv v0, v8, v0
; RV64V-NEXT:    li a6, -1
; RV64V-NEXT:    slli a6, a6, 63
; RV64V-NEXT:    vand.vx v16, v16, a6
; RV64V-NEXT:    vmul.vv v8, v8, v16
; RV64V-NEXT:    vxor.vv v16, v24, v0
; RV64V-NEXT:    vxor.vv v8, v16, v8
; RV64V-NEXT:    csrr a6, vlenb
; RV64V-NEXT:    slli a6, a6, 5
; RV64V-NEXT:    add a6, sp, a6
; RV64V-NEXT:    addi a6, a6, 16
; RV64V-NEXT:    vl8r.v v16, (a6) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsrl.vx v16, v16, a1
; RV64V-NEXT:    vand.vx v16, v16, a2
; RV64V-NEXT:    vsrl.vx v8, v8, a0
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 4
; RV64V-NEXT:    vand.vx v16, v16, a3
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 2
; RV64V-NEXT:    vand.vx v16, v16, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 1
; RV64V-NEXT:    vand.vx v16, v16, a5
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    mv a1, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a1
; RV64V-NEXT:    add sp, sp, a0
; RV64V-NEXT:    addi sp, sp, 16
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv8i64_vv:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v16
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv8i64_vv:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vv v8, v8, v16
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 8 x i64> %va to <vscale x 8 x i128>
  %vb.ext = zext <vscale x 8 x i64> %vb to <vscale x 8 x i128>
  %clmul = call <vscale x 8 x i128> @llvm.clmul.nxv8i128(<vscale x 8 x i128> %va.ext, <vscale x 8 x i128> %vb.ext)
  %res.ext = lshr <vscale x 8 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 8 x i128> %res.ext to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %res
}

define <vscale x 8 x i64> @clmulh_nxv8i64_vx(<vscale x 8 x i64> %va, i64 %b) nounwind {
; RV32V-LABEL: clmulh_nxv8i64_vx:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -368
; RV32V-NEXT:    sw ra, 364(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 360(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 356(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 352(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 348(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 344(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 340(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 336(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a2, vlenb
; RV32V-NEXT:    slli a2, a2, 3
; RV32V-NEXT:    mv a3, a2
; RV32V-NEXT:    slli a2, a2, 3
; RV32V-NEXT:    add a2, a2, a3
; RV32V-NEXT:    sub sp, sp, a2
; RV32V-NEXT:    li a3, 56
; RV32V-NEXT:    vsetvli a2, zero, e64, m8, ta, ma
; RV32V-NEXT:    vsrl.vx v24, v8, a3
; RV32V-NEXT:    li a7, 56
; RV32V-NEXT:    li a2, 40
; RV32V-NEXT:    vsrl.vx v0, v8, a2
; RV32V-NEXT:    li a2, 40
; RV32V-NEXT:    sw a0, 16(sp)
; RV32V-NEXT:    sw a1, 20(sp)
; RV32V-NEXT:    addi a0, sp, 16
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    sw a0, 288(sp)
; RV32V-NEXT:    sw zero, 292(sp)
; RV32V-NEXT:    lui a4, 524288
; RV32V-NEXT:    li s8, 1
; RV32V-NEXT:    li a1, 2
; RV32V-NEXT:    lui a6, 16
; RV32V-NEXT:    addi a5, a6, -256
; RV32V-NEXT:    sw a4, 280(sp)
; RV32V-NEXT:    sw zero, 284(sp)
; RV32V-NEXT:    sw zero, 272(sp)
; RV32V-NEXT:    sw s8, 276(sp)
; RV32V-NEXT:    vand.vx v0, v0, a5
; RV32V-NEXT:    sw zero, 264(sp)
; RV32V-NEXT:    sw a1, 268(sp)
; RV32V-NEXT:    li a1, 4
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw a1, 260(sp)
; RV32V-NEXT:    li a1, 8
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a1, 252(sp)
; RV32V-NEXT:    li a1, 16
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a1, 244(sp)
; RV32V-NEXT:    li a3, 32
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a3, 236(sp)
; RV32V-NEXT:    li t0, 64
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw t0, 228(sp)
; RV32V-NEXT:    li ra, 128
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw ra, 220(sp)
; RV32V-NEXT:    li s11, 256
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw s11, 212(sp)
; RV32V-NEXT:    li s10, 512
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw s10, 204(sp)
; RV32V-NEXT:    li s9, 1024
; RV32V-NEXT:    slli s8, s8, 11
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw s9, 196(sp)
; RV32V-NEXT:    vor.vv v16, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv t1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add t1, t1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, t1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s8, 188(sp)
; RV32V-NEXT:    lui s7, 1
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s7, 180(sp)
; RV32V-NEXT:    lui s6, 2
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s6, 172(sp)
; RV32V-NEXT:    lui s5, 4
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s5, 164(sp)
; RV32V-NEXT:    lui s4, 8
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s4, 156(sp)
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw a6, 148(sp)
; RV32V-NEXT:    lui s3, 32
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s3, 140(sp)
; RV32V-NEXT:    lui s2, 64
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s2, 132(sp)
; RV32V-NEXT:    lui s1, 128
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s1, 124(sp)
; RV32V-NEXT:    lui s0, 256
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw s0, 116(sp)
; RV32V-NEXT:    lui t6, 512
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw t6, 108(sp)
; RV32V-NEXT:    lui t4, 1024
; RV32V-NEXT:    vsrl.vi v0, v8, 24
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw t4, 100(sp)
; RV32V-NEXT:    lui t5, 2048
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t5, 92(sp)
; RV32V-NEXT:    lui t3, 4096
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t3, 84(sp)
; RV32V-NEXT:    lui t2, 8192
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t2, 76(sp)
; RV32V-NEXT:    lui t1, 16384
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw t1, 68(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a0, 60(sp)
; RV32V-NEXT:    lui a0, 65536
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a0, 52(sp)
; RV32V-NEXT:    lui a0, 131072
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    lui a0, 262144
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a0, 36(sp)
; RV32V-NEXT:    sw a4, 28(sp)
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    lui a4, 4080
; RV32V-NEXT:    addi a0, sp, 288
; RV32V-NEXT:    vand.vx v0, v0, a4
; RV32V-NEXT:    vlse64.v v16, (a0), zero
; RV32V-NEXT:    sw a1, 4(sp) # 4-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v8, 8
; RV32V-NEXT:    vand.vv v24, v24, v16
; RV32V-NEXT:    vor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, a5
; RV32V-NEXT:    vsll.vx v0, v0, a2
; RV32V-NEXT:    vsll.vx v16, v8, a7
; RV32V-NEXT:    vor.vv v24, v16, v0
; RV32V-NEXT:    vand.vx v0, v8, a4
; RV32V-NEXT:    vmv8r.v v16, v8
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v8, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v16, v8
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v8, v8, v16
; RV32V-NEXT:    vsrl.vi v16, v8, 4
; RV32V-NEXT:    lui a0, 61681
; RV32V-NEXT:    addi a0, a0, -241
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 2
; RV32V-NEXT:    lui a0, 209715
; RV32V-NEXT:    addi a0, a0, 819
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    lui a0, 349525
; RV32V-NEXT:    addi a0, a0, 1365
; RV32V-NEXT:    vsetvli a7, zero, e32, m8, ta, ma
; RV32V-NEXT:    vmv.v.x v24, a0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsetvli a0, zero, e64, m8, ta, ma
; RV32V-NEXT:    vsrl.vi v16, v8, 1
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v8, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    li a2, 40
; RV32V-NEXT:    vsrl.vx v8, v16, a2
; RV32V-NEXT:    vand.vx v8, v8, a5
; RV32V-NEXT:    li a0, 56
; RV32V-NEXT:    vsrl.vx v24, v16, a0
; RV32V-NEXT:    vor.vv v8, v8, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v16, 24
; RV32V-NEXT:    vand.vx v24, v24, a4
; RV32V-NEXT:    vsrl.vi v0, v16, 8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v0, v0, v8
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 4
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vs8r.v v8, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 6
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vl8r.v v8, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v8, v16, v8
; RV32V-NEXT:    vsll.vi v8, v8, 8
; RV32V-NEXT:    vand.vx v0, v16, a4
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vsll.vx v0, v16, a0
; RV32V-NEXT:    li a7, 56
; RV32V-NEXT:    vand.vx v24, v16, a5
; RV32V-NEXT:    vsll.vx v24, v24, a2
; RV32V-NEXT:    li a2, 40
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v8, v8, v16
; RV32V-NEXT:    vsrl.vi v24, v8, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v8, v8, v0
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    vsrl.vi v24, v8, 2
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v8, v8, v0
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    vsrl.vi v24, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v24, v0
; RV32V-NEXT:    vand.vv v8, v8, v0
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v24, v8
; RV32V-NEXT:    vand.vi v24, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    lw a1, 4(sp) # 4-byte Folded Reload
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    vand.vi v0, v8, 2
; RV32V-NEXT:    vmul.vv v0, v16, v0
; RV32V-NEXT:    vxor.vi v24, v24, 0
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v0, v8, 4
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vi v24, v8, 8
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, a1
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, a3
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, t0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, ra
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s11
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s10
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s9
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s8
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s7
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s6
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s5
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s4
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, a6
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s3
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s2
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, s1
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, s0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, t6
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, t4
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, t5
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, t3
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v24, v8, t2
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vx v0, v8, t1
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    vand.vx v24, v8, a0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 65536
; RV32V-NEXT:    vand.vx v0, v8, a0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    lui a0, 131072
; RV32V-NEXT:    vand.vx v24, v8, a0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v0, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 280
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    lui a0, 262144
; RV32V-NEXT:    vand.vx v24, v8, a0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 272
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 264
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 256
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 248
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 240
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 232
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 224
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 216
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 208
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 200
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 192
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 184
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 176
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 168
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 160
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 152
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 144
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 136
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 128
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 120
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 112
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 104
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 96
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 88
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 80
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 72
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 64
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 56
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 48
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    addi a0, sp, 40
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vand.vv v0, v8, v0
; RV32V-NEXT:    addi a0, sp, 32
; RV32V-NEXT:    vmul.vv v24, v16, v0
; RV32V-NEXT:    addi a1, sp, 304
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v24, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v24, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v0, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    addi a0, sp, 24
; RV32V-NEXT:    vl8r.v v0, (a1) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v24, v24, v0
; RV32V-NEXT:    csrr a1, vlenb
; RV32V-NEXT:    slli a1, a1, 5
; RV32V-NEXT:    add a1, sp, a1
; RV32V-NEXT:    addi a1, a1, 304
; RV32V-NEXT:    vs8r.v v24, (a1) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vlse64.v v0, (a0), zero
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vand.vv v8, v8, v0
; RV32V-NEXT:    vmul.vv v24, v16, v24
; RV32V-NEXT:    vmul.vv v8, v16, v8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vxor.vv v16, v16, v24
; RV32V-NEXT:    vxor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vx v16, v8, a2
; RV32V-NEXT:    vand.vx v16, v16, a5
; RV32V-NEXT:    vsrl.vx v24, v8, a7
; RV32V-NEXT:    vor.vv v16, v16, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vs8r.v v16, (a0) # vscale x 64-byte Folded Spill
; RV32V-NEXT:    vsrl.vi v24, v8, 24
; RV32V-NEXT:    vand.vx v24, v24, a4
; RV32V-NEXT:    vsrl.vi v0, v8, 8
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v0, v0, v16
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 5
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vor.vv v16, v24, v16
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 6
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v24, v8, v24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vand.vx v0, v8, a4
; RV32V-NEXT:    vsll.vi v0, v0, 24
; RV32V-NEXT:    vor.vv v24, v0, v24
; RV32V-NEXT:    vsll.vx v0, v8, a7
; RV32V-NEXT:    vand.vx v8, v8, a5
; RV32V-NEXT:    vsll.vx v8, v8, a2
; RV32V-NEXT:    vor.vv v8, v0, v8
; RV32V-NEXT:    vor.vv v8, v8, v24
; RV32V-NEXT:    vor.vv v8, v8, v16
; RV32V-NEXT:    vsrl.vi v16, v8, 4
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a1, a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 4
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 2
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 4
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 1
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vsll.vi v8, v8, 2
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v16, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 2
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add a0, sp, a0
; RV32V-NEXT:    addi a0, a0, 304
; RV32V-NEXT:    vl8r.v v24, (a0) # vscale x 64-byte Folded Reload
; RV32V-NEXT:    vand.vv v16, v16, v24
; RV32V-NEXT:    vand.vv v8, v8, v24
; RV32V-NEXT:    vadd.vv v8, v8, v8
; RV32V-NEXT:    vor.vv v8, v16, v8
; RV32V-NEXT:    vsrl.vi v8, v8, 1
; RV32V-NEXT:    csrr a0, vlenb
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    mv a1, a0
; RV32V-NEXT:    slli a0, a0, 3
; RV32V-NEXT:    add a0, a0, a1
; RV32V-NEXT:    add sp, sp, a0
; RV32V-NEXT:    lw ra, 364(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 360(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 356(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 352(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 348(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 344(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 340(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 336(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    addi sp, sp, 368
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv8i64_vx:
; RV64V:       # %bb.0:
; RV64V-NEXT:    addi sp, sp, -16
; RV64V-NEXT:    csrr a1, vlenb
; RV64V-NEXT:    slli a1, a1, 5
; RV64V-NEXT:    sub sp, sp, a1
; RV64V-NEXT:    li a1, 56
; RV64V-NEXT:    vsetvli a2, zero, e64, m8, ta, ma
; RV64V-NEXT:    vsrl.vx v16, v8, a1
; RV64V-NEXT:    li a2, 40
; RV64V-NEXT:    vsrl.vx v24, v8, a2
; RV64V-NEXT:    lui t2, 16
; RV64V-NEXT:    addi a3, t2, -256
; RV64V-NEXT:    vand.vx v24, v24, a3
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    vsrl.vi v24, v8, 24
; RV64V-NEXT:    lui a7, 4080
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vsrl.vi v0, v8, 8
; RV64V-NEXT:    li t0, 255
; RV64V-NEXT:    slli t0, t0, 24
; RV64V-NEXT:    vand.vx v0, v0, t0
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    vand.vx v24, v8, a7
; RV64V-NEXT:    vsll.vi v24, v24, 24
; RV64V-NEXT:    vand.vx v0, v8, t0
; RV64V-NEXT:    vsll.vi v0, v0, 8
; RV64V-NEXT:    vor.vv v24, v24, v0
; RV64V-NEXT:    vsll.vx v0, v8, a1
; RV64V-NEXT:    vand.vx v8, v8, a3
; RV64V-NEXT:    vsll.vx v8, v8, a2
; RV64V-NEXT:    vor.vv v8, v0, v8
; RV64V-NEXT:    vor.vv v8, v8, v24
; RV64V-NEXT:    vor.vv v8, v8, v16
; RV64V-NEXT:    vsrl.vi v16, v8, 4
; RV64V-NEXT:    lui a4, 61681
; RV64V-NEXT:    addi a4, a4, -241
; RV64V-NEXT:    slli a5, a4, 32
; RV64V-NEXT:    add a4, a4, a5
; RV64V-NEXT:    vand.vx v16, v16, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 2
; RV64V-NEXT:    lui a5, 209715
; RV64V-NEXT:    addi a5, a5, 819
; RV64V-NEXT:    slli a6, a5, 32
; RV64V-NEXT:    add a5, a5, a6
; RV64V-NEXT:    vand.vx v16, v16, a5
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 1
; RV64V-NEXT:    lui a6, 349525
; RV64V-NEXT:    addi a6, a6, 1365
; RV64V-NEXT:    slli t1, a6, 32
; RV64V-NEXT:    add a6, a6, t1
; RV64V-NEXT:    vand.vx v16, v16, a6
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    srli t1, a0, 24
; RV64V-NEXT:    srli t3, a0, 8
; RV64V-NEXT:    and t1, t1, a7
; RV64V-NEXT:    and t3, t3, t0
; RV64V-NEXT:    or t1, t3, t1
; RV64V-NEXT:    srli t3, a0, 40
; RV64V-NEXT:    and t3, t3, a3
; RV64V-NEXT:    srli t4, a0, 56
; RV64V-NEXT:    or t3, t3, t4
; RV64V-NEXT:    and t4, a0, a7
; RV64V-NEXT:    slli t4, t4, 24
; RV64V-NEXT:    srliw t5, a0, 24
; RV64V-NEXT:    slli t5, t5, 32
; RV64V-NEXT:    and t6, a0, a3
; RV64V-NEXT:    slli t6, t6, 40
; RV64V-NEXT:    slli a0, a0, 56
; RV64V-NEXT:    or t4, t4, t5
; RV64V-NEXT:    or a0, a0, t6
; RV64V-NEXT:    or t1, t1, t3
; RV64V-NEXT:    or a0, a0, t4
; RV64V-NEXT:    or a0, a0, t1
; RV64V-NEXT:    srli t1, a0, 4
; RV64V-NEXT:    and a0, a0, a4
; RV64V-NEXT:    and t1, t1, a4
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 2
; RV64V-NEXT:    and a0, a0, a5
; RV64V-NEXT:    and t1, t1, a5
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 1
; RV64V-NEXT:    and a0, a0, a6
; RV64V-NEXT:    and t1, t1, a6
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    andi t1, a0, 2
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    andi t1, a0, 1
; RV64V-NEXT:    vmul.vx v24, v8, t1
; RV64V-NEXT:    andi t1, a0, 4
; RV64V-NEXT:    vmul.vx v0, v8, t1
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    andi t1, a0, 8
; RV64V-NEXT:    vmul.vx v24, v8, t1
; RV64V-NEXT:    andi t1, a0, 16
; RV64V-NEXT:    vmul.vx v0, v8, t1
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    andi t1, a0, 32
; RV64V-NEXT:    vmul.vx v24, v8, t1
; RV64V-NEXT:    andi t1, a0, 64
; RV64V-NEXT:    vmul.vx v0, v8, t1
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    andi t1, a0, 128
; RV64V-NEXT:    vmul.vx v24, v8, t1
; RV64V-NEXT:    andi t1, a0, 256
; RV64V-NEXT:    vmul.vx v0, v8, t1
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    csrr t1, vlenb
; RV64V-NEXT:    slli t1, t1, 3
; RV64V-NEXT:    mv t3, t1
; RV64V-NEXT:    slli t1, t1, 1
; RV64V-NEXT:    add t1, t1, t3
; RV64V-NEXT:    add t1, sp, t1
; RV64V-NEXT:    addi t1, t1, 16
; RV64V-NEXT:    vs8r.v v16, (t1) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vxor.vv v24, v16, v0
; RV64V-NEXT:    andi t1, a0, 512
; RV64V-NEXT:    vmul.vx v0, v8, t1
; RV64V-NEXT:    andi t1, a0, 1024
; RV64V-NEXT:    vmul.vx v16, v8, t1
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    vxor.vv v24, v24, v16
; RV64V-NEXT:    li t1, 1
; RV64V-NEXT:    slli t3, t1, 11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v16, v8, t3
; RV64V-NEXT:    lui t3, 1
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v0, v8, t3
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t3, 2
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v24, v8, t3
; RV64V-NEXT:    lui t3, 4
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v0, v8, t3
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t3, 8
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v24, v8, t3
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 64
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 128
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 256
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 512
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 1024
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 2048
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsll.vx v0, v0, a1
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vand.vx v24, v16, a3
; RV64V-NEXT:    vsll.vx v24, v24, a2
; RV64V-NEXT:    vor.vv v24, v0, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    lui t2, 4096
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 8192
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 16384
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 32768
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 65536
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    lui t2, 131072
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    lui t2, 262144
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    srliw t2, a0, 31
; RV64V-NEXT:    slli t2, t2, 31
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 32
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 33
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 34
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 35
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 36
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 37
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 38
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 39
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v24, v16, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v24, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t2, t1, 40
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    slli t2, t1, 41
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    vxor.vv v24, v16, v0
; RV64V-NEXT:    slli t2, t1, 42
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    slli t2, t1, 43
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    slli t2, t1, 44
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 45
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 46
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 47
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    addi t2, sp, 16
; RV64V-NEXT:    vs8r.v v16, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli t2, t1, 48
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 49
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v24, v16, v24
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    slli t2, t1, 50
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    slli t2, t1, 51
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v8, t2
; RV64V-NEXT:    vxor.vv v24, v24, v0
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    slli t2, t1, 52
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 53
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t2, t1, 54
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v24, v8, t2
; RV64V-NEXT:    slli t2, t1, 55
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v0, v8, t2
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v16, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v0, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vand.vx v16, v0, a7
; RV64V-NEXT:    vsll.vi v16, v16, 24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 3
; RV64V-NEXT:    mv t3, t2
; RV64V-NEXT:    slli t2, t2, 1
; RV64V-NEXT:    add t2, t2, t3
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vand.vx v24, v24, t0
; RV64V-NEXT:    vsll.vi v24, v24, 8
; RV64V-NEXT:    vor.vv v16, v16, v24
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vl8r.v v24, (t2) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v16, v24, v16
; RV64V-NEXT:    csrr t2, vlenb
; RV64V-NEXT:    slli t2, t2, 4
; RV64V-NEXT:    add t2, sp, t2
; RV64V-NEXT:    addi t2, t2, 16
; RV64V-NEXT:    vs8r.v v16, (t2) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    vsrl.vi v16, v0, 8
; RV64V-NEXT:    vand.vx v16, v16, t0
; RV64V-NEXT:    addi t0, sp, 16
; RV64V-NEXT:    vl8r.v v24, (t0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsrl.vi v24, v24, 24
; RV64V-NEXT:    vand.vx v24, v24, a7
; RV64V-NEXT:    vor.vv v16, v16, v24
; RV64V-NEXT:    csrr a7, vlenb
; RV64V-NEXT:    slli a7, a7, 3
; RV64V-NEXT:    add a7, sp, a7
; RV64V-NEXT:    addi a7, a7, 16
; RV64V-NEXT:    vs8r.v v16, (a7) # vscale x 64-byte Folded Spill
; RV64V-NEXT:    slli a7, t1, 56
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v24, v8, a7
; RV64V-NEXT:    slli a7, t1, 57
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v16, v8, a7
; RV64V-NEXT:    csrr a7, vlenb
; RV64V-NEXT:    slli a7, a7, 3
; RV64V-NEXT:    mv t0, a7
; RV64V-NEXT:    slli a7, a7, 1
; RV64V-NEXT:    add a7, a7, t0
; RV64V-NEXT:    add a7, sp, a7
; RV64V-NEXT:    addi a7, a7, 16
; RV64V-NEXT:    vl8r.v v0, (a7) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    vxor.vv v0, v24, v16
; RV64V-NEXT:    slli a7, t1, 58
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v24, v8, a7
; RV64V-NEXT:    slli a7, t1, 59
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v16, v8, a7
; RV64V-NEXT:    vxor.vv v24, v0, v24
; RV64V-NEXT:    vxor.vv v16, v24, v16
; RV64V-NEXT:    slli a7, t1, 60
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v24, v8, a7
; RV64V-NEXT:    slli a7, t1, 61
; RV64V-NEXT:    and a7, a0, a7
; RV64V-NEXT:    vmul.vx v0, v8, a7
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v16, v16, v0
; RV64V-NEXT:    slli t1, t1, 62
; RV64V-NEXT:    and a7, a0, t1
; RV64V-NEXT:    vmul.vx v24, v8, a7
; RV64V-NEXT:    srli a0, a0, 63
; RV64V-NEXT:    slli a0, a0, 63
; RV64V-NEXT:    vmul.vx v8, v8, a0
; RV64V-NEXT:    vxor.vv v16, v16, v24
; RV64V-NEXT:    vxor.vv v8, v16, v8
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    mv a7, a0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    add a0, a0, a7
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vsrl.vx v16, v16, a2
; RV64V-NEXT:    vand.vx v16, v16, a3
; RV64V-NEXT:    vsrl.vx v8, v8, a1
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 3
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    add a0, sp, a0
; RV64V-NEXT:    addi a0, a0, 16
; RV64V-NEXT:    vl8r.v v16, (a0) # vscale x 64-byte Folded Reload
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 4
; RV64V-NEXT:    vand.vx v16, v16, a4
; RV64V-NEXT:    vand.vx v8, v8, a4
; RV64V-NEXT:    vsll.vi v8, v8, 4
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 2
; RV64V-NEXT:    vand.vx v16, v16, a5
; RV64V-NEXT:    vand.vx v8, v8, a5
; RV64V-NEXT:    vsll.vi v8, v8, 2
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v16, v8, 1
; RV64V-NEXT:    vand.vx v16, v16, a6
; RV64V-NEXT:    vand.vx v8, v8, a6
; RV64V-NEXT:    vadd.vv v8, v8, v8
; RV64V-NEXT:    vor.vv v8, v16, v8
; RV64V-NEXT:    vsrl.vi v8, v8, 1
; RV64V-NEXT:    csrr a0, vlenb
; RV64V-NEXT:    slli a0, a0, 5
; RV64V-NEXT:    add sp, sp, a0
; RV64V-NEXT:    addi sp, sp, 16
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv8i64_vx:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    addi sp, sp, -16
; RV32ZVBC-NEXT:    sw a0, 8(sp)
; RV32ZVBC-NEXT:    sw a1, 12(sp)
; RV32ZVBC-NEXT:    addi a0, sp, 8
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV32ZVBC-NEXT:    vlse64.v v16, (a0), zero
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v16
; RV32ZVBC-NEXT:    addi sp, sp, 16
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv8i64_vx:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m8, ta, ma
; RV64ZVBC-NEXT:    vclmulh.vx v8, v8, a0
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 8 x i64> poison, i64 %b, i128 0
  %vb = shufflevector <vscale x 8 x i64> %elt.head, <vscale x 8 x i64> poison, <vscale x 8 x i32> zeroinitializer
  %va.ext = zext <vscale x 8 x i64> %va to <vscale x 8 x i128>
  %vb.ext = zext <vscale x 8 x i64> %vb to <vscale x 8 x i128>
  %clmul = call <vscale x 8 x i128> @llvm.clmul.nxv8i128(<vscale x 8 x i128> %va.ext, <vscale x 8 x i128> %vb.ext)
  %res.ext = lshr <vscale x 8 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 8 x i128> %res.ext to <vscale x 8 x i64>
  ret <vscale x 8 x i64> %res
}

define <vscale x 1 x i64> @clmulh_nxv1i64_vv_mask(<vscale x 1 x i64> %va, <vscale x 1 x i64> %vb, <vscale x 1 x i1> %mask) {
; RV32V-LABEL: clmulh_nxv1i64_vv_mask:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    .cfi_def_cfa_offset 336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    .cfi_offset ra, -4
; RV32V-NEXT:    .cfi_offset s0, -8
; RV32V-NEXT:    .cfi_offset s1, -12
; RV32V-NEXT:    .cfi_offset s2, -16
; RV32V-NEXT:    .cfi_offset s3, -20
; RV32V-NEXT:    .cfi_offset s4, -24
; RV32V-NEXT:    .cfi_offset s5, -28
; RV32V-NEXT:    .cfi_offset s6, -32
; RV32V-NEXT:    .cfi_offset s7, -36
; RV32V-NEXT:    .cfi_offset s8, -40
; RV32V-NEXT:    .cfi_offset s9, -44
; RV32V-NEXT:    .cfi_offset s10, -48
; RV32V-NEXT:    .cfi_offset s11, -52
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li s6, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s6, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li s11, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw s11, 228(sp)
; RV32V-NEXT:    li ra, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw ra, 220(sp)
; RV32V-NEXT:    li a6, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a6, 212(sp)
; RV32V-NEXT:    li a5, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a5, 204(sp)
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    li s7, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s7, 188(sp)
; RV32V-NEXT:    li s8, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s8, 180(sp)
; RV32V-NEXT:    slli s6, s6, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s6, 172(sp)
; RV32V-NEXT:    lui s5, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s5, 164(sp)
; RV32V-NEXT:    lui s3, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s3, 156(sp)
; RV32V-NEXT:    lui s4, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw s4, 148(sp)
; RV32V-NEXT:    lui s2, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s2, 140(sp)
; RV32V-NEXT:    lui s1, 16
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s1, 132(sp)
; RV32V-NEXT:    lui s0, 32
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    lui t5, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw t5, 108(sp)
; RV32V-NEXT:    lui t4, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw t4, 100(sp)
; RV32V-NEXT:    lui t3, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t3, 92(sp)
; RV32V-NEXT:    lui t2, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t2, 84(sp)
; RV32V-NEXT:    lui t1, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t1, 76(sp)
; RV32V-NEXT:    lui a0, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a0, 68(sp)
; RV32V-NEXT:    lui a0, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a0, 60(sp)
; RV32V-NEXT:    lui a0, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a0, 52(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    lui a2, 65536
; RV32V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vx v10, v9, a0
; RV32V-NEXT:    li a1, 56
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a2, 36(sp)
; RV32V-NEXT:    vsrl.vx v11, v9, a1
; RV32V-NEXT:    lui s10, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s10, 28(sp)
; RV32V-NEXT:    addi a2, s1, -256
; RV32V-NEXT:    vand.vx v12, v10, a2
; RV32V-NEXT:    lui s9, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s9, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    vsrl.vi v13, v9, 24
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    addi a3, sp, 272
; RV32V-NEXT:    vlse64.v v10, (a3), zero
; RV32V-NEXT:    vsrl.vi v14, v9, 8
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vv v14, v14, v10
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v12, v14, v13
; RV32V-NEXT:    vand.vx v13, v9, a2
; RV32V-NEXT:    vsll.vx v14, v9, a1
; RV32V-NEXT:    vsll.vx v13, v13, a0
; RV32V-NEXT:    vand.vx v15, v9, a3
; RV32V-NEXT:    vand.vv v9, v9, v10
; RV32V-NEXT:    vsll.vi v15, v15, 24
; RV32V-NEXT:    vsll.vi v9, v9, 8
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v9, v15, v9
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v9, v13, v9
; RV32V-NEXT:    vor.vv v11, v9, v11
; RV32V-NEXT:    lui a7, 61681
; RV32V-NEXT:    addi a7, a7, -241
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v9, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v12, v11, 4
; RV32V-NEXT:    vand.vv v11, v11, v9
; RV32V-NEXT:    vand.vv v12, v12, v9
; RV32V-NEXT:    vsll.vi v11, v11, 4
; RV32V-NEXT:    vor.vv v12, v12, v11
; RV32V-NEXT:    lui a7, 209715
; RV32V-NEXT:    addi a7, a7, 819
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v11, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v13, v12, 2
; RV32V-NEXT:    vand.vv v12, v12, v11
; RV32V-NEXT:    vand.vv v13, v13, v11
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v13, v13, v12
; RV32V-NEXT:    lui a7, 349525
; RV32V-NEXT:    addi a7, a7, 1365
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, mu
; RV32V-NEXT:    vsrl.vi v14, v13, 1
; RV32V-NEXT:    vand.vv v13, v13, v12
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vadd.vv v13, v13, v13
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vand.vx v17, v13, s11
; RV32V-NEXT:    vand.vx v19, v13, ra
; RV32V-NEXT:    vand.vx v16, v13, a6
; RV32V-NEXT:    vand.vx v18, v13, a5
; RV32V-NEXT:    vand.vx v15, v13, a4
; RV32V-NEXT:    vsrl.vx v14, v8, a0
; RV32V-NEXT:    vsrl.vx v20, v8, a1
; RV32V-NEXT:    vand.vx v14, v14, a2
; RV32V-NEXT:    vsrl.vi v21, v8, 24
; RV32V-NEXT:    vsrl.vi v22, v8, 8
; RV32V-NEXT:    vand.vx v21, v21, a3
; RV32V-NEXT:    vand.vv v22, v22, v10
; RV32V-NEXT:    vor.vv v14, v14, v20
; RV32V-NEXT:    vor.vv v20, v22, v21
; RV32V-NEXT:    vand.vx v21, v8, a2
; RV32V-NEXT:    vsll.vx v22, v8, a1
; RV32V-NEXT:    vsll.vx v21, v21, a0
; RV32V-NEXT:    vand.vx v23, v8, a3
; RV32V-NEXT:    vand.vv v24, v8, v10
; RV32V-NEXT:    vsll.vi v23, v23, 24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vor.vv v21, v22, v21
; RV32V-NEXT:    vor.vv v22, v23, v24
; RV32V-NEXT:    vor.vv v14, v20, v14
; RV32V-NEXT:    vor.vv v20, v21, v22
; RV32V-NEXT:    vand.vx v21, v13, s7
; RV32V-NEXT:    vor.vv v14, v20, v14
; RV32V-NEXT:    vand.vx v20, v13, s8
; RV32V-NEXT:    vsrl.vi v22, v14, 4
; RV32V-NEXT:    vand.vv v14, v14, v9
; RV32V-NEXT:    vand.vv v22, v22, v9
; RV32V-NEXT:    vsll.vi v14, v14, 4
; RV32V-NEXT:    vand.vx v23, v13, s6
; RV32V-NEXT:    vor.vv v14, v22, v14
; RV32V-NEXT:    vand.vx v22, v13, s5
; RV32V-NEXT:    vsrl.vi v24, v14, 2
; RV32V-NEXT:    vand.vv v14, v14, v11
; RV32V-NEXT:    vand.vv v24, v24, v11
; RV32V-NEXT:    vsll.vi v14, v14, 2
; RV32V-NEXT:    vand.vx v25, v13, s3
; RV32V-NEXT:    vor.vv v14, v24, v14
; RV32V-NEXT:    vand.vx v24, v13, s4
; RV32V-NEXT:    vsrl.vi v26, v14, 1
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vand.vv v26, v26, v12
; RV32V-NEXT:    vadd.vv v14, v14, v14
; RV32V-NEXT:    vand.vx v27, v13, s2
; RV32V-NEXT:    vor.vv v14, v26, v14
; RV32V-NEXT:    vand.vi v26, v13, 1
; RV32V-NEXT:    vand.vi v28, v13, 2
; RV32V-NEXT:    vmul.vv v26, v14, v26
; RV32V-NEXT:    vand.vx v29, v13, s1
; RV32V-NEXT:    vmul.vv v28, v14, v28
; RV32V-NEXT:    vand.vi v30, v13, 4
; RV32V-NEXT:    vxor.vi v26, v26, 0
; RV32V-NEXT:    vmul.vv v30, v14, v30
; RV32V-NEXT:    vand.vi v31, v13, 8
; RV32V-NEXT:    vxor.vv v26, v26, v28
; RV32V-NEXT:    vmul.vv v28, v14, v31
; RV32V-NEXT:    vand.vx v31, v13, s0
; RV32V-NEXT:    vxor.vv v26, v26, v30
; RV32V-NEXT:    vmul.vv v17, v14, v17
; RV32V-NEXT:    vxor.vv v26, v26, v28
; RV32V-NEXT:    vmul.vv v19, v14, v19
; RV32V-NEXT:    vand.vx v28, v13, t6
; RV32V-NEXT:    vxor.vv v17, v26, v17
; RV32V-NEXT:    vmul.vv v16, v14, v16
; RV32V-NEXT:    vxor.vv v17, v17, v19
; RV32V-NEXT:    vmul.vv v18, v14, v18
; RV32V-NEXT:    vand.vx v19, v13, t5
; RV32V-NEXT:    vxor.vv v16, v17, v16
; RV32V-NEXT:    vmul.vv v15, v14, v15
; RV32V-NEXT:    vxor.vv v16, v16, v18
; RV32V-NEXT:    vmul.vv v17, v14, v21
; RV32V-NEXT:    vand.vx v18, v13, t4
; RV32V-NEXT:    vxor.vv v15, v16, v15
; RV32V-NEXT:    vmul.vv v16, v14, v20
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v23
; RV32V-NEXT:    vand.vx v20, v13, t3
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v22
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v25
; RV32V-NEXT:    vand.vx v21, v13, t2
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v24
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v27
; RV32V-NEXT:    vand.vx v22, v13, t1
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v29
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v31
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    vand.vx v23, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v28
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v19
; RV32V-NEXT:    lui a4, 8192
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v20
; RV32V-NEXT:    lui a4, 16384
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v21
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v22
; RV32V-NEXT:    lui a4, 32768
; RV32V-NEXT:    vand.vx v20, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v23
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v19
; RV32V-NEXT:    lui a4, 65536
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v20
; RV32V-NEXT:    vand.vx v18, v13, s10
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v19
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vx v18, v13, s9
; RV32V-NEXT:    addi a4, sp, 264
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 256
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 248
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 240
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 232
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 224
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 216
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 208
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 200
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 192
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 184
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 176
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 168
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 160
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 152
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 144
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 136
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 128
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 120
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 112
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 104
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 96
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 88
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 80
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 72
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 64
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 56
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 48
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 40
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 32
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 24
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 16
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 8
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v13, v13, v19
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v13, v14, v13
; RV32V-NEXT:    vxor.vv v14, v15, v16
; RV32V-NEXT:    vxor.vv v13, v14, v13
; RV32V-NEXT:    vsrl.vi v14, v13, 24
; RV32V-NEXT:    vsrl.vx v15, v13, a0
; RV32V-NEXT:    vand.vx v14, v14, a3
; RV32V-NEXT:    vand.vx v16, v13, a3
; RV32V-NEXT:    vand.vx v15, v15, a2
; RV32V-NEXT:    vand.vx v17, v13, a2
; RV32V-NEXT:    vsrl.vx v18, v13, a1
; RV32V-NEXT:    vsll.vx v19, v13, a1
; RV32V-NEXT:    vsll.vx v17, v17, a0
; RV32V-NEXT:    vor.vv v15, v15, v18
; RV32V-NEXT:    vsrl.vi v18, v13, 8
; RV32V-NEXT:    vand.vv v13, v13, v10
; RV32V-NEXT:    vand.vv v10, v18, v10
; RV32V-NEXT:    vsll.vi v13, v13, 8
; RV32V-NEXT:    vsll.vi v16, v16, 24
; RV32V-NEXT:    vor.vv v10, v10, v14
; RV32V-NEXT:    vor.vv v13, v16, v13
; RV32V-NEXT:    vor.vv v14, v19, v17
; RV32V-NEXT:    vor.vv v10, v10, v15
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v10, v13, v10
; RV32V-NEXT:    vsrl.vi v13, v10, 4
; RV32V-NEXT:    vand.vv v10, v10, v9
; RV32V-NEXT:    vand.vv v9, v13, v9
; RV32V-NEXT:    vsll.vi v10, v10, 4
; RV32V-NEXT:    vor.vv v9, v9, v10
; RV32V-NEXT:    vsrl.vi v10, v9, 2
; RV32V-NEXT:    vand.vv v9, v9, v11
; RV32V-NEXT:    vand.vv v10, v10, v11
; RV32V-NEXT:    vsll.vi v9, v9, 2
; RV32V-NEXT:    vor.vv v9, v10, v9
; RV32V-NEXT:    vsrl.vi v10, v9, 1
; RV32V-NEXT:    vand.vv v9, v9, v12
; RV32V-NEXT:    vand.vv v10, v10, v12
; RV32V-NEXT:    vadd.vv v9, v9, v9
; RV32V-NEXT:    vor.vv v9, v10, v9
; RV32V-NEXT:    vsrl.vi v8, v9, 1, v0.t
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    .cfi_restore ra
; RV32V-NEXT:    .cfi_restore s0
; RV32V-NEXT:    .cfi_restore s1
; RV32V-NEXT:    .cfi_restore s2
; RV32V-NEXT:    .cfi_restore s3
; RV32V-NEXT:    .cfi_restore s4
; RV32V-NEXT:    .cfi_restore s5
; RV32V-NEXT:    .cfi_restore s6
; RV32V-NEXT:    .cfi_restore s7
; RV32V-NEXT:    .cfi_restore s8
; RV32V-NEXT:    .cfi_restore s9
; RV32V-NEXT:    .cfi_restore s10
; RV32V-NEXT:    .cfi_restore s11
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    .cfi_def_cfa_offset 0
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i64_vv_mask:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a0, 56
; RV64V-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; RV64V-NEXT:    vsrl.vx v10, v8, a0
; RV64V-NEXT:    li a1, 40
; RV64V-NEXT:    vsrl.vx v11, v8, a1
; RV64V-NEXT:    lui t1, 16
; RV64V-NEXT:    vsrl.vi v12, v8, 24
; RV64V-NEXT:    addi a2, t1, -256
; RV64V-NEXT:    vand.vx v11, v11, a2
; RV64V-NEXT:    vsrl.vi v13, v8, 8
; RV64V-NEXT:    lui a3, 4080
; RV64V-NEXT:    vand.vx v12, v12, a3
; RV64V-NEXT:    li a4, 255
; RV64V-NEXT:    slli a4, a4, 24
; RV64V-NEXT:    vand.vx v13, v13, a4
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vor.vv v11, v13, v12
; RV64V-NEXT:    vand.vx v12, v8, a3
; RV64V-NEXT:    vand.vx v13, v8, a4
; RV64V-NEXT:    vsll.vi v12, v12, 24
; RV64V-NEXT:    vsll.vi v13, v13, 8
; RV64V-NEXT:    vand.vx v14, v8, a2
; RV64V-NEXT:    vsll.vx v15, v8, a0
; RV64V-NEXT:    vsll.vx v14, v14, a1
; RV64V-NEXT:    vor.vv v12, v12, v13
; RV64V-NEXT:    vor.vv v13, v15, v14
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vor.vv v11, v13, v12
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    lui a5, 61681
; RV64V-NEXT:    addi a5, a5, -241
; RV64V-NEXT:    vsrl.vi v11, v10, 4
; RV64V-NEXT:    slli a6, a5, 32
; RV64V-NEXT:    add a5, a5, a6
; RV64V-NEXT:    vand.vx v10, v10, a5
; RV64V-NEXT:    vand.vx v11, v11, a5
; RV64V-NEXT:    vsll.vi v10, v10, 4
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vsrl.vi v11, v10, 2
; RV64V-NEXT:    lui a6, 209715
; RV64V-NEXT:    addi a6, a6, 819
; RV64V-NEXT:    slli a7, a6, 32
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    vand.vx v11, v11, a6
; RV64V-NEXT:    vsrl.vx v12, v9, a1
; RV64V-NEXT:    vsrl.vx v13, v9, a0
; RV64V-NEXT:    vand.vx v12, v12, a2
; RV64V-NEXT:    vsrl.vi v14, v9, 24
; RV64V-NEXT:    vsrl.vi v15, v9, 8
; RV64V-NEXT:    vand.vx v14, v14, a3
; RV64V-NEXT:    vand.vx v15, v15, a4
; RV64V-NEXT:    vor.vv v12, v12, v13
; RV64V-NEXT:    vor.vv v13, v15, v14
; RV64V-NEXT:    vand.vx v14, v9, a3
; RV64V-NEXT:    vand.vx v15, v9, a4
; RV64V-NEXT:    vsll.vi v14, v14, 24
; RV64V-NEXT:    vsll.vi v15, v15, 8
; RV64V-NEXT:    vand.vx v16, v9, a2
; RV64V-NEXT:    vsll.vx v9, v9, a0
; RV64V-NEXT:    vsll.vx v16, v16, a1
; RV64V-NEXT:    vor.vv v14, v14, v15
; RV64V-NEXT:    vor.vv v9, v9, v16
; RV64V-NEXT:    vor.vv v12, v13, v12
; RV64V-NEXT:    vor.vv v9, v9, v14
; RV64V-NEXT:    vand.vx v10, v10, a6
; RV64V-NEXT:    vor.vv v9, v9, v12
; RV64V-NEXT:    vsll.vi v10, v10, 2
; RV64V-NEXT:    vsrl.vi v12, v9, 4
; RV64V-NEXT:    vand.vx v9, v9, a5
; RV64V-NEXT:    vand.vx v12, v12, a5
; RV64V-NEXT:    vsll.vi v9, v9, 4
; RV64V-NEXT:    vor.vv v10, v11, v10
; RV64V-NEXT:    vor.vv v9, v12, v9
; RV64V-NEXT:    vsrl.vi v11, v10, 1
; RV64V-NEXT:    vsrl.vi v12, v9, 2
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    lui a7, 349525
; RV64V-NEXT:    vand.vx v12, v12, a6
; RV64V-NEXT:    addi a7, a7, 1365
; RV64V-NEXT:    slli t0, a7, 32
; RV64V-NEXT:    vsll.vi v9, v9, 2
; RV64V-NEXT:    add a7, a7, t0
; RV64V-NEXT:    vand.vx v11, v11, a7
; RV64V-NEXT:    vor.vv v9, v12, v9
; RV64V-NEXT:    vand.vx v10, v10, a7
; RV64V-NEXT:    vsrl.vi v12, v9, 1
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vand.vx v12, v12, a7
; RV64V-NEXT:    vadd.vv v9, v9, v9
; RV64V-NEXT:    vadd.vv v13, v10, v10
; RV64V-NEXT:    vor.vv v10, v12, v9
; RV64V-NEXT:    vor.vv v9, v11, v13
; RV64V-NEXT:    vand.vi v11, v10, 2
; RV64V-NEXT:    vand.vi v12, v10, 1
; RV64V-NEXT:    vmul.vv v11, v9, v11
; RV64V-NEXT:    vmul.vv v12, v9, v12
; RV64V-NEXT:    vand.vi v13, v10, 4
; RV64V-NEXT:    vmul.vv v13, v9, v13
; RV64V-NEXT:    vand.vi v14, v10, 8
; RV64V-NEXT:    vxor.vv v11, v12, v11
; RV64V-NEXT:    vmul.vv v12, v9, v14
; RV64V-NEXT:    li t0, 16
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v9, v14
; RV64V-NEXT:    li t0, 32
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v9, v14
; RV64V-NEXT:    li t0, 64
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v9, v14
; RV64V-NEXT:    li t0, 128
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v9, v14
; RV64V-NEXT:    li t0, 256
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v9, v14
; RV64V-NEXT:    li t0, 512
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    vmul.vv v12, v9, v14
; RV64V-NEXT:    li t0, 1024
; RV64V-NEXT:    vand.vx v14, v10, t0
; RV64V-NEXT:    vxor.vv v13, v11, v13
; RV64V-NEXT:    li t0, 1
; RV64V-NEXT:    vmul.vv v14, v9, v14
; RV64V-NEXT:    slli t2, t0, 11
; RV64V-NEXT:    vand.vx v15, v10, t2
; RV64V-NEXT:    vxor.vv v12, v13, v12
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t2, 1
; RV64V-NEXT:    vand.vx v15, v10, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t2, 2
; RV64V-NEXT:    vand.vx v15, v10, t2
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t2, 4
; RV64V-NEXT:    vand.vx v15, v10, t2
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t2, 8
; RV64V-NEXT:    vand.vx v15, v10, t2
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 32
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 64
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 128
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 256
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 512
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 1024
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 2048
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vsll.vx v11, v11, a0
; RV64V-NEXT:    vand.vx v13, v12, a2
; RV64V-NEXT:    lui t1, 4096
; RV64V-NEXT:    vand.vx v14, v10, t1
; RV64V-NEXT:    vsll.vx v13, v13, a1
; RV64V-NEXT:    vmul.vv v14, v9, v14
; RV64V-NEXT:    lui t1, 8192
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vor.vv v11, v11, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 16384
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 32768
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 65536
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    lui t1, 131072
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    lui t1, 262144
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 31
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 33
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 34
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 35
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 36
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 37
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 38
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 39
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 40
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    vmul.vv v14, v9, v15
; RV64V-NEXT:    slli t1, t0, 41
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    vmul.vv v13, v9, v15
; RV64V-NEXT:    slli t1, t0, 42
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vxor.vv v14, v12, v14
; RV64V-NEXT:    vmul.vv v15, v9, v15
; RV64V-NEXT:    slli t1, t0, 43
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v14, v13
; RV64V-NEXT:    vmul.vv v14, v9, v16
; RV64V-NEXT:    slli t1, t0, 44
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vmul.vv v15, v9, v16
; RV64V-NEXT:    slli t1, t0, 45
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vmul.vv v14, v9, v16
; RV64V-NEXT:    slli t1, t0, 46
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vmul.vv v15, v9, v16
; RV64V-NEXT:    slli t1, t0, 47
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vmul.vv v14, v9, v16
; RV64V-NEXT:    slli t1, t0, 48
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    vmul.vv v15, v9, v16
; RV64V-NEXT:    slli t1, t0, 49
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    vmul.vv v14, v9, v16
; RV64V-NEXT:    slli t1, t0, 50
; RV64V-NEXT:    vand.vx v16, v10, t1
; RV64V-NEXT:    vxor.vv v15, v13, v15
; RV64V-NEXT:    vmul.vv v16, v9, v16
; RV64V-NEXT:    slli t1, t0, 51
; RV64V-NEXT:    vand.vx v17, v10, t1
; RV64V-NEXT:    vxor.vv v14, v15, v14
; RV64V-NEXT:    vmul.vv v15, v9, v17
; RV64V-NEXT:    slli t1, t0, 52
; RV64V-NEXT:    vand.vx v17, v10, t1
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vmul.vv v16, v9, v17
; RV64V-NEXT:    slli t1, t0, 53
; RV64V-NEXT:    vand.vx v17, v10, t1
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    vmul.vv v15, v9, v17
; RV64V-NEXT:    slli t1, t0, 54
; RV64V-NEXT:    vand.vx v17, v10, t1
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vmul.vv v16, v9, v17
; RV64V-NEXT:    slli t1, t0, 55
; RV64V-NEXT:    vand.vx v17, v10, t1
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    vmul.vv v15, v9, v17
; RV64V-NEXT:    vand.vx v17, v12, a3
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    vsll.vi v16, v17, 24
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    slli t1, t0, 56
; RV64V-NEXT:    vand.vx v15, v10, t1
; RV64V-NEXT:    vand.vx v17, v14, a4
; RV64V-NEXT:    vmul.vv v15, v9, v15
; RV64V-NEXT:    slli t1, t0, 57
; RV64V-NEXT:    vand.vx v18, v10, t1
; RV64V-NEXT:    vsll.vi v17, v17, 8
; RV64V-NEXT:    vmul.vv v18, v9, v18
; RV64V-NEXT:    slli t1, t0, 58
; RV64V-NEXT:    vand.vx v19, v10, t1
; RV64V-NEXT:    vxor.vv v15, v14, v15
; RV64V-NEXT:    vmul.vv v19, v9, v19
; RV64V-NEXT:    slli t1, t0, 59
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vxor.vv v15, v15, v18
; RV64V-NEXT:    vmul.vv v18, v9, v20
; RV64V-NEXT:    slli t1, t0, 60
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vxor.vv v15, v15, v19
; RV64V-NEXT:    vmul.vv v19, v9, v20
; RV64V-NEXT:    slli t1, t0, 61
; RV64V-NEXT:    vand.vx v20, v10, t1
; RV64V-NEXT:    vxor.vv v15, v15, v18
; RV64V-NEXT:    vmul.vv v18, v9, v20
; RV64V-NEXT:    vor.vv v16, v16, v17
; RV64V-NEXT:    vxor.vv v15, v15, v19
; RV64V-NEXT:    vsrl.vi v12, v12, 8
; RV64V-NEXT:    vxor.vv v15, v15, v18
; RV64V-NEXT:    slli t0, t0, 62
; RV64V-NEXT:    vand.vx v17, v10, t0
; RV64V-NEXT:    vsrl.vi v13, v13, 24
; RV64V-NEXT:    li t0, -1
; RV64V-NEXT:    vmul.vv v17, v9, v17
; RV64V-NEXT:    slli t0, t0, 63
; RV64V-NEXT:    vand.vx v10, v10, t0
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vmul.vv v9, v9, v10
; RV64V-NEXT:    vand.vx v10, v13, a3
; RV64V-NEXT:    vxor.vv v13, v15, v17
; RV64V-NEXT:    vsrl.vx v14, v14, a1
; RV64V-NEXT:    vxor.vv v9, v13, v9
; RV64V-NEXT:    vand.vx v13, v14, a2
; RV64V-NEXT:    vsrl.vx v9, v9, a0
; RV64V-NEXT:    vor.vv v10, v12, v10
; RV64V-NEXT:    vor.vv v9, v13, v9
; RV64V-NEXT:    vor.vv v11, v11, v16
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 4
; RV64V-NEXT:    vand.vx v9, v9, a5
; RV64V-NEXT:    vand.vx v10, v10, a5
; RV64V-NEXT:    vsll.vi v9, v9, 4
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 2
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vand.vx v10, v10, a6
; RV64V-NEXT:    vsll.vi v9, v9, 2
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 1
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vand.vx v10, v10, a7
; RV64V-NEXT:    vadd.vv v9, v9, v9
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v8, v9, 1, v0.t
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i64_vv_mask:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v9, v0.t
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i64_vv_mask:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a0, zero, e64, m1, ta, mu
; RV64ZVBC-NEXT:    vclmulh.vv v8, v8, v9, v0.t
; RV64ZVBC-NEXT:    ret
  %va.ext = zext <vscale x 1 x i64> %va to <vscale x 1 x i128>
  %vb.ext = zext <vscale x 1 x i64> %vb to <vscale x 1 x i128>
  %clmul = call <vscale x 1 x i128> @llvm.clmul.nxv1i128(<vscale x 1 x i128> %va.ext, <vscale x 1 x i128> %vb.ext)
  %res.ext = lshr <vscale x 1 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 1 x i128> %res.ext to <vscale x 1 x i64>
  %sel = select <vscale x 1 x i1> %mask, <vscale x 1 x i64> %res, <vscale x 1 x i64> %va
  ret <vscale x 1 x i64> %sel
}

define <vscale x 1 x i64> @clmulh_nxv1i64_vx_mask(<vscale x 1 x i64> %va, i64 %b, <vscale x 1 x i1> %mask) {
; RV32V-LABEL: clmulh_nxv1i64_vx_mask:
; RV32V:       # %bb.0:
; RV32V-NEXT:    addi sp, sp, -336
; RV32V-NEXT:    .cfi_def_cfa_offset 336
; RV32V-NEXT:    sw ra, 332(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s0, 328(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s1, 324(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s2, 320(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s3, 316(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s4, 312(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s5, 308(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s6, 304(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s7, 300(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s8, 296(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s9, 292(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s10, 288(sp) # 4-byte Folded Spill
; RV32V-NEXT:    sw s11, 284(sp) # 4-byte Folded Spill
; RV32V-NEXT:    .cfi_offset ra, -4
; RV32V-NEXT:    .cfi_offset s0, -8
; RV32V-NEXT:    .cfi_offset s1, -12
; RV32V-NEXT:    .cfi_offset s2, -16
; RV32V-NEXT:    .cfi_offset s3, -20
; RV32V-NEXT:    .cfi_offset s4, -24
; RV32V-NEXT:    .cfi_offset s5, -28
; RV32V-NEXT:    .cfi_offset s6, -32
; RV32V-NEXT:    .cfi_offset s7, -36
; RV32V-NEXT:    .cfi_offset s8, -40
; RV32V-NEXT:    .cfi_offset s9, -44
; RV32V-NEXT:    .cfi_offset s10, -48
; RV32V-NEXT:    .cfi_offset s11, -52
; RV32V-NEXT:    sw a0, 0(sp)
; RV32V-NEXT:    sw a1, 4(sp)
; RV32V-NEXT:    mv a0, sp
; RV32V-NEXT:    vsetvli a1, zero, e64, m1, ta, ma
; RV32V-NEXT:    vlse64.v v10, (a0), zero
; RV32V-NEXT:    lui a0, 1044480
; RV32V-NEXT:    lui a3, 524288
; RV32V-NEXT:    sw a0, 272(sp)
; RV32V-NEXT:    sw zero, 276(sp)
; RV32V-NEXT:    sw a3, 264(sp)
; RV32V-NEXT:    sw zero, 268(sp)
; RV32V-NEXT:    li s6, 1
; RV32V-NEXT:    sw zero, 256(sp)
; RV32V-NEXT:    sw s6, 260(sp)
; RV32V-NEXT:    li a0, 2
; RV32V-NEXT:    sw zero, 248(sp)
; RV32V-NEXT:    sw a0, 252(sp)
; RV32V-NEXT:    li a0, 4
; RV32V-NEXT:    sw zero, 240(sp)
; RV32V-NEXT:    sw a0, 244(sp)
; RV32V-NEXT:    li a0, 8
; RV32V-NEXT:    sw zero, 232(sp)
; RV32V-NEXT:    sw a0, 236(sp)
; RV32V-NEXT:    li s11, 16
; RV32V-NEXT:    sw zero, 224(sp)
; RV32V-NEXT:    sw s11, 228(sp)
; RV32V-NEXT:    li ra, 32
; RV32V-NEXT:    sw zero, 216(sp)
; RV32V-NEXT:    sw ra, 220(sp)
; RV32V-NEXT:    li a6, 64
; RV32V-NEXT:    sw zero, 208(sp)
; RV32V-NEXT:    sw a6, 212(sp)
; RV32V-NEXT:    li a5, 128
; RV32V-NEXT:    sw zero, 200(sp)
; RV32V-NEXT:    sw a5, 204(sp)
; RV32V-NEXT:    li a4, 256
; RV32V-NEXT:    sw zero, 192(sp)
; RV32V-NEXT:    sw a4, 196(sp)
; RV32V-NEXT:    li s7, 512
; RV32V-NEXT:    sw zero, 184(sp)
; RV32V-NEXT:    sw s7, 188(sp)
; RV32V-NEXT:    li s8, 1024
; RV32V-NEXT:    sw zero, 176(sp)
; RV32V-NEXT:    sw s8, 180(sp)
; RV32V-NEXT:    slli s6, s6, 11
; RV32V-NEXT:    sw zero, 168(sp)
; RV32V-NEXT:    sw s6, 172(sp)
; RV32V-NEXT:    lui s5, 1
; RV32V-NEXT:    sw zero, 160(sp)
; RV32V-NEXT:    sw s5, 164(sp)
; RV32V-NEXT:    lui s3, 2
; RV32V-NEXT:    sw zero, 152(sp)
; RV32V-NEXT:    sw s3, 156(sp)
; RV32V-NEXT:    lui s4, 4
; RV32V-NEXT:    sw zero, 144(sp)
; RV32V-NEXT:    sw s4, 148(sp)
; RV32V-NEXT:    lui s2, 8
; RV32V-NEXT:    sw zero, 136(sp)
; RV32V-NEXT:    sw s2, 140(sp)
; RV32V-NEXT:    lui s1, 16
; RV32V-NEXT:    sw zero, 128(sp)
; RV32V-NEXT:    sw s1, 132(sp)
; RV32V-NEXT:    lui s0, 32
; RV32V-NEXT:    sw zero, 120(sp)
; RV32V-NEXT:    sw s0, 124(sp)
; RV32V-NEXT:    lui t6, 64
; RV32V-NEXT:    sw zero, 112(sp)
; RV32V-NEXT:    sw t6, 116(sp)
; RV32V-NEXT:    lui t5, 128
; RV32V-NEXT:    sw zero, 104(sp)
; RV32V-NEXT:    sw t5, 108(sp)
; RV32V-NEXT:    lui t4, 256
; RV32V-NEXT:    sw zero, 96(sp)
; RV32V-NEXT:    sw t4, 100(sp)
; RV32V-NEXT:    lui t3, 512
; RV32V-NEXT:    sw zero, 88(sp)
; RV32V-NEXT:    sw t3, 92(sp)
; RV32V-NEXT:    lui t2, 1024
; RV32V-NEXT:    sw zero, 80(sp)
; RV32V-NEXT:    sw t2, 84(sp)
; RV32V-NEXT:    lui t1, 2048
; RV32V-NEXT:    sw zero, 72(sp)
; RV32V-NEXT:    sw t1, 76(sp)
; RV32V-NEXT:    lui a0, 4096
; RV32V-NEXT:    sw zero, 64(sp)
; RV32V-NEXT:    sw a0, 68(sp)
; RV32V-NEXT:    lui a0, 8192
; RV32V-NEXT:    sw zero, 56(sp)
; RV32V-NEXT:    sw a0, 60(sp)
; RV32V-NEXT:    lui a0, 16384
; RV32V-NEXT:    sw zero, 48(sp)
; RV32V-NEXT:    sw a0, 52(sp)
; RV32V-NEXT:    lui a0, 32768
; RV32V-NEXT:    sw zero, 40(sp)
; RV32V-NEXT:    sw a0, 44(sp)
; RV32V-NEXT:    li a0, 40
; RV32V-NEXT:    lui a2, 65536
; RV32V-NEXT:    vsrl.vx v9, v10, a0
; RV32V-NEXT:    li a1, 56
; RV32V-NEXT:    sw zero, 32(sp)
; RV32V-NEXT:    sw a2, 36(sp)
; RV32V-NEXT:    vsrl.vx v11, v10, a1
; RV32V-NEXT:    lui s10, 131072
; RV32V-NEXT:    sw zero, 24(sp)
; RV32V-NEXT:    sw s10, 28(sp)
; RV32V-NEXT:    addi a2, s1, -256
; RV32V-NEXT:    vand.vx v12, v9, a2
; RV32V-NEXT:    lui s9, 262144
; RV32V-NEXT:    sw zero, 16(sp)
; RV32V-NEXT:    sw s9, 20(sp)
; RV32V-NEXT:    sw a3, 12(sp)
; RV32V-NEXT:    vsrl.vi v13, v10, 24
; RV32V-NEXT:    sw zero, 8(sp)
; RV32V-NEXT:    addi a3, sp, 272
; RV32V-NEXT:    vlse64.v v9, (a3), zero
; RV32V-NEXT:    vsrl.vi v14, v10, 8
; RV32V-NEXT:    lui a3, 4080
; RV32V-NEXT:    vand.vx v13, v13, a3
; RV32V-NEXT:    vand.vv v14, v14, v9
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v12, v14, v13
; RV32V-NEXT:    vand.vv v13, v10, v9
; RV32V-NEXT:    vand.vx v14, v10, a3
; RV32V-NEXT:    vsll.vi v13, v13, 8
; RV32V-NEXT:    vsll.vi v14, v14, 24
; RV32V-NEXT:    vand.vx v15, v10, a2
; RV32V-NEXT:    vsll.vx v10, v10, a1
; RV32V-NEXT:    vsll.vx v15, v15, a0
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v10, v10, v15
; RV32V-NEXT:    vor.vv v11, v12, v11
; RV32V-NEXT:    vor.vv v10, v10, v13
; RV32V-NEXT:    vor.vv v11, v10, v11
; RV32V-NEXT:    lui a7, 61681
; RV32V-NEXT:    addi a7, a7, -241
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v10, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v12, v11, 4
; RV32V-NEXT:    vand.vv v11, v11, v10
; RV32V-NEXT:    vand.vv v12, v12, v10
; RV32V-NEXT:    vsll.vi v11, v11, 4
; RV32V-NEXT:    vor.vv v12, v12, v11
; RV32V-NEXT:    lui a7, 209715
; RV32V-NEXT:    addi a7, a7, 819
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v11, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, ma
; RV32V-NEXT:    vsrl.vi v13, v12, 2
; RV32V-NEXT:    vand.vv v12, v12, v11
; RV32V-NEXT:    vand.vv v13, v13, v11
; RV32V-NEXT:    vsll.vi v12, v12, 2
; RV32V-NEXT:    vor.vv v13, v13, v12
; RV32V-NEXT:    lui a7, 349525
; RV32V-NEXT:    addi a7, a7, 1365
; RV32V-NEXT:    vsetvli t0, zero, e32, m1, ta, ma
; RV32V-NEXT:    vmv.v.x v12, a7
; RV32V-NEXT:    vsetvli a7, zero, e64, m1, ta, mu
; RV32V-NEXT:    vsrl.vi v14, v13, 1
; RV32V-NEXT:    vand.vv v13, v13, v12
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vadd.vv v13, v13, v13
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vand.vx v17, v13, s11
; RV32V-NEXT:    vand.vx v19, v13, ra
; RV32V-NEXT:    vand.vx v16, v13, a6
; RV32V-NEXT:    vand.vx v18, v13, a5
; RV32V-NEXT:    vand.vx v15, v13, a4
; RV32V-NEXT:    vsrl.vx v14, v8, a0
; RV32V-NEXT:    vsrl.vx v20, v8, a1
; RV32V-NEXT:    vand.vx v14, v14, a2
; RV32V-NEXT:    vsrl.vi v21, v8, 24
; RV32V-NEXT:    vsrl.vi v22, v8, 8
; RV32V-NEXT:    vand.vx v21, v21, a3
; RV32V-NEXT:    vand.vv v22, v22, v9
; RV32V-NEXT:    vor.vv v14, v14, v20
; RV32V-NEXT:    vor.vv v20, v22, v21
; RV32V-NEXT:    vand.vx v21, v8, a2
; RV32V-NEXT:    vsll.vx v22, v8, a1
; RV32V-NEXT:    vsll.vx v21, v21, a0
; RV32V-NEXT:    vand.vx v23, v8, a3
; RV32V-NEXT:    vand.vv v24, v8, v9
; RV32V-NEXT:    vsll.vi v23, v23, 24
; RV32V-NEXT:    vsll.vi v24, v24, 8
; RV32V-NEXT:    vor.vv v21, v22, v21
; RV32V-NEXT:    vor.vv v22, v23, v24
; RV32V-NEXT:    vor.vv v14, v20, v14
; RV32V-NEXT:    vor.vv v20, v21, v22
; RV32V-NEXT:    vand.vx v21, v13, s7
; RV32V-NEXT:    vor.vv v14, v20, v14
; RV32V-NEXT:    vand.vx v20, v13, s8
; RV32V-NEXT:    vsrl.vi v22, v14, 4
; RV32V-NEXT:    vand.vv v14, v14, v10
; RV32V-NEXT:    vand.vv v22, v22, v10
; RV32V-NEXT:    vsll.vi v14, v14, 4
; RV32V-NEXT:    vand.vx v23, v13, s6
; RV32V-NEXT:    vor.vv v14, v22, v14
; RV32V-NEXT:    vand.vx v22, v13, s5
; RV32V-NEXT:    vsrl.vi v24, v14, 2
; RV32V-NEXT:    vand.vv v14, v14, v11
; RV32V-NEXT:    vand.vv v24, v24, v11
; RV32V-NEXT:    vsll.vi v14, v14, 2
; RV32V-NEXT:    vand.vx v25, v13, s3
; RV32V-NEXT:    vor.vv v14, v24, v14
; RV32V-NEXT:    vand.vx v24, v13, s4
; RV32V-NEXT:    vsrl.vi v26, v14, 1
; RV32V-NEXT:    vand.vv v14, v14, v12
; RV32V-NEXT:    vand.vv v26, v26, v12
; RV32V-NEXT:    vadd.vv v14, v14, v14
; RV32V-NEXT:    vand.vx v27, v13, s2
; RV32V-NEXT:    vor.vv v14, v26, v14
; RV32V-NEXT:    vand.vi v26, v13, 1
; RV32V-NEXT:    vand.vi v28, v13, 2
; RV32V-NEXT:    vmul.vv v26, v14, v26
; RV32V-NEXT:    vand.vx v29, v13, s1
; RV32V-NEXT:    vmul.vv v28, v14, v28
; RV32V-NEXT:    vand.vi v30, v13, 4
; RV32V-NEXT:    vxor.vi v26, v26, 0
; RV32V-NEXT:    vmul.vv v30, v14, v30
; RV32V-NEXT:    vand.vi v31, v13, 8
; RV32V-NEXT:    vxor.vv v26, v26, v28
; RV32V-NEXT:    vmul.vv v28, v14, v31
; RV32V-NEXT:    vand.vx v31, v13, s0
; RV32V-NEXT:    vxor.vv v26, v26, v30
; RV32V-NEXT:    vmul.vv v17, v14, v17
; RV32V-NEXT:    vxor.vv v26, v26, v28
; RV32V-NEXT:    vmul.vv v19, v14, v19
; RV32V-NEXT:    vand.vx v28, v13, t6
; RV32V-NEXT:    vxor.vv v17, v26, v17
; RV32V-NEXT:    vmul.vv v16, v14, v16
; RV32V-NEXT:    vxor.vv v17, v17, v19
; RV32V-NEXT:    vmul.vv v18, v14, v18
; RV32V-NEXT:    vand.vx v19, v13, t5
; RV32V-NEXT:    vxor.vv v16, v17, v16
; RV32V-NEXT:    vmul.vv v15, v14, v15
; RV32V-NEXT:    vxor.vv v16, v16, v18
; RV32V-NEXT:    vmul.vv v17, v14, v21
; RV32V-NEXT:    vand.vx v18, v13, t4
; RV32V-NEXT:    vxor.vv v15, v16, v15
; RV32V-NEXT:    vmul.vv v16, v14, v20
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v23
; RV32V-NEXT:    vand.vx v20, v13, t3
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v22
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v25
; RV32V-NEXT:    vand.vx v21, v13, t2
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v24
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v27
; RV32V-NEXT:    vand.vx v22, v13, t1
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v29
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v31
; RV32V-NEXT:    lui a4, 4096
; RV32V-NEXT:    vand.vx v23, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v28
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v19
; RV32V-NEXT:    lui a4, 8192
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v20
; RV32V-NEXT:    lui a4, 16384
; RV32V-NEXT:    vand.vx v18, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v21
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v22
; RV32V-NEXT:    lui a4, 32768
; RV32V-NEXT:    vand.vx v20, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v23
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v19
; RV32V-NEXT:    lui a4, 65536
; RV32V-NEXT:    vand.vx v19, v13, a4
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v20
; RV32V-NEXT:    vand.vx v18, v13, s10
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v19
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vx v18, v13, s9
; RV32V-NEXT:    addi a4, sp, 264
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 256
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 248
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 240
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 232
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 224
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 216
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 208
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 200
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 192
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 184
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 176
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 168
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 160
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 152
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 144
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 136
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 128
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 120
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 112
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 104
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 96
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 88
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 80
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 72
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 64
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 56
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 48
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 40
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 32
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 24
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 16
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v17, v14, v18
; RV32V-NEXT:    vand.vv v18, v13, v19
; RV32V-NEXT:    addi a4, sp, 8
; RV32V-NEXT:    vlse64.v v19, (a4), zero
; RV32V-NEXT:    vxor.vv v15, v15, v16
; RV32V-NEXT:    vmul.vv v16, v14, v18
; RV32V-NEXT:    vand.vv v13, v13, v19
; RV32V-NEXT:    vxor.vv v15, v15, v17
; RV32V-NEXT:    vmul.vv v13, v14, v13
; RV32V-NEXT:    vxor.vv v14, v15, v16
; RV32V-NEXT:    vxor.vv v13, v14, v13
; RV32V-NEXT:    vsrl.vi v14, v13, 24
; RV32V-NEXT:    vsrl.vx v15, v13, a0
; RV32V-NEXT:    vand.vx v14, v14, a3
; RV32V-NEXT:    vand.vx v16, v13, a3
; RV32V-NEXT:    vand.vx v15, v15, a2
; RV32V-NEXT:    vand.vx v17, v13, a2
; RV32V-NEXT:    vsrl.vx v18, v13, a1
; RV32V-NEXT:    vsll.vx v19, v13, a1
; RV32V-NEXT:    vsll.vx v17, v17, a0
; RV32V-NEXT:    vor.vv v15, v15, v18
; RV32V-NEXT:    vsrl.vi v18, v13, 8
; RV32V-NEXT:    vand.vv v13, v13, v9
; RV32V-NEXT:    vand.vv v9, v18, v9
; RV32V-NEXT:    vsll.vi v13, v13, 8
; RV32V-NEXT:    vsll.vi v16, v16, 24
; RV32V-NEXT:    vor.vv v9, v9, v14
; RV32V-NEXT:    vor.vv v13, v16, v13
; RV32V-NEXT:    vor.vv v14, v19, v17
; RV32V-NEXT:    vor.vv v9, v9, v15
; RV32V-NEXT:    vor.vv v13, v14, v13
; RV32V-NEXT:    vor.vv v9, v13, v9
; RV32V-NEXT:    vsrl.vi v13, v9, 4
; RV32V-NEXT:    vand.vv v9, v9, v10
; RV32V-NEXT:    vand.vv v10, v13, v10
; RV32V-NEXT:    vsll.vi v9, v9, 4
; RV32V-NEXT:    vor.vv v9, v10, v9
; RV32V-NEXT:    vsrl.vi v10, v9, 2
; RV32V-NEXT:    vand.vv v9, v9, v11
; RV32V-NEXT:    vand.vv v10, v10, v11
; RV32V-NEXT:    vsll.vi v9, v9, 2
; RV32V-NEXT:    vor.vv v9, v10, v9
; RV32V-NEXT:    vsrl.vi v10, v9, 1
; RV32V-NEXT:    vand.vv v9, v9, v12
; RV32V-NEXT:    vand.vv v10, v10, v12
; RV32V-NEXT:    vadd.vv v9, v9, v9
; RV32V-NEXT:    vor.vv v9, v10, v9
; RV32V-NEXT:    vsrl.vi v8, v9, 1, v0.t
; RV32V-NEXT:    lw ra, 332(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s0, 328(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s1, 324(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s2, 320(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s3, 316(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s4, 312(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s5, 308(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s6, 304(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s7, 300(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s8, 296(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s9, 292(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s10, 288(sp) # 4-byte Folded Reload
; RV32V-NEXT:    lw s11, 284(sp) # 4-byte Folded Reload
; RV32V-NEXT:    .cfi_restore ra
; RV32V-NEXT:    .cfi_restore s0
; RV32V-NEXT:    .cfi_restore s1
; RV32V-NEXT:    .cfi_restore s2
; RV32V-NEXT:    .cfi_restore s3
; RV32V-NEXT:    .cfi_restore s4
; RV32V-NEXT:    .cfi_restore s5
; RV32V-NEXT:    .cfi_restore s6
; RV32V-NEXT:    .cfi_restore s7
; RV32V-NEXT:    .cfi_restore s8
; RV32V-NEXT:    .cfi_restore s9
; RV32V-NEXT:    .cfi_restore s10
; RV32V-NEXT:    .cfi_restore s11
; RV32V-NEXT:    addi sp, sp, 336
; RV32V-NEXT:    .cfi_def_cfa_offset 0
; RV32V-NEXT:    ret
;
; RV64V-LABEL: clmulh_nxv1i64_vx_mask:
; RV64V:       # %bb.0:
; RV64V-NEXT:    li a1, 56
; RV64V-NEXT:    vsetvli a2, zero, e64, m1, ta, mu
; RV64V-NEXT:    vsrl.vx v9, v8, a1
; RV64V-NEXT:    li a2, 40
; RV64V-NEXT:    vsrl.vx v10, v8, a2
; RV64V-NEXT:    lui t2, 16
; RV64V-NEXT:    vsrl.vi v11, v8, 24
; RV64V-NEXT:    addi a3, t2, -256
; RV64V-NEXT:    vand.vx v10, v10, a3
; RV64V-NEXT:    vsrl.vi v12, v8, 8
; RV64V-NEXT:    lui a5, 4080
; RV64V-NEXT:    vand.vx v11, v11, a5
; RV64V-NEXT:    li a4, 255
; RV64V-NEXT:    slli a4, a4, 24
; RV64V-NEXT:    vand.vx v12, v12, a4
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vor.vv v10, v12, v11
; RV64V-NEXT:    vand.vx v11, v8, a5
; RV64V-NEXT:    vand.vx v12, v8, a4
; RV64V-NEXT:    vsll.vi v11, v11, 24
; RV64V-NEXT:    vsll.vi v12, v12, 8
; RV64V-NEXT:    vand.vx v13, v8, a3
; RV64V-NEXT:    vsll.vx v14, v8, a1
; RV64V-NEXT:    vsll.vx v13, v13, a2
; RV64V-NEXT:    vor.vv v11, v11, v12
; RV64V-NEXT:    vor.vv v12, v14, v13
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vor.vv v10, v12, v11
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    lui a6, 61681
; RV64V-NEXT:    addi a6, a6, -241
; RV64V-NEXT:    vsrl.vi v10, v9, 4
; RV64V-NEXT:    slli a7, a6, 32
; RV64V-NEXT:    add a6, a6, a7
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vand.vx v10, v10, a6
; RV64V-NEXT:    vsll.vi v9, v9, 4
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    lui a7, 209715
; RV64V-NEXT:    addi a7, a7, 819
; RV64V-NEXT:    vsrl.vi v10, v9, 2
; RV64V-NEXT:    slli t0, a7, 32
; RV64V-NEXT:    add a7, a7, t0
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vand.vx v10, v10, a7
; RV64V-NEXT:    vsll.vi v9, v9, 2
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    lui t0, 349525
; RV64V-NEXT:    addi t0, t0, 1365
; RV64V-NEXT:    vsrl.vi v10, v9, 1
; RV64V-NEXT:    slli t1, t0, 32
; RV64V-NEXT:    add t0, t0, t1
; RV64V-NEXT:    vand.vx v9, v9, t0
; RV64V-NEXT:    srli t1, a0, 24
; RV64V-NEXT:    and t1, t1, a5
; RV64V-NEXT:    srli t3, a0, 8
; RV64V-NEXT:    and t3, t3, a4
; RV64V-NEXT:    srli t4, a0, 40
; RV64V-NEXT:    and t4, t4, a3
; RV64V-NEXT:    srli t5, a0, 56
; RV64V-NEXT:    or t1, t3, t1
; RV64V-NEXT:    or t3, t4, t5
; RV64V-NEXT:    and t4, a0, a5
; RV64V-NEXT:    srliw t5, a0, 24
; RV64V-NEXT:    slli t4, t4, 24
; RV64V-NEXT:    slli t5, t5, 32
; RV64V-NEXT:    or t4, t4, t5
; RV64V-NEXT:    and t5, a0, a3
; RV64V-NEXT:    slli t5, t5, 40
; RV64V-NEXT:    slli a0, a0, 56
; RV64V-NEXT:    or a0, a0, t5
; RV64V-NEXT:    vand.vx v10, v10, t0
; RV64V-NEXT:    or t1, t1, t3
; RV64V-NEXT:    or a0, a0, t4
; RV64V-NEXT:    vadd.vv v9, v9, v9
; RV64V-NEXT:    or a0, a0, t1
; RV64V-NEXT:    srli t1, a0, 4
; RV64V-NEXT:    and a0, a0, a6
; RV64V-NEXT:    and t1, t1, a6
; RV64V-NEXT:    slli a0, a0, 4
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 2
; RV64V-NEXT:    and a0, a0, a7
; RV64V-NEXT:    and t1, t1, a7
; RV64V-NEXT:    slli a0, a0, 2
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    srli t1, a0, 1
; RV64V-NEXT:    and a0, a0, t0
; RV64V-NEXT:    and t1, t1, t0
; RV64V-NEXT:    slli a0, a0, 1
; RV64V-NEXT:    or a0, t1, a0
; RV64V-NEXT:    andi t1, a0, 2
; RV64V-NEXT:    vmul.vx v10, v9, t1
; RV64V-NEXT:    andi t1, a0, 1
; RV64V-NEXT:    vmul.vx v11, v9, t1
; RV64V-NEXT:    andi t1, a0, 4
; RV64V-NEXT:    vmul.vx v12, v9, t1
; RV64V-NEXT:    vxor.vv v10, v11, v10
; RV64V-NEXT:    andi t1, a0, 8
; RV64V-NEXT:    vmul.vx v11, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    andi t1, a0, 16
; RV64V-NEXT:    vmul.vx v12, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    andi t1, a0, 32
; RV64V-NEXT:    vmul.vx v11, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    andi t1, a0, 64
; RV64V-NEXT:    vmul.vx v12, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    andi t1, a0, 128
; RV64V-NEXT:    vmul.vx v11, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v12
; RV64V-NEXT:    andi t1, a0, 256
; RV64V-NEXT:    vmul.vx v12, v9, t1
; RV64V-NEXT:    vxor.vv v10, v10, v11
; RV64V-NEXT:    andi t1, a0, 512
; RV64V-NEXT:    vmul.vx v11, v9, t1
; RV64V-NEXT:    vxor.vv v12, v10, v12
; RV64V-NEXT:    andi t1, a0, 1024
; RV64V-NEXT:    vmul.vx v13, v9, t1
; RV64V-NEXT:    li t1, 1
; RV64V-NEXT:    vxor.vv v11, v12, v11
; RV64V-NEXT:    slli t3, t1, 11
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v12, v9, t3
; RV64V-NEXT:    lui t3, 1
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v13, v9, t3
; RV64V-NEXT:    lui t3, 2
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v12, v9, t3
; RV64V-NEXT:    lui t3, 4
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v13, v9, t3
; RV64V-NEXT:    lui t3, 8
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t3, a0, t3
; RV64V-NEXT:    vmul.vx v12, v9, t3
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 32
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v9, t2
; RV64V-NEXT:    lui t2, 64
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 128
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v9, t2
; RV64V-NEXT:    lui t2, 256
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 512
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v9, t2
; RV64V-NEXT:    lui t2, 1024
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 2048
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v9, t2
; RV64V-NEXT:    lui t2, 4096
; RV64V-NEXT:    vxor.vv v11, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 8192
; RV64V-NEXT:    vxor.vv v11, v11, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v12, v9, t2
; RV64V-NEXT:    lui t2, 16384
; RV64V-NEXT:    vxor.vv v13, v11, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    lui t2, 32768
; RV64V-NEXT:    vxor.vv v12, v13, v12
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 65536
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    lui t2, 131072
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    lui t2, 262144
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    srliw t2, a0, 31
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    slli t2, t2, 31
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 32
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 33
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 34
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 35
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 36
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 37
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 38
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 39
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 40
; RV64V-NEXT:    vxor.vv v12, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 41
; RV64V-NEXT:    vxor.vv v12, v12, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v13, v9, t2
; RV64V-NEXT:    slli t2, t1, 42
; RV64V-NEXT:    vxor.vv v14, v12, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t2, t1, 43
; RV64V-NEXT:    vxor.vv v13, v14, v13
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 44
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t2, t1, 45
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 46
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t2, t1, 47
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 48
; RV64V-NEXT:    vxor.vv v13, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t2, t1, 49
; RV64V-NEXT:    vxor.vv v13, v13, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    slli t2, t1, 50
; RV64V-NEXT:    vxor.vv v15, v13, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v9, t2
; RV64V-NEXT:    slli t2, t1, 51
; RV64V-NEXT:    vxor.vv v14, v15, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t2, t1, 52
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v9, t2
; RV64V-NEXT:    vsll.vx v10, v10, a1
; RV64V-NEXT:    slli t2, t1, 53
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    vand.vx v11, v11, a3
; RV64V-NEXT:    slli t2, t1, 54
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v16, v9, t2
; RV64V-NEXT:    vsll.vx v11, v11, a2
; RV64V-NEXT:    slli t2, t1, 55
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    vor.vv v10, v10, v11
; RV64V-NEXT:    slli t2, t1, 56
; RV64V-NEXT:    vxor.vv v11, v14, v16
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v14, v9, t2
; RV64V-NEXT:    vsrl.vi v16, v12, 8
; RV64V-NEXT:    slli t2, t1, 57
; RV64V-NEXT:    vxor.vv v11, v11, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    vand.vx v16, v16, a4
; RV64V-NEXT:    slli t2, t1, 58
; RV64V-NEXT:    vxor.vv v14, v11, v14
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v17, v9, t2
; RV64V-NEXT:    vsrl.vi v13, v13, 24
; RV64V-NEXT:    slli t2, t1, 59
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    vand.vx v13, v13, a5
; RV64V-NEXT:    slli t2, t1, 60
; RV64V-NEXT:    vxor.vv v14, v14, v17
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v17, v9, t2
; RV64V-NEXT:    vor.vv v13, v16, v13
; RV64V-NEXT:    slli t2, t1, 61
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    and t2, a0, t2
; RV64V-NEXT:    vmul.vx v15, v9, t2
; RV64V-NEXT:    slli t1, t1, 62
; RV64V-NEXT:    vxor.vv v14, v14, v17
; RV64V-NEXT:    and t1, a0, t1
; RV64V-NEXT:    vmul.vx v16, v9, t1
; RV64V-NEXT:    vand.vx v12, v12, a5
; RV64V-NEXT:    vxor.vv v14, v14, v15
; RV64V-NEXT:    vsll.vi v12, v12, 24
; RV64V-NEXT:    srli a0, a0, 63
; RV64V-NEXT:    vxor.vv v14, v14, v16
; RV64V-NEXT:    slli a0, a0, 63
; RV64V-NEXT:    vmul.vx v9, v9, a0
; RV64V-NEXT:    vand.vx v15, v11, a4
; RV64V-NEXT:    vsrl.vx v11, v11, a2
; RV64V-NEXT:    vsll.vi v15, v15, 8
; RV64V-NEXT:    vxor.vv v9, v14, v9
; RV64V-NEXT:    vand.vx v11, v11, a3
; RV64V-NEXT:    vsrl.vx v9, v9, a1
; RV64V-NEXT:    vor.vv v12, v12, v15
; RV64V-NEXT:    vor.vv v9, v11, v9
; RV64V-NEXT:    vor.vv v10, v10, v12
; RV64V-NEXT:    vor.vv v9, v13, v9
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 4
; RV64V-NEXT:    vand.vx v9, v9, a6
; RV64V-NEXT:    vand.vx v10, v10, a6
; RV64V-NEXT:    vsll.vi v9, v9, 4
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 2
; RV64V-NEXT:    vand.vx v9, v9, a7
; RV64V-NEXT:    vand.vx v10, v10, a7
; RV64V-NEXT:    vsll.vi v9, v9, 2
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v10, v9, 1
; RV64V-NEXT:    vand.vx v9, v9, t0
; RV64V-NEXT:    vand.vx v10, v10, t0
; RV64V-NEXT:    vadd.vv v9, v9, v9
; RV64V-NEXT:    vor.vv v9, v10, v9
; RV64V-NEXT:    vsrl.vi v8, v9, 1, v0.t
; RV64V-NEXT:    ret
;
; RV32ZVBC-LABEL: clmulh_nxv1i64_vx_mask:
; RV32ZVBC:       # %bb.0:
; RV32ZVBC-NEXT:    addi sp, sp, -16
; RV32ZVBC-NEXT:    .cfi_def_cfa_offset 16
; RV32ZVBC-NEXT:    sw a0, 8(sp)
; RV32ZVBC-NEXT:    sw a1, 12(sp)
; RV32ZVBC-NEXT:    addi a0, sp, 8
; RV32ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; RV32ZVBC-NEXT:    vlse64.v v9, (a0), zero
; RV32ZVBC-NEXT:    vclmulh.vv v8, v8, v9, v0.t
; RV32ZVBC-NEXT:    addi sp, sp, 16
; RV32ZVBC-NEXT:    .cfi_def_cfa_offset 0
; RV32ZVBC-NEXT:    ret
;
; RV64ZVBC-LABEL: clmulh_nxv1i64_vx_mask:
; RV64ZVBC:       # %bb.0:
; RV64ZVBC-NEXT:    vsetvli a1, zero, e64, m1, ta, mu
; RV64ZVBC-NEXT:    vclmulh.vx v8, v8, a0, v0.t
; RV64ZVBC-NEXT:    ret
  %elt.head = insertelement <vscale x 1 x i64> poison, i64 %b, i128 0
  %vb = shufflevector <vscale x 1 x i64> %elt.head, <vscale x 1 x i64> poison, <vscale x 1 x i32> zeroinitializer
  %va.ext = zext <vscale x 1 x i64> %va to <vscale x 1 x i128>
  %vb.ext = zext <vscale x 1 x i64> %vb to <vscale x 1 x i128>
  %clmul = call <vscale x 1 x i128> @llvm.clmul.nxv1i128(<vscale x 1 x i128> %va.ext, <vscale x 1 x i128> %vb.ext)
  %res.ext = lshr <vscale x 1 x i128> %clmul, splat(i128 64)
  %res = trunc <vscale x 1 x i128> %res.ext to <vscale x 1 x i64>
  %sel = select <vscale x 1 x i1> %mask, <vscale x 1 x i64> %res, <vscale x 1 x i64> %va
  ret <vscale x 1 x i64> %sel
}
