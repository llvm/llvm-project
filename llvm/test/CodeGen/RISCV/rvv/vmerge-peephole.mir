# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc %s -o - -mtriple=riscv64 -mattr=+v -run-pass=riscv-vector-peephole -verify-machineinstrs | FileCheck %s

---
name: vle32
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: vle32
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %y:vrnov0 = PseudoVLE32_V_M1_MASK %passthru, $noreg, %mask, %avl, 5 /* e32 */, 0 /* tu, mu */ :: (load unknown-size, align 1)
    %avl:gprnox0 = COPY $x8
    %passthru:vrnov0 = COPY $v8
    %x:vrnov0 = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size)
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %x, %mask, %avl, 5 /* e32 */
...
---
name: vle32_no_passthru
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: vle32_no_passthru
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %false:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %y:vrnov0 = PseudoVLE32_V_M1_MASK %false, $noreg, %mask, %avl, 5 /* e32 */, 1 /* ta, mu */ :: (load unknown-size, align 1)
    %avl:gprnox0 = COPY $x8
    %false:vrnov0 = COPY $v8
    %x:vrnov0 = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size)
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = PseudoVMERGE_VVM_M1 $noreg, %false, %x, %mask, %avl, 5 /* e32 */
...
---
name: vle32_move_past_passthru
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: vle32_move_past_passthru
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %y:vrnov0 = PseudoVLE32_V_M1_MASK %passthru, $noreg, %mask, %avl, 5 /* e32 */, 0 /* tu, mu */ :: (load unknown-size, align 1)
    %avl:gprnox0 = COPY $x8
    %x:vrnov0 = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size)
    %passthru:vrnov0 = COPY $v8
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %x, %mask, %avl, 5 /* e32 */
...
---
name: vnclip_move_past_passthru
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: vnclip_move_past_passthru
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %y:vrnov0 = PseudoVNCLIPU_WV_MF2_MASK %passthru, $noreg, $noreg, %mask, 0, %avl, 5 /* e32 */, 0 /* tu, mu */, implicit-def $vxsat
    %avl:gprnox0 = COPY $x8
    %x:vrnov0 = PseudoVNCLIPU_WV_MF2 $noreg, $noreg, $noreg, 0, -1, 5, 3, implicit-def $vxsat
    %passthru:vrnov0 = COPY $v8
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %x, %mask, %avl, 5 /* e32 */
...
---
name: vnclip_cant_move_past_passthru
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: vnclip_cant_move_past_passthru
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %x:vrnov0 = PseudoVNCLIPU_WV_MF2 $noreg, $noreg, $noreg, 0, -1, 5 /* e32 */, 3 /* ta, ma */, implicit-def $vxsat
    ; CHECK-NEXT: %vxsat:gpr = COPY $vxsat
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %x, %mask, %avl, 5 /* e32 */
    %avl:gprnox0 = COPY $x8
    %x:vrnov0 = PseudoVNCLIPU_WV_MF2 $noreg, $noreg, $noreg, 0, -1, 5, 3, implicit-def $vxsat
    %vxsat:gpr = COPY $vxsat
    %passthru:vrnov0 = COPY $v8
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %x, %mask, %avl, 5 /* e32 */
...
---
name: commute_vfmadd
body: |
  bb.0:
    liveins: $x8, $v0, $v8, $v9, $v10
    ; CHECK-LABEL: name: commute_vfmadd
    ; CHECK: liveins: $x8, $v0, $v8, $v9, $v10
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %x:vr = COPY $v9
    ; CHECK-NEXT: %y:vr = COPY $v10
    ; CHECK-NEXT: %vmerge:vrnov0 = nofpexcept PseudoVFMACC_VV_M1_E32_MASK %passthru, %y, %x, %mask, 7, %avl, 5 /* e32 */, 0 /* tu, mu */, implicit $frm
    %avl:gprnox0 = COPY $x8
    %mask:vmv0 = COPY $v0
    %passthru:vrnov0 = COPY $v8
    %x:vr = COPY $v9
    %y:vr = COPY $v10
    %vfmadd:vrnov0 = nofpexcept PseudoVFMADD_VV_M1_E32 %x, %y, %passthru, 7, -1, 5 /* e32 */, 3 /* ta, ma */, implicit $frm
    %vmerge:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %vfmadd, %mask, %avl, 5
...
---
name: true_copy
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: true_copy
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %z:vrnov0 = PseudoVLE32_V_M1_MASK %passthru, $noreg, %mask, %avl, 5 /* e32 */, 0 /* tu, mu */ :: (load unknown-size, align 1)
    %avl:gprnox0 = COPY $x8
    %passthru:vrnov0 = COPY $v8
    %x:vr = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size)
    %mask:vmv0 = COPY $v0
    %y:vrnov0 = COPY %x
    %z:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %y, %mask, %avl, 5 /* e32 */
...
---
name: copy_is_killed
body: |
  bb.0:
    liveins: $v0, $v8, $v9
    ; CHECK-LABEL: name: copy_is_killed
    ; CHECK: liveins: $v0, $v8, $v9
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %x:vr = COPY $v8
    ; CHECK-NEXT: %y:vr = COPY $v9
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: %add0:vr = PseudoVADD_VV_M1 $noreg, %x, %y, -1, 5 /* e32 */, 3 /* ta, ma */
    ; CHECK-NEXT: %add1:vmnov0 = COPY %add:vmnov0
    ; CHECK-NEXT: %merge:vrnov0 = PseudoVOR_VV_M1_MASK %add:vmnov0, %add1, %y, %mask, -1, 5 /* e32 */, 1 /* ta, mu */
    %x:vr = COPY $v8
    %y:vr = COPY $v9
    %mask:vmv0 = COPY $v0
    %add0:vr = PseudoVADD_VV_M1 $noreg, %x:vr, %y:vr, -1, 5, 3
    %add1:vrnov0 = COPY killed %add:vr
    %or:vrnov0 = PseudoVOR_VV_M1 $noreg, %add1:vrnov0, %y:vr, -1, 5, 3
    %merge:vrnov0 = PseudoVMERGE_VVM_M1 $noreg, %add1:vrnov0, killed %or:vrnov0, killed %mask:vmv0, -1, 5 /* e32 */

...
---
name: copy_multiple_use
body: |
  bb.0:
    liveins: $x8, $v0, $v8
    ; CHECK-LABEL: name: copy_multiple_use
    ; CHECK: liveins: $x8, $v0, $v8
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: %avl:gprnox0 = COPY $x8
    ; CHECK-NEXT: %passthru:vrnov0 = COPY $v8
    ; CHECK-NEXT: %x:vrnov0 = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size, align 1)
    ; CHECK-NEXT: %copy:vrnov0 = COPY %x
    ; CHECK-NEXT: %mask:vmv0 = COPY $v0
    ; CHECK-NEXT: PseudoVSE8_V_M1 %copy, $noreg, %avl, 5 /* e32 */
    ; CHECK-NEXT: %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %copy, %mask, %avl, 5 /* e32 */
    %avl:gprnox0 = COPY $x8
    %passthru:vrnov0 = COPY $v8
    %x:vrnov0 = PseudoVLE32_V_M1 $noreg, $noreg, %avl, 5 /* e32 */, 2 /* tu, ma */ :: (load unknown-size)
    %copy:vrnov0 = COPY %x
    %mask:vmv0 = COPY $v0
    PseudoVSE8_V_M1 %copy, $noreg, %avl, 5 /* e8 */
    %y:vrnov0 = PseudoVMERGE_VVM_M1 %passthru, %passthru, %copy, %mask, %avl, 5 /* e32 */
...
---
name: true_copy_elimination
body:             |
  ; CHECK-LABEL: name: true_copy_elimination
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   PseudoBR %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   [[DEF:%[0-9]+]]:vmv0 = IMPLICIT_DEF
  ; CHECK-NEXT:   early-clobber %5:vrm8nov0 = PseudoVZEXT_VF8_M8_MASK $noreg, $noreg, [[DEF]], 16, 6 /* e64 */, 1 /* ta, mu */
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vrm8 = COPY %5
  ; CHECK-NEXT:   PseudoRET
  bb.0:
    successors: %bb.1

    PseudoBR %bb.1

  bb.1:

    %102:vrm8 = PseudoVZEXT_VF8_M8 $noreg, $noreg, 16, 6 /* e64 */, 3 /* ta, ma */
    %123:vrm8nov0 = COPY %102
    %a123:vrm8 = COPY %123
    %b123:vrm8nov0 = COPY %a123
    %124:vmv0 = IMPLICIT_DEF
    %121:vrm8nov0 = PseudoVMERGE_VVM_M8 $noreg, $noreg, %b123, undef %124, 16, 6 /* e64 */
    %125:vrm8 = COPY %121
    PseudoRET
...
