; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+v -stop-after=finalize-isel < %s \
; RUN:   -target-abi=ilp32 | FileCheck %s
declare { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.nxv8i8(<vscale x 8 x i8>, <vscale x 8 x i8>*, i32);
declare { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.nxv16i8(<vscale x 16 x i8>, <vscale x 16 x i8>*, i32);
declare { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.nxv32i8(<vscale x 32 x i8>, <vscale x 32 x i8>*, i32);
declare { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.nxv64i8(<vscale x 64 x i8>, <vscale x 64 x i8>*, i32);
declare { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.nxv4i16(<vscale x 4 x i16>, <vscale x 4 x i16>*, i32);
declare { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.nxv8i16(<vscale x 8 x i16>, <vscale x 8 x i16>*, i32);
declare { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.nxv16i16(<vscale x 16 x i16>, <vscale x 16 x i16>*, i32);
declare { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.nxv32i16(<vscale x 32 x i16>, <vscale x 32 x i16>*, i32);
declare { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.nxv2i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, i32);
declare { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.nxv4i32(<vscale x 4 x i32>, <vscale x 4 x i32>*, i32);
declare { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.nxv8i32(<vscale x 8 x i32>, <vscale x 8 x i32>*, i32);
declare { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.nxv16i32(<vscale x 16 x i32>, <vscale x 16 x i32>*, i32);
declare { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.nxv1i64(<vscale x 1 x i64>, <vscale x 1 x i64>*, i32);
declare { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.nxv2i64(<vscale x 2 x i64>, <vscale x 2 x i64>*, i32);
declare { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.nxv4i64(<vscale x 4 x i64>, <vscale x 4 x i64>*, i32);
declare { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.nxv8i64(<vscale x 8 x i64>, <vscale x 8 x i64>*, i32);
declare { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.mask.nxv8i8.i32(<vscale x 8 x i8>, <vscale x 8 x i8>*, <vscale x 8 x i1>, i32, i32 immarg)
declare { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.mask.nxv16i8.i32(<vscale x 16 x i8>, <vscale x 16 x i8>*, <vscale x 16 x i1>, i32, i32 immarg)
declare { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.mask.nxv32i8.i32(<vscale x 32 x i8>, <vscale x 32 x i8>*, <vscale x 32 x i1>, i32, i32 immarg)
declare { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.mask.nxv64i8.i32(<vscale x 64 x i8>, <vscale x 64 x i8>*, <vscale x 64 x i1>, i32, i32 immarg)
declare { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.mask.nxv4i16.i32(<vscale x 4 x i16>, <vscale x 4 x i16>*, <vscale x 4 x i1>, i32, i32 immarg)
declare { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.mask.nxv8i16.i32(<vscale x 8 x i16>, <vscale x 8 x i16>*, <vscale x 8 x i1>, i32, i32 immarg)
declare { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.mask.nxv16i16.i32(<vscale x 16 x i16>, <vscale x 16 x i16>*, <vscale x 16 x i1>, i32, i32 immarg)
declare { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.mask.nxv32i16.i32(<vscale x 32 x i16>, <vscale x 32 x i16>*, <vscale x 32 x i1>, i32, i32 immarg)
declare { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.mask.nxv2i32.i32(<vscale x 2 x i32>, <vscale x 2 x i32>*, <vscale x 2 x i1>, i32, i32 immarg)
declare { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.mask.nxv4i32.i32(<vscale x 4 x i32>, <vscale x 4 x i32>*, <vscale x 4 x i1>, i32, i32 immarg)
declare { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.mask.nxv8i32.i32(<vscale x 8 x i32>, <vscale x 8 x i32>*, <vscale x 8 x i1>, i32, i32 immarg)
declare { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.mask.nxv16i32.i32(<vscale x 16 x i32>, <vscale x 16 x i32>*, <vscale x 16 x i1>, i32, i32 immarg)
declare { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.mask.nxv1i64.i32(<vscale x 1 x i64>, <vscale x 1 x i64>*, <vscale x 1 x i1>, i32, i32 immarg)
declare { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.mask.nxv2i64.i32(<vscale x 2 x i64>, <vscale x 2 x i64>*, <vscale x 2 x i1>, i32, i32 immarg)
declare { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.mask.nxv4i64.i32(<vscale x 4 x i64>, <vscale x 4 x i64>*, <vscale x 4 x i1>, i32, i32 immarg)
declare { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.mask.nxv8i64.i32(<vscale x 8 x i64>, <vscale x 8 x i64>*, <vscale x 8 x i1>, i32, i32 immarg)

define i32 @vleffe8m1(<vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_:%[0-9]+]]:vr = PseudoVLE8FF_V_M1 [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 64 /* e8, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2(<vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_:%[0-9]+]]:vrm2 = PseudoVLE8FF_V_M2 [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 65 /* e8, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.nxv16i8(<vscale x 16 x i8> undef, <vscale x 16 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4(<vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_:%[0-9]+]]:vrm4 = PseudoVLE8FF_V_M4 [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 66 /* e8, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.nxv32i8(<vscale x 32 x i8> undef, <vscale x 32 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8(<vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_:%[0-9]+]]:vrm8 = PseudoVLE8FF_V_M8 [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 67 /* e8, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.nxv64i8(<vscale x 64 x i8> undef, <vscale x 64 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m1_tu(<vscale x 8 x i8> %merge, <vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_TU:%[0-9]+]]:vr = PseudoVLE8FF_V_M1_TU [[COPY2]], [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 0 /* e8, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.nxv8i8(<vscale x 8 x i8> %merge, <vscale x 8 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2_tu(<vscale x 16 x i8> %merge, <vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_TU:%[0-9]+]]:vrm2 = PseudoVLE8FF_V_M2_TU [[COPY2]], [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 1 /* e8, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.nxv16i8(<vscale x 16 x i8> %merge, <vscale x 16 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4_tu(<vscale x 32 x i8> %merge, <vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_TU:%[0-9]+]]:vrm4 = PseudoVLE8FF_V_M4_TU [[COPY2]], [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 2 /* e8, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.nxv32i8(<vscale x 32 x i8> %merge, <vscale x 32 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8_tu(<vscale x 64 x i8> %merge, <vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_TU:%[0-9]+]]:vrm8 = PseudoVLE8FF_V_M8_TU [[COPY2]], [[COPY1]], [[COPY]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 3 /* e8, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.nxv64i8(<vscale x 64 x i8> %merge, <vscale x 64 x i8>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m1_tumu(<vscale x 8 x i1> %mask, <vscale x 8 x i8> %maskedoff, <vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE8FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 0 /* e8, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.mask.nxv8i8.i32(<vscale x 8 x i8> %maskedoff, <vscale x 8 x i8>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2_tumu(<vscale x 16 x i1> %mask, <vscale x 16 x i8> %maskedoff, <vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE8FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 1 /* e8, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.mask.nxv16i8.i32(<vscale x 16 x i8> %maskedoff, <vscale x 16 x i8>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4_tumu(<vscale x 32 x i1> %mask, <vscale x 32 x i8> %maskedoff, <vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE8FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 2 /* e8, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.mask.nxv32i8.i32(<vscale x 32 x i8> %maskedoff, <vscale x 32 x i8>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8_tumu(<vscale x 64 x i1> %mask, <vscale x 64 x i8> %maskedoff, <vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE8FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 3 /* e8, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.mask.nxv64i8.i32(<vscale x 64 x i8> %maskedoff, <vscale x 64 x i8>* %p, <vscale x 64 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m1_tamu(<vscale x 8 x i1> %mask, <vscale x 8 x i8> %maskedoff, <vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE8FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 64 /* e8, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.mask.nxv8i8.i32(<vscale x 8 x i8> %maskedoff, <vscale x 8 x i8>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2_tamu(<vscale x 16 x i1> %mask, <vscale x 16 x i8> %maskedoff, <vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE8FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 65 /* e8, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.mask.nxv16i8.i32(<vscale x 16 x i8> %maskedoff, <vscale x 16 x i8>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4_tamu(<vscale x 32 x i1> %mask, <vscale x 32 x i8> %maskedoff, <vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE8FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 66 /* e8, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.mask.nxv32i8.i32(<vscale x 32 x i8> %maskedoff, <vscale x 32 x i8>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8_tamu(<vscale x 64 x i1> %mask, <vscale x 64 x i8> %maskedoff, <vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE8FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 67 /* e8, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.mask.nxv64i8.i32(<vscale x 64 x i8> %maskedoff, <vscale x 64 x i8>* %p, <vscale x 64 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m1_tuma(<vscale x 8 x i1> %mask, <vscale x 8 x i8> %maskedoff, <vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE8FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 128 /* e8, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.mask.nxv8i8.i32(<vscale x 8 x i8> %maskedoff, <vscale x 8 x i8>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2_tuma(<vscale x 16 x i1> %mask, <vscale x 16 x i8> %maskedoff, <vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE8FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 129 /* e8, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.mask.nxv16i8.i32(<vscale x 16 x i8> %maskedoff, <vscale x 16 x i8>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4_tuma(<vscale x 32 x i1> %mask, <vscale x 32 x i8> %maskedoff, <vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE8FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 130 /* e8, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.mask.nxv32i8.i32(<vscale x 32 x i8> %maskedoff, <vscale x 32 x i8>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8_tuma(<vscale x 64 x i1> %mask, <vscale x 64 x i8> %maskedoff, <vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE8FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 131 /* e8, m8, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.mask.nxv64i8.i32(<vscale x 64 x i8> %maskedoff, <vscale x 64 x i8>* %p, <vscale x 64 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m1_tama(<vscale x 8 x i1> %mask, <vscale x 8 x i8> %maskedoff, <vscale x 8 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m1_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE8FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 192 /* e8, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i8>, i32 } @llvm.riscv.vleff.mask.nxv8i8.i32(<vscale x 8 x i8> %maskedoff, <vscale x 8 x i8>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 8 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m2_tama(<vscale x 16 x i1> %mask, <vscale x 16 x i8> %maskedoff, <vscale x 16 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m2_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE8FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 193 /* e8, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i8>, i32 } @llvm.riscv.vleff.mask.nxv16i8.i32(<vscale x 16 x i8> %maskedoff, <vscale x 16 x i8>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 16 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m4_tama(<vscale x 32 x i1> %mask, <vscale x 32 x i8> %maskedoff, <vscale x 32 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m4_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE8FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 194 /* e8, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i8>, i32 } @llvm.riscv.vleff.mask.nxv32i8.i32(<vscale x 32 x i8> %maskedoff, <vscale x 32 x i8>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 32 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe8m8_tama(<vscale x 64 x i1> %mask, <vscale x 64 x i8> %maskedoff, <vscale x 64 x i8> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe8m8_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE8FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE8FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 195 /* e8, m8, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 64 x i8>, i32 } @llvm.riscv.vleff.mask.nxv64i8.i32(<vscale x 64 x i8> %maskedoff, <vscale x 64 x i8>* %p, <vscale x 64 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 64 x i8>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1(<vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_:%[0-9]+]]:vr = PseudoVLE16FF_V_M1 [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 72 /* e16, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2(<vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_:%[0-9]+]]:vrm2 = PseudoVLE16FF_V_M2 [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 73 /* e16, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.nxv8i16(<vscale x 8 x i16> undef, <vscale x 8 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4(<vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_:%[0-9]+]]:vrm4 = PseudoVLE16FF_V_M4 [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 74 /* e16, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.nxv16i16(<vscale x 16 x i16> undef, <vscale x 16 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8(<vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_:%[0-9]+]]:vrm8 = PseudoVLE16FF_V_M8 [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 75 /* e16, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.nxv32i16(<vscale x 32 x i16> undef, <vscale x 32 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1_tu(<vscale x 4 x i16> %merge, <vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_TU:%[0-9]+]]:vr = PseudoVLE16FF_V_M1_TU [[COPY2]], [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 8 /* e16, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.nxv4i16(<vscale x 4 x i16> %merge, <vscale x 4 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2_tu(<vscale x 8 x i16> %merge, <vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_TU:%[0-9]+]]:vrm2 = PseudoVLE16FF_V_M2_TU [[COPY2]], [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 9 /* e16, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.nxv8i16(<vscale x 8 x i16> %merge, <vscale x 8 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4_tu(<vscale x 16 x i16> %merge, <vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_TU:%[0-9]+]]:vrm4 = PseudoVLE16FF_V_M4_TU [[COPY2]], [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 10 /* e16, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.nxv16i16(<vscale x 16 x i16> %merge, <vscale x 16 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8_tu(<vscale x 32 x i16> %merge, <vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_TU:%[0-9]+]]:vrm8 = PseudoVLE16FF_V_M8_TU [[COPY2]], [[COPY1]], [[COPY]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 11 /* e16, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.nxv32i16(<vscale x 32 x i16> %merge, <vscale x 32 x i16>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1_tumu(<vscale x 4 x i1> %mask, <vscale x 4 x i16> %maskedoff, <vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE16FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 8 /* e16, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.mask.nxv4i16.i32(<vscale x 4 x i16> %maskedoff, <vscale x 4 x i16>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2_tumu(<vscale x 8 x i1> %mask, <vscale x 8 x i16> %maskedoff, <vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE16FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 9 /* e16, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.mask.nxv8i16.i32(<vscale x 8 x i16> %maskedoff, <vscale x 8 x i16>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4_tumu(<vscale x 16 x i1> %mask, <vscale x 16 x i16> %maskedoff, <vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE16FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 10 /* e16, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.mask.nxv16i16.i32(<vscale x 16 x i16> %maskedoff, <vscale x 16 x i16>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8_tumu(<vscale x 32 x i1> %mask, <vscale x 32 x i16> %maskedoff, <vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE16FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 11 /* e16, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.mask.nxv32i16.i32(<vscale x 32 x i16> %maskedoff, <vscale x 32 x i16>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1_tamu(<vscale x 4 x i1> %mask, <vscale x 4 x i16> %maskedoff, <vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE16FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 72 /* e16, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.mask.nxv4i16.i32(<vscale x 4 x i16> %maskedoff, <vscale x 4 x i16>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2_tamu(<vscale x 8 x i1> %mask, <vscale x 8 x i16> %maskedoff, <vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE16FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 73 /* e16, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.mask.nxv8i16.i32(<vscale x 8 x i16> %maskedoff, <vscale x 8 x i16>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4_tamu(<vscale x 16 x i1> %mask, <vscale x 16 x i16> %maskedoff, <vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE16FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 74 /* e16, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.mask.nxv16i16.i32(<vscale x 16 x i16> %maskedoff, <vscale x 16 x i16>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8_tamu(<vscale x 32 x i1> %mask, <vscale x 32 x i16> %maskedoff, <vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE16FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 75 /* e16, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.mask.nxv32i16.i32(<vscale x 32 x i16> %maskedoff, <vscale x 32 x i16>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1_tuma(<vscale x 4 x i1> %mask, <vscale x 4 x i16> %maskedoff, <vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE16FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 136 /* e16, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.mask.nxv4i16.i32(<vscale x 4 x i16> %maskedoff, <vscale x 4 x i16>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2_tuma(<vscale x 8 x i1> %mask, <vscale x 8 x i16> %maskedoff, <vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE16FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 137 /* e16, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.mask.nxv8i16.i32(<vscale x 8 x i16> %maskedoff, <vscale x 8 x i16>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4_tuma(<vscale x 16 x i1> %mask, <vscale x 16 x i16> %maskedoff, <vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE16FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 138 /* e16, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.mask.nxv16i16.i32(<vscale x 16 x i16> %maskedoff, <vscale x 16 x i16>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8_tuma(<vscale x 32 x i1> %mask, <vscale x 32 x i16> %maskedoff, <vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE16FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 139 /* e16, m8, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.mask.nxv32i16.i32(<vscale x 32 x i16> %maskedoff, <vscale x 32 x i16>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m1_tama(<vscale x 4 x i1> %mask, <vscale x 4 x i16> %maskedoff, <vscale x 4 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m1_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE16FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 200 /* e16, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i16>, i32 } @llvm.riscv.vleff.mask.nxv4i16.i32(<vscale x 4 x i16> %maskedoff, <vscale x 4 x i16>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 4 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m2_tama(<vscale x 8 x i1> %mask, <vscale x 8 x i16> %maskedoff, <vscale x 8 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m2_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE16FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 201 /* e16, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i16>, i32 } @llvm.riscv.vleff.mask.nxv8i16.i32(<vscale x 8 x i16> %maskedoff, <vscale x 8 x i16>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 8 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m4_tama(<vscale x 16 x i1> %mask, <vscale x 16 x i16> %maskedoff, <vscale x 16 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m4_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE16FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 202 /* e16, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i16>, i32 } @llvm.riscv.vleff.mask.nxv16i16.i32(<vscale x 16 x i16> %maskedoff, <vscale x 16 x i16>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 16 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe16m8_tama(<vscale x 32 x i1> %mask, <vscale x 32 x i16> %maskedoff, <vscale x 32 x i16> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe16m8_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE16FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE16FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 203 /* e16, m8, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 32 x i16>, i32 } @llvm.riscv.vleff.mask.nxv32i16.i32(<vscale x 32 x i16> %maskedoff, <vscale x 32 x i16>* %p, <vscale x 32 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 32 x i16>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1(<vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_:%[0-9]+]]:vr = PseudoVLE32FF_V_M1 [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 80 /* e32, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2(<vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_:%[0-9]+]]:vrm2 = PseudoVLE32FF_V_M2 [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 81 /* e32, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.nxv4i32(<vscale x 4 x i32> undef, <vscale x 4 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4(<vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_:%[0-9]+]]:vrm4 = PseudoVLE32FF_V_M4 [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 82 /* e32, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.nxv8i32(<vscale x 8 x i32> undef, <vscale x 8 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8(<vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_:%[0-9]+]]:vrm8 = PseudoVLE32FF_V_M8 [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 83 /* e32, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.nxv16i32(<vscale x 16 x i32> undef, <vscale x 16 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1_tu(<vscale x 2 x i32> %merge, <vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_TU:%[0-9]+]]:vr = PseudoVLE32FF_V_M1_TU [[COPY2]], [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 16 /* e32, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.nxv2i32(<vscale x 2 x i32> %merge, <vscale x 2 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2_tu(<vscale x 4 x i32> %merge, <vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_TU:%[0-9]+]]:vrm2 = PseudoVLE32FF_V_M2_TU [[COPY2]], [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 17 /* e32, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.nxv4i32(<vscale x 4 x i32> %merge, <vscale x 4 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4_tu(<vscale x 8 x i32> %merge, <vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_TU:%[0-9]+]]:vrm4 = PseudoVLE32FF_V_M4_TU [[COPY2]], [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 18 /* e32, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.nxv8i32(<vscale x 8 x i32> %merge, <vscale x 8 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8_tu(<vscale x 16 x i32> %merge, <vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_TU:%[0-9]+]]:vrm8 = PseudoVLE32FF_V_M8_TU [[COPY2]], [[COPY1]], [[COPY]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 19 /* e32, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.nxv16i32(<vscale x 16 x i32> %merge, <vscale x 16 x i32>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1_tumu(<vscale x 2 x i1> %mask, <vscale x 2 x i32> %maskedoff, <vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE32FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 16 /* e32, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.mask.nxv2i32.i32(<vscale x 2 x i32> %maskedoff, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2_tumu(<vscale x 4 x i1> %mask, <vscale x 4 x i32> %maskedoff, <vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE32FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 17 /* e32, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.mask.nxv4i32.i32(<vscale x 4 x i32> %maskedoff, <vscale x 4 x i32>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4_tumu(<vscale x 8 x i1> %mask, <vscale x 8 x i32> %maskedoff, <vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE32FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 18 /* e32, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.mask.nxv8i32.i32(<vscale x 8 x i32> %maskedoff, <vscale x 8 x i32>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8_tumu(<vscale x 16 x i1> %mask, <vscale x 16 x i32> %maskedoff, <vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE32FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 19 /* e32, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.mask.nxv16i32.i32(<vscale x 16 x i32> %maskedoff, <vscale x 16 x i32>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1_tamu(<vscale x 2 x i1> %mask, <vscale x 2 x i32> %maskedoff, <vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE32FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 80 /* e32, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.mask.nxv2i32.i32(<vscale x 2 x i32> %maskedoff, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2_tamu(<vscale x 4 x i1> %mask, <vscale x 4 x i32> %maskedoff, <vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE32FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 81 /* e32, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.mask.nxv4i32.i32(<vscale x 4 x i32> %maskedoff, <vscale x 4 x i32>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4_tamu(<vscale x 8 x i1> %mask, <vscale x 8 x i32> %maskedoff, <vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE32FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 82 /* e32, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.mask.nxv8i32.i32(<vscale x 8 x i32> %maskedoff, <vscale x 8 x i32>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8_tamu(<vscale x 16 x i1> %mask, <vscale x 16 x i32> %maskedoff, <vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE32FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 83 /* e32, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.mask.nxv16i32.i32(<vscale x 16 x i32> %maskedoff, <vscale x 16 x i32>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1_tuma(<vscale x 2 x i1> %mask, <vscale x 2 x i32> %maskedoff, <vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE32FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 144 /* e32, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.mask.nxv2i32.i32(<vscale x 2 x i32> %maskedoff, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2_tuma(<vscale x 4 x i1> %mask, <vscale x 4 x i32> %maskedoff, <vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE32FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 145 /* e32, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.mask.nxv4i32.i32(<vscale x 4 x i32> %maskedoff, <vscale x 4 x i32>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4_tuma(<vscale x 8 x i1> %mask, <vscale x 8 x i32> %maskedoff, <vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE32FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 146 /* e32, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.mask.nxv8i32.i32(<vscale x 8 x i32> %maskedoff, <vscale x 8 x i32>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8_tuma(<vscale x 16 x i1> %mask, <vscale x 16 x i32> %maskedoff, <vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE32FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 147 /* e32, m8, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.mask.nxv16i32.i32(<vscale x 16 x i32> %maskedoff, <vscale x 16 x i32>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m1_tama(<vscale x 2 x i1> %mask, <vscale x 2 x i32> %maskedoff, <vscale x 2 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m1_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE32FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 208 /* e32, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i32>, i32 } @llvm.riscv.vleff.mask.nxv2i32.i32(<vscale x 2 x i32> %maskedoff, <vscale x 2 x i32>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 2 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m2_tama(<vscale x 4 x i1> %mask, <vscale x 4 x i32> %maskedoff, <vscale x 4 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m2_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE32FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 209 /* e32, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i32>, i32 } @llvm.riscv.vleff.mask.nxv4i32.i32(<vscale x 4 x i32> %maskedoff, <vscale x 4 x i32>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 4 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m4_tama(<vscale x 8 x i1> %mask, <vscale x 8 x i32> %maskedoff, <vscale x 8 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m4_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE32FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 210 /* e32, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i32>, i32 } @llvm.riscv.vleff.mask.nxv8i32.i32(<vscale x 8 x i32> %maskedoff, <vscale x 8 x i32>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 8 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe32m8_tama(<vscale x 16 x i1> %mask, <vscale x 16 x i32> %maskedoff, <vscale x 16 x i32> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe32m8_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE32FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE32FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 211 /* e32, m8, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 16 x i32>, i32 } @llvm.riscv.vleff.mask.nxv16i32.i32(<vscale x 16 x i32> %maskedoff, <vscale x 16 x i32>* %p, <vscale x 16 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 16 x i32>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1(<vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_:%[0-9]+]]:vr = PseudoVLE64FF_V_M1 [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 88 /* e64, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2(<vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_:%[0-9]+]]:vrm2 = PseudoVLE64FF_V_M2 [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 89 /* e64, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.nxv2i64(<vscale x 2 x i64> undef, <vscale x 2 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4(<vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_:%[0-9]+]]:vrm4 = PseudoVLE64FF_V_M4 [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 90 /* e64, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.nxv4i64(<vscale x 4 x i64> undef, <vscale x 4 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8(<vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_:%[0-9]+]]:vrm8 = PseudoVLE64FF_V_M8 [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 91 /* e64, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.nxv8i64(<vscale x 8 x i64> undef, <vscale x 8 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1_tu(<vscale x 1 x i64> %merge, <vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_TU:%[0-9]+]]:vr = PseudoVLE64FF_V_M1_TU [[COPY2]], [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 24 /* e64, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.nxv1i64(<vscale x 1 x i64> %merge, <vscale x 1 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2_tu(<vscale x 2 x i64> %merge, <vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_TU:%[0-9]+]]:vrm2 = PseudoVLE64FF_V_M2_TU [[COPY2]], [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 25 /* e64, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.nxv2i64(<vscale x 2 x i64> %merge, <vscale x 2 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4_tu(<vscale x 4 x i64> %merge, <vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_TU:%[0-9]+]]:vrm4 = PseudoVLE64FF_V_M4_TU [[COPY2]], [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 26 /* e64, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.nxv4i64(<vscale x 4 x i64> %merge, <vscale x 4 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8_tu(<vscale x 8 x i64> %merge, <vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8_tu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_TU:%[0-9]+]]:vrm8 = PseudoVLE64FF_V_M8_TU [[COPY2]], [[COPY1]], [[COPY]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 27 /* e64, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.nxv8i64(<vscale x 8 x i64> %merge, <vscale x 8 x i64>* %p, i32 %vl)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1_tumu(<vscale x 1 x i1> %mask, <vscale x 1 x i64> %maskedoff, <vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE64FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 24 /* e64, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.mask.nxv1i64.i32(<vscale x 1 x i64> %maskedoff, <vscale x 1 x i64>* %p, <vscale x 1 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2_tumu(<vscale x 2 x i1> %mask, <vscale x 2 x i64> %maskedoff, <vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE64FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 25 /* e64, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.mask.nxv2i64.i32(<vscale x 2 x i64> %maskedoff, <vscale x 2 x i64>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4_tumu(<vscale x 4 x i1> %mask, <vscale x 4 x i64> %maskedoff, <vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE64FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 26 /* e64, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.mask.nxv4i64.i32(<vscale x 4 x i64> %maskedoff, <vscale x 4 x i64>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8_tumu(<vscale x 8 x i1> %mask, <vscale x 8 x i64> %maskedoff, <vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8_tumu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE64FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 27 /* e64, m8, tu, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.mask.nxv8i64.i32(<vscale x 8 x i64> %maskedoff, <vscale x 8 x i64>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 0)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1_tamu(<vscale x 1 x i1> %mask, <vscale x 1 x i64> %maskedoff, <vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE64FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 88 /* e64, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.mask.nxv1i64.i32(<vscale x 1 x i64> %maskedoff, <vscale x 1 x i64>* %p, <vscale x 1 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2_tamu(<vscale x 2 x i1> %mask, <vscale x 2 x i64> %maskedoff, <vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE64FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 89 /* e64, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.mask.nxv2i64.i32(<vscale x 2 x i64> %maskedoff, <vscale x 2 x i64>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4_tamu(<vscale x 4 x i1> %mask, <vscale x 4 x i64> %maskedoff, <vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE64FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 90 /* e64, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.mask.nxv4i64.i32(<vscale x 4 x i64> %maskedoff, <vscale x 4 x i64>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8_tamu(<vscale x 8 x i1> %mask, <vscale x 8 x i64> %maskedoff, <vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8_tamu
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE64FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 91 /* e64, m8, ta, mu */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.mask.nxv8i64.i32(<vscale x 8 x i64> %maskedoff, <vscale x 8 x i64>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 1)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1_tuma(<vscale x 1 x i1> %mask, <vscale x 1 x i64> %maskedoff, <vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE64FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 152 /* e64, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.mask.nxv1i64.i32(<vscale x 1 x i64> %maskedoff, <vscale x 1 x i64>* %p, <vscale x 1 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2_tuma(<vscale x 2 x i1> %mask, <vscale x 2 x i64> %maskedoff, <vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE64FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 153 /* e64, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.mask.nxv2i64.i32(<vscale x 2 x i64> %maskedoff, <vscale x 2 x i64>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4_tuma(<vscale x 4 x i1> %mask, <vscale x 4 x i64> %maskedoff, <vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE64FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 154 /* e64, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.mask.nxv4i64.i32(<vscale x 4 x i64> %maskedoff, <vscale x 4 x i64>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8_tuma(<vscale x 8 x i1> %mask, <vscale x 8 x i64> %maskedoff, <vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8_tuma
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE64FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 155 /* e64, m8, tu, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.mask.nxv8i64.i32(<vscale x 8 x i64> %maskedoff, <vscale x 8 x i64>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 2)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m1_tama(<vscale x 1 x i1> %mask, <vscale x 1 x i64> %maskedoff, <vscale x 1 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m1_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrnov0 = COPY $v8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M1_MASK:%[0-9]+]]:vrnov0 = PseudoVLE64FF_V_M1_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 216 /* e64, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 1 x i64>, i32 } @llvm.riscv.vleff.mask.nxv1i64.i32(<vscale x 1 x i64> %maskedoff, <vscale x 1 x i64>* %p, <vscale x 1 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 1 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m2_tama(<vscale x 2 x i1> %mask, <vscale x 2 x i64> %maskedoff, <vscale x 2 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m2_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m2, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm2nov0 = COPY $v8m2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M2_MASK:%[0-9]+]]:vrm2nov0 = PseudoVLE64FF_V_M2_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 217 /* e64, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 2 x i64>, i32 } @llvm.riscv.vleff.mask.nxv2i64.i32(<vscale x 2 x i64> %maskedoff, <vscale x 2 x i64>* %p, <vscale x 2 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 2 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m4_tama(<vscale x 4 x i1> %mask, <vscale x 4 x i64> %maskedoff, <vscale x 4 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m4_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m4, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm4nov0 = COPY $v8m4
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M4_MASK:%[0-9]+]]:vrm4nov0 = PseudoVLE64FF_V_M4_MASK [[COPY2]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 218 /* e64, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 4 x i64>, i32 } @llvm.riscv.vleff.mask.nxv4i64.i32(<vscale x 4 x i64> %maskedoff, <vscale x 4 x i64>* %p, <vscale x 4 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 4 x i64>, i32 } %0, 1
  ret i32 %1
}

define i32 @vleffe64m8_tama(<vscale x 8 x i1> %mask, <vscale x 8 x i64> %maskedoff, <vscale x 8 x i64> *%p, i32 %vl) {
  ; CHECK-LABEL: name: vleffe64m8_tama
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v0, $v8m8, $x10, $x11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vrm8 = COPY $v8m8
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   $v0 = COPY [[COPY3]]
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm8nov0 = COPY [[COPY2]]
  ; CHECK-NEXT:   [[PseudoVLE64FF_V_M8_MASK:%[0-9]+]]:vrm8nov0 = PseudoVLE64FF_V_M8_MASK [[COPY4]], [[COPY1]], $v0, [[COPY]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 219 /* e64, m8, ta, ma */, implicit $vl
  ; CHECK-NEXT:   $x10 = COPY [[PseudoReadVL]]
  ; CHECK-NEXT:   PseudoRET implicit $x10
entry:
  %0 = tail call { <vscale x 8 x i64>, i32 } @llvm.riscv.vleff.mask.nxv8i64.i32(<vscale x 8 x i64> %maskedoff, <vscale x 8 x i64>* %p, <vscale x 8 x i1> %mask, i32 %vl, i32 3)
  %1 = extractvalue { <vscale x 8 x i64>, i32 } %0, 1
  ret i32 %1
}
