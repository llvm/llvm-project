; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+d,+experimental-zfh,+experimental-v -target-abi=ilp32d -riscv-v-vector-bits-min=128 \
; RUN:   -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV32
; RUN: llc -mtriple=riscv64 -mattr=+d,+experimental-zfh,+experimental-v -target-abi=lp64d -riscv-v-vector-bits-min=128 \
; RUN:   -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV64

define <2 x half> @select_v2f16(i1 zeroext %c, <2 x half> %a, <2 x half> %b) {
; CHECK-LABEL: select_v2f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    bnez a0, .LBB0_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB0_3
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB0_3:
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; CHECK-NEXT:    vfmv.v.f v8, ft1
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, tu, mu
; CHECK-NEXT:    vfmv.s.f v8, ft0
; CHECK-NEXT:    ret
  %v = select i1 %c, <2 x half> %a, <2 x half> %b
  ret <2 x half> %v
}

define <2 x half> @selectcc_v2f16(half %a, half %b, <2 x half> %c, <2 x half> %d) {
; CHECK-LABEL: selectcc_v2f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.h a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB1_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB1_3
; CHECK-NEXT:  .LBB1_2:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB1_3:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    bnez a0, .LBB1_5
; CHECK-NEXT:  # %bb.4:
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    j .LBB1_6
; CHECK-NEXT:  .LBB1_5:
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:  .LBB1_6:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf4, tu, mu
; CHECK-NEXT:    vfmv.s.f v25, ft0
; CHECK-NEXT:    vmv1r.v v8, v25
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <2 x half> %c, <2 x half> %d
  ret <2 x half> %v
}

define <4 x half> @select_v4f16(i1 zeroext %c, <4 x half> %a, <4 x half> %b) {
; CHECK-LABEL: select_v4f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    bnez a0, .LBB2_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsh ft0, 8(sp)
; CHECK-NEXT:    beqz a0, .LBB2_4
; CHECK-NEXT:  .LBB2_2:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:    j .LBB2_5
; CHECK-NEXT:  .LBB2_3:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsh ft0, 8(sp)
; CHECK-NEXT:    bnez a0, .LBB2_2
; CHECK-NEXT:  .LBB2_4:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:  .LBB2_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 14(sp)
; CHECK-NEXT:    bnez a0, .LBB2_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB2_8
; CHECK-NEXT:  .LBB2_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB2_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 12(sp)
; CHECK-NEXT:    bnez a0, .LBB2_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB2_11
; CHECK-NEXT:  .LBB2_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB2_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 10(sp)
; CHECK-NEXT:    vsetivli zero, 4, e16, mf2, ta, mu
; CHECK-NEXT:    addi a0, sp, 8
; CHECK-NEXT:    vle16.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %v = select i1 %c, <4 x half> %a, <4 x half> %b
  ret <4 x half> %v
}

define <4 x half> @selectcc_v4f16(half %a, half %b, <4 x half> %c, <4 x half> %d) {
; CHECK-LABEL: selectcc_v4f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    feq.h a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB3_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsh ft0, 8(sp)
; CHECK-NEXT:    beqz a0, .LBB3_4
; CHECK-NEXT:  .LBB3_2:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:    j .LBB3_5
; CHECK-NEXT:  .LBB3_3:
; CHECK-NEXT:    vsetvli zero, zero, e16, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsh ft0, 8(sp)
; CHECK-NEXT:    bnez a0, .LBB3_2
; CHECK-NEXT:  .LBB3_4:
; CHECK-NEXT:    vsetivli zero, 1, e16, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:  .LBB3_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 14(sp)
; CHECK-NEXT:    bnez a0, .LBB3_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB3_8
; CHECK-NEXT:  .LBB3_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB3_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 12(sp)
; CHECK-NEXT:    bnez a0, .LBB3_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB3_11
; CHECK-NEXT:  .LBB3_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB3_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 10(sp)
; CHECK-NEXT:    vsetivli zero, 4, e16, mf2, ta, mu
; CHECK-NEXT:    addi a0, sp, 8
; CHECK-NEXT:    vle16.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <4 x half> %c, <4 x half> %d
  ret <4 x half> %v
}

define <8 x half> @select_v8f16(i1 zeroext %c, <8 x half> %a, <8 x half> %b) {
; CHECK-LABEL: select_v8f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    bnez a0, .LBB4_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsh ft0, 16(sp)
; CHECK-NEXT:    beqz a0, .LBB4_4
; CHECK-NEXT:  .LBB4_2:
; CHECK-NEXT:    vsetivli zero, 1, e16, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 7
; CHECK-NEXT:    j .LBB4_5
; CHECK-NEXT:  .LBB4_3:
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsh ft0, 16(sp)
; CHECK-NEXT:    bnez a0, .LBB4_2
; CHECK-NEXT:  .LBB4_4:
; CHECK-NEXT:    vsetivli zero, 1, e16, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 7
; CHECK-NEXT:  .LBB4_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 30(sp)
; CHECK-NEXT:    bnez a0, .LBB4_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 6
; CHECK-NEXT:    j .LBB4_8
; CHECK-NEXT:  .LBB4_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 6
; CHECK-NEXT:  .LBB4_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 28(sp)
; CHECK-NEXT:    bnez a0, .LBB4_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 5
; CHECK-NEXT:    j .LBB4_11
; CHECK-NEXT:  .LBB4_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 5
; CHECK-NEXT:  .LBB4_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 26(sp)
; CHECK-NEXT:    bnez a0, .LBB4_13
; CHECK-NEXT:  # %bb.12:
; CHECK-NEXT:    vslidedown.vi v25, v9, 4
; CHECK-NEXT:    j .LBB4_14
; CHECK-NEXT:  .LBB4_13:
; CHECK-NEXT:    vslidedown.vi v25, v8, 4
; CHECK-NEXT:  .LBB4_14:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 24(sp)
; CHECK-NEXT:    bnez a0, .LBB4_16
; CHECK-NEXT:  # %bb.15:
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:    j .LBB4_17
; CHECK-NEXT:  .LBB4_16:
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:  .LBB4_17:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 22(sp)
; CHECK-NEXT:    bnez a0, .LBB4_19
; CHECK-NEXT:  # %bb.18:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB4_20
; CHECK-NEXT:  .LBB4_19:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB4_20:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 20(sp)
; CHECK-NEXT:    bnez a0, .LBB4_22
; CHECK-NEXT:  # %bb.21:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB4_23
; CHECK-NEXT:  .LBB4_22:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB4_23:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 18(sp)
; CHECK-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vle16.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    ret
  %v = select i1 %c, <8 x half> %a, <8 x half> %b
  ret <8 x half> %v
}

define <8 x half> @selectcc_v8f16(half %a, half %b, <8 x half> %c, <8 x half> %d) {
; CHECK-LABEL: selectcc_v8f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    feq.h a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB5_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsh ft0, 16(sp)
; CHECK-NEXT:    beqz a0, .LBB5_4
; CHECK-NEXT:  .LBB5_2:
; CHECK-NEXT:    vsetivli zero, 1, e16, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 7
; CHECK-NEXT:    j .LBB5_5
; CHECK-NEXT:  .LBB5_3:
; CHECK-NEXT:    vsetvli zero, zero, e16, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsh ft0, 16(sp)
; CHECK-NEXT:    bnez a0, .LBB5_2
; CHECK-NEXT:  .LBB5_4:
; CHECK-NEXT:    vsetivli zero, 1, e16, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 7
; CHECK-NEXT:  .LBB5_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 30(sp)
; CHECK-NEXT:    bnez a0, .LBB5_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 6
; CHECK-NEXT:    j .LBB5_8
; CHECK-NEXT:  .LBB5_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 6
; CHECK-NEXT:  .LBB5_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 28(sp)
; CHECK-NEXT:    bnez a0, .LBB5_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 5
; CHECK-NEXT:    j .LBB5_11
; CHECK-NEXT:  .LBB5_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 5
; CHECK-NEXT:  .LBB5_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 26(sp)
; CHECK-NEXT:    bnez a0, .LBB5_13
; CHECK-NEXT:  # %bb.12:
; CHECK-NEXT:    vslidedown.vi v25, v9, 4
; CHECK-NEXT:    j .LBB5_14
; CHECK-NEXT:  .LBB5_13:
; CHECK-NEXT:    vslidedown.vi v25, v8, 4
; CHECK-NEXT:  .LBB5_14:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 24(sp)
; CHECK-NEXT:    bnez a0, .LBB5_16
; CHECK-NEXT:  # %bb.15:
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:    j .LBB5_17
; CHECK-NEXT:  .LBB5_16:
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:  .LBB5_17:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 22(sp)
; CHECK-NEXT:    bnez a0, .LBB5_19
; CHECK-NEXT:  # %bb.18:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB5_20
; CHECK-NEXT:  .LBB5_19:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB5_20:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 20(sp)
; CHECK-NEXT:    bnez a0, .LBB5_22
; CHECK-NEXT:  # %bb.21:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB5_23
; CHECK-NEXT:  .LBB5_22:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB5_23:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsh ft0, 18(sp)
; CHECK-NEXT:    vsetivli zero, 8, e16, m1, ta, mu
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vle16.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <8 x half> %c, <8 x half> %d
  ret <8 x half> %v
}

define <16 x half> @select_v16f16(i1 zeroext %c, <16 x half> %a, <16 x half> %b) {
; RV32-LABEL: select_v16f16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    bnez a0, .LBB6_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsh ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB6_4
; RV32-NEXT:  .LBB6_2:
; RV32-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 15
; RV32-NEXT:    j .LBB6_5
; RV32-NEXT:  .LBB6_3:
; RV32-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsh ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB6_2
; RV32-NEXT:  .LBB6_4:
; RV32-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 15
; RV32-NEXT:  .LBB6_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 62(sp)
; RV32-NEXT:    bnez a0, .LBB6_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 14
; RV32-NEXT:    j .LBB6_8
; RV32-NEXT:  .LBB6_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 14
; RV32-NEXT:  .LBB6_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 60(sp)
; RV32-NEXT:    bnez a0, .LBB6_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 13
; RV32-NEXT:    j .LBB6_11
; RV32-NEXT:  .LBB6_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 13
; RV32-NEXT:  .LBB6_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 58(sp)
; RV32-NEXT:    bnez a0, .LBB6_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v26, v10, 12
; RV32-NEXT:    j .LBB6_14
; RV32-NEXT:  .LBB6_13:
; RV32-NEXT:    vslidedown.vi v26, v8, 12
; RV32-NEXT:  .LBB6_14:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB6_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v26, v10, 11
; RV32-NEXT:    j .LBB6_17
; RV32-NEXT:  .LBB6_16:
; RV32-NEXT:    vslidedown.vi v26, v8, 11
; RV32-NEXT:  .LBB6_17:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 54(sp)
; RV32-NEXT:    bnez a0, .LBB6_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v26, v10, 10
; RV32-NEXT:    j .LBB6_20
; RV32-NEXT:  .LBB6_19:
; RV32-NEXT:    vslidedown.vi v26, v8, 10
; RV32-NEXT:  .LBB6_20:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 52(sp)
; RV32-NEXT:    bnez a0, .LBB6_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v26, v10, 9
; RV32-NEXT:    j .LBB6_23
; RV32-NEXT:  .LBB6_22:
; RV32-NEXT:    vslidedown.vi v26, v8, 9
; RV32-NEXT:  .LBB6_23:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 50(sp)
; RV32-NEXT:    bnez a0, .LBB6_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v26, v10, 8
; RV32-NEXT:    j .LBB6_26
; RV32-NEXT:  .LBB6_25:
; RV32-NEXT:    vslidedown.vi v26, v8, 8
; RV32-NEXT:  .LBB6_26:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB6_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v26, v10, 7
; RV32-NEXT:    j .LBB6_29
; RV32-NEXT:  .LBB6_28:
; RV32-NEXT:    vslidedown.vi v26, v8, 7
; RV32-NEXT:  .LBB6_29:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 46(sp)
; RV32-NEXT:    bnez a0, .LBB6_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v26, v10, 6
; RV32-NEXT:    j .LBB6_32
; RV32-NEXT:  .LBB6_31:
; RV32-NEXT:    vslidedown.vi v26, v8, 6
; RV32-NEXT:  .LBB6_32:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 44(sp)
; RV32-NEXT:    bnez a0, .LBB6_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v26, v10, 5
; RV32-NEXT:    j .LBB6_35
; RV32-NEXT:  .LBB6_34:
; RV32-NEXT:    vslidedown.vi v26, v8, 5
; RV32-NEXT:  .LBB6_35:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 42(sp)
; RV32-NEXT:    bnez a0, .LBB6_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v26, v10, 4
; RV32-NEXT:    j .LBB6_38
; RV32-NEXT:  .LBB6_37:
; RV32-NEXT:    vslidedown.vi v26, v8, 4
; RV32-NEXT:  .LBB6_38:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 40(sp)
; RV32-NEXT:    bnez a0, .LBB6_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:    j .LBB6_41
; RV32-NEXT:  .LBB6_40:
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:  .LBB6_41:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 38(sp)
; RV32-NEXT:    bnez a0, .LBB6_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB6_44
; RV32-NEXT:  .LBB6_43:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB6_44:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 36(sp)
; RV32-NEXT:    bnez a0, .LBB6_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB6_47
; RV32-NEXT:  .LBB6_46:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB6_47:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 34(sp)
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v16f16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    bnez a0, .LBB6_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsh ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB6_4
; RV64-NEXT:  .LBB6_2:
; RV64-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 15
; RV64-NEXT:    j .LBB6_5
; RV64-NEXT:  .LBB6_3:
; RV64-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsh ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB6_2
; RV64-NEXT:  .LBB6_4:
; RV64-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 15
; RV64-NEXT:  .LBB6_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 62(sp)
; RV64-NEXT:    bnez a0, .LBB6_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 14
; RV64-NEXT:    j .LBB6_8
; RV64-NEXT:  .LBB6_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 14
; RV64-NEXT:  .LBB6_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 60(sp)
; RV64-NEXT:    bnez a0, .LBB6_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 13
; RV64-NEXT:    j .LBB6_11
; RV64-NEXT:  .LBB6_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 13
; RV64-NEXT:  .LBB6_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 58(sp)
; RV64-NEXT:    bnez a0, .LBB6_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v26, v10, 12
; RV64-NEXT:    j .LBB6_14
; RV64-NEXT:  .LBB6_13:
; RV64-NEXT:    vslidedown.vi v26, v8, 12
; RV64-NEXT:  .LBB6_14:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB6_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v26, v10, 11
; RV64-NEXT:    j .LBB6_17
; RV64-NEXT:  .LBB6_16:
; RV64-NEXT:    vslidedown.vi v26, v8, 11
; RV64-NEXT:  .LBB6_17:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 54(sp)
; RV64-NEXT:    bnez a0, .LBB6_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v26, v10, 10
; RV64-NEXT:    j .LBB6_20
; RV64-NEXT:  .LBB6_19:
; RV64-NEXT:    vslidedown.vi v26, v8, 10
; RV64-NEXT:  .LBB6_20:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 52(sp)
; RV64-NEXT:    bnez a0, .LBB6_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v26, v10, 9
; RV64-NEXT:    j .LBB6_23
; RV64-NEXT:  .LBB6_22:
; RV64-NEXT:    vslidedown.vi v26, v8, 9
; RV64-NEXT:  .LBB6_23:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 50(sp)
; RV64-NEXT:    bnez a0, .LBB6_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v26, v10, 8
; RV64-NEXT:    j .LBB6_26
; RV64-NEXT:  .LBB6_25:
; RV64-NEXT:    vslidedown.vi v26, v8, 8
; RV64-NEXT:  .LBB6_26:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB6_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v26, v10, 7
; RV64-NEXT:    j .LBB6_29
; RV64-NEXT:  .LBB6_28:
; RV64-NEXT:    vslidedown.vi v26, v8, 7
; RV64-NEXT:  .LBB6_29:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 46(sp)
; RV64-NEXT:    bnez a0, .LBB6_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v26, v10, 6
; RV64-NEXT:    j .LBB6_32
; RV64-NEXT:  .LBB6_31:
; RV64-NEXT:    vslidedown.vi v26, v8, 6
; RV64-NEXT:  .LBB6_32:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 44(sp)
; RV64-NEXT:    bnez a0, .LBB6_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v26, v10, 5
; RV64-NEXT:    j .LBB6_35
; RV64-NEXT:  .LBB6_34:
; RV64-NEXT:    vslidedown.vi v26, v8, 5
; RV64-NEXT:  .LBB6_35:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 42(sp)
; RV64-NEXT:    bnez a0, .LBB6_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v26, v10, 4
; RV64-NEXT:    j .LBB6_38
; RV64-NEXT:  .LBB6_37:
; RV64-NEXT:    vslidedown.vi v26, v8, 4
; RV64-NEXT:  .LBB6_38:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 40(sp)
; RV64-NEXT:    bnez a0, .LBB6_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:    j .LBB6_41
; RV64-NEXT:  .LBB6_40:
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:  .LBB6_41:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 38(sp)
; RV64-NEXT:    bnez a0, .LBB6_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB6_44
; RV64-NEXT:  .LBB6_43:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB6_44:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 36(sp)
; RV64-NEXT:    bnez a0, .LBB6_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB6_47
; RV64-NEXT:  .LBB6_46:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB6_47:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 34(sp)
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %v = select i1 %c, <16 x half> %a, <16 x half> %b
  ret <16 x half> %v
}

define <16 x half> @selectcc_v16f16(half %a, half %b, <16 x half> %c, <16 x half> %d) {
; RV32-LABEL: selectcc_v16f16:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    feq.h a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB7_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsh ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB7_4
; RV32-NEXT:  .LBB7_2:
; RV32-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 15
; RV32-NEXT:    j .LBB7_5
; RV32-NEXT:  .LBB7_3:
; RV32-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsh ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB7_2
; RV32-NEXT:  .LBB7_4:
; RV32-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 15
; RV32-NEXT:  .LBB7_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 62(sp)
; RV32-NEXT:    bnez a0, .LBB7_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 14
; RV32-NEXT:    j .LBB7_8
; RV32-NEXT:  .LBB7_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 14
; RV32-NEXT:  .LBB7_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 60(sp)
; RV32-NEXT:    bnez a0, .LBB7_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 13
; RV32-NEXT:    j .LBB7_11
; RV32-NEXT:  .LBB7_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 13
; RV32-NEXT:  .LBB7_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 58(sp)
; RV32-NEXT:    bnez a0, .LBB7_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v26, v10, 12
; RV32-NEXT:    j .LBB7_14
; RV32-NEXT:  .LBB7_13:
; RV32-NEXT:    vslidedown.vi v26, v8, 12
; RV32-NEXT:  .LBB7_14:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB7_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v26, v10, 11
; RV32-NEXT:    j .LBB7_17
; RV32-NEXT:  .LBB7_16:
; RV32-NEXT:    vslidedown.vi v26, v8, 11
; RV32-NEXT:  .LBB7_17:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 54(sp)
; RV32-NEXT:    bnez a0, .LBB7_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v26, v10, 10
; RV32-NEXT:    j .LBB7_20
; RV32-NEXT:  .LBB7_19:
; RV32-NEXT:    vslidedown.vi v26, v8, 10
; RV32-NEXT:  .LBB7_20:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 52(sp)
; RV32-NEXT:    bnez a0, .LBB7_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v26, v10, 9
; RV32-NEXT:    j .LBB7_23
; RV32-NEXT:  .LBB7_22:
; RV32-NEXT:    vslidedown.vi v26, v8, 9
; RV32-NEXT:  .LBB7_23:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 50(sp)
; RV32-NEXT:    bnez a0, .LBB7_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v26, v10, 8
; RV32-NEXT:    j .LBB7_26
; RV32-NEXT:  .LBB7_25:
; RV32-NEXT:    vslidedown.vi v26, v8, 8
; RV32-NEXT:  .LBB7_26:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB7_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v26, v10, 7
; RV32-NEXT:    j .LBB7_29
; RV32-NEXT:  .LBB7_28:
; RV32-NEXT:    vslidedown.vi v26, v8, 7
; RV32-NEXT:  .LBB7_29:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 46(sp)
; RV32-NEXT:    bnez a0, .LBB7_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v26, v10, 6
; RV32-NEXT:    j .LBB7_32
; RV32-NEXT:  .LBB7_31:
; RV32-NEXT:    vslidedown.vi v26, v8, 6
; RV32-NEXT:  .LBB7_32:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 44(sp)
; RV32-NEXT:    bnez a0, .LBB7_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v26, v10, 5
; RV32-NEXT:    j .LBB7_35
; RV32-NEXT:  .LBB7_34:
; RV32-NEXT:    vslidedown.vi v26, v8, 5
; RV32-NEXT:  .LBB7_35:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 42(sp)
; RV32-NEXT:    bnez a0, .LBB7_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v26, v10, 4
; RV32-NEXT:    j .LBB7_38
; RV32-NEXT:  .LBB7_37:
; RV32-NEXT:    vslidedown.vi v26, v8, 4
; RV32-NEXT:  .LBB7_38:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 40(sp)
; RV32-NEXT:    bnez a0, .LBB7_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:    j .LBB7_41
; RV32-NEXT:  .LBB7_40:
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:  .LBB7_41:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 38(sp)
; RV32-NEXT:    bnez a0, .LBB7_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB7_44
; RV32-NEXT:  .LBB7_43:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB7_44:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 36(sp)
; RV32-NEXT:    bnez a0, .LBB7_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB7_47
; RV32-NEXT:  .LBB7_46:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB7_47:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsh ft0, 34(sp)
; RV32-NEXT:    vsetivli zero, 16, e16, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v16f16:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    feq.h a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB7_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsh ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB7_4
; RV64-NEXT:  .LBB7_2:
; RV64-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 15
; RV64-NEXT:    j .LBB7_5
; RV64-NEXT:  .LBB7_3:
; RV64-NEXT:    vsetvli zero, zero, e16, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsh ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB7_2
; RV64-NEXT:  .LBB7_4:
; RV64-NEXT:    vsetivli zero, 1, e16, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 15
; RV64-NEXT:  .LBB7_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 62(sp)
; RV64-NEXT:    bnez a0, .LBB7_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 14
; RV64-NEXT:    j .LBB7_8
; RV64-NEXT:  .LBB7_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 14
; RV64-NEXT:  .LBB7_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 60(sp)
; RV64-NEXT:    bnez a0, .LBB7_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 13
; RV64-NEXT:    j .LBB7_11
; RV64-NEXT:  .LBB7_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 13
; RV64-NEXT:  .LBB7_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 58(sp)
; RV64-NEXT:    bnez a0, .LBB7_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v26, v10, 12
; RV64-NEXT:    j .LBB7_14
; RV64-NEXT:  .LBB7_13:
; RV64-NEXT:    vslidedown.vi v26, v8, 12
; RV64-NEXT:  .LBB7_14:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB7_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v26, v10, 11
; RV64-NEXT:    j .LBB7_17
; RV64-NEXT:  .LBB7_16:
; RV64-NEXT:    vslidedown.vi v26, v8, 11
; RV64-NEXT:  .LBB7_17:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 54(sp)
; RV64-NEXT:    bnez a0, .LBB7_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v26, v10, 10
; RV64-NEXT:    j .LBB7_20
; RV64-NEXT:  .LBB7_19:
; RV64-NEXT:    vslidedown.vi v26, v8, 10
; RV64-NEXT:  .LBB7_20:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 52(sp)
; RV64-NEXT:    bnez a0, .LBB7_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v26, v10, 9
; RV64-NEXT:    j .LBB7_23
; RV64-NEXT:  .LBB7_22:
; RV64-NEXT:    vslidedown.vi v26, v8, 9
; RV64-NEXT:  .LBB7_23:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 50(sp)
; RV64-NEXT:    bnez a0, .LBB7_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v26, v10, 8
; RV64-NEXT:    j .LBB7_26
; RV64-NEXT:  .LBB7_25:
; RV64-NEXT:    vslidedown.vi v26, v8, 8
; RV64-NEXT:  .LBB7_26:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB7_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v26, v10, 7
; RV64-NEXT:    j .LBB7_29
; RV64-NEXT:  .LBB7_28:
; RV64-NEXT:    vslidedown.vi v26, v8, 7
; RV64-NEXT:  .LBB7_29:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 46(sp)
; RV64-NEXT:    bnez a0, .LBB7_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v26, v10, 6
; RV64-NEXT:    j .LBB7_32
; RV64-NEXT:  .LBB7_31:
; RV64-NEXT:    vslidedown.vi v26, v8, 6
; RV64-NEXT:  .LBB7_32:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 44(sp)
; RV64-NEXT:    bnez a0, .LBB7_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v26, v10, 5
; RV64-NEXT:    j .LBB7_35
; RV64-NEXT:  .LBB7_34:
; RV64-NEXT:    vslidedown.vi v26, v8, 5
; RV64-NEXT:  .LBB7_35:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 42(sp)
; RV64-NEXT:    bnez a0, .LBB7_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v26, v10, 4
; RV64-NEXT:    j .LBB7_38
; RV64-NEXT:  .LBB7_37:
; RV64-NEXT:    vslidedown.vi v26, v8, 4
; RV64-NEXT:  .LBB7_38:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 40(sp)
; RV64-NEXT:    bnez a0, .LBB7_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:    j .LBB7_41
; RV64-NEXT:  .LBB7_40:
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:  .LBB7_41:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 38(sp)
; RV64-NEXT:    bnez a0, .LBB7_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB7_44
; RV64-NEXT:  .LBB7_43:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB7_44:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 36(sp)
; RV64-NEXT:    bnez a0, .LBB7_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB7_47
; RV64-NEXT:  .LBB7_46:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB7_47:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsh ft0, 34(sp)
; RV64-NEXT:    vsetivli zero, 16, e16, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %cmp = fcmp oeq half %a, %b
  %v = select i1 %cmp, <16 x half> %c, <16 x half> %d
  ret <16 x half> %v
}

define <2 x float> @select_v2f32(i1 zeroext %c, <2 x float> %a, <2 x float> %b) {
; CHECK-LABEL: select_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    bnez a0, .LBB8_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB8_3
; CHECK-NEXT:  .LBB8_2:
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB8_3:
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; CHECK-NEXT:    vfmv.v.f v8, ft1
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, tu, mu
; CHECK-NEXT:    vfmv.s.f v8, ft0
; CHECK-NEXT:    ret
  %v = select i1 %c, <2 x float> %a, <2 x float> %b
  ret <2 x float> %v
}

define <2 x float> @selectcc_v2f32(float %a, float %b, <2 x float> %c, <2 x float> %d) {
; CHECK-LABEL: selectcc_v2f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.s a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB9_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB9_3
; CHECK-NEXT:  .LBB9_2:
; CHECK-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB9_3:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    bnez a0, .LBB9_5
; CHECK-NEXT:  # %bb.4:
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    j .LBB9_6
; CHECK-NEXT:  .LBB9_5:
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:  .LBB9_6:
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, tu, mu
; CHECK-NEXT:    vfmv.s.f v25, ft0
; CHECK-NEXT:    vmv1r.v v8, v25
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <2 x float> %c, <2 x float> %d
  ret <2 x float> %v
}

define <4 x float> @select_v4f32(i1 zeroext %c, <4 x float> %a, <4 x float> %b) {
; CHECK-LABEL: select_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    bnez a0, .LBB10_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsw ft0, 16(sp)
; CHECK-NEXT:    beqz a0, .LBB10_4
; CHECK-NEXT:  .LBB10_2:
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:    j .LBB10_5
; CHECK-NEXT:  .LBB10_3:
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsw ft0, 16(sp)
; CHECK-NEXT:    bnez a0, .LBB10_2
; CHECK-NEXT:  .LBB10_4:
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:  .LBB10_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 28(sp)
; CHECK-NEXT:    bnez a0, .LBB10_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB10_8
; CHECK-NEXT:  .LBB10_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB10_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 24(sp)
; CHECK-NEXT:    bnez a0, .LBB10_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB10_11
; CHECK-NEXT:  .LBB10_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB10_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 20(sp)
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    ret
  %v = select i1 %c, <4 x float> %a, <4 x float> %b
  ret <4 x float> %v
}

define <4 x float> @selectcc_v4f32(float %a, float %b, <4 x float> %c, <4 x float> %d) {
; CHECK-LABEL: selectcc_v4f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -32
; CHECK-NEXT:    .cfi_def_cfa_offset 32
; CHECK-NEXT:    feq.s a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB11_3
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    fsw ft0, 16(sp)
; CHECK-NEXT:    beqz a0, .LBB11_4
; CHECK-NEXT:  .LBB11_2:
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 3
; CHECK-NEXT:    j .LBB11_5
; CHECK-NEXT:  .LBB11_3:
; CHECK-NEXT:    vsetvli zero, zero, e32, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    fsw ft0, 16(sp)
; CHECK-NEXT:    bnez a0, .LBB11_2
; CHECK-NEXT:  .LBB11_4:
; CHECK-NEXT:    vsetivli zero, 1, e32, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 3
; CHECK-NEXT:  .LBB11_5:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 28(sp)
; CHECK-NEXT:    bnez a0, .LBB11_7
; CHECK-NEXT:  # %bb.6:
; CHECK-NEXT:    vslidedown.vi v25, v9, 2
; CHECK-NEXT:    j .LBB11_8
; CHECK-NEXT:  .LBB11_7:
; CHECK-NEXT:    vslidedown.vi v25, v8, 2
; CHECK-NEXT:  .LBB11_8:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 24(sp)
; CHECK-NEXT:    bnez a0, .LBB11_10
; CHECK-NEXT:  # %bb.9:
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB11_11
; CHECK-NEXT:  .LBB11_10:
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB11_11:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    fsw ft0, 20(sp)
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; CHECK-NEXT:    addi a0, sp, 16
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    addi sp, sp, 32
; CHECK-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <4 x float> %c, <4 x float> %d
  ret <4 x float> %v
}

define <8 x float> @select_v8f32(i1 zeroext %c, <8 x float> %a, <8 x float> %b) {
; RV32-LABEL: select_v8f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    bnez a0, .LBB12_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsw ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB12_4
; RV32-NEXT:  .LBB12_2:
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 7
; RV32-NEXT:    j .LBB12_5
; RV32-NEXT:  .LBB12_3:
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsw ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB12_2
; RV32-NEXT:  .LBB12_4:
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 7
; RV32-NEXT:  .LBB12_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 60(sp)
; RV32-NEXT:    bnez a0, .LBB12_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 6
; RV32-NEXT:    j .LBB12_8
; RV32-NEXT:  .LBB12_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 6
; RV32-NEXT:  .LBB12_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB12_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 5
; RV32-NEXT:    j .LBB12_11
; RV32-NEXT:  .LBB12_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 5
; RV32-NEXT:  .LBB12_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 52(sp)
; RV32-NEXT:    bnez a0, .LBB12_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v26, v10, 4
; RV32-NEXT:    j .LBB12_14
; RV32-NEXT:  .LBB12_13:
; RV32-NEXT:    vslidedown.vi v26, v8, 4
; RV32-NEXT:  .LBB12_14:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB12_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:    j .LBB12_17
; RV32-NEXT:  .LBB12_16:
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:  .LBB12_17:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 44(sp)
; RV32-NEXT:    bnez a0, .LBB12_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB12_20
; RV32-NEXT:  .LBB12_19:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB12_20:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 40(sp)
; RV32-NEXT:    bnez a0, .LBB12_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB12_23
; RV32-NEXT:  .LBB12_22:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB12_23:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 36(sp)
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v8f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    bnez a0, .LBB12_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsw ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB12_4
; RV64-NEXT:  .LBB12_2:
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 7
; RV64-NEXT:    j .LBB12_5
; RV64-NEXT:  .LBB12_3:
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsw ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB12_2
; RV64-NEXT:  .LBB12_4:
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 7
; RV64-NEXT:  .LBB12_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 60(sp)
; RV64-NEXT:    bnez a0, .LBB12_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 6
; RV64-NEXT:    j .LBB12_8
; RV64-NEXT:  .LBB12_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 6
; RV64-NEXT:  .LBB12_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB12_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 5
; RV64-NEXT:    j .LBB12_11
; RV64-NEXT:  .LBB12_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 5
; RV64-NEXT:  .LBB12_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 52(sp)
; RV64-NEXT:    bnez a0, .LBB12_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v26, v10, 4
; RV64-NEXT:    j .LBB12_14
; RV64-NEXT:  .LBB12_13:
; RV64-NEXT:    vslidedown.vi v26, v8, 4
; RV64-NEXT:  .LBB12_14:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB12_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:    j .LBB12_17
; RV64-NEXT:  .LBB12_16:
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:  .LBB12_17:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 44(sp)
; RV64-NEXT:    bnez a0, .LBB12_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB12_20
; RV64-NEXT:  .LBB12_19:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB12_20:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 40(sp)
; RV64-NEXT:    bnez a0, .LBB12_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB12_23
; RV64-NEXT:  .LBB12_22:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB12_23:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 36(sp)
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %v = select i1 %c, <8 x float> %a, <8 x float> %b
  ret <8 x float> %v
}

define <8 x float> @selectcc_v8f32(float %a, float %b, <8 x float> %c, <8 x float> %d) {
; RV32-LABEL: selectcc_v8f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    feq.s a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB13_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsw ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB13_4
; RV32-NEXT:  .LBB13_2:
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 7
; RV32-NEXT:    j .LBB13_5
; RV32-NEXT:  .LBB13_3:
; RV32-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsw ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB13_2
; RV32-NEXT:  .LBB13_4:
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 7
; RV32-NEXT:  .LBB13_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 60(sp)
; RV32-NEXT:    bnez a0, .LBB13_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 6
; RV32-NEXT:    j .LBB13_8
; RV32-NEXT:  .LBB13_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 6
; RV32-NEXT:  .LBB13_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB13_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 5
; RV32-NEXT:    j .LBB13_11
; RV32-NEXT:  .LBB13_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 5
; RV32-NEXT:  .LBB13_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 52(sp)
; RV32-NEXT:    bnez a0, .LBB13_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v26, v10, 4
; RV32-NEXT:    j .LBB13_14
; RV32-NEXT:  .LBB13_13:
; RV32-NEXT:    vslidedown.vi v26, v8, 4
; RV32-NEXT:  .LBB13_14:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB13_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:    j .LBB13_17
; RV32-NEXT:  .LBB13_16:
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:  .LBB13_17:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 44(sp)
; RV32-NEXT:    bnez a0, .LBB13_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB13_20
; RV32-NEXT:  .LBB13_19:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB13_20:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 40(sp)
; RV32-NEXT:    bnez a0, .LBB13_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB13_23
; RV32-NEXT:  .LBB13_22:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB13_23:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsw ft0, 36(sp)
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v8f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    feq.s a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB13_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsw ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB13_4
; RV64-NEXT:  .LBB13_2:
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 7
; RV64-NEXT:    j .LBB13_5
; RV64-NEXT:  .LBB13_3:
; RV64-NEXT:    vsetvli zero, zero, e32, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsw ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB13_2
; RV64-NEXT:  .LBB13_4:
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 7
; RV64-NEXT:  .LBB13_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 60(sp)
; RV64-NEXT:    bnez a0, .LBB13_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 6
; RV64-NEXT:    j .LBB13_8
; RV64-NEXT:  .LBB13_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 6
; RV64-NEXT:  .LBB13_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB13_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 5
; RV64-NEXT:    j .LBB13_11
; RV64-NEXT:  .LBB13_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 5
; RV64-NEXT:  .LBB13_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 52(sp)
; RV64-NEXT:    bnez a0, .LBB13_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v26, v10, 4
; RV64-NEXT:    j .LBB13_14
; RV64-NEXT:  .LBB13_13:
; RV64-NEXT:    vslidedown.vi v26, v8, 4
; RV64-NEXT:  .LBB13_14:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB13_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:    j .LBB13_17
; RV64-NEXT:  .LBB13_16:
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:  .LBB13_17:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 44(sp)
; RV64-NEXT:    bnez a0, .LBB13_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB13_20
; RV64-NEXT:  .LBB13_19:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB13_20:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 40(sp)
; RV64-NEXT:    bnez a0, .LBB13_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB13_23
; RV64-NEXT:  .LBB13_22:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB13_23:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsw ft0, 36(sp)
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <8 x float> %c, <8 x float> %d
  ret <8 x float> %v
}

define <16 x float> @select_v16f32(i1 zeroext %c, <16 x float> %a, <16 x float> %b) {
; RV32-LABEL: select_v16f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -192
; RV32-NEXT:    .cfi_def_cfa_offset 192
; RV32-NEXT:    sw ra, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 192
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    bnez a0, .LBB14_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v12
; RV32-NEXT:    fsw ft0, 64(sp)
; RV32-NEXT:    beqz a0, .LBB14_4
; RV32-NEXT:  .LBB14_2:
; RV32-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v8, 15
; RV32-NEXT:    j .LBB14_5
; RV32-NEXT:  .LBB14_3:
; RV32-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsw ft0, 64(sp)
; RV32-NEXT:    bnez a0, .LBB14_2
; RV32-NEXT:  .LBB14_4:
; RV32-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v12, 15
; RV32-NEXT:  .LBB14_5:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 124(sp)
; RV32-NEXT:    bnez a0, .LBB14_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v28, v12, 14
; RV32-NEXT:    j .LBB14_8
; RV32-NEXT:  .LBB14_7:
; RV32-NEXT:    vslidedown.vi v28, v8, 14
; RV32-NEXT:  .LBB14_8:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 120(sp)
; RV32-NEXT:    bnez a0, .LBB14_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v28, v12, 13
; RV32-NEXT:    j .LBB14_11
; RV32-NEXT:  .LBB14_10:
; RV32-NEXT:    vslidedown.vi v28, v8, 13
; RV32-NEXT:  .LBB14_11:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 116(sp)
; RV32-NEXT:    bnez a0, .LBB14_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v28, v12, 12
; RV32-NEXT:    j .LBB14_14
; RV32-NEXT:  .LBB14_13:
; RV32-NEXT:    vslidedown.vi v28, v8, 12
; RV32-NEXT:  .LBB14_14:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 112(sp)
; RV32-NEXT:    bnez a0, .LBB14_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v28, v12, 11
; RV32-NEXT:    j .LBB14_17
; RV32-NEXT:  .LBB14_16:
; RV32-NEXT:    vslidedown.vi v28, v8, 11
; RV32-NEXT:  .LBB14_17:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 108(sp)
; RV32-NEXT:    bnez a0, .LBB14_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v28, v12, 10
; RV32-NEXT:    j .LBB14_20
; RV32-NEXT:  .LBB14_19:
; RV32-NEXT:    vslidedown.vi v28, v8, 10
; RV32-NEXT:  .LBB14_20:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 104(sp)
; RV32-NEXT:    bnez a0, .LBB14_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v28, v12, 9
; RV32-NEXT:    j .LBB14_23
; RV32-NEXT:  .LBB14_22:
; RV32-NEXT:    vslidedown.vi v28, v8, 9
; RV32-NEXT:  .LBB14_23:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 100(sp)
; RV32-NEXT:    bnez a0, .LBB14_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v28, v12, 8
; RV32-NEXT:    j .LBB14_26
; RV32-NEXT:  .LBB14_25:
; RV32-NEXT:    vslidedown.vi v28, v8, 8
; RV32-NEXT:  .LBB14_26:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 96(sp)
; RV32-NEXT:    bnez a0, .LBB14_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v28, v12, 7
; RV32-NEXT:    j .LBB14_29
; RV32-NEXT:  .LBB14_28:
; RV32-NEXT:    vslidedown.vi v28, v8, 7
; RV32-NEXT:  .LBB14_29:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 92(sp)
; RV32-NEXT:    bnez a0, .LBB14_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v28, v12, 6
; RV32-NEXT:    j .LBB14_32
; RV32-NEXT:  .LBB14_31:
; RV32-NEXT:    vslidedown.vi v28, v8, 6
; RV32-NEXT:  .LBB14_32:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 88(sp)
; RV32-NEXT:    bnez a0, .LBB14_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v28, v12, 5
; RV32-NEXT:    j .LBB14_35
; RV32-NEXT:  .LBB14_34:
; RV32-NEXT:    vslidedown.vi v28, v8, 5
; RV32-NEXT:  .LBB14_35:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 84(sp)
; RV32-NEXT:    bnez a0, .LBB14_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v28, v12, 4
; RV32-NEXT:    j .LBB14_38
; RV32-NEXT:  .LBB14_37:
; RV32-NEXT:    vslidedown.vi v28, v8, 4
; RV32-NEXT:  .LBB14_38:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 80(sp)
; RV32-NEXT:    bnez a0, .LBB14_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v28, v12, 3
; RV32-NEXT:    j .LBB14_41
; RV32-NEXT:  .LBB14_40:
; RV32-NEXT:    vslidedown.vi v28, v8, 3
; RV32-NEXT:  .LBB14_41:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 76(sp)
; RV32-NEXT:    bnez a0, .LBB14_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v28, v12, 2
; RV32-NEXT:    j .LBB14_44
; RV32-NEXT:  .LBB14_43:
; RV32-NEXT:    vslidedown.vi v28, v8, 2
; RV32-NEXT:  .LBB14_44:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 72(sp)
; RV32-NEXT:    bnez a0, .LBB14_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v28, v12, 1
; RV32-NEXT:    j .LBB14_47
; RV32-NEXT:  .LBB14_46:
; RV32-NEXT:    vslidedown.vi v28, v8, 1
; RV32-NEXT:  .LBB14_47:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 68(sp)
; RV32-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -192
; RV32-NEXT:    lw s0, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 192
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v16f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -192
; RV64-NEXT:    .cfi_def_cfa_offset 192
; RV64-NEXT:    sd ra, 184(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 176(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 192
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    bnez a0, .LBB14_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v12
; RV64-NEXT:    fsw ft0, 64(sp)
; RV64-NEXT:    beqz a0, .LBB14_4
; RV64-NEXT:  .LBB14_2:
; RV64-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v8, 15
; RV64-NEXT:    j .LBB14_5
; RV64-NEXT:  .LBB14_3:
; RV64-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsw ft0, 64(sp)
; RV64-NEXT:    bnez a0, .LBB14_2
; RV64-NEXT:  .LBB14_4:
; RV64-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v12, 15
; RV64-NEXT:  .LBB14_5:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 124(sp)
; RV64-NEXT:    bnez a0, .LBB14_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v28, v12, 14
; RV64-NEXT:    j .LBB14_8
; RV64-NEXT:  .LBB14_7:
; RV64-NEXT:    vslidedown.vi v28, v8, 14
; RV64-NEXT:  .LBB14_8:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 120(sp)
; RV64-NEXT:    bnez a0, .LBB14_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v28, v12, 13
; RV64-NEXT:    j .LBB14_11
; RV64-NEXT:  .LBB14_10:
; RV64-NEXT:    vslidedown.vi v28, v8, 13
; RV64-NEXT:  .LBB14_11:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 116(sp)
; RV64-NEXT:    bnez a0, .LBB14_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v28, v12, 12
; RV64-NEXT:    j .LBB14_14
; RV64-NEXT:  .LBB14_13:
; RV64-NEXT:    vslidedown.vi v28, v8, 12
; RV64-NEXT:  .LBB14_14:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 112(sp)
; RV64-NEXT:    bnez a0, .LBB14_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v28, v12, 11
; RV64-NEXT:    j .LBB14_17
; RV64-NEXT:  .LBB14_16:
; RV64-NEXT:    vslidedown.vi v28, v8, 11
; RV64-NEXT:  .LBB14_17:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 108(sp)
; RV64-NEXT:    bnez a0, .LBB14_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v28, v12, 10
; RV64-NEXT:    j .LBB14_20
; RV64-NEXT:  .LBB14_19:
; RV64-NEXT:    vslidedown.vi v28, v8, 10
; RV64-NEXT:  .LBB14_20:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 104(sp)
; RV64-NEXT:    bnez a0, .LBB14_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v28, v12, 9
; RV64-NEXT:    j .LBB14_23
; RV64-NEXT:  .LBB14_22:
; RV64-NEXT:    vslidedown.vi v28, v8, 9
; RV64-NEXT:  .LBB14_23:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 100(sp)
; RV64-NEXT:    bnez a0, .LBB14_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v28, v12, 8
; RV64-NEXT:    j .LBB14_26
; RV64-NEXT:  .LBB14_25:
; RV64-NEXT:    vslidedown.vi v28, v8, 8
; RV64-NEXT:  .LBB14_26:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 96(sp)
; RV64-NEXT:    bnez a0, .LBB14_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v28, v12, 7
; RV64-NEXT:    j .LBB14_29
; RV64-NEXT:  .LBB14_28:
; RV64-NEXT:    vslidedown.vi v28, v8, 7
; RV64-NEXT:  .LBB14_29:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 92(sp)
; RV64-NEXT:    bnez a0, .LBB14_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v28, v12, 6
; RV64-NEXT:    j .LBB14_32
; RV64-NEXT:  .LBB14_31:
; RV64-NEXT:    vslidedown.vi v28, v8, 6
; RV64-NEXT:  .LBB14_32:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 88(sp)
; RV64-NEXT:    bnez a0, .LBB14_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v28, v12, 5
; RV64-NEXT:    j .LBB14_35
; RV64-NEXT:  .LBB14_34:
; RV64-NEXT:    vslidedown.vi v28, v8, 5
; RV64-NEXT:  .LBB14_35:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 84(sp)
; RV64-NEXT:    bnez a0, .LBB14_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v28, v12, 4
; RV64-NEXT:    j .LBB14_38
; RV64-NEXT:  .LBB14_37:
; RV64-NEXT:    vslidedown.vi v28, v8, 4
; RV64-NEXT:  .LBB14_38:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 80(sp)
; RV64-NEXT:    bnez a0, .LBB14_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v28, v12, 3
; RV64-NEXT:    j .LBB14_41
; RV64-NEXT:  .LBB14_40:
; RV64-NEXT:    vslidedown.vi v28, v8, 3
; RV64-NEXT:  .LBB14_41:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 76(sp)
; RV64-NEXT:    bnez a0, .LBB14_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v28, v12, 2
; RV64-NEXT:    j .LBB14_44
; RV64-NEXT:  .LBB14_43:
; RV64-NEXT:    vslidedown.vi v28, v8, 2
; RV64-NEXT:  .LBB14_44:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 72(sp)
; RV64-NEXT:    bnez a0, .LBB14_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v28, v12, 1
; RV64-NEXT:    j .LBB14_47
; RV64-NEXT:  .LBB14_46:
; RV64-NEXT:    vslidedown.vi v28, v8, 1
; RV64-NEXT:  .LBB14_47:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 68(sp)
; RV64-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV64-NEXT:    addi a0, sp, 64
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -192
; RV64-NEXT:    ld s0, 176(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 184(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 192
; RV64-NEXT:    ret
  %v = select i1 %c, <16 x float> %a, <16 x float> %b
  ret <16 x float> %v
}

define <16 x float> @selectcc_v16f32(float %a, float %b, <16 x float> %c, <16 x float> %d) {
; RV32-LABEL: selectcc_v16f32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -192
; RV32-NEXT:    .cfi_def_cfa_offset 192
; RV32-NEXT:    sw ra, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 192
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    feq.s a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB15_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v12
; RV32-NEXT:    fsw ft0, 64(sp)
; RV32-NEXT:    beqz a0, .LBB15_4
; RV32-NEXT:  .LBB15_2:
; RV32-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v8, 15
; RV32-NEXT:    j .LBB15_5
; RV32-NEXT:  .LBB15_3:
; RV32-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsw ft0, 64(sp)
; RV32-NEXT:    bnez a0, .LBB15_2
; RV32-NEXT:  .LBB15_4:
; RV32-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v12, 15
; RV32-NEXT:  .LBB15_5:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 124(sp)
; RV32-NEXT:    bnez a0, .LBB15_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v28, v12, 14
; RV32-NEXT:    j .LBB15_8
; RV32-NEXT:  .LBB15_7:
; RV32-NEXT:    vslidedown.vi v28, v8, 14
; RV32-NEXT:  .LBB15_8:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 120(sp)
; RV32-NEXT:    bnez a0, .LBB15_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v28, v12, 13
; RV32-NEXT:    j .LBB15_11
; RV32-NEXT:  .LBB15_10:
; RV32-NEXT:    vslidedown.vi v28, v8, 13
; RV32-NEXT:  .LBB15_11:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 116(sp)
; RV32-NEXT:    bnez a0, .LBB15_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v28, v12, 12
; RV32-NEXT:    j .LBB15_14
; RV32-NEXT:  .LBB15_13:
; RV32-NEXT:    vslidedown.vi v28, v8, 12
; RV32-NEXT:  .LBB15_14:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 112(sp)
; RV32-NEXT:    bnez a0, .LBB15_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v28, v12, 11
; RV32-NEXT:    j .LBB15_17
; RV32-NEXT:  .LBB15_16:
; RV32-NEXT:    vslidedown.vi v28, v8, 11
; RV32-NEXT:  .LBB15_17:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 108(sp)
; RV32-NEXT:    bnez a0, .LBB15_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v28, v12, 10
; RV32-NEXT:    j .LBB15_20
; RV32-NEXT:  .LBB15_19:
; RV32-NEXT:    vslidedown.vi v28, v8, 10
; RV32-NEXT:  .LBB15_20:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 104(sp)
; RV32-NEXT:    bnez a0, .LBB15_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v28, v12, 9
; RV32-NEXT:    j .LBB15_23
; RV32-NEXT:  .LBB15_22:
; RV32-NEXT:    vslidedown.vi v28, v8, 9
; RV32-NEXT:  .LBB15_23:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 100(sp)
; RV32-NEXT:    bnez a0, .LBB15_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v28, v12, 8
; RV32-NEXT:    j .LBB15_26
; RV32-NEXT:  .LBB15_25:
; RV32-NEXT:    vslidedown.vi v28, v8, 8
; RV32-NEXT:  .LBB15_26:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 96(sp)
; RV32-NEXT:    bnez a0, .LBB15_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v28, v12, 7
; RV32-NEXT:    j .LBB15_29
; RV32-NEXT:  .LBB15_28:
; RV32-NEXT:    vslidedown.vi v28, v8, 7
; RV32-NEXT:  .LBB15_29:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 92(sp)
; RV32-NEXT:    bnez a0, .LBB15_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v28, v12, 6
; RV32-NEXT:    j .LBB15_32
; RV32-NEXT:  .LBB15_31:
; RV32-NEXT:    vslidedown.vi v28, v8, 6
; RV32-NEXT:  .LBB15_32:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 88(sp)
; RV32-NEXT:    bnez a0, .LBB15_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v28, v12, 5
; RV32-NEXT:    j .LBB15_35
; RV32-NEXT:  .LBB15_34:
; RV32-NEXT:    vslidedown.vi v28, v8, 5
; RV32-NEXT:  .LBB15_35:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 84(sp)
; RV32-NEXT:    bnez a0, .LBB15_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v28, v12, 4
; RV32-NEXT:    j .LBB15_38
; RV32-NEXT:  .LBB15_37:
; RV32-NEXT:    vslidedown.vi v28, v8, 4
; RV32-NEXT:  .LBB15_38:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 80(sp)
; RV32-NEXT:    bnez a0, .LBB15_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v28, v12, 3
; RV32-NEXT:    j .LBB15_41
; RV32-NEXT:  .LBB15_40:
; RV32-NEXT:    vslidedown.vi v28, v8, 3
; RV32-NEXT:  .LBB15_41:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 76(sp)
; RV32-NEXT:    bnez a0, .LBB15_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v28, v12, 2
; RV32-NEXT:    j .LBB15_44
; RV32-NEXT:  .LBB15_43:
; RV32-NEXT:    vslidedown.vi v28, v8, 2
; RV32-NEXT:  .LBB15_44:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 72(sp)
; RV32-NEXT:    bnez a0, .LBB15_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v28, v12, 1
; RV32-NEXT:    j .LBB15_47
; RV32-NEXT:  .LBB15_46:
; RV32-NEXT:    vslidedown.vi v28, v8, 1
; RV32-NEXT:  .LBB15_47:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsw ft0, 68(sp)
; RV32-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -192
; RV32-NEXT:    lw s0, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 192
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v16f32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -192
; RV64-NEXT:    .cfi_def_cfa_offset 192
; RV64-NEXT:    sd ra, 184(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 176(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 192
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    feq.s a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB15_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v12
; RV64-NEXT:    fsw ft0, 64(sp)
; RV64-NEXT:    beqz a0, .LBB15_4
; RV64-NEXT:  .LBB15_2:
; RV64-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v8, 15
; RV64-NEXT:    j .LBB15_5
; RV64-NEXT:  .LBB15_3:
; RV64-NEXT:    vsetvli zero, zero, e32, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsw ft0, 64(sp)
; RV64-NEXT:    bnez a0, .LBB15_2
; RV64-NEXT:  .LBB15_4:
; RV64-NEXT:    vsetivli zero, 1, e32, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v12, 15
; RV64-NEXT:  .LBB15_5:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 124(sp)
; RV64-NEXT:    bnez a0, .LBB15_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v28, v12, 14
; RV64-NEXT:    j .LBB15_8
; RV64-NEXT:  .LBB15_7:
; RV64-NEXT:    vslidedown.vi v28, v8, 14
; RV64-NEXT:  .LBB15_8:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 120(sp)
; RV64-NEXT:    bnez a0, .LBB15_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v28, v12, 13
; RV64-NEXT:    j .LBB15_11
; RV64-NEXT:  .LBB15_10:
; RV64-NEXT:    vslidedown.vi v28, v8, 13
; RV64-NEXT:  .LBB15_11:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 116(sp)
; RV64-NEXT:    bnez a0, .LBB15_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v28, v12, 12
; RV64-NEXT:    j .LBB15_14
; RV64-NEXT:  .LBB15_13:
; RV64-NEXT:    vslidedown.vi v28, v8, 12
; RV64-NEXT:  .LBB15_14:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 112(sp)
; RV64-NEXT:    bnez a0, .LBB15_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v28, v12, 11
; RV64-NEXT:    j .LBB15_17
; RV64-NEXT:  .LBB15_16:
; RV64-NEXT:    vslidedown.vi v28, v8, 11
; RV64-NEXT:  .LBB15_17:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 108(sp)
; RV64-NEXT:    bnez a0, .LBB15_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v28, v12, 10
; RV64-NEXT:    j .LBB15_20
; RV64-NEXT:  .LBB15_19:
; RV64-NEXT:    vslidedown.vi v28, v8, 10
; RV64-NEXT:  .LBB15_20:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 104(sp)
; RV64-NEXT:    bnez a0, .LBB15_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v28, v12, 9
; RV64-NEXT:    j .LBB15_23
; RV64-NEXT:  .LBB15_22:
; RV64-NEXT:    vslidedown.vi v28, v8, 9
; RV64-NEXT:  .LBB15_23:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 100(sp)
; RV64-NEXT:    bnez a0, .LBB15_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v28, v12, 8
; RV64-NEXT:    j .LBB15_26
; RV64-NEXT:  .LBB15_25:
; RV64-NEXT:    vslidedown.vi v28, v8, 8
; RV64-NEXT:  .LBB15_26:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 96(sp)
; RV64-NEXT:    bnez a0, .LBB15_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v28, v12, 7
; RV64-NEXT:    j .LBB15_29
; RV64-NEXT:  .LBB15_28:
; RV64-NEXT:    vslidedown.vi v28, v8, 7
; RV64-NEXT:  .LBB15_29:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 92(sp)
; RV64-NEXT:    bnez a0, .LBB15_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v28, v12, 6
; RV64-NEXT:    j .LBB15_32
; RV64-NEXT:  .LBB15_31:
; RV64-NEXT:    vslidedown.vi v28, v8, 6
; RV64-NEXT:  .LBB15_32:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 88(sp)
; RV64-NEXT:    bnez a0, .LBB15_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v28, v12, 5
; RV64-NEXT:    j .LBB15_35
; RV64-NEXT:  .LBB15_34:
; RV64-NEXT:    vslidedown.vi v28, v8, 5
; RV64-NEXT:  .LBB15_35:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 84(sp)
; RV64-NEXT:    bnez a0, .LBB15_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v28, v12, 4
; RV64-NEXT:    j .LBB15_38
; RV64-NEXT:  .LBB15_37:
; RV64-NEXT:    vslidedown.vi v28, v8, 4
; RV64-NEXT:  .LBB15_38:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 80(sp)
; RV64-NEXT:    bnez a0, .LBB15_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v28, v12, 3
; RV64-NEXT:    j .LBB15_41
; RV64-NEXT:  .LBB15_40:
; RV64-NEXT:    vslidedown.vi v28, v8, 3
; RV64-NEXT:  .LBB15_41:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 76(sp)
; RV64-NEXT:    bnez a0, .LBB15_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v28, v12, 2
; RV64-NEXT:    j .LBB15_44
; RV64-NEXT:  .LBB15_43:
; RV64-NEXT:    vslidedown.vi v28, v8, 2
; RV64-NEXT:  .LBB15_44:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 72(sp)
; RV64-NEXT:    bnez a0, .LBB15_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v28, v12, 1
; RV64-NEXT:    j .LBB15_47
; RV64-NEXT:  .LBB15_46:
; RV64-NEXT:    vslidedown.vi v28, v8, 1
; RV64-NEXT:  .LBB15_47:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsw ft0, 68(sp)
; RV64-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV64-NEXT:    addi a0, sp, 64
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -192
; RV64-NEXT:    ld s0, 176(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 184(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 192
; RV64-NEXT:    ret
  %cmp = fcmp oeq float %a, %b
  %v = select i1 %cmp, <16 x float> %c, <16 x float> %d
  ret <16 x float> %v
}

define <2 x double> @select_v2f64(i1 zeroext %c, <2 x double> %a, <2 x double> %b) {
; CHECK-LABEL: select_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    bnez a0, .LBB16_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB16_3
; CHECK-NEXT:  .LBB16_2:
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB16_3:
; CHECK-NEXT:    vfmv.f.s ft1, v25
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.v.f v8, ft1
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; CHECK-NEXT:    vfmv.s.f v8, ft0
; CHECK-NEXT:    ret
  %v = select i1 %c, <2 x double> %a, <2 x double> %b
  ret <2 x double> %v
}

define <2 x double> @selectcc_v2f64(double %a, double %b, <2 x double> %c, <2 x double> %d) {
; CHECK-LABEL: selectcc_v2f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    feq.d a0, fa0, fa1
; CHECK-NEXT:    bnez a0, .LBB17_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v9, 1
; CHECK-NEXT:    j .LBB17_3
; CHECK-NEXT:  .LBB17_2:
; CHECK-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; CHECK-NEXT:    vslidedown.vi v25, v8, 1
; CHECK-NEXT:  .LBB17_3:
; CHECK-NEXT:    vfmv.f.s ft0, v25
; CHECK-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; CHECK-NEXT:    vfmv.v.f v25, ft0
; CHECK-NEXT:    bnez a0, .LBB17_5
; CHECK-NEXT:  # %bb.4:
; CHECK-NEXT:    vfmv.f.s ft0, v9
; CHECK-NEXT:    j .LBB17_6
; CHECK-NEXT:  .LBB17_5:
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:  .LBB17_6:
; CHECK-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; CHECK-NEXT:    vfmv.s.f v25, ft0
; CHECK-NEXT:    vmv1r.v v8, v25
; CHECK-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <2 x double> %c, <2 x double> %d
  ret <2 x double> %v
}

define <4 x double> @select_v4f64(i1 zeroext %c, <4 x double> %a, <4 x double> %b) {
; RV32-LABEL: select_v4f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    bnez a0, .LBB18_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsd ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB18_4
; RV32-NEXT:  .LBB18_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:    j .LBB18_5
; RV32-NEXT:  .LBB18_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB18_2
; RV32-NEXT:  .LBB18_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:  .LBB18_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB18_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB18_8
; RV32-NEXT:  .LBB18_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB18_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB18_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB18_11
; RV32-NEXT:  .LBB18_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB18_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 40(sp)
; RV32-NEXT:    vsetivli zero, 4, e64, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v4f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    bnez a0, .LBB18_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsd ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB18_4
; RV64-NEXT:  .LBB18_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:    j .LBB18_5
; RV64-NEXT:  .LBB18_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB18_2
; RV64-NEXT:  .LBB18_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:  .LBB18_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB18_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB18_8
; RV64-NEXT:  .LBB18_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB18_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB18_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB18_11
; RV64-NEXT:  .LBB18_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB18_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 40(sp)
; RV64-NEXT:    vsetivli zero, 4, e64, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %v = select i1 %c, <4 x double> %a, <4 x double> %b
  ret <4 x double> %v
}

define <4 x double> @selectcc_v4f64(double %a, double %b, <4 x double> %c, <4 x double> %d) {
; RV32-LABEL: selectcc_v4f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 96
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    feq.d a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB19_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    fsd ft0, 32(sp)
; RV32-NEXT:    beqz a0, .LBB19_4
; RV32-NEXT:  .LBB19_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v8, 3
; RV32-NEXT:    j .LBB19_5
; RV32-NEXT:  .LBB19_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 32(sp)
; RV32-NEXT:    bnez a0, .LBB19_2
; RV32-NEXT:  .LBB19_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v26, v10, 3
; RV32-NEXT:  .LBB19_5:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 56(sp)
; RV32-NEXT:    bnez a0, .LBB19_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v26, v10, 2
; RV32-NEXT:    j .LBB19_8
; RV32-NEXT:  .LBB19_7:
; RV32-NEXT:    vslidedown.vi v26, v8, 2
; RV32-NEXT:  .LBB19_8:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 48(sp)
; RV32-NEXT:    bnez a0, .LBB19_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v26, v10, 1
; RV32-NEXT:    j .LBB19_11
; RV32-NEXT:  .LBB19_10:
; RV32-NEXT:    vslidedown.vi v26, v8, 1
; RV32-NEXT:  .LBB19_11:
; RV32-NEXT:    vfmv.f.s ft0, v26
; RV32-NEXT:    fsd ft0, 40(sp)
; RV32-NEXT:    vsetivli zero, 4, e64, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -96
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v4f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -96
; RV64-NEXT:    .cfi_def_cfa_offset 96
; RV64-NEXT:    sd ra, 88(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 80(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 96
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    feq.d a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB19_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    fsd ft0, 32(sp)
; RV64-NEXT:    beqz a0, .LBB19_4
; RV64-NEXT:  .LBB19_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v8, 3
; RV64-NEXT:    j .LBB19_5
; RV64-NEXT:  .LBB19_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m2, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 32(sp)
; RV64-NEXT:    bnez a0, .LBB19_2
; RV64-NEXT:  .LBB19_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v26, v10, 3
; RV64-NEXT:  .LBB19_5:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 56(sp)
; RV64-NEXT:    bnez a0, .LBB19_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v26, v10, 2
; RV64-NEXT:    j .LBB19_8
; RV64-NEXT:  .LBB19_7:
; RV64-NEXT:    vslidedown.vi v26, v8, 2
; RV64-NEXT:  .LBB19_8:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 48(sp)
; RV64-NEXT:    bnez a0, .LBB19_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v26, v10, 1
; RV64-NEXT:    j .LBB19_11
; RV64-NEXT:  .LBB19_10:
; RV64-NEXT:    vslidedown.vi v26, v8, 1
; RV64-NEXT:  .LBB19_11:
; RV64-NEXT:    vfmv.f.s ft0, v26
; RV64-NEXT:    fsd ft0, 40(sp)
; RV64-NEXT:    vsetivli zero, 4, e64, m2, ta, mu
; RV64-NEXT:    addi a0, sp, 32
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -96
; RV64-NEXT:    ld s0, 80(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 88(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 96
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <4 x double> %c, <4 x double> %d
  ret <4 x double> %v
}

define <8 x double> @select_v8f64(i1 zeroext %c, <8 x double> %a, <8 x double> %b) {
; RV32-LABEL: select_v8f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -192
; RV32-NEXT:    .cfi_def_cfa_offset 192
; RV32-NEXT:    sw ra, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 192
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    bnez a0, .LBB20_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v12
; RV32-NEXT:    fsd ft0, 64(sp)
; RV32-NEXT:    beqz a0, .LBB20_4
; RV32-NEXT:  .LBB20_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v8, 7
; RV32-NEXT:    j .LBB20_5
; RV32-NEXT:  .LBB20_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 64(sp)
; RV32-NEXT:    bnez a0, .LBB20_2
; RV32-NEXT:  .LBB20_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v12, 7
; RV32-NEXT:  .LBB20_5:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 120(sp)
; RV32-NEXT:    bnez a0, .LBB20_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v28, v12, 6
; RV32-NEXT:    j .LBB20_8
; RV32-NEXT:  .LBB20_7:
; RV32-NEXT:    vslidedown.vi v28, v8, 6
; RV32-NEXT:  .LBB20_8:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 112(sp)
; RV32-NEXT:    bnez a0, .LBB20_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v28, v12, 5
; RV32-NEXT:    j .LBB20_11
; RV32-NEXT:  .LBB20_10:
; RV32-NEXT:    vslidedown.vi v28, v8, 5
; RV32-NEXT:  .LBB20_11:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 104(sp)
; RV32-NEXT:    bnez a0, .LBB20_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v28, v12, 4
; RV32-NEXT:    j .LBB20_14
; RV32-NEXT:  .LBB20_13:
; RV32-NEXT:    vslidedown.vi v28, v8, 4
; RV32-NEXT:  .LBB20_14:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 96(sp)
; RV32-NEXT:    bnez a0, .LBB20_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v28, v12, 3
; RV32-NEXT:    j .LBB20_17
; RV32-NEXT:  .LBB20_16:
; RV32-NEXT:    vslidedown.vi v28, v8, 3
; RV32-NEXT:  .LBB20_17:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 88(sp)
; RV32-NEXT:    bnez a0, .LBB20_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v28, v12, 2
; RV32-NEXT:    j .LBB20_20
; RV32-NEXT:  .LBB20_19:
; RV32-NEXT:    vslidedown.vi v28, v8, 2
; RV32-NEXT:  .LBB20_20:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 80(sp)
; RV32-NEXT:    bnez a0, .LBB20_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v28, v12, 1
; RV32-NEXT:    j .LBB20_23
; RV32-NEXT:  .LBB20_22:
; RV32-NEXT:    vslidedown.vi v28, v8, 1
; RV32-NEXT:  .LBB20_23:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 72(sp)
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -192
; RV32-NEXT:    lw s0, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 192
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v8f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -192
; RV64-NEXT:    .cfi_def_cfa_offset 192
; RV64-NEXT:    sd ra, 184(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 176(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 192
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    bnez a0, .LBB20_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v12
; RV64-NEXT:    fsd ft0, 64(sp)
; RV64-NEXT:    beqz a0, .LBB20_4
; RV64-NEXT:  .LBB20_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v8, 7
; RV64-NEXT:    j .LBB20_5
; RV64-NEXT:  .LBB20_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 64(sp)
; RV64-NEXT:    bnez a0, .LBB20_2
; RV64-NEXT:  .LBB20_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v12, 7
; RV64-NEXT:  .LBB20_5:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 120(sp)
; RV64-NEXT:    bnez a0, .LBB20_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v28, v12, 6
; RV64-NEXT:    j .LBB20_8
; RV64-NEXT:  .LBB20_7:
; RV64-NEXT:    vslidedown.vi v28, v8, 6
; RV64-NEXT:  .LBB20_8:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 112(sp)
; RV64-NEXT:    bnez a0, .LBB20_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v28, v12, 5
; RV64-NEXT:    j .LBB20_11
; RV64-NEXT:  .LBB20_10:
; RV64-NEXT:    vslidedown.vi v28, v8, 5
; RV64-NEXT:  .LBB20_11:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 104(sp)
; RV64-NEXT:    bnez a0, .LBB20_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v28, v12, 4
; RV64-NEXT:    j .LBB20_14
; RV64-NEXT:  .LBB20_13:
; RV64-NEXT:    vslidedown.vi v28, v8, 4
; RV64-NEXT:  .LBB20_14:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 96(sp)
; RV64-NEXT:    bnez a0, .LBB20_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v28, v12, 3
; RV64-NEXT:    j .LBB20_17
; RV64-NEXT:  .LBB20_16:
; RV64-NEXT:    vslidedown.vi v28, v8, 3
; RV64-NEXT:  .LBB20_17:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 88(sp)
; RV64-NEXT:    bnez a0, .LBB20_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v28, v12, 2
; RV64-NEXT:    j .LBB20_20
; RV64-NEXT:  .LBB20_19:
; RV64-NEXT:    vslidedown.vi v28, v8, 2
; RV64-NEXT:  .LBB20_20:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 80(sp)
; RV64-NEXT:    bnez a0, .LBB20_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v28, v12, 1
; RV64-NEXT:    j .LBB20_23
; RV64-NEXT:  .LBB20_22:
; RV64-NEXT:    vslidedown.vi v28, v8, 1
; RV64-NEXT:  .LBB20_23:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 72(sp)
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    addi a0, sp, 64
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -192
; RV64-NEXT:    ld s0, 176(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 184(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 192
; RV64-NEXT:    ret
  %v = select i1 %c, <8 x double> %a, <8 x double> %b
  ret <8 x double> %v
}

define <8 x double> @selectcc_v8f64(double %a, double %b, <8 x double> %c, <8 x double> %d) {
; RV32-LABEL: selectcc_v8f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -192
; RV32-NEXT:    .cfi_def_cfa_offset 192
; RV32-NEXT:    sw ra, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 192
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    feq.d a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB21_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v12
; RV32-NEXT:    fsd ft0, 64(sp)
; RV32-NEXT:    beqz a0, .LBB21_4
; RV32-NEXT:  .LBB21_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v8, 7
; RV32-NEXT:    j .LBB21_5
; RV32-NEXT:  .LBB21_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 64(sp)
; RV32-NEXT:    bnez a0, .LBB21_2
; RV32-NEXT:  .LBB21_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v28, v12, 7
; RV32-NEXT:  .LBB21_5:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 120(sp)
; RV32-NEXT:    bnez a0, .LBB21_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v28, v12, 6
; RV32-NEXT:    j .LBB21_8
; RV32-NEXT:  .LBB21_7:
; RV32-NEXT:    vslidedown.vi v28, v8, 6
; RV32-NEXT:  .LBB21_8:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 112(sp)
; RV32-NEXT:    bnez a0, .LBB21_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v28, v12, 5
; RV32-NEXT:    j .LBB21_11
; RV32-NEXT:  .LBB21_10:
; RV32-NEXT:    vslidedown.vi v28, v8, 5
; RV32-NEXT:  .LBB21_11:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 104(sp)
; RV32-NEXT:    bnez a0, .LBB21_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v28, v12, 4
; RV32-NEXT:    j .LBB21_14
; RV32-NEXT:  .LBB21_13:
; RV32-NEXT:    vslidedown.vi v28, v8, 4
; RV32-NEXT:  .LBB21_14:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 96(sp)
; RV32-NEXT:    bnez a0, .LBB21_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v28, v12, 3
; RV32-NEXT:    j .LBB21_17
; RV32-NEXT:  .LBB21_16:
; RV32-NEXT:    vslidedown.vi v28, v8, 3
; RV32-NEXT:  .LBB21_17:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 88(sp)
; RV32-NEXT:    bnez a0, .LBB21_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v28, v12, 2
; RV32-NEXT:    j .LBB21_20
; RV32-NEXT:  .LBB21_19:
; RV32-NEXT:    vslidedown.vi v28, v8, 2
; RV32-NEXT:  .LBB21_20:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 80(sp)
; RV32-NEXT:    bnez a0, .LBB21_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v28, v12, 1
; RV32-NEXT:    j .LBB21_23
; RV32-NEXT:  .LBB21_22:
; RV32-NEXT:    vslidedown.vi v28, v8, 1
; RV32-NEXT:  .LBB21_23:
; RV32-NEXT:    vfmv.f.s ft0, v28
; RV32-NEXT:    fsd ft0, 72(sp)
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -192
; RV32-NEXT:    lw s0, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 192
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v8f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -192
; RV64-NEXT:    .cfi_def_cfa_offset 192
; RV64-NEXT:    sd ra, 184(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 176(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 192
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    feq.d a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB21_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v12
; RV64-NEXT:    fsd ft0, 64(sp)
; RV64-NEXT:    beqz a0, .LBB21_4
; RV64-NEXT:  .LBB21_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v8, 7
; RV64-NEXT:    j .LBB21_5
; RV64-NEXT:  .LBB21_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m4, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 64(sp)
; RV64-NEXT:    bnez a0, .LBB21_2
; RV64-NEXT:  .LBB21_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v28, v12, 7
; RV64-NEXT:  .LBB21_5:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 120(sp)
; RV64-NEXT:    bnez a0, .LBB21_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v28, v12, 6
; RV64-NEXT:    j .LBB21_8
; RV64-NEXT:  .LBB21_7:
; RV64-NEXT:    vslidedown.vi v28, v8, 6
; RV64-NEXT:  .LBB21_8:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 112(sp)
; RV64-NEXT:    bnez a0, .LBB21_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v28, v12, 5
; RV64-NEXT:    j .LBB21_11
; RV64-NEXT:  .LBB21_10:
; RV64-NEXT:    vslidedown.vi v28, v8, 5
; RV64-NEXT:  .LBB21_11:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 104(sp)
; RV64-NEXT:    bnez a0, .LBB21_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v28, v12, 4
; RV64-NEXT:    j .LBB21_14
; RV64-NEXT:  .LBB21_13:
; RV64-NEXT:    vslidedown.vi v28, v8, 4
; RV64-NEXT:  .LBB21_14:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 96(sp)
; RV64-NEXT:    bnez a0, .LBB21_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v28, v12, 3
; RV64-NEXT:    j .LBB21_17
; RV64-NEXT:  .LBB21_16:
; RV64-NEXT:    vslidedown.vi v28, v8, 3
; RV64-NEXT:  .LBB21_17:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 88(sp)
; RV64-NEXT:    bnez a0, .LBB21_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v28, v12, 2
; RV64-NEXT:    j .LBB21_20
; RV64-NEXT:  .LBB21_19:
; RV64-NEXT:    vslidedown.vi v28, v8, 2
; RV64-NEXT:  .LBB21_20:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 80(sp)
; RV64-NEXT:    bnez a0, .LBB21_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v28, v12, 1
; RV64-NEXT:    j .LBB21_23
; RV64-NEXT:  .LBB21_22:
; RV64-NEXT:    vslidedown.vi v28, v8, 1
; RV64-NEXT:  .LBB21_23:
; RV64-NEXT:    vfmv.f.s ft0, v28
; RV64-NEXT:    fsd ft0, 72(sp)
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    addi a0, sp, 64
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -192
; RV64-NEXT:    ld s0, 176(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 184(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 192
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <8 x double> %c, <8 x double> %d
  ret <8 x double> %v
}

define <16 x double> @select_v16f64(i1 zeroext %c, <16 x double> %a, <16 x double> %b) {
; RV32-LABEL: select_v16f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -384
; RV32-NEXT:    .cfi_def_cfa_offset 384
; RV32-NEXT:    sw ra, 380(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 376(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 384
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -128
; RV32-NEXT:    bnez a0, .LBB22_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v16
; RV32-NEXT:    fsd ft0, 128(sp)
; RV32-NEXT:    beqz a0, .LBB22_4
; RV32-NEXT:  .LBB22_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV32-NEXT:    vslidedown.vi v24, v8, 15
; RV32-NEXT:    j .LBB22_5
; RV32-NEXT:  .LBB22_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 128(sp)
; RV32-NEXT:    bnez a0, .LBB22_2
; RV32-NEXT:  .LBB22_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV32-NEXT:    vslidedown.vi v24, v16, 15
; RV32-NEXT:  .LBB22_5:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 248(sp)
; RV32-NEXT:    bnez a0, .LBB22_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v24, v16, 14
; RV32-NEXT:    j .LBB22_8
; RV32-NEXT:  .LBB22_7:
; RV32-NEXT:    vslidedown.vi v24, v8, 14
; RV32-NEXT:  .LBB22_8:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 240(sp)
; RV32-NEXT:    bnez a0, .LBB22_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v24, v16, 13
; RV32-NEXT:    j .LBB22_11
; RV32-NEXT:  .LBB22_10:
; RV32-NEXT:    vslidedown.vi v24, v8, 13
; RV32-NEXT:  .LBB22_11:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 232(sp)
; RV32-NEXT:    bnez a0, .LBB22_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v24, v16, 12
; RV32-NEXT:    j .LBB22_14
; RV32-NEXT:  .LBB22_13:
; RV32-NEXT:    vslidedown.vi v24, v8, 12
; RV32-NEXT:  .LBB22_14:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 224(sp)
; RV32-NEXT:    bnez a0, .LBB22_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v24, v16, 11
; RV32-NEXT:    j .LBB22_17
; RV32-NEXT:  .LBB22_16:
; RV32-NEXT:    vslidedown.vi v24, v8, 11
; RV32-NEXT:  .LBB22_17:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 216(sp)
; RV32-NEXT:    bnez a0, .LBB22_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v24, v16, 10
; RV32-NEXT:    j .LBB22_20
; RV32-NEXT:  .LBB22_19:
; RV32-NEXT:    vslidedown.vi v24, v8, 10
; RV32-NEXT:  .LBB22_20:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 208(sp)
; RV32-NEXT:    bnez a0, .LBB22_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v24, v16, 9
; RV32-NEXT:    j .LBB22_23
; RV32-NEXT:  .LBB22_22:
; RV32-NEXT:    vslidedown.vi v24, v8, 9
; RV32-NEXT:  .LBB22_23:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 200(sp)
; RV32-NEXT:    bnez a0, .LBB22_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v24, v16, 8
; RV32-NEXT:    j .LBB22_26
; RV32-NEXT:  .LBB22_25:
; RV32-NEXT:    vslidedown.vi v24, v8, 8
; RV32-NEXT:  .LBB22_26:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 192(sp)
; RV32-NEXT:    bnez a0, .LBB22_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v24, v16, 7
; RV32-NEXT:    j .LBB22_29
; RV32-NEXT:  .LBB22_28:
; RV32-NEXT:    vslidedown.vi v24, v8, 7
; RV32-NEXT:  .LBB22_29:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 184(sp)
; RV32-NEXT:    bnez a0, .LBB22_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v24, v16, 6
; RV32-NEXT:    j .LBB22_32
; RV32-NEXT:  .LBB22_31:
; RV32-NEXT:    vslidedown.vi v24, v8, 6
; RV32-NEXT:  .LBB22_32:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 176(sp)
; RV32-NEXT:    bnez a0, .LBB22_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v24, v16, 5
; RV32-NEXT:    j .LBB22_35
; RV32-NEXT:  .LBB22_34:
; RV32-NEXT:    vslidedown.vi v24, v8, 5
; RV32-NEXT:  .LBB22_35:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 168(sp)
; RV32-NEXT:    bnez a0, .LBB22_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v24, v16, 4
; RV32-NEXT:    j .LBB22_38
; RV32-NEXT:  .LBB22_37:
; RV32-NEXT:    vslidedown.vi v24, v8, 4
; RV32-NEXT:  .LBB22_38:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 160(sp)
; RV32-NEXT:    bnez a0, .LBB22_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v24, v16, 3
; RV32-NEXT:    j .LBB22_41
; RV32-NEXT:  .LBB22_40:
; RV32-NEXT:    vslidedown.vi v24, v8, 3
; RV32-NEXT:  .LBB22_41:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 152(sp)
; RV32-NEXT:    bnez a0, .LBB22_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v24, v16, 2
; RV32-NEXT:    j .LBB22_44
; RV32-NEXT:  .LBB22_43:
; RV32-NEXT:    vslidedown.vi v24, v8, 2
; RV32-NEXT:  .LBB22_44:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 144(sp)
; RV32-NEXT:    bnez a0, .LBB22_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v8, v16, 1
; RV32-NEXT:    j .LBB22_47
; RV32-NEXT:  .LBB22_46:
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:  .LBB22_47:
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 136(sp)
; RV32-NEXT:    vsetivli zero, 16, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 128
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -384
; RV32-NEXT:    lw s0, 376(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 380(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 384
; RV32-NEXT:    ret
;
; RV64-LABEL: select_v16f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -384
; RV64-NEXT:    .cfi_def_cfa_offset 384
; RV64-NEXT:    sd ra, 376(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 368(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 384
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -128
; RV64-NEXT:    bnez a0, .LBB22_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v16
; RV64-NEXT:    fsd ft0, 128(sp)
; RV64-NEXT:    beqz a0, .LBB22_4
; RV64-NEXT:  .LBB22_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV64-NEXT:    vslidedown.vi v24, v8, 15
; RV64-NEXT:    j .LBB22_5
; RV64-NEXT:  .LBB22_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 128(sp)
; RV64-NEXT:    bnez a0, .LBB22_2
; RV64-NEXT:  .LBB22_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV64-NEXT:    vslidedown.vi v24, v16, 15
; RV64-NEXT:  .LBB22_5:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 248(sp)
; RV64-NEXT:    bnez a0, .LBB22_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v24, v16, 14
; RV64-NEXT:    j .LBB22_8
; RV64-NEXT:  .LBB22_7:
; RV64-NEXT:    vslidedown.vi v24, v8, 14
; RV64-NEXT:  .LBB22_8:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 240(sp)
; RV64-NEXT:    bnez a0, .LBB22_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v24, v16, 13
; RV64-NEXT:    j .LBB22_11
; RV64-NEXT:  .LBB22_10:
; RV64-NEXT:    vslidedown.vi v24, v8, 13
; RV64-NEXT:  .LBB22_11:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 232(sp)
; RV64-NEXT:    bnez a0, .LBB22_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v24, v16, 12
; RV64-NEXT:    j .LBB22_14
; RV64-NEXT:  .LBB22_13:
; RV64-NEXT:    vslidedown.vi v24, v8, 12
; RV64-NEXT:  .LBB22_14:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 224(sp)
; RV64-NEXT:    bnez a0, .LBB22_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v24, v16, 11
; RV64-NEXT:    j .LBB22_17
; RV64-NEXT:  .LBB22_16:
; RV64-NEXT:    vslidedown.vi v24, v8, 11
; RV64-NEXT:  .LBB22_17:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 216(sp)
; RV64-NEXT:    bnez a0, .LBB22_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v24, v16, 10
; RV64-NEXT:    j .LBB22_20
; RV64-NEXT:  .LBB22_19:
; RV64-NEXT:    vslidedown.vi v24, v8, 10
; RV64-NEXT:  .LBB22_20:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 208(sp)
; RV64-NEXT:    bnez a0, .LBB22_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v24, v16, 9
; RV64-NEXT:    j .LBB22_23
; RV64-NEXT:  .LBB22_22:
; RV64-NEXT:    vslidedown.vi v24, v8, 9
; RV64-NEXT:  .LBB22_23:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 200(sp)
; RV64-NEXT:    bnez a0, .LBB22_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v24, v16, 8
; RV64-NEXT:    j .LBB22_26
; RV64-NEXT:  .LBB22_25:
; RV64-NEXT:    vslidedown.vi v24, v8, 8
; RV64-NEXT:  .LBB22_26:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 192(sp)
; RV64-NEXT:    bnez a0, .LBB22_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v24, v16, 7
; RV64-NEXT:    j .LBB22_29
; RV64-NEXT:  .LBB22_28:
; RV64-NEXT:    vslidedown.vi v24, v8, 7
; RV64-NEXT:  .LBB22_29:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 184(sp)
; RV64-NEXT:    bnez a0, .LBB22_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v24, v16, 6
; RV64-NEXT:    j .LBB22_32
; RV64-NEXT:  .LBB22_31:
; RV64-NEXT:    vslidedown.vi v24, v8, 6
; RV64-NEXT:  .LBB22_32:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 176(sp)
; RV64-NEXT:    bnez a0, .LBB22_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v24, v16, 5
; RV64-NEXT:    j .LBB22_35
; RV64-NEXT:  .LBB22_34:
; RV64-NEXT:    vslidedown.vi v24, v8, 5
; RV64-NEXT:  .LBB22_35:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 168(sp)
; RV64-NEXT:    bnez a0, .LBB22_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v24, v16, 4
; RV64-NEXT:    j .LBB22_38
; RV64-NEXT:  .LBB22_37:
; RV64-NEXT:    vslidedown.vi v24, v8, 4
; RV64-NEXT:  .LBB22_38:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 160(sp)
; RV64-NEXT:    bnez a0, .LBB22_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v24, v16, 3
; RV64-NEXT:    j .LBB22_41
; RV64-NEXT:  .LBB22_40:
; RV64-NEXT:    vslidedown.vi v24, v8, 3
; RV64-NEXT:  .LBB22_41:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 152(sp)
; RV64-NEXT:    bnez a0, .LBB22_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v24, v16, 2
; RV64-NEXT:    j .LBB22_44
; RV64-NEXT:  .LBB22_43:
; RV64-NEXT:    vslidedown.vi v24, v8, 2
; RV64-NEXT:  .LBB22_44:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 144(sp)
; RV64-NEXT:    bnez a0, .LBB22_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v8, v16, 1
; RV64-NEXT:    j .LBB22_47
; RV64-NEXT:  .LBB22_46:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:  .LBB22_47:
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 136(sp)
; RV64-NEXT:    vsetivli zero, 16, e64, m8, ta, mu
; RV64-NEXT:    addi a0, sp, 128
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -384
; RV64-NEXT:    ld s0, 368(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 376(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 384
; RV64-NEXT:    ret
  %v = select i1 %c, <16 x double> %a, <16 x double> %b
  ret <16 x double> %v
}

define <16 x double> @selectcc_v16f64(double %a, double %b, <16 x double> %c, <16 x double> %d) {
; RV32-LABEL: selectcc_v16f64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -384
; RV32-NEXT:    .cfi_def_cfa_offset 384
; RV32-NEXT:    sw ra, 380(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 376(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 384
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -128
; RV32-NEXT:    feq.d a0, fa0, fa1
; RV32-NEXT:    bnez a0, .LBB23_3
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v16
; RV32-NEXT:    fsd ft0, 128(sp)
; RV32-NEXT:    beqz a0, .LBB23_4
; RV32-NEXT:  .LBB23_2:
; RV32-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV32-NEXT:    vslidedown.vi v24, v8, 15
; RV32-NEXT:    j .LBB23_5
; RV32-NEXT:  .LBB23_3:
; RV32-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 128(sp)
; RV32-NEXT:    bnez a0, .LBB23_2
; RV32-NEXT:  .LBB23_4:
; RV32-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV32-NEXT:    vslidedown.vi v24, v16, 15
; RV32-NEXT:  .LBB23_5:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 248(sp)
; RV32-NEXT:    bnez a0, .LBB23_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    vslidedown.vi v24, v16, 14
; RV32-NEXT:    j .LBB23_8
; RV32-NEXT:  .LBB23_7:
; RV32-NEXT:    vslidedown.vi v24, v8, 14
; RV32-NEXT:  .LBB23_8:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 240(sp)
; RV32-NEXT:    bnez a0, .LBB23_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    vslidedown.vi v24, v16, 13
; RV32-NEXT:    j .LBB23_11
; RV32-NEXT:  .LBB23_10:
; RV32-NEXT:    vslidedown.vi v24, v8, 13
; RV32-NEXT:  .LBB23_11:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 232(sp)
; RV32-NEXT:    bnez a0, .LBB23_13
; RV32-NEXT:  # %bb.12:
; RV32-NEXT:    vslidedown.vi v24, v16, 12
; RV32-NEXT:    j .LBB23_14
; RV32-NEXT:  .LBB23_13:
; RV32-NEXT:    vslidedown.vi v24, v8, 12
; RV32-NEXT:  .LBB23_14:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 224(sp)
; RV32-NEXT:    bnez a0, .LBB23_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    vslidedown.vi v24, v16, 11
; RV32-NEXT:    j .LBB23_17
; RV32-NEXT:  .LBB23_16:
; RV32-NEXT:    vslidedown.vi v24, v8, 11
; RV32-NEXT:  .LBB23_17:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 216(sp)
; RV32-NEXT:    bnez a0, .LBB23_19
; RV32-NEXT:  # %bb.18:
; RV32-NEXT:    vslidedown.vi v24, v16, 10
; RV32-NEXT:    j .LBB23_20
; RV32-NEXT:  .LBB23_19:
; RV32-NEXT:    vslidedown.vi v24, v8, 10
; RV32-NEXT:  .LBB23_20:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 208(sp)
; RV32-NEXT:    bnez a0, .LBB23_22
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    vslidedown.vi v24, v16, 9
; RV32-NEXT:    j .LBB23_23
; RV32-NEXT:  .LBB23_22:
; RV32-NEXT:    vslidedown.vi v24, v8, 9
; RV32-NEXT:  .LBB23_23:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 200(sp)
; RV32-NEXT:    bnez a0, .LBB23_25
; RV32-NEXT:  # %bb.24:
; RV32-NEXT:    vslidedown.vi v24, v16, 8
; RV32-NEXT:    j .LBB23_26
; RV32-NEXT:  .LBB23_25:
; RV32-NEXT:    vslidedown.vi v24, v8, 8
; RV32-NEXT:  .LBB23_26:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 192(sp)
; RV32-NEXT:    bnez a0, .LBB23_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    vslidedown.vi v24, v16, 7
; RV32-NEXT:    j .LBB23_29
; RV32-NEXT:  .LBB23_28:
; RV32-NEXT:    vslidedown.vi v24, v8, 7
; RV32-NEXT:  .LBB23_29:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 184(sp)
; RV32-NEXT:    bnez a0, .LBB23_31
; RV32-NEXT:  # %bb.30:
; RV32-NEXT:    vslidedown.vi v24, v16, 6
; RV32-NEXT:    j .LBB23_32
; RV32-NEXT:  .LBB23_31:
; RV32-NEXT:    vslidedown.vi v24, v8, 6
; RV32-NEXT:  .LBB23_32:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 176(sp)
; RV32-NEXT:    bnez a0, .LBB23_34
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    vslidedown.vi v24, v16, 5
; RV32-NEXT:    j .LBB23_35
; RV32-NEXT:  .LBB23_34:
; RV32-NEXT:    vslidedown.vi v24, v8, 5
; RV32-NEXT:  .LBB23_35:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 168(sp)
; RV32-NEXT:    bnez a0, .LBB23_37
; RV32-NEXT:  # %bb.36:
; RV32-NEXT:    vslidedown.vi v24, v16, 4
; RV32-NEXT:    j .LBB23_38
; RV32-NEXT:  .LBB23_37:
; RV32-NEXT:    vslidedown.vi v24, v8, 4
; RV32-NEXT:  .LBB23_38:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 160(sp)
; RV32-NEXT:    bnez a0, .LBB23_40
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    vslidedown.vi v24, v16, 3
; RV32-NEXT:    j .LBB23_41
; RV32-NEXT:  .LBB23_40:
; RV32-NEXT:    vslidedown.vi v24, v8, 3
; RV32-NEXT:  .LBB23_41:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 152(sp)
; RV32-NEXT:    bnez a0, .LBB23_43
; RV32-NEXT:  # %bb.42:
; RV32-NEXT:    vslidedown.vi v24, v16, 2
; RV32-NEXT:    j .LBB23_44
; RV32-NEXT:  .LBB23_43:
; RV32-NEXT:    vslidedown.vi v24, v8, 2
; RV32-NEXT:  .LBB23_44:
; RV32-NEXT:    vfmv.f.s ft0, v24
; RV32-NEXT:    fsd ft0, 144(sp)
; RV32-NEXT:    bnez a0, .LBB23_46
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    vslidedown.vi v8, v16, 1
; RV32-NEXT:    j .LBB23_47
; RV32-NEXT:  .LBB23_46:
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:  .LBB23_47:
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fsd ft0, 136(sp)
; RV32-NEXT:    vsetivli zero, 16, e64, m8, ta, mu
; RV32-NEXT:    addi a0, sp, 128
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    addi sp, s0, -384
; RV32-NEXT:    lw s0, 376(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw ra, 380(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 384
; RV32-NEXT:    ret
;
; RV64-LABEL: selectcc_v16f64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -384
; RV64-NEXT:    .cfi_def_cfa_offset 384
; RV64-NEXT:    sd ra, 376(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 368(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 384
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -128
; RV64-NEXT:    feq.d a0, fa0, fa1
; RV64-NEXT:    bnez a0, .LBB23_3
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v16
; RV64-NEXT:    fsd ft0, 128(sp)
; RV64-NEXT:    beqz a0, .LBB23_4
; RV64-NEXT:  .LBB23_2:
; RV64-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV64-NEXT:    vslidedown.vi v24, v8, 15
; RV64-NEXT:    j .LBB23_5
; RV64-NEXT:  .LBB23_3:
; RV64-NEXT:    vsetvli zero, zero, e64, m8, ta, mu
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 128(sp)
; RV64-NEXT:    bnez a0, .LBB23_2
; RV64-NEXT:  .LBB23_4:
; RV64-NEXT:    vsetivli zero, 1, e64, m8, ta, mu
; RV64-NEXT:    vslidedown.vi v24, v16, 15
; RV64-NEXT:  .LBB23_5:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 248(sp)
; RV64-NEXT:    bnez a0, .LBB23_7
; RV64-NEXT:  # %bb.6:
; RV64-NEXT:    vslidedown.vi v24, v16, 14
; RV64-NEXT:    j .LBB23_8
; RV64-NEXT:  .LBB23_7:
; RV64-NEXT:    vslidedown.vi v24, v8, 14
; RV64-NEXT:  .LBB23_8:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 240(sp)
; RV64-NEXT:    bnez a0, .LBB23_10
; RV64-NEXT:  # %bb.9:
; RV64-NEXT:    vslidedown.vi v24, v16, 13
; RV64-NEXT:    j .LBB23_11
; RV64-NEXT:  .LBB23_10:
; RV64-NEXT:    vslidedown.vi v24, v8, 13
; RV64-NEXT:  .LBB23_11:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 232(sp)
; RV64-NEXT:    bnez a0, .LBB23_13
; RV64-NEXT:  # %bb.12:
; RV64-NEXT:    vslidedown.vi v24, v16, 12
; RV64-NEXT:    j .LBB23_14
; RV64-NEXT:  .LBB23_13:
; RV64-NEXT:    vslidedown.vi v24, v8, 12
; RV64-NEXT:  .LBB23_14:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 224(sp)
; RV64-NEXT:    bnez a0, .LBB23_16
; RV64-NEXT:  # %bb.15:
; RV64-NEXT:    vslidedown.vi v24, v16, 11
; RV64-NEXT:    j .LBB23_17
; RV64-NEXT:  .LBB23_16:
; RV64-NEXT:    vslidedown.vi v24, v8, 11
; RV64-NEXT:  .LBB23_17:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 216(sp)
; RV64-NEXT:    bnez a0, .LBB23_19
; RV64-NEXT:  # %bb.18:
; RV64-NEXT:    vslidedown.vi v24, v16, 10
; RV64-NEXT:    j .LBB23_20
; RV64-NEXT:  .LBB23_19:
; RV64-NEXT:    vslidedown.vi v24, v8, 10
; RV64-NEXT:  .LBB23_20:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 208(sp)
; RV64-NEXT:    bnez a0, .LBB23_22
; RV64-NEXT:  # %bb.21:
; RV64-NEXT:    vslidedown.vi v24, v16, 9
; RV64-NEXT:    j .LBB23_23
; RV64-NEXT:  .LBB23_22:
; RV64-NEXT:    vslidedown.vi v24, v8, 9
; RV64-NEXT:  .LBB23_23:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 200(sp)
; RV64-NEXT:    bnez a0, .LBB23_25
; RV64-NEXT:  # %bb.24:
; RV64-NEXT:    vslidedown.vi v24, v16, 8
; RV64-NEXT:    j .LBB23_26
; RV64-NEXT:  .LBB23_25:
; RV64-NEXT:    vslidedown.vi v24, v8, 8
; RV64-NEXT:  .LBB23_26:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 192(sp)
; RV64-NEXT:    bnez a0, .LBB23_28
; RV64-NEXT:  # %bb.27:
; RV64-NEXT:    vslidedown.vi v24, v16, 7
; RV64-NEXT:    j .LBB23_29
; RV64-NEXT:  .LBB23_28:
; RV64-NEXT:    vslidedown.vi v24, v8, 7
; RV64-NEXT:  .LBB23_29:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 184(sp)
; RV64-NEXT:    bnez a0, .LBB23_31
; RV64-NEXT:  # %bb.30:
; RV64-NEXT:    vslidedown.vi v24, v16, 6
; RV64-NEXT:    j .LBB23_32
; RV64-NEXT:  .LBB23_31:
; RV64-NEXT:    vslidedown.vi v24, v8, 6
; RV64-NEXT:  .LBB23_32:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 176(sp)
; RV64-NEXT:    bnez a0, .LBB23_34
; RV64-NEXT:  # %bb.33:
; RV64-NEXT:    vslidedown.vi v24, v16, 5
; RV64-NEXT:    j .LBB23_35
; RV64-NEXT:  .LBB23_34:
; RV64-NEXT:    vslidedown.vi v24, v8, 5
; RV64-NEXT:  .LBB23_35:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 168(sp)
; RV64-NEXT:    bnez a0, .LBB23_37
; RV64-NEXT:  # %bb.36:
; RV64-NEXT:    vslidedown.vi v24, v16, 4
; RV64-NEXT:    j .LBB23_38
; RV64-NEXT:  .LBB23_37:
; RV64-NEXT:    vslidedown.vi v24, v8, 4
; RV64-NEXT:  .LBB23_38:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 160(sp)
; RV64-NEXT:    bnez a0, .LBB23_40
; RV64-NEXT:  # %bb.39:
; RV64-NEXT:    vslidedown.vi v24, v16, 3
; RV64-NEXT:    j .LBB23_41
; RV64-NEXT:  .LBB23_40:
; RV64-NEXT:    vslidedown.vi v24, v8, 3
; RV64-NEXT:  .LBB23_41:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 152(sp)
; RV64-NEXT:    bnez a0, .LBB23_43
; RV64-NEXT:  # %bb.42:
; RV64-NEXT:    vslidedown.vi v24, v16, 2
; RV64-NEXT:    j .LBB23_44
; RV64-NEXT:  .LBB23_43:
; RV64-NEXT:    vslidedown.vi v24, v8, 2
; RV64-NEXT:  .LBB23_44:
; RV64-NEXT:    vfmv.f.s ft0, v24
; RV64-NEXT:    fsd ft0, 144(sp)
; RV64-NEXT:    bnez a0, .LBB23_46
; RV64-NEXT:  # %bb.45:
; RV64-NEXT:    vslidedown.vi v8, v16, 1
; RV64-NEXT:    j .LBB23_47
; RV64-NEXT:  .LBB23_46:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:  .LBB23_47:
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    fsd ft0, 136(sp)
; RV64-NEXT:    vsetivli zero, 16, e64, m8, ta, mu
; RV64-NEXT:    addi a0, sp, 128
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    addi sp, s0, -384
; RV64-NEXT:    ld s0, 368(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld ra, 376(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 384
; RV64-NEXT:    ret
  %cmp = fcmp oeq double %a, %b
  %v = select i1 %cmp, <16 x double> %c, <16 x double> %d
  ret <16 x double> %v
}
