; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=riscv64 -mcpu=spacemit-x60 -verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=DEFAULT
; RUN: llc -mtriple=riscv64 -mcpu=spacemit-x60 -misched-prera-direction=bottomup \
; RUN:   -riscv-enable-vtype-sched-heuristic -verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=VTYPE-SCHED-BOTTOMUP
; RUN: llc -mtriple=riscv64 -mcpu=spacemit-x60 -misched-prera-direction=topdown \
; RUN:   -riscv-enable-vtype-sched-heuristic -verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=VTYPE-SCHED-TOPDOWN
; RUN: llc -mtriple=riscv64 -mcpu=spacemit-x60 -misched-prera-direction=bidirectional \
; RUN:   -riscv-enable-vtype-sched-heuristic -verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=VTYPE-SCHED-BIDIRECTIONAL

define void @test(i16 %0, i16 %1, i16 %2, i16 %3, i16 %4, i16 %5, i16 %6, ptr %7, ptr %8, ptr %9, ptr %10, ptr %11, i32 %12) {
; DEFAULT-LABEL: test:
; DEFAULT:       # %bb.0: # %entry
; DEFAULT-NEXT:    ld a6, 0(sp)
; DEFAULT-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; DEFAULT-NEXT:    vle8.v v9, (a7)
; DEFAULT-NEXT:    ld a7, 8(sp)
; DEFAULT-NEXT:    vle8.v v8, (a6)
; DEFAULT-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; DEFAULT-NEXT:    vslidedown.vi v10, v9, 1
; DEFAULT-NEXT:    ld a6, 16(sp)
; DEFAULT-NEXT:    vslidedown.vi v11, v8, 1
; DEFAULT-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; DEFAULT-NEXT:    vle8.v v12, (a7)
; DEFAULT-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; DEFAULT-NEXT:    vslidedown.vi v13, v12, 1
; DEFAULT-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; DEFAULT-NEXT:    vle8.v v14, (a6)
; DEFAULT-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; DEFAULT-NEXT:    vslidedown.vi v15, v14, 1
; DEFAULT-NEXT:    vslidedown.vi v16, v9, 2
; DEFAULT-NEXT:    vslidedown.vi v17, v8, 2
; DEFAULT-NEXT:    vslidedown.vi v18, v12, 2
; DEFAULT-NEXT:    vslidedown.vi v19, v14, 2
; DEFAULT-NEXT:    vslidedown.vi v20, v9, 3
; DEFAULT-NEXT:    vslidedown.vi v21, v8, 3
; DEFAULT-NEXT:    vslidedown.vi v22, v12, 3
; DEFAULT-NEXT:    vslidedown.vi v23, v14, 3
; DEFAULT-NEXT:    vslidedown.vi v24, v9, 4
; DEFAULT-NEXT:    vslidedown.vi v25, v8, 4
; DEFAULT-NEXT:    vslidedown.vi v26, v12, 4
; DEFAULT-NEXT:    vslidedown.vi v27, v14, 4
; DEFAULT-NEXT:    vslidedown.vi v28, v9, 5
; DEFAULT-NEXT:    vslidedown.vi v29, v8, 5
; DEFAULT-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; DEFAULT-NEXT:    vmv.v.i v30, 0
; DEFAULT-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; DEFAULT-NEXT:    vslidedown.vi v31, v12, 5
; DEFAULT-NEXT:    vmv1r.v v7, v30
; DEFAULT-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; DEFAULT-NEXT:    vwmaccsu.vx v7, a0, v9
; DEFAULT-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; DEFAULT-NEXT:    vslidedown.vi v9, v14, 5
; DEFAULT-NEXT:    vmv1r.v v6, v30
; DEFAULT-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; DEFAULT-NEXT:    vwmaccsu.vx v6, a0, v8
; DEFAULT-NEXT:    vwmaccsu.vx v7, a1, v10
; DEFAULT-NEXT:    vmv1r.v v8, v30
; DEFAULT-NEXT:    vwmaccsu.vx v8, a0, v12
; DEFAULT-NEXT:    vwmaccsu.vx v6, a1, v11
; DEFAULT-NEXT:    vwmaccsu.vx v7, a2, v16
; DEFAULT-NEXT:    vwmaccsu.vx v30, a0, v14
; DEFAULT-NEXT:    vwmaccsu.vx v8, a1, v13
; DEFAULT-NEXT:    vwmaccsu.vx v6, a2, v17
; DEFAULT-NEXT:    vwmaccsu.vx v7, a3, v20
; DEFAULT-NEXT:    vwmaccsu.vx v30, a1, v15
; DEFAULT-NEXT:    vwmaccsu.vx v8, a2, v18
; DEFAULT-NEXT:    vwmaccsu.vx v6, a3, v21
; DEFAULT-NEXT:    vwmaccsu.vx v7, a4, v24
; DEFAULT-NEXT:    vwmaccsu.vx v30, a2, v19
; DEFAULT-NEXT:    vwmaccsu.vx v8, a3, v22
; DEFAULT-NEXT:    vwmaccsu.vx v6, a4, v25
; DEFAULT-NEXT:    vwmaccsu.vx v7, a5, v28
; DEFAULT-NEXT:    vwmaccsu.vx v30, a3, v23
; DEFAULT-NEXT:    vwmaccsu.vx v8, a4, v26
; DEFAULT-NEXT:    vwmaccsu.vx v6, a5, v29
; DEFAULT-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; DEFAULT-NEXT:    vmax.vx v10, v7, zero
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; DEFAULT-NEXT:    vwmaccsu.vx v30, a4, v27
; DEFAULT-NEXT:    vwmaccsu.vx v8, a5, v31
; DEFAULT-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; DEFAULT-NEXT:    vmax.vx v11, v6, zero
; DEFAULT-NEXT:    csrwi vxrm, 0
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; DEFAULT-NEXT:    vnclipu.wi v10, v10, 6
; DEFAULT-NEXT:    vwmaccsu.vx v30, a5, v9
; DEFAULT-NEXT:    ld a0, 24(sp)
; DEFAULT-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; DEFAULT-NEXT:    vmax.vx v8, v8, zero
; DEFAULT-NEXT:    lw a1, 32(sp)
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; DEFAULT-NEXT:    vnclipu.wi v9, v11, 6
; DEFAULT-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; DEFAULT-NEXT:    vmax.vx v11, v30, zero
; DEFAULT-NEXT:    vse8.v v10, (a0)
; DEFAULT-NEXT:    add a2, a0, a1
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; DEFAULT-NEXT:    vnclipu.wi v8, v8, 6
; DEFAULT-NEXT:    vse8.v v9, (a2)
; DEFAULT-NEXT:    sh1add a2, a1, a0
; DEFAULT-NEXT:    vnclipu.wi v9, v11, 6
; DEFAULT-NEXT:    sh1add a1, a1, a1
; DEFAULT-NEXT:    vse8.v v8, (a2)
; DEFAULT-NEXT:    add a0, a0, a1
; DEFAULT-NEXT:    vse8.v v9, (a0)
; DEFAULT-NEXT:    ret
;
; VTYPE-SCHED-BOTTOMUP-LABEL: test:
; VTYPE-SCHED-BOTTOMUP:       # %bb.0: # %entry
; VTYPE-SCHED-BOTTOMUP-NEXT:    ld a6, 0(sp)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle8.v v9, (a7)
; VTYPE-SCHED-BOTTOMUP-NEXT:    ld a7, 8(sp)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle8.v v8, (a6)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v10, v9, 1
; VTYPE-SCHED-BOTTOMUP-NEXT:    ld a6, 16(sp)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle8.v v11, (a7)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v12, v8, 1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle8.v v13, (a6)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v14, v11, 1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v15, v13, 1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v16, v9, 2
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v17, v8, 2
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v18, v11, 2
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v19, v13, 2
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v20, v9, 3
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v21, v8, 3
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v22, v11, 3
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v23, v13, 3
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v24, v9, 4
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v25, v8, 4
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v26, v11, 4
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v27, v13, 4
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v28, v9, 5
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v29, v8, 5
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v30, v11, 5
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.v.i v31, 0
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vslidedown.vi v7, v13, 5
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv1r.v v6, v31
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a0, v9
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv1r.v v9, v31
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a0, v8
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a1, v10
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv1r.v v8, v31
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a0, v11
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a0, v13
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a1, v12
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a2, v16
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a1, v14
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a1, v15
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a2, v17
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a3, v20
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a2, v18
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a2, v19
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a3, v21
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a4, v24
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a3, v22
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a3, v23
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a4, v25
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v6, a5, v28
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a4, v26
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a4, v27
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v9, a5, v29
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmax.vx v10, v6, zero
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v8, a5, v30
; VTYPE-SCHED-BOTTOMUP-NEXT:    vwmaccsu.vx v31, a5, v7
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmax.vx v9, v9, zero
; VTYPE-SCHED-BOTTOMUP-NEXT:    csrwi vxrm, 0
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vnclipu.wi v10, v10, 6
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmax.vx v8, v8, zero
; VTYPE-SCHED-BOTTOMUP-NEXT:    ld a0, 24(sp)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmax.vx v11, v31, zero
; VTYPE-SCHED-BOTTOMUP-NEXT:    lw a1, 32(sp)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vnclipu.wi v9, v9, 6
; VTYPE-SCHED-BOTTOMUP-NEXT:    vnclipu.wi v8, v8, 6
; VTYPE-SCHED-BOTTOMUP-NEXT:    vse8.v v10, (a0)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vnclipu.wi v10, v11, 6
; VTYPE-SCHED-BOTTOMUP-NEXT:    add a2, a0, a1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vse8.v v9, (a2)
; VTYPE-SCHED-BOTTOMUP-NEXT:    sh1add a2, a1, a0
; VTYPE-SCHED-BOTTOMUP-NEXT:    sh1add a1, a1, a1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vse8.v v8, (a2)
; VTYPE-SCHED-BOTTOMUP-NEXT:    add a0, a0, a1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vse8.v v10, (a0)
; VTYPE-SCHED-BOTTOMUP-NEXT:    ret
;
; VTYPE-SCHED-TOPDOWN-LABEL: test:
; VTYPE-SCHED-TOPDOWN:       # %bb.0: # %entry
; VTYPE-SCHED-TOPDOWN-NEXT:    lw t1, 32(sp)
; VTYPE-SCHED-TOPDOWN-NEXT:    ld a6, 24(sp)
; VTYPE-SCHED-TOPDOWN-NEXT:    ld t2, 16(sp)
; VTYPE-SCHED-TOPDOWN-NEXT:    ld t3, 8(sp)
; VTYPE-SCHED-TOPDOWN-NEXT:    ld t4, 0(sp)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vle8.v v12, (a7)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.v.i v8, 0
; VTYPE-SCHED-TOPDOWN-NEXT:    csrwi vxrm, 0
; VTYPE-SCHED-TOPDOWN-NEXT:    sh1add t5, t1, t1
; VTYPE-SCHED-TOPDOWN-NEXT:    add t0, a6, t1
; VTYPE-SCHED-TOPDOWN-NEXT:    sh1add a7, t1, a6
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vle8.v v13, (t4)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v16, v12, 1
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv1r.v v9, v8
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv1r.v v10, v8
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv1r.v v11, v8
; VTYPE-SCHED-TOPDOWN-NEXT:    add t1, a6, t5
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vle8.v v14, (t2)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a0, v12
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a0, v13
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v17, v13, 1
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vle8.v v15, (t3)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a0, v14
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a0, v15
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a1, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a1, v17
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v16, v15, 1
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v17, v14, 1
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v18, v12, 2
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a1, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a1, v17
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v16, v13, 2
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v17, v15, 2
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v19, v14, 2
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a2, v18
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a2, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a2, v17
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a2, v19
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v16, v12, 3
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v17, v13, 3
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v18, v15, 3
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v19, v14, 3
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a3, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a3, v17
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a3, v18
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a3, v19
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v16, v12, 4
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v17, v13, 4
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v18, v15, 4
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v19, v14, 4
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a4, v17
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a4, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a4, v18
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a4, v19
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v12, v12, 5
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v13, v13, 5
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v15, v15, 5
; VTYPE-SCHED-TOPDOWN-NEXT:    vslidedown.vi v14, v14, 5
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v9, a5, v12
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v10, a5, v13
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v11, a5, v15
; VTYPE-SCHED-TOPDOWN-NEXT:    vwmaccsu.vx v8, a5, v14
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmax.vx v9, v9, zero
; VTYPE-SCHED-TOPDOWN-NEXT:    vmax.vx v10, v10, zero
; VTYPE-SCHED-TOPDOWN-NEXT:    vmax.vx v11, v11, zero
; VTYPE-SCHED-TOPDOWN-NEXT:    vmax.vx v8, v8, zero
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vnclipu.wi v9, v9, 6
; VTYPE-SCHED-TOPDOWN-NEXT:    vnclipu.wi v10, v10, 6
; VTYPE-SCHED-TOPDOWN-NEXT:    vnclipu.wi v11, v11, 6
; VTYPE-SCHED-TOPDOWN-NEXT:    vse8.v v9, (a6)
; VTYPE-SCHED-TOPDOWN-NEXT:    vnclipu.wi v8, v8, 6
; VTYPE-SCHED-TOPDOWN-NEXT:    vse8.v v10, (t0)
; VTYPE-SCHED-TOPDOWN-NEXT:    vse8.v v11, (a7)
; VTYPE-SCHED-TOPDOWN-NEXT:    vse8.v v8, (t1)
; VTYPE-SCHED-TOPDOWN-NEXT:    ret
;
; VTYPE-SCHED-BIDIRECTIONAL-LABEL: test:
; VTYPE-SCHED-BIDIRECTIONAL:       # %bb.0: # %entry
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ld a6, 0(sp)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle8.v v8, (a7)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ld a7, 8(sp)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle8.v v9, (a6)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v10, v8, 1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ld a6, 16(sp)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v11, v9, 1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle8.v v12, (a7)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v13, v12, 1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle8.v v14, (a6)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v15, v14, 1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v16, v8, 2
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v17, v9, 2
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v18, v12, 2
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v19, v14, 2
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v20, v8, 3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v21, v9, 3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v22, v12, 3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v23, v14, 3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v24, v8, 4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v25, v9, 4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v26, v12, 4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v27, v14, 4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v28, v8, 5
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v29, v9, 5
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v30, v12, 5
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.v.i v31, 0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vslidedown.vi v7, v14, 5
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv1r.v v6, v31
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e8, mf2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a0, v8
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv1r.v v8, v31
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a0, v9
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a1, v10
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv1r.v v9, v31
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a0, v12
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a0, v14
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a1, v11
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a2, v16
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a1, v13
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a1, v15
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a2, v17
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a3, v20
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a2, v18
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a2, v19
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a3, v21
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a4, v24
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a3, v22
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a3, v23
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a4, v25
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v6, a5, v28
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a4, v26
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a4, v27
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v8, a5, v29
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmax.vx v10, v6, zero
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v9, a5, v30
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vwmaccsu.vx v31, a5, v7
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmax.vx v8, v8, zero
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    csrwi vxrm, 0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vnclipu.wi v10, v10, 6
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ld a0, 24(sp)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e16, m1, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmax.vx v9, v9, zero
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    lw a1, 32(sp)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmax.vx v11, v31, zero
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e8, mf2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vnclipu.wi v8, v8, 6
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vse8.v v10, (a0)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vnclipu.wi v9, v9, 6
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    add a2, a0, a1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    sh1add a3, a1, a0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vse8.v v8, (a2)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vnclipu.wi v8, v11, 6
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    sh1add a1, a1, a1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vse8.v v9, (a3)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    add a0, a0, a1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vse8.v v8, (a0)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ret
entry:
  %14 = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.p0.i64(<vscale x 8 x i8> poison, ptr %7, i64 16)
  %15 = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.p0.i64(<vscale x 8 x i8> poison, ptr %8, i64 16)
  %16 = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.p0.i64(<vscale x 8 x i8> poison, ptr %9, i64 16)
  %17 = tail call <vscale x 8 x i8> @llvm.riscv.vle.nxv8i8.p0.i64(<vscale x 8 x i8> poison, ptr %10, i64 16)
  %18 = tail call <vscale x 4 x i16> @llvm.riscv.vmv.v.x.nxv4i16.i64(<vscale x 4 x i16> poison, i16 0, i64 8)
  %19 = trunc i16 %0 to i8
  %20 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %14, i64 0)
  %21 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %18, i8 %19, <vscale x 4 x i8> %20, i64 8, i64 3)
  %22 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %15, i64 0)
  %23 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %18, i8 %19, <vscale x 4 x i8> %22, i64 8, i64 3)
  %24 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %16, i64 0)
  %25 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %18, i8 %19, <vscale x 4 x i8> %24, i64 8, i64 3)
  %26 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %17, i64 0)
  %27 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %18, i8 %19, <vscale x 4 x i8> %26, i64 8, i64 3)
  %28 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %14, i64 1, i64 8, i64 3)
  %29 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %15, i64 1, i64 8, i64 3)
  %30 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %16, i64 1, i64 8, i64 3)
  %31 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %17, i64 1, i64 8, i64 3)
  %32 = trunc i16 %1 to i8
  %33 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %28, i64 0)
  %34 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %21, i8 %32, <vscale x 4 x i8> %33, i64 8, i64 3)
  %35 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %29, i64 0)
  %36 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %23, i8 %32, <vscale x 4 x i8> %35, i64 8, i64 3)
  %37 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %30, i64 0)
  %38 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %25, i8 %32, <vscale x 4 x i8> %37, i64 8, i64 3)
  %39 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %31, i64 0)
  %40 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %27, i8 %32, <vscale x 4 x i8> %39, i64 8, i64 3)
  %41 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %14, i64 2, i64 8, i64 3)
  %42 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %15, i64 2, i64 8, i64 3)
  %43 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %16, i64 2, i64 8, i64 3)
  %44 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %17, i64 2, i64 8, i64 3)
  %45 = trunc i16 %2 to i8
  %46 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %41, i64 0)
  %47 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %34, i8 %45, <vscale x 4 x i8> %46, i64 8, i64 3)
  %48 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %42, i64 0)
  %49 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %36, i8 %45, <vscale x 4 x i8> %48, i64 8, i64 3)
  %50 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %43, i64 0)
  %51 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %38, i8 %45, <vscale x 4 x i8> %50, i64 8, i64 3)
  %52 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %44, i64 0)
  %53 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %40, i8 %45, <vscale x 4 x i8> %52, i64 8, i64 3)
  %54 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %14, i64 3, i64 8, i64 3)
  %55 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %15, i64 3, i64 8, i64 3)
  %56 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %16, i64 3, i64 8, i64 3)
  %57 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %17, i64 3, i64 8, i64 3)
  %58 = trunc i16 %3 to i8
  %59 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %54, i64 0)
  %60 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %47, i8 %58, <vscale x 4 x i8> %59, i64 8, i64 3)
  %61 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %55, i64 0)
  %62 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %49, i8 %58, <vscale x 4 x i8> %61, i64 8, i64 3)
  %63 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %56, i64 0)
  %64 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %51, i8 %58, <vscale x 4 x i8> %63, i64 8, i64 3)
  %65 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %57, i64 0)
  %66 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %53, i8 %58, <vscale x 4 x i8> %65, i64 8, i64 3)
  %67 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %14, i64 4, i64 8, i64 3)
  %68 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %15, i64 4, i64 8, i64 3)
  %69 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %16, i64 4, i64 8, i64 3)
  %70 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %17, i64 4, i64 8, i64 3)
  %71 = trunc i16 %4 to i8
  %72 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %67, i64 0)
  %73 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %60, i8 %71, <vscale x 4 x i8> %72, i64 8, i64 3)
  %74 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %68, i64 0)
  %75 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %62, i8 %71, <vscale x 4 x i8> %74, i64 8, i64 3)
  %76 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %69, i64 0)
  %77 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %64, i8 %71, <vscale x 4 x i8> %76, i64 8, i64 3)
  %78 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %70, i64 0)
  %79 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %66, i8 %71, <vscale x 4 x i8> %78, i64 8, i64 3)
  %80 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %14, i64 5, i64 8, i64 3)
  %81 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %15, i64 5, i64 8, i64 3)
  %82 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %16, i64 5, i64 8, i64 3)
  %83 = tail call <vscale x 8 x i8> @llvm.riscv.vslidedown.nxv8i8.i64(<vscale x 8 x i8> poison, <vscale x 8 x i8> %17, i64 5, i64 8, i64 3)
  %84 = trunc i16 %5 to i8
  %85 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %80, i64 0)
  %86 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %73, i8 %84, <vscale x 4 x i8> %85, i64 8, i64 3)
  %87 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %81, i64 0)
  %88 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %75, i8 %84, <vscale x 4 x i8> %87, i64 8, i64 3)
  %89 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %82, i64 0)
  %90 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %77, i8 %84, <vscale x 4 x i8> %89, i64 8, i64 3)
  %91 = tail call <vscale x 4 x i8> @llvm.vector.extract.nxv4i8.nxv8i8(<vscale x 8 x i8> %83, i64 0)
  %92 = tail call <vscale x 4 x i16> @llvm.riscv.vwmaccsu.nxv4i16.i8.nxv4i8.i64(<vscale x 4 x i16> %79, i8 %84, <vscale x 4 x i8> %91, i64 8, i64 3)
  %93 = tail call <vscale x 4 x i16> @llvm.riscv.vmax.nxv4i16.i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %86, i16 0, i64 8)
  %94 = tail call <vscale x 4 x i16> @llvm.riscv.vmax.nxv4i16.i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %88, i16 0, i64 8)
  %95 = tail call <vscale x 4 x i16> @llvm.riscv.vmax.nxv4i16.i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %90, i16 0, i64 8)
  %96 = tail call <vscale x 4 x i16> @llvm.riscv.vmax.nxv4i16.i16.i64(<vscale x 4 x i16> poison, <vscale x 4 x i16> %92, i16 0, i64 8)
  %97 = tail call <vscale x 4 x i8> @llvm.riscv.vnclipu.nxv4i8.nxv4i16.i64.i64(<vscale x 4 x i8> poison, <vscale x 4 x i16> %93, i64 6, i64 0, i64 8)
  %98 = tail call <vscale x 4 x i8> @llvm.riscv.vnclipu.nxv4i8.nxv4i16.i64.i64(<vscale x 4 x i8> poison, <vscale x 4 x i16> %94, i64 6, i64 0, i64 8)
  %99 = tail call <vscale x 4 x i8> @llvm.riscv.vnclipu.nxv4i8.nxv4i16.i64.i64(<vscale x 4 x i8> poison, <vscale x 4 x i16> %95, i64 6, i64 0, i64 8)
  %100 = tail call <vscale x 4 x i8> @llvm.riscv.vnclipu.nxv4i8.nxv4i16.i64.i64(<vscale x 4 x i8> poison, <vscale x 4 x i16> %96, i64 6, i64 0, i64 8)
  tail call void @llvm.riscv.vse.nxv4i8.p0.i64(<vscale x 4 x i8> %97, ptr %11, i64 8)
  %101 = sext i32 %12 to i64
  %102 = getelementptr inbounds i8, ptr %11, i64 %101
  tail call void @llvm.riscv.vse.nxv4i8.p0.i64(<vscale x 4 x i8> %98, ptr %102, i64 8)
  %103 = shl nsw i32 %12, 1
  %104 = sext i32 %103 to i64
  %105 = getelementptr inbounds i8, ptr %11, i64 %104
  tail call void @llvm.riscv.vse.nxv4i8.p0.i64(<vscale x 4 x i8> %99, ptr %105, i64 8)
  %106 = mul nsw i32 %12, 3
  %107 = sext i32 %106 to i64
  %108 = getelementptr inbounds i8, ptr %11, i64 %107
  tail call void @llvm.riscv.vse.nxv4i8.p0.i64(<vscale x 4 x i8> %100, ptr %108, i64 8)
  ret void
}

define void @foo(ptr %0, ptr %1, ptr %2, ptr %3, ptr %4) {
; DEFAULT-LABEL: foo:
; DEFAULT:       # %bb.0: # %entry
; DEFAULT-NEXT:    vsetivli zero, 8, e64, m2, ta, ma
; DEFAULT-NEXT:    vle64.v v8, (a1)
; DEFAULT-NEXT:    vle64.v v10, (a2)
; DEFAULT-NEXT:    vmsltu.vv v12, v8, v10
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; DEFAULT-NEXT:    vmv.x.s a1, v12
; DEFAULT-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; DEFAULT-NEXT:    vmseq.vv v14, v8, v10
; DEFAULT-NEXT:    vle64.v v12, (a3)
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; DEFAULT-NEXT:    vmv.x.s a2, v14
; DEFAULT-NEXT:    vle64.v v14, (a4)
; DEFAULT-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; DEFAULT-NEXT:    vmsltu.vv v16, v12, v14
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; DEFAULT-NEXT:    vmv.x.s a3, v16
; DEFAULT-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; DEFAULT-NEXT:    vmseq.vv v16, v12, v14
; DEFAULT-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; DEFAULT-NEXT:    vmv.x.s a4, v16
; DEFAULT-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; DEFAULT-NEXT:    vsub.vv v8, v8, v10
; DEFAULT-NEXT:    sh1add a1, a1, a2
; DEFAULT-NEXT:    xor a1, a1, a2
; DEFAULT-NEXT:    vmv.s.x v0, a1
; DEFAULT-NEXT:    vmv.v.i v10, 0
; DEFAULT-NEXT:    vmerge.vim v16, v10, 1, v0
; DEFAULT-NEXT:    vsub.vv v8, v8, v16
; DEFAULT-NEXT:    sh1add a1, a3, a4
; DEFAULT-NEXT:    xor a1, a1, a4
; DEFAULT-NEXT:    vmv.s.x v0, a1
; DEFAULT-NEXT:    vsub.vv v12, v12, v14
; DEFAULT-NEXT:    vmerge.vim v10, v10, 1, v0
; DEFAULT-NEXT:    vsub.vv v10, v12, v10
; DEFAULT-NEXT:    vand.vv v8, v10, v8
; DEFAULT-NEXT:    vse64.v v8, (a0)
; DEFAULT-NEXT:    ret
;
; VTYPE-SCHED-BOTTOMUP-LABEL: foo:
; VTYPE-SCHED-BOTTOMUP:       # %bb.0: # %entry
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetivli zero, 8, e64, m2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle64.v v12, (a1)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle64.v v14, (a2)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle64.v v8, (a3)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vle64.v v10, (a4)
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmsltu.vv v16, v12, v14
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmsltu.vv v17, v8, v10
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.x.s a1, v16
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.x.s a2, v17
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmseq.vv v16, v12, v14
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmseq.vv v17, v8, v10
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.x.s a3, v16
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.x.s a4, v17
; VTYPE-SCHED-BOTTOMUP-NEXT:    sh1add a1, a1, a3
; VTYPE-SCHED-BOTTOMUP-NEXT:    xor a1, a1, a3
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsub.vv v12, v12, v14
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.v.i v14, 0
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmerge.vim v16, v14, 1, v0
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsub.vv v12, v12, v16
; VTYPE-SCHED-BOTTOMUP-NEXT:    sh1add a1, a2, a4
; VTYPE-SCHED-BOTTOMUP-NEXT:    xor a1, a1, a4
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsub.vv v8, v8, v10
; VTYPE-SCHED-BOTTOMUP-NEXT:    vmerge.vim v10, v14, 1, v0
; VTYPE-SCHED-BOTTOMUP-NEXT:    vsub.vv v8, v8, v10
; VTYPE-SCHED-BOTTOMUP-NEXT:    vand.vv v8, v8, v12
; VTYPE-SCHED-BOTTOMUP-NEXT:    vse64.v v8, (a0)
; VTYPE-SCHED-BOTTOMUP-NEXT:    ret
;
; VTYPE-SCHED-TOPDOWN-LABEL: foo:
; VTYPE-SCHED-TOPDOWN:       # %bb.0: # %entry
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetivli zero, 8, e64, m2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vle64.v v10, (a1)
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.v.i v8, 0
; VTYPE-SCHED-TOPDOWN-NEXT:    vle64.v v12, (a2)
; VTYPE-SCHED-TOPDOWN-NEXT:    vle64.v v14, (a3)
; VTYPE-SCHED-TOPDOWN-NEXT:    vmsltu.vv v18, v10, v12
; VTYPE-SCHED-TOPDOWN-NEXT:    vmseq.vv v19, v10, v12
; VTYPE-SCHED-TOPDOWN-NEXT:    vle64.v v16, (a4)
; VTYPE-SCHED-TOPDOWN-NEXT:    vsub.vv v10, v10, v12
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.x.s a1, v18
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.x.s a2, v19
; VTYPE-SCHED-TOPDOWN-NEXT:    sh1add a1, a1, a2
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmsltu.vv v18, v14, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    xor a1, a1, a2
; VTYPE-SCHED-TOPDOWN-NEXT:    vsub.vv v12, v14, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vmseq.vv v19, v14, v16
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.x.s a1, v18
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.x.s a2, v19
; VTYPE-SCHED-TOPDOWN-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-TOPDOWN-NEXT:    vmerge.vim v14, v8, 1, v0
; VTYPE-SCHED-TOPDOWN-NEXT:    sh1add a1, a1, a2
; VTYPE-SCHED-TOPDOWN-NEXT:    xor a1, a1, a2
; VTYPE-SCHED-TOPDOWN-NEXT:    vsub.vv v10, v10, v14
; VTYPE-SCHED-TOPDOWN-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-TOPDOWN-NEXT:    vmerge.vim v8, v8, 1, v0
; VTYPE-SCHED-TOPDOWN-NEXT:    vsub.vv v8, v12, v8
; VTYPE-SCHED-TOPDOWN-NEXT:    vand.vv v8, v8, v10
; VTYPE-SCHED-TOPDOWN-NEXT:    vse64.v v8, (a0)
; VTYPE-SCHED-TOPDOWN-NEXT:    ret
;
; VTYPE-SCHED-BIDIRECTIONAL-LABEL: foo:
; VTYPE-SCHED-BIDIRECTIONAL:       # %bb.0: # %entry
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetivli zero, 8, e64, m2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle64.v v12, (a1)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle64.v v14, (a2)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle64.v v8, (a3)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmsltu.vv v16, v12, v14
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vle64.v v10, (a4)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmsltu.vv v17, v8, v10
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.x.s a1, v16
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.x.s a2, v17
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmseq.vv v16, v12, v14
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmseq.vv v17, v8, v10
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e8, mf4, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.x.s a3, v16
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.x.s a4, v17
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    sh1add a1, a1, a3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    xor a1, a1, a3
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsetvli zero, zero, e64, m2, ta, ma
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsub.vv v12, v12, v14
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.v.i v14, 0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmerge.vim v16, v14, 1, v0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsub.vv v12, v12, v16
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    sh1add a1, a2, a4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    xor a1, a1, a4
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmv.s.x v0, a1
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsub.vv v8, v8, v10
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vmerge.vim v10, v14, 1, v0
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vsub.vv v8, v8, v10
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vand.vv v8, v8, v12
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    vse64.v v8, (a0)
; VTYPE-SCHED-BIDIRECTIONAL-NEXT:    ret
entry:
  %5 = load <8 x i64>, ptr %1, align 64
  %6 = load <8 x i64>, ptr %2, align 64
  %7 = load <8 x i64>, ptr %3, align 64
  %8 = load <8 x i64>, ptr %4, align 64
  %9 = icmp ult <8 x i64> %5, %6
  %10 = bitcast <8 x i1> %9 to i8
  %11 = icmp eq <8 x i64> %5, %6
  %12 = bitcast <8 x i1> %11 to i8
  %13 = sub <8 x i64> %5, %6
  %14 = shl i8 %10, 1
  %15 = add i8 %14, %12
  %16 = xor i8 %15, %12
  %17 = bitcast i8 %16 to <8 x i1>
  %18 = sext <8 x i1> %17 to <8 x i64>
  %19 = add <8 x i64> %13, %18
  %20 = icmp ult <8 x i64> %7, %8
  %21 = bitcast <8 x i1> %20 to i8
  %22 = icmp eq <8 x i64> %7, %8
  %23 = bitcast <8 x i1> %22 to i8
  %24 = sub <8 x i64> %7, %8
  %25 = shl i8 %21, 1
  %26 = add i8 %25, %23
  %27 = xor i8 %26, %23
  %28 = bitcast i8 %27 to <8 x i1>
  %29 = sext <8 x i1> %28 to <8 x i64>
  %30 = add <8 x i64> %24, %29
  %31 = and <8 x i64> %30, %19
  store <8 x i64> %31, ptr %0, align 64
  ret void
}
