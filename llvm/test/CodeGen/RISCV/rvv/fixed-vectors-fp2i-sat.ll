; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -target-abi=ilp32d -mattr=+v,+zfh,+experimental-zvfh,+f,+d -riscv-v-vector-bits-min=128 -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV32
; RUN: llc -mtriple=riscv64 -target-abi=lp64d -mattr=+v,+zfh,+experimental-zvfh,+f,+d -riscv-v-vector-bits-min=128 -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV64

define void @fp2si_v2f32_v2i32(<2 x float>* %x, <2 x i32>* %y) {
; CHECK-LABEL: fp2si_v2f32_v2i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; CHECK-NEXT:    vle32.v v8, (a0)
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    feq.s a0, ft0, ft0
; CHECK-NEXT:    beqz a0, .LBB0_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    fcvt.w.s a0, ft0, rtz
; CHECK-NEXT:  .LBB0_2:
; CHECK-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; CHECK-NEXT:    vslidedown.vi v8, v8, 1
; CHECK-NEXT:    vfmv.f.s ft0, v8
; CHECK-NEXT:    feq.s a2, ft0, ft0
; CHECK-NEXT:    beqz a2, .LBB0_4
; CHECK-NEXT:  # %bb.3:
; CHECK-NEXT:    fcvt.w.s a2, ft0, rtz
; CHECK-NEXT:  .LBB0_4:
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; CHECK-NEXT:    vmv.v.x v8, a2
; CHECK-NEXT:    vsetvli zero, zero, e32, mf2, tu, mu
; CHECK-NEXT:    vmv.s.x v8, a0
; CHECK-NEXT:    vse32.v v8, (a1)
; CHECK-NEXT:    ret
  %a = load <2 x float>, <2 x float>* %x
  %d = call <2 x i32> @llvm.fptosi.sat.v2i32.v2f32(<2 x float> %a)
  store <2 x i32> %d, <2 x i32>* %y
  ret void
}
declare <2 x i32> @llvm.fptosi.sat.v2i32.v2f32(<2 x float>)

define void @fp2ui_v2f32_v2i32(<2 x float>* %x, <2 x i32>* %y) {
; RV32-LABEL: fp2ui_v2f32_v2i32:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a0, ft0, ft0
; RV32-NEXT:    beqz a0, .LBB1_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.wu.s a0, ft0, rtz
; RV32-NEXT:  .LBB1_2:
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    beqz a2, .LBB1_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.wu.s a2, ft0, rtz
; RV32-NEXT:  .LBB1_4:
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV32-NEXT:    vmv.v.x v8, a2
; RV32-NEXT:    vsetvli zero, zero, e32, mf2, tu, mu
; RV32-NEXT:    vmv.s.x v8, a0
; RV32-NEXT:    vse32.v v8, (a1)
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2ui_v2f32_v2i32:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB1_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.wu.s a0, ft0, rtz
; RV64-NEXT:    slli a0, a0, 32
; RV64-NEXT:    srli a0, a0, 32
; RV64-NEXT:  .LBB1_2:
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    beqz a2, .LBB1_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.wu.s a2, ft0, rtz
; RV64-NEXT:    slli a2, a2, 32
; RV64-NEXT:    srli a2, a2, 32
; RV64-NEXT:  .LBB1_4:
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e32, mf2, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse32.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x float>, <2 x float>* %x
  %d = call <2 x i32> @llvm.fptoui.sat.v2i32.v2f32(<2 x float> %a)
  store <2 x i32> %d, <2 x i32>* %y
  ret void
}
declare <2 x i32> @llvm.fptoui.sat.v2i32.v2f32(<2 x float>)

define void @fp2si_v8f32_v8i32(<8 x float>* %x, <8 x i32>* %y) {
;
; RV32-LABEL: fp2si_v8f32_v8i32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 64
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a0, ft0, ft0
; RV32-NEXT:    beqz a0, .LBB2_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_2:
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v10, v8, 7
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a0, ft0, ft0
; RV32-NEXT:    beqz a0, .LBB2_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_4:
; RV32-NEXT:    vslidedown.vi v10, v8, 6
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 28(sp)
; RV32-NEXT:    bnez a2, .LBB2_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_7
; RV32-NEXT:  .LBB2_6:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_7:
; RV32-NEXT:    vslidedown.vi v10, v8, 5
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 24(sp)
; RV32-NEXT:    bnez a2, .LBB2_9
; RV32-NEXT:  # %bb.8:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_10
; RV32-NEXT:  .LBB2_9:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_10:
; RV32-NEXT:    vslidedown.vi v10, v8, 4
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 20(sp)
; RV32-NEXT:    bnez a2, .LBB2_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_13
; RV32-NEXT:  .LBB2_12:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_13:
; RV32-NEXT:    vslidedown.vi v10, v8, 3
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 16(sp)
; RV32-NEXT:    bnez a2, .LBB2_15
; RV32-NEXT:  # %bb.14:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_16
; RV32-NEXT:  .LBB2_15:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_16:
; RV32-NEXT:    vslidedown.vi v10, v8, 2
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    bnez a2, .LBB2_18
; RV32-NEXT:  # %bb.17:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_19
; RV32-NEXT:  .LBB2_18:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_19:
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    bnez a2, .LBB2_21
; RV32-NEXT:  # %bb.20:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB2_22
; RV32-NEXT:  .LBB2_21:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB2_22:
; RV32-NEXT:    sw a0, 4(sp)
; RV32-NEXT:    mv a0, sp
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vse32.v v8, (a1)
; RV32-NEXT:    addi sp, s0, -64
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2si_v8f32_v8i32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 64
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB2_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_2:
; RV64-NEXT:    sw a0, 0(sp)
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v10, v8, 7
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB2_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_4:
; RV64-NEXT:    vslidedown.vi v10, v8, 6
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 28(sp)
; RV64-NEXT:    bnez a2, .LBB2_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_7
; RV64-NEXT:  .LBB2_6:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_7:
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 24(sp)
; RV64-NEXT:    bnez a2, .LBB2_9
; RV64-NEXT:  # %bb.8:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_10
; RV64-NEXT:  .LBB2_9:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_10:
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 20(sp)
; RV64-NEXT:    bnez a2, .LBB2_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_13
; RV64-NEXT:  .LBB2_12:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_13:
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 16(sp)
; RV64-NEXT:    bnez a2, .LBB2_15
; RV64-NEXT:  # %bb.14:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_16
; RV64-NEXT:  .LBB2_15:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_16:
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 12(sp)
; RV64-NEXT:    bnez a2, .LBB2_18
; RV64-NEXT:  # %bb.17:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_19
; RV64-NEXT:  .LBB2_18:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_19:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 8(sp)
; RV64-NEXT:    bnez a2, .LBB2_21
; RV64-NEXT:  # %bb.20:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB2_22
; RV64-NEXT:  .LBB2_21:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB2_22:
; RV64-NEXT:    sw a0, 4(sp)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vse32.v v8, (a1)
; RV64-NEXT:    addi sp, s0, -64
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x float>, <8 x float>* %x
  %d = call <8 x i32> @llvm.fptosi.sat.v8i32.v8f32(<8 x float> %a)
  store <8 x i32> %d, <8 x i32>* %y
  ret void
}
declare <8 x i32> @llvm.fptosi.sat.v8i32.v8f32(<8 x float>)

define void @fp2ui_v8f32_v8i32(<8 x float>* %x, <8 x i32>* %y) {
;
; RV32-LABEL: fp2ui_v8f32_v8i32:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -64
; RV32-NEXT:    .cfi_def_cfa_offset 64
; RV32-NEXT:    sw ra, 60(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 56(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    addi s0, sp, 64
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    andi sp, sp, -32
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a0, ft0, ft0
; RV32-NEXT:    beqz a0, .LBB3_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_2:
; RV32-NEXT:    sw a0, 0(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    vslidedown.vi v10, v8, 7
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a0, ft0, ft0
; RV32-NEXT:    beqz a0, .LBB3_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_4:
; RV32-NEXT:    vslidedown.vi v10, v8, 6
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 28(sp)
; RV32-NEXT:    bnez a2, .LBB3_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_7
; RV32-NEXT:  .LBB3_6:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_7:
; RV32-NEXT:    vslidedown.vi v10, v8, 5
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 24(sp)
; RV32-NEXT:    bnez a2, .LBB3_9
; RV32-NEXT:  # %bb.8:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_10
; RV32-NEXT:  .LBB3_9:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_10:
; RV32-NEXT:    vslidedown.vi v10, v8, 4
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 20(sp)
; RV32-NEXT:    bnez a2, .LBB3_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_13
; RV32-NEXT:  .LBB3_12:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_13:
; RV32-NEXT:    vslidedown.vi v10, v8, 3
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 16(sp)
; RV32-NEXT:    bnez a2, .LBB3_15
; RV32-NEXT:  # %bb.14:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_16
; RV32-NEXT:  .LBB3_15:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_16:
; RV32-NEXT:    vslidedown.vi v10, v8, 2
; RV32-NEXT:    vfmv.f.s ft0, v10
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 12(sp)
; RV32-NEXT:    bnez a2, .LBB3_18
; RV32-NEXT:  # %bb.17:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_19
; RV32-NEXT:  .LBB3_18:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_19:
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    feq.s a2, ft0, ft0
; RV32-NEXT:    sw a0, 8(sp)
; RV32-NEXT:    bnez a2, .LBB3_21
; RV32-NEXT:  # %bb.20:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB3_22
; RV32-NEXT:  .LBB3_21:
; RV32-NEXT:    fcvt.w.s a0, ft0, rtz
; RV32-NEXT:  .LBB3_22:
; RV32-NEXT:    sw a0, 4(sp)
; RV32-NEXT:    mv a0, sp
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vse32.v v8, (a1)
; RV32-NEXT:    addi sp, s0, -64
; RV32-NEXT:    lw ra, 60(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 56(sp) # 4-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 64
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2ui_v8f32_v8i32:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -64
; RV64-NEXT:    .cfi_def_cfa_offset 64
; RV64-NEXT:    sd ra, 56(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 48(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 64
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -32
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB3_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_2:
; RV64-NEXT:    sw a0, 0(sp)
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v10, v8, 7
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB3_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_4:
; RV64-NEXT:    vslidedown.vi v10, v8, 6
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 28(sp)
; RV64-NEXT:    bnez a2, .LBB3_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_7
; RV64-NEXT:  .LBB3_6:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_7:
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 24(sp)
; RV64-NEXT:    bnez a2, .LBB3_9
; RV64-NEXT:  # %bb.8:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_10
; RV64-NEXT:  .LBB3_9:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_10:
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 20(sp)
; RV64-NEXT:    bnez a2, .LBB3_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_13
; RV64-NEXT:  .LBB3_12:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_13:
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 16(sp)
; RV64-NEXT:    bnez a2, .LBB3_15
; RV64-NEXT:  # %bb.14:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_16
; RV64-NEXT:  .LBB3_15:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_16:
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 12(sp)
; RV64-NEXT:    bnez a2, .LBB3_18
; RV64-NEXT:  # %bb.17:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_19
; RV64-NEXT:  .LBB3_18:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_19:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sw a0, 8(sp)
; RV64-NEXT:    bnez a2, .LBB3_21
; RV64-NEXT:  # %bb.20:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB3_22
; RV64-NEXT:  .LBB3_21:
; RV64-NEXT:    fcvt.w.s a0, ft0, rtz
; RV64-NEXT:  .LBB3_22:
; RV64-NEXT:    sw a0, 4(sp)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vse32.v v8, (a1)
; RV64-NEXT:    addi sp, s0, -64
; RV64-NEXT:    ld ra, 56(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 48(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 64
; RV64-NEXT:    ret
  %a = load <8 x float>, <8 x float>* %x
  %d = call <8 x i32> @llvm.fptosi.sat.v8i32.v8f32(<8 x float> %a)
  store <8 x i32> %d, <8 x i32>* %y
  ret void
}
declare <8 x i32> @llvm.fptoui.sat.v8i32.v8f32(<8 x float>)

define void @fp2si_v2f32_v2i64(<2 x float>* %x, <2 x i64>* %y) {
; RV32-LABEL: fp2si_v2f32_v2i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 64(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 56(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 48(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset fs0, -32
; RV32-NEXT:    .cfi_offset fs1, -40
; RV32-NEXT:    .cfi_offset fs2, -48
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI4_0)
; RV32-NEXT:    flw fs1, %lo(.LCPI4_0)(a0)
; RV32-NEXT:    mv s0, a1
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    lui a3, 524288
; RV32-NEXT:    bnez s2, .LBB4_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB4_2:
; RV32-NEXT:    lui a2, %hi(.LCPI4_1)
; RV32-NEXT:    flw fs2, %lo(.LCPI4_1)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    addi s1, a3, -1
; RV32-NEXT:    mv a3, s1
; RV32-NEXT:    beqz a2, .LBB4_18
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB4_19
; RV32-NEXT:  .LBB4_4:
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    bnez s2, .LBB4_6
; RV32-NEXT:  .LBB4_5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:  .LBB4_6:
; RV32-NEXT:    li s2, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB4_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:  .LBB4_8:
; RV32-NEXT:    bnez a1, .LBB4_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB4_10:
; RV32-NEXT:    sw a3, 16(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s3, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    beqz s3, .LBB4_20
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    beqz a2, .LBB4_21
; RV32-NEXT:  .LBB4_12:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB4_22
; RV32-NEXT:  .LBB4_13:
; RV32-NEXT:    sw s1, 28(sp)
; RV32-NEXT:    beqz s3, .LBB4_23
; RV32-NEXT:  .LBB4_14:
; RV32-NEXT:    beqz a2, .LBB4_24
; RV32-NEXT:  .LBB4_15:
; RV32-NEXT:    bnez a1, .LBB4_17
; RV32-NEXT:  .LBB4_16:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:  .LBB4_17:
; RV32-NEXT:    sw s2, 24(sp)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vse64.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 64(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 56(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 48(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB4_18:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB4_4
; RV32-NEXT:  .LBB4_19:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    beqz s2, .LBB4_5
; RV32-NEXT:    j .LBB4_6
; RV32-NEXT:  .LBB4_20:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    bnez a2, .LBB4_12
; RV32-NEXT:  .LBB4_21:
; RV32-NEXT:    mv s1, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB4_13
; RV32-NEXT:  .LBB4_22:
; RV32-NEXT:    li s1, 0
; RV32-NEXT:    sw s1, 28(sp)
; RV32-NEXT:    bnez s3, .LBB4_14
; RV32-NEXT:  .LBB4_23:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    bnez a2, .LBB4_15
; RV32-NEXT:  .LBB4_24:
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    beqz a1, .LBB4_16
; RV32-NEXT:    j .LBB4_17
;
; RV64-LABEL: fp2si_v2f32_v2i64:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB4_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB4_2:
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    beqz a2, .LBB4_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.l.s a2, ft0, rtz
; RV64-NEXT:  .LBB4_4:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x float>, <2 x float>* %x
  %d = call <2 x i64> @llvm.fptosi.sat.v2i64.v2f32(<2 x float> %a)
  store <2 x i64> %d, <2 x i64>* %y
  ret void
}
declare <2 x i64> @llvm.fptosi.sat.v2i64.v2f32(<2 x float>)

define void @fp2ui_v2f32_v2i64(<2 x float>* %x, <2 x i64>* %y) {
; RV32-LABEL: fp2ui_v2f32_v2i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 72(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 64(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 56(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    mv s0, a1
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fmv.w.x fs1, zero
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s2, .LBB5_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB5_2:
; RV32-NEXT:    lui a2, %hi(.LCPI5_0)
; RV32-NEXT:    flw fs2, %lo(.LCPI5_0)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li s1, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB5_13
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    beqz s2, .LBB5_14
; RV32-NEXT:  .LBB5_4:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB5_6
; RV32-NEXT:  .LBB5_5:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB5_6:
; RV32-NEXT:    sw a1, 16(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s2, .LBB5_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB5_8:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB5_15
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    sw a3, 28(sp)
; RV32-NEXT:    beqz s2, .LBB5_16
; RV32-NEXT:  .LBB5_10:
; RV32-NEXT:    bnez a2, .LBB5_12
; RV32-NEXT:  .LBB5_11:
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:  .LBB5_12:
; RV32-NEXT:    sw s1, 24(sp)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vse64.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 72(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 64(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 56(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB5_13:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    bnez s2, .LBB5_4
; RV32-NEXT:  .LBB5_14:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB5_5
; RV32-NEXT:    j .LBB5_6
; RV32-NEXT:  .LBB5_15:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 28(sp)
; RV32-NEXT:    bnez s2, .LBB5_10
; RV32-NEXT:  .LBB5_16:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    beqz a2, .LBB5_11
; RV32-NEXT:    j .LBB5_12
;
; RV64-LABEL: fp2ui_v2f32_v2i64:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e32, mf2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB5_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB5_2:
; RV64-NEXT:    vsetivli zero, 1, e32, mf2, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    beqz a2, .LBB5_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.lu.s a2, ft0, rtz
; RV64-NEXT:  .LBB5_4:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x float>, <2 x float>* %x
  %d = call <2 x i64> @llvm.fptoui.sat.v2i64.v2f32(<2 x float> %a)
  store <2 x i64> %d, <2 x i64>* %y
  ret void
}
declare <2 x i64> @llvm.fptoui.sat.v2i64.v2f32(<2 x float>)

define void @fp2si_v8f32_v8i64(<8 x float>* %x, <8 x i64>* %y) {
;
; RV32-LABEL: fp2si_v8f32_v8i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -192
; RV32-NEXT:    .cfi_def_cfa_offset 192
; RV32-NEXT:    sw ra, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 184(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 180(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 176(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 172(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 168(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 160(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 152(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 144(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s2, -12
; RV32-NEXT:    .cfi_offset s3, -16
; RV32-NEXT:    .cfi_offset s4, -20
; RV32-NEXT:    .cfi_offset s5, -24
; RV32-NEXT:    .cfi_offset fs0, -32
; RV32-NEXT:    .cfi_offset fs1, -40
; RV32-NEXT:    .cfi_offset fs2, -48
; RV32-NEXT:    addi s0, sp, 192
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI6_0)
; RV32-NEXT:    flw fs1, %lo(.LCPI6_0)(a0)
; RV32-NEXT:    mv s2, a1
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    lui a3, 524288
; RV32-NEXT:    bnez s4, .LBB6_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_2:
; RV32-NEXT:    lui a2, %hi(.LCPI6_1)
; RV32-NEXT:    flw fs2, %lo(.LCPI6_1)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    addi s3, a3, -1
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_66
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_67
; RV32-NEXT:  .LBB6_4:
; RV32-NEXT:    sw a3, 68(sp)
; RV32-NEXT:    bnez s4, .LBB6_6
; RV32-NEXT:  .LBB6_5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:  .LBB6_6:
; RV32-NEXT:    li s4, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:  .LBB6_8:
; RV32-NEXT:    bnez a1, .LBB6_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_10:
; RV32-NEXT:    sw a3, 64(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_12:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_68
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_69
; RV32-NEXT:  .LBB6_14:
; RV32-NEXT:    sw a3, 124(sp)
; RV32-NEXT:    beqz s5, .LBB6_70
; RV32-NEXT:  .LBB6_15:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_71
; RV32-NEXT:  .LBB6_16:
; RV32-NEXT:    bnez a1, .LBB6_18
; RV32-NEXT:  .LBB6_17:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_18:
; RV32-NEXT:    sw a3, 120(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_20
; RV32-NEXT:  # %bb.19:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_20:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_72
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_73
; RV32-NEXT:  .LBB6_22:
; RV32-NEXT:    sw a3, 116(sp)
; RV32-NEXT:    beqz s5, .LBB6_74
; RV32-NEXT:  .LBB6_23:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_75
; RV32-NEXT:  .LBB6_24:
; RV32-NEXT:    bnez a1, .LBB6_26
; RV32-NEXT:  .LBB6_25:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_26:
; RV32-NEXT:    sw a3, 112(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_28
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_28:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_76
; RV32-NEXT:  # %bb.29:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_77
; RV32-NEXT:  .LBB6_30:
; RV32-NEXT:    sw a3, 108(sp)
; RV32-NEXT:    beqz s5, .LBB6_78
; RV32-NEXT:  .LBB6_31:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_79
; RV32-NEXT:  .LBB6_32:
; RV32-NEXT:    bnez a1, .LBB6_34
; RV32-NEXT:  .LBB6_33:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_34:
; RV32-NEXT:    sw a3, 104(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_36
; RV32-NEXT:  # %bb.35:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_36:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_80
; RV32-NEXT:  # %bb.37:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_81
; RV32-NEXT:  .LBB6_38:
; RV32-NEXT:    sw a3, 100(sp)
; RV32-NEXT:    beqz s5, .LBB6_82
; RV32-NEXT:  .LBB6_39:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_83
; RV32-NEXT:  .LBB6_40:
; RV32-NEXT:    bnez a1, .LBB6_42
; RV32-NEXT:  .LBB6_41:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_42:
; RV32-NEXT:    sw a3, 96(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_44
; RV32-NEXT:  # %bb.43:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_44:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_84
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_85
; RV32-NEXT:  .LBB6_46:
; RV32-NEXT:    sw a3, 92(sp)
; RV32-NEXT:    beqz s5, .LBB6_86
; RV32-NEXT:  .LBB6_47:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_87
; RV32-NEXT:  .LBB6_48:
; RV32-NEXT:    bnez a1, .LBB6_50
; RV32-NEXT:  .LBB6_49:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_50:
; RV32-NEXT:    sw a3, 88(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    bnez s5, .LBB6_52
; RV32-NEXT:  # %bb.51:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB6_52:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    beqz a2, .LBB6_88
; RV32-NEXT:  # %bb.53:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_89
; RV32-NEXT:  .LBB6_54:
; RV32-NEXT:    sw a3, 84(sp)
; RV32-NEXT:    beqz s5, .LBB6_90
; RV32-NEXT:  .LBB6_55:
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB6_91
; RV32-NEXT:  .LBB6_56:
; RV32-NEXT:    bnez a1, .LBB6_58
; RV32-NEXT:  .LBB6_57:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB6_58:
; RV32-NEXT:    sw a3, 80(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 144
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s5, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    beqz s5, .LBB6_92
; RV32-NEXT:  # %bb.59:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    beqz a2, .LBB6_93
; RV32-NEXT:  .LBB6_60:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB6_94
; RV32-NEXT:  .LBB6_61:
; RV32-NEXT:    sw s3, 76(sp)
; RV32-NEXT:    beqz s5, .LBB6_95
; RV32-NEXT:  .LBB6_62:
; RV32-NEXT:    beqz a2, .LBB6_96
; RV32-NEXT:  .LBB6_63:
; RV32-NEXT:    bnez a1, .LBB6_65
; RV32-NEXT:  .LBB6_64:
; RV32-NEXT:    li s4, 0
; RV32-NEXT:  .LBB6_65:
; RV32-NEXT:    sw s4, 72(sp)
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    vse64.v v8, (s2)
; RV32-NEXT:    addi sp, s0, -192
; RV32-NEXT:    lw ra, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 184(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 180(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 176(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 172(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 168(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 160(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 152(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 144(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 192
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB6_66:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_4
; RV32-NEXT:  .LBB6_67:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 68(sp)
; RV32-NEXT:    beqz s4, .LBB6_5
; RV32-NEXT:    j .LBB6_6
; RV32-NEXT:  .LBB6_68:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_14
; RV32-NEXT:  .LBB6_69:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 124(sp)
; RV32-NEXT:    bnez s5, .LBB6_15
; RV32-NEXT:  .LBB6_70:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_16
; RV32-NEXT:  .LBB6_71:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_17
; RV32-NEXT:    j .LBB6_18
; RV32-NEXT:  .LBB6_72:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_22
; RV32-NEXT:  .LBB6_73:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 116(sp)
; RV32-NEXT:    bnez s5, .LBB6_23
; RV32-NEXT:  .LBB6_74:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_24
; RV32-NEXT:  .LBB6_75:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_25
; RV32-NEXT:    j .LBB6_26
; RV32-NEXT:  .LBB6_76:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_30
; RV32-NEXT:  .LBB6_77:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 108(sp)
; RV32-NEXT:    bnez s5, .LBB6_31
; RV32-NEXT:  .LBB6_78:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_32
; RV32-NEXT:  .LBB6_79:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_33
; RV32-NEXT:    j .LBB6_34
; RV32-NEXT:  .LBB6_80:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_38
; RV32-NEXT:  .LBB6_81:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 100(sp)
; RV32-NEXT:    bnez s5, .LBB6_39
; RV32-NEXT:  .LBB6_82:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_40
; RV32-NEXT:  .LBB6_83:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_41
; RV32-NEXT:    j .LBB6_42
; RV32-NEXT:  .LBB6_84:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_46
; RV32-NEXT:  .LBB6_85:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 92(sp)
; RV32-NEXT:    bnez s5, .LBB6_47
; RV32-NEXT:  .LBB6_86:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_48
; RV32-NEXT:  .LBB6_87:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_49
; RV32-NEXT:    j .LBB6_50
; RV32-NEXT:  .LBB6_88:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_54
; RV32-NEXT:  .LBB6_89:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 84(sp)
; RV32-NEXT:    bnez s5, .LBB6_55
; RV32-NEXT:  .LBB6_90:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB6_56
; RV32-NEXT:  .LBB6_91:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:    beqz a1, .LBB6_57
; RV32-NEXT:    j .LBB6_58
; RV32-NEXT:  .LBB6_92:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    bnez a2, .LBB6_60
; RV32-NEXT:  .LBB6_93:
; RV32-NEXT:    mv s3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB6_61
; RV32-NEXT:  .LBB6_94:
; RV32-NEXT:    li s3, 0
; RV32-NEXT:    sw s3, 76(sp)
; RV32-NEXT:    bnez s5, .LBB6_62
; RV32-NEXT:  .LBB6_95:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    bnez a2, .LBB6_63
; RV32-NEXT:  .LBB6_96:
; RV32-NEXT:    mv s4, a0
; RV32-NEXT:    beqz a1, .LBB6_64
; RV32-NEXT:    j .LBB6_65
;
; RV64-LABEL: fp2si_v8f32_v8i64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -128
; RV64-NEXT:    .cfi_def_cfa_offset 128
; RV64-NEXT:    sd ra, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 128
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB6_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_2:
; RV64-NEXT:    sd a0, 0(sp)
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v10, v8, 7
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB6_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_4:
; RV64-NEXT:    vslidedown.vi v10, v8, 6
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 56(sp)
; RV64-NEXT:    bnez a2, .LBB6_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_7
; RV64-NEXT:  .LBB6_6:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_7:
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 48(sp)
; RV64-NEXT:    bnez a2, .LBB6_9
; RV64-NEXT:  # %bb.8:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_10
; RV64-NEXT:  .LBB6_9:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_10:
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 40(sp)
; RV64-NEXT:    bnez a2, .LBB6_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_13
; RV64-NEXT:  .LBB6_12:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_13:
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 32(sp)
; RV64-NEXT:    bnez a2, .LBB6_15
; RV64-NEXT:  # %bb.14:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_16
; RV64-NEXT:  .LBB6_15:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_16:
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 24(sp)
; RV64-NEXT:    bnez a2, .LBB6_18
; RV64-NEXT:  # %bb.17:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_19
; RV64-NEXT:  .LBB6_18:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_19:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 16(sp)
; RV64-NEXT:    bnez a2, .LBB6_21
; RV64-NEXT:  # %bb.20:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB6_22
; RV64-NEXT:  .LBB6_21:
; RV64-NEXT:    fcvt.l.s a0, ft0, rtz
; RV64-NEXT:  .LBB6_22:
; RV64-NEXT:    sd a0, 8(sp)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    addi sp, s0, -128
; RV64-NEXT:    ld ra, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 128
; RV64-NEXT:    ret
  %a = load <8 x float>, <8 x float>* %x
  %d = call <8 x i64> @llvm.fptosi.sat.v8i64.v8f32(<8 x float> %a)
  store <8 x i64> %d, <8 x i64>* %y
  ret void
}
declare <8 x i64> @llvm.fptosi.sat.v8i64.v8f32(<8 x float>)

define void @fp2ui_v8f32_v8i64(<8 x float>* %x, <8 x i64>* %y) {
;
; RV32-LABEL: fp2ui_v8f32_v8i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -208
; RV32-NEXT:    .cfi_def_cfa_offset 208
; RV32-NEXT:    sw ra, 204(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 200(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 196(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 192(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 188(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 176(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 168(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 160(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s2, -12
; RV32-NEXT:    .cfi_offset s3, -16
; RV32-NEXT:    .cfi_offset s4, -20
; RV32-NEXT:    .cfi_offset fs0, -32
; RV32-NEXT:    .cfi_offset fs1, -40
; RV32-NEXT:    .cfi_offset fs2, -48
; RV32-NEXT:    addi s0, sp, 208
; RV32-NEXT:    .cfi_def_cfa s0, 0
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    andi sp, sp, -64
; RV32-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    mv s2, a1
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fmv.w.x fs1, zero
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_2:
; RV32-NEXT:    lui a2, %hi(.LCPI7_0)
; RV32-NEXT:    flw fs2, %lo(.LCPI7_0)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li s3, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_49
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    sw a3, 68(sp)
; RV32-NEXT:    beqz s4, .LBB7_50
; RV32-NEXT:  .LBB7_4:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_6
; RV32-NEXT:  .LBB7_5:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_6:
; RV32-NEXT:    sw a1, 64(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 7
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_8:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_51
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    sw a3, 124(sp)
; RV32-NEXT:    beqz s4, .LBB7_52
; RV32-NEXT:  .LBB7_10:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_12
; RV32-NEXT:  .LBB7_11:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_12:
; RV32-NEXT:    sw a1, 120(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 6
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_14
; RV32-NEXT:  # %bb.13:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_14:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_53
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    sw a3, 116(sp)
; RV32-NEXT:    beqz s4, .LBB7_54
; RV32-NEXT:  .LBB7_16:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_18
; RV32-NEXT:  .LBB7_17:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_18:
; RV32-NEXT:    sw a1, 112(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 5
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_20
; RV32-NEXT:  # %bb.19:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_20:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_55
; RV32-NEXT:  # %bb.21:
; RV32-NEXT:    sw a3, 108(sp)
; RV32-NEXT:    beqz s4, .LBB7_56
; RV32-NEXT:  .LBB7_22:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_24
; RV32-NEXT:  .LBB7_23:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_24:
; RV32-NEXT:    sw a1, 104(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 4
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_26
; RV32-NEXT:  # %bb.25:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_26:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_57
; RV32-NEXT:  # %bb.27:
; RV32-NEXT:    sw a3, 100(sp)
; RV32-NEXT:    beqz s4, .LBB7_58
; RV32-NEXT:  .LBB7_28:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_30
; RV32-NEXT:  .LBB7_29:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_30:
; RV32-NEXT:    sw a1, 96(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_32
; RV32-NEXT:  # %bb.31:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_32:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_59
; RV32-NEXT:  # %bb.33:
; RV32-NEXT:    sw a3, 92(sp)
; RV32-NEXT:    beqz s4, .LBB7_60
; RV32-NEXT:  .LBB7_34:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_36
; RV32-NEXT:  .LBB7_35:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_36:
; RV32-NEXT:    sw a1, 88(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_38
; RV32-NEXT:  # %bb.37:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_38:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_61
; RV32-NEXT:  # %bb.39:
; RV32-NEXT:    sw a3, 84(sp)
; RV32-NEXT:    beqz s4, .LBB7_62
; RV32-NEXT:  .LBB7_40:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB7_42
; RV32-NEXT:  .LBB7_41:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB7_42:
; RV32-NEXT:    sw a1, 80(sp)
; RV32-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV32-NEXT:    addi a0, sp, 160
; RV32-NEXT:    vl2re8.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s fs0, v8
; RV32-NEXT:    fle.s s4, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s4, .LBB7_44
; RV32-NEXT:  # %bb.43:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB7_44:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB7_63
; RV32-NEXT:  # %bb.45:
; RV32-NEXT:    sw a3, 76(sp)
; RV32-NEXT:    beqz s4, .LBB7_64
; RV32-NEXT:  .LBB7_46:
; RV32-NEXT:    bnez a2, .LBB7_48
; RV32-NEXT:  .LBB7_47:
; RV32-NEXT:    mv s3, a0
; RV32-NEXT:  .LBB7_48:
; RV32-NEXT:    sw s3, 72(sp)
; RV32-NEXT:    addi a0, sp, 64
; RV32-NEXT:    vsetivli zero, 16, e32, m4, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    vse64.v v8, (s2)
; RV32-NEXT:    addi sp, s0, -208
; RV32-NEXT:    lw ra, 204(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 200(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 196(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 192(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 188(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 176(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 168(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 160(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 208
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB7_49:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 68(sp)
; RV32-NEXT:    bnez s4, .LBB7_4
; RV32-NEXT:  .LBB7_50:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_5
; RV32-NEXT:    j .LBB7_6
; RV32-NEXT:  .LBB7_51:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 124(sp)
; RV32-NEXT:    bnez s4, .LBB7_10
; RV32-NEXT:  .LBB7_52:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_11
; RV32-NEXT:    j .LBB7_12
; RV32-NEXT:  .LBB7_53:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 116(sp)
; RV32-NEXT:    bnez s4, .LBB7_16
; RV32-NEXT:  .LBB7_54:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_17
; RV32-NEXT:    j .LBB7_18
; RV32-NEXT:  .LBB7_55:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 108(sp)
; RV32-NEXT:    bnez s4, .LBB7_22
; RV32-NEXT:  .LBB7_56:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_23
; RV32-NEXT:    j .LBB7_24
; RV32-NEXT:  .LBB7_57:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 100(sp)
; RV32-NEXT:    bnez s4, .LBB7_28
; RV32-NEXT:  .LBB7_58:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_29
; RV32-NEXT:    j .LBB7_30
; RV32-NEXT:  .LBB7_59:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 92(sp)
; RV32-NEXT:    bnez s4, .LBB7_34
; RV32-NEXT:  .LBB7_60:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_35
; RV32-NEXT:    j .LBB7_36
; RV32-NEXT:  .LBB7_61:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 84(sp)
; RV32-NEXT:    bnez s4, .LBB7_40
; RV32-NEXT:  .LBB7_62:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB7_41
; RV32-NEXT:    j .LBB7_42
; RV32-NEXT:  .LBB7_63:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 76(sp)
; RV32-NEXT:    bnez s4, .LBB7_46
; RV32-NEXT:  .LBB7_64:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    beqz a2, .LBB7_47
; RV32-NEXT:    j .LBB7_48
;
; RV64-LABEL: fp2ui_v8f32_v8i64:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -128
; RV64-NEXT:    .cfi_def_cfa_offset 128
; RV64-NEXT:    sd ra, 120(sp) # 8-byte Folded Spill
; RV64-NEXT:    sd s0, 112(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset s0, -16
; RV64-NEXT:    addi s0, sp, 128
; RV64-NEXT:    .cfi_def_cfa s0, 0
; RV64-NEXT:    andi sp, sp, -64
; RV64-NEXT:    vsetivli zero, 8, e32, m2, ta, mu
; RV64-NEXT:    vle32.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB7_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_2:
; RV64-NEXT:    sd a0, 0(sp)
; RV64-NEXT:    vsetivli zero, 1, e32, m2, ta, mu
; RV64-NEXT:    vslidedown.vi v10, v8, 7
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB7_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_4:
; RV64-NEXT:    vslidedown.vi v10, v8, 6
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 56(sp)
; RV64-NEXT:    bnez a2, .LBB7_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_7
; RV64-NEXT:  .LBB7_6:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_7:
; RV64-NEXT:    vslidedown.vi v10, v8, 5
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 48(sp)
; RV64-NEXT:    bnez a2, .LBB7_9
; RV64-NEXT:  # %bb.8:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_10
; RV64-NEXT:  .LBB7_9:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_10:
; RV64-NEXT:    vslidedown.vi v10, v8, 4
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 40(sp)
; RV64-NEXT:    bnez a2, .LBB7_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_13
; RV64-NEXT:  .LBB7_12:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_13:
; RV64-NEXT:    vslidedown.vi v10, v8, 3
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 32(sp)
; RV64-NEXT:    bnez a2, .LBB7_15
; RV64-NEXT:  # %bb.14:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_16
; RV64-NEXT:  .LBB7_15:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_16:
; RV64-NEXT:    vslidedown.vi v10, v8, 2
; RV64-NEXT:    vfmv.f.s ft0, v10
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 24(sp)
; RV64-NEXT:    bnez a2, .LBB7_18
; RV64-NEXT:  # %bb.17:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_19
; RV64-NEXT:  .LBB7_18:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_19:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.s a2, ft0, ft0
; RV64-NEXT:    sd a0, 16(sp)
; RV64-NEXT:    bnez a2, .LBB7_21
; RV64-NEXT:  # %bb.20:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB7_22
; RV64-NEXT:  .LBB7_21:
; RV64-NEXT:    fcvt.lu.s a0, ft0, rtz
; RV64-NEXT:  .LBB7_22:
; RV64-NEXT:    sd a0, 8(sp)
; RV64-NEXT:    mv a0, sp
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    addi sp, s0, -128
; RV64-NEXT:    ld ra, 120(sp) # 8-byte Folded Reload
; RV64-NEXT:    ld s0, 112(sp) # 8-byte Folded Reload
; RV64-NEXT:    addi sp, sp, 128
; RV64-NEXT:    ret
  %a = load <8 x float>, <8 x float>* %x
  %d = call <8 x i64> @llvm.fptoui.sat.v8i64.v8f32(<8 x float> %a)
  store <8 x i64> %d, <8 x i64>* %y
  ret void
}
declare <8 x i64> @llvm.fptoui.sat.v8i64.v8f32(<8 x float>)

define void @fp2si_v2f16_v2i64(<2 x half>* %x, <2 x i64>* %y) {
; RV32-LABEL: fp2si_v2f16_v2i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 64(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 56(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 48(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset fs0, -32
; RV32-NEXT:    .cfi_offset fs1, -40
; RV32-NEXT:    .cfi_offset fs2, -48
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI8_0)
; RV32-NEXT:    flw fs1, %lo(.LCPI8_0)(a0)
; RV32-NEXT:    mv s0, a1
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fcvt.s.h fs0, ft0
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    lui a3, 524288
; RV32-NEXT:    bnez s2, .LBB8_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB8_2:
; RV32-NEXT:    lui a2, %hi(.LCPI8_1)
; RV32-NEXT:    flw fs2, %lo(.LCPI8_1)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    addi s1, a3, -1
; RV32-NEXT:    mv a3, s1
; RV32-NEXT:    beqz a2, .LBB8_18
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB8_19
; RV32-NEXT:  .LBB8_4:
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    bnez s2, .LBB8_6
; RV32-NEXT:  .LBB8_5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:  .LBB8_6:
; RV32-NEXT:    li s2, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    bnez a2, .LBB8_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    mv a3, a0
; RV32-NEXT:  .LBB8_8:
; RV32-NEXT:    bnez a1, .LBB8_10
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:  .LBB8_10:
; RV32-NEXT:    sw a3, 16(sp)
; RV32-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fcvt.s.h fs0, ft0
; RV32-NEXT:    fle.s s3, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixsfdi@plt
; RV32-NEXT:    beqz s3, .LBB8_20
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    beqz a2, .LBB8_21
; RV32-NEXT:  .LBB8_12:
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    beqz a1, .LBB8_22
; RV32-NEXT:  .LBB8_13:
; RV32-NEXT:    sw s1, 28(sp)
; RV32-NEXT:    beqz s3, .LBB8_23
; RV32-NEXT:  .LBB8_14:
; RV32-NEXT:    beqz a2, .LBB8_24
; RV32-NEXT:  .LBB8_15:
; RV32-NEXT:    bnez a1, .LBB8_17
; RV32-NEXT:  .LBB8_16:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:  .LBB8_17:
; RV32-NEXT:    sw s2, 24(sp)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vse64.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 64(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 56(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 48(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB8_18:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB8_4
; RV32-NEXT:  .LBB8_19:
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    beqz s2, .LBB8_5
; RV32-NEXT:    j .LBB8_6
; RV32-NEXT:  .LBB8_20:
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    bnez a2, .LBB8_12
; RV32-NEXT:  .LBB8_21:
; RV32-NEXT:    mv s1, a1
; RV32-NEXT:    feq.s a1, fs0, fs0
; RV32-NEXT:    bnez a1, .LBB8_13
; RV32-NEXT:  .LBB8_22:
; RV32-NEXT:    li s1, 0
; RV32-NEXT:    sw s1, 28(sp)
; RV32-NEXT:    bnez s3, .LBB8_14
; RV32-NEXT:  .LBB8_23:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    bnez a2, .LBB8_15
; RV32-NEXT:  .LBB8_24:
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    beqz a1, .LBB8_16
; RV32-NEXT:    j .LBB8_17
;
; RV64-LABEL: fp2si_v2f16_v2i64:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.h a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB8_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.l.h a0, ft0, rtz
; RV64-NEXT:  .LBB8_2:
; RV64-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.h a2, ft0, ft0
; RV64-NEXT:    beqz a2, .LBB8_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.l.h a2, ft0, rtz
; RV64-NEXT:  .LBB8_4:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x half>, <2 x half>* %x
  %d = call <2 x i64> @llvm.fptosi.sat.v2i64.v2f16(<2 x half> %a)
  store <2 x i64> %d, <2 x i64>* %y
  ret void
}
declare <2 x i64> @llvm.fptosi.sat.v2i64.v2f16(<2 x half>)

define void @fp2ui_v2f16_v2i64(<2 x half>* %x, <2 x i64>* %y) {
; RV32-LABEL: fp2ui_v2f16_v2i64:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -96
; RV32-NEXT:    .cfi_def_cfa_offset 96
; RV32-NEXT:    sw ra, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 72(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs1, 64(sp) # 8-byte Folded Spill
; RV32-NEXT:    fsd fs2, 56(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset fs0, -24
; RV32-NEXT:    .cfi_offset fs1, -32
; RV32-NEXT:    .cfi_offset fs2, -40
; RV32-NEXT:    csrr a2, vlenb
; RV32-NEXT:    slli a2, a2, 1
; RV32-NEXT:    sub sp, sp, a2
; RV32-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; RV32-NEXT:    vle16.v v8, (a0)
; RV32-NEXT:    mv s0, a1
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fcvt.s.h fs0, ft0
; RV32-NEXT:    fmv.w.x fs1, zero
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s2, .LBB9_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB9_2:
; RV32-NEXT:    lui a2, %hi(.LCPI9_0)
; RV32-NEXT:    flw fs2, %lo(.LCPI9_0)(a2)
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li s1, -1
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB9_13
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    beqz s2, .LBB9_14
; RV32-NEXT:  .LBB9_4:
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    bnez a2, .LBB9_6
; RV32-NEXT:  .LBB9_5:
; RV32-NEXT:    mv a1, a0
; RV32-NEXT:  .LBB9_6:
; RV32-NEXT:    sw a1, 16(sp)
; RV32-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; RV32-NEXT:    addi a0, sp, 48
; RV32-NEXT:    vl1r.v v8, (a0) # Unknown-size Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft0, v8
; RV32-NEXT:    fcvt.s.h fs0, ft0
; RV32-NEXT:    fle.s s2, fs1, fs0
; RV32-NEXT:    fmv.s fa0, fs0
; RV32-NEXT:    call __fixunssfdi@plt
; RV32-NEXT:    bnez s2, .LBB9_8
; RV32-NEXT:  # %bb.7:
; RV32-NEXT:    li a1, 0
; RV32-NEXT:  .LBB9_8:
; RV32-NEXT:    flt.s a2, fs2, fs0
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beqz a2, .LBB9_15
; RV32-NEXT:  # %bb.9:
; RV32-NEXT:    sw a3, 28(sp)
; RV32-NEXT:    beqz s2, .LBB9_16
; RV32-NEXT:  .LBB9_10:
; RV32-NEXT:    bnez a2, .LBB9_12
; RV32-NEXT:  .LBB9_11:
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:  .LBB9_12:
; RV32-NEXT:    sw s1, 24(sp)
; RV32-NEXT:    addi a0, sp, 16
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, mu
; RV32-NEXT:    vle32.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vse64.v v8, (s0)
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    lw ra, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 72(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs1, 64(sp) # 8-byte Folded Reload
; RV32-NEXT:    fld fs2, 56(sp) # 8-byte Folded Reload
; RV32-NEXT:    addi sp, sp, 96
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB9_13:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 20(sp)
; RV32-NEXT:    bnez s2, .LBB9_4
; RV32-NEXT:  .LBB9_14:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, -1
; RV32-NEXT:    beqz a2, .LBB9_5
; RV32-NEXT:    j .LBB9_6
; RV32-NEXT:  .LBB9_15:
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    sw a3, 28(sp)
; RV32-NEXT:    bnez s2, .LBB9_10
; RV32-NEXT:  .LBB9_16:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    beqz a2, .LBB9_11
; RV32-NEXT:    j .LBB9_12
;
; RV64-LABEL: fp2ui_v2f16_v2i64:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e16, mf4, ta, mu
; RV64-NEXT:    vle16.v v8, (a0)
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.h a0, ft0, ft0
; RV64-NEXT:    beqz a0, .LBB9_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fcvt.lu.h a0, ft0, rtz
; RV64-NEXT:  .LBB9_2:
; RV64-NEXT:    vsetivli zero, 1, e16, mf4, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft0, v8
; RV64-NEXT:    feq.h a2, ft0, ft0
; RV64-NEXT:    beqz a2, .LBB9_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fcvt.lu.h a2, ft0, rtz
; RV64-NEXT:  .LBB9_4:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e64, m1, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse64.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x half>, <2 x half>* %x
  %d = call <2 x i64> @llvm.fptoui.sat.v2i64.v2f16(<2 x half> %a)
  store <2 x i64> %d, <2 x i64>* %y
  ret void
}
declare <2 x i64> @llvm.fptoui.sat.v2i64.v2f16(<2 x half>)

define void @fp2si_v2f64_v2i8(<2 x double>* %x, <2 x i8>* %y) {
; RV32-LABEL: fp2si_v2f64_v2i8:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v9, v8, 1
; RV32-NEXT:    lui a0, %hi(.LCPI10_0)
; RV32-NEXT:    fld ft0, %lo(.LCPI10_0)(a0)
; RV32-NEXT:    lui a0, %hi(.LCPI10_1)
; RV32-NEXT:    fld ft1, %lo(.LCPI10_1)(a0)
; RV32-NEXT:    vfmv.f.s ft2, v9
; RV32-NEXT:    feq.d a0, ft2, ft2
; RV32-NEXT:    beqz a0, .LBB10_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB10_2:
; RV32-NEXT:    vsetivli zero, 2, e8, mf8, ta, mu
; RV32-NEXT:    vmv.v.x v9, a0
; RV32-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; RV32-NEXT:    vfmv.f.s ft2, v8
; RV32-NEXT:    feq.d a0, ft2, ft2
; RV32-NEXT:    beqz a0, .LBB10_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fmax.d ft0, ft2, ft0
; RV32-NEXT:    fmin.d ft0, ft0, ft1
; RV32-NEXT:    fcvt.w.d a0, ft0, rtz
; RV32-NEXT:  .LBB10_4:
; RV32-NEXT:    vsetivli zero, 2, e8, mf8, tu, mu
; RV32-NEXT:    vmv.s.x v9, a0
; RV32-NEXT:    vse8.v v9, (a1)
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2si_v2f64_v2i8:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v9, v8, 1
; RV64-NEXT:    lui a0, %hi(.LCPI10_0)
; RV64-NEXT:    fld ft0, %lo(.LCPI10_0)(a0)
; RV64-NEXT:    lui a0, %hi(.LCPI10_1)
; RV64-NEXT:    fld ft1, %lo(.LCPI10_1)(a0)
; RV64-NEXT:    vfmv.f.s ft2, v9
; RV64-NEXT:    feq.d a0, ft2, ft2
; RV64-NEXT:    beqz a0, .LBB10_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB10_2:
; RV64-NEXT:    vsetivli zero, 2, e8, mf8, ta, mu
; RV64-NEXT:    vmv.v.x v9, a0
; RV64-NEXT:    vsetvli zero, zero, e64, m1, ta, mu
; RV64-NEXT:    vfmv.f.s ft2, v8
; RV64-NEXT:    feq.d a0, ft2, ft2
; RV64-NEXT:    beqz a0, .LBB10_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fmax.d ft0, ft2, ft0
; RV64-NEXT:    fmin.d ft0, ft0, ft1
; RV64-NEXT:    fcvt.l.d a0, ft0, rtz
; RV64-NEXT:  .LBB10_4:
; RV64-NEXT:    vsetivli zero, 2, e8, mf8, tu, mu
; RV64-NEXT:    vmv.s.x v9, a0
; RV64-NEXT:    vse8.v v9, (a1)
; RV64-NEXT:    ret
  %a = load <2 x double>, <2 x double>* %x
  %d = call <2 x i8> @llvm.fptosi.sat.v2i8.v2f64(<2 x double> %a)
  store <2 x i8> %d, <2 x i8>* %y
  ret void
}
declare <2 x i8> @llvm.fptosi.sat.v2i8.v2f64(<2 x double>)

define void @fp2ui_v2f64_v2i8(<2 x double>* %x, <2 x i8>* %y) {
; RV32-LABEL: fp2ui_v2f64_v2i8:
; RV32:       # %bb.0:
; RV32-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI11_0)
; RV32-NEXT:    fld ft0, %lo(.LCPI11_0)(a0)
; RV32-NEXT:    vfmv.f.s ft1, v8
; RV32-NEXT:    fcvt.d.w ft2, zero
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft1, v8
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft0, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a2, ft0, rtz
; RV32-NEXT:    vsetivli zero, 2, e8, mf8, ta, mu
; RV32-NEXT:    vmv.v.x v8, a2
; RV32-NEXT:    vsetvli zero, zero, e8, mf8, tu, mu
; RV32-NEXT:    vmv.s.x v8, a0
; RV32-NEXT:    vse8.v v8, (a1)
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2ui_v2f64_v2i8:
; RV64:       # %bb.0:
; RV64-NEXT:    vsetivli zero, 2, e64, m1, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    lui a0, %hi(.LCPI11_0)
; RV64-NEXT:    fld ft0, %lo(.LCPI11_0)(a0)
; RV64-NEXT:    vfmv.f.s ft1, v8
; RV64-NEXT:    fmv.d.x ft2, zero
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    vsetivli zero, 1, e64, m1, ta, mu
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft1, v8
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft0, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a2, ft0, rtz
; RV64-NEXT:    vsetivli zero, 2, e8, mf8, ta, mu
; RV64-NEXT:    vmv.v.x v8, a2
; RV64-NEXT:    vsetvli zero, zero, e8, mf8, tu, mu
; RV64-NEXT:    vmv.s.x v8, a0
; RV64-NEXT:    vse8.v v8, (a1)
; RV64-NEXT:    ret
  %a = load <2 x double>, <2 x double>* %x
  %d = call <2 x i8> @llvm.fptoui.sat.v2i8.v2f64(<2 x double> %a)
  store <2 x i8> %d, <2 x i8>* %y
  ret void
}
declare <2 x i8> @llvm.fptoui.sat.v2i8.v2f64(<2 x double>)

define void @fp2si_v8f64_v8i8(<8 x double>* %x, <8 x i8>* %y) {
;
; RV32-LABEL: fp2si_v8f64_v8i8:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI12_0)
; RV32-NEXT:    fld ft0, %lo(.LCPI12_0)(a0)
; RV32-NEXT:    lui a0, %hi(.LCPI12_1)
; RV32-NEXT:    fld ft1, %lo(.LCPI12_1)(a0)
; RV32-NEXT:    vfmv.f.s ft2, v8
; RV32-NEXT:    feq.d a0, ft2, ft2
; RV32-NEXT:    beqz a0, .LBB12_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_2:
; RV32-NEXT:    sb a0, 8(sp)
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v12, v8, 7
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a0, ft2, ft2
; RV32-NEXT:    beqz a0, .LBB12_4
; RV32-NEXT:  # %bb.3:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_4:
; RV32-NEXT:    vslidedown.vi v12, v8, 6
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 15(sp)
; RV32-NEXT:    bnez a2, .LBB12_6
; RV32-NEXT:  # %bb.5:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_7
; RV32-NEXT:  .LBB12_6:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_7:
; RV32-NEXT:    vslidedown.vi v12, v8, 5
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 14(sp)
; RV32-NEXT:    bnez a2, .LBB12_9
; RV32-NEXT:  # %bb.8:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_10
; RV32-NEXT:  .LBB12_9:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_10:
; RV32-NEXT:    vslidedown.vi v12, v8, 4
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 13(sp)
; RV32-NEXT:    bnez a2, .LBB12_12
; RV32-NEXT:  # %bb.11:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_13
; RV32-NEXT:  .LBB12_12:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_13:
; RV32-NEXT:    vslidedown.vi v12, v8, 3
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 12(sp)
; RV32-NEXT:    bnez a2, .LBB12_15
; RV32-NEXT:  # %bb.14:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_16
; RV32-NEXT:  .LBB12_15:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_16:
; RV32-NEXT:    vslidedown.vi v12, v8, 2
; RV32-NEXT:    vfmv.f.s ft2, v12
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 11(sp)
; RV32-NEXT:    bnez a2, .LBB12_18
; RV32-NEXT:  # %bb.17:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_19
; RV32-NEXT:  .LBB12_18:
; RV32-NEXT:    fmax.d ft2, ft2, ft0
; RV32-NEXT:    fmin.d ft2, ft2, ft1
; RV32-NEXT:    fcvt.w.d a0, ft2, rtz
; RV32-NEXT:  .LBB12_19:
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft2, v8
; RV32-NEXT:    feq.d a2, ft2, ft2
; RV32-NEXT:    sb a0, 10(sp)
; RV32-NEXT:    bnez a2, .LBB12_21
; RV32-NEXT:  # %bb.20:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB12_22
; RV32-NEXT:  .LBB12_21:
; RV32-NEXT:    fmax.d ft0, ft2, ft0
; RV32-NEXT:    fmin.d ft0, ft0, ft1
; RV32-NEXT:    fcvt.w.d a0, ft0, rtz
; RV32-NEXT:  .LBB12_22:
; RV32-NEXT:    sb a0, 9(sp)
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vsetivli zero, 8, e8, mf2, ta, mu
; RV32-NEXT:    vle8.v v8, (a0)
; RV32-NEXT:    vse8.v v8, (a1)
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2si_v8f64_v8i8:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    lui a0, %hi(.LCPI12_0)
; RV64-NEXT:    fld ft0, %lo(.LCPI12_0)(a0)
; RV64-NEXT:    lui a0, %hi(.LCPI12_1)
; RV64-NEXT:    fld ft1, %lo(.LCPI12_1)(a0)
; RV64-NEXT:    vfmv.f.s ft2, v8
; RV64-NEXT:    feq.d a0, ft2, ft2
; RV64-NEXT:    beqz a0, .LBB12_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_2:
; RV64-NEXT:    sb a0, 8(sp)
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v12, v8, 7
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a0, ft2, ft2
; RV64-NEXT:    beqz a0, .LBB12_4
; RV64-NEXT:  # %bb.3:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_4:
; RV64-NEXT:    vslidedown.vi v12, v8, 6
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 15(sp)
; RV64-NEXT:    bnez a2, .LBB12_6
; RV64-NEXT:  # %bb.5:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_7
; RV64-NEXT:  .LBB12_6:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_7:
; RV64-NEXT:    vslidedown.vi v12, v8, 5
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 14(sp)
; RV64-NEXT:    bnez a2, .LBB12_9
; RV64-NEXT:  # %bb.8:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_10
; RV64-NEXT:  .LBB12_9:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_10:
; RV64-NEXT:    vslidedown.vi v12, v8, 4
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 13(sp)
; RV64-NEXT:    bnez a2, .LBB12_12
; RV64-NEXT:  # %bb.11:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_13
; RV64-NEXT:  .LBB12_12:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_13:
; RV64-NEXT:    vslidedown.vi v12, v8, 3
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 12(sp)
; RV64-NEXT:    bnez a2, .LBB12_15
; RV64-NEXT:  # %bb.14:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_16
; RV64-NEXT:  .LBB12_15:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_16:
; RV64-NEXT:    vslidedown.vi v12, v8, 2
; RV64-NEXT:    vfmv.f.s ft2, v12
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 11(sp)
; RV64-NEXT:    bnez a2, .LBB12_18
; RV64-NEXT:  # %bb.17:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_19
; RV64-NEXT:  .LBB12_18:
; RV64-NEXT:    fmax.d ft2, ft2, ft0
; RV64-NEXT:    fmin.d ft2, ft2, ft1
; RV64-NEXT:    fcvt.l.d a0, ft2, rtz
; RV64-NEXT:  .LBB12_19:
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft2, v8
; RV64-NEXT:    feq.d a2, ft2, ft2
; RV64-NEXT:    sb a0, 10(sp)
; RV64-NEXT:    bnez a2, .LBB12_21
; RV64-NEXT:  # %bb.20:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    j .LBB12_22
; RV64-NEXT:  .LBB12_21:
; RV64-NEXT:    fmax.d ft0, ft2, ft0
; RV64-NEXT:    fmin.d ft0, ft0, ft1
; RV64-NEXT:    fcvt.l.d a0, ft0, rtz
; RV64-NEXT:  .LBB12_22:
; RV64-NEXT:    sb a0, 9(sp)
; RV64-NEXT:    addi a0, sp, 8
; RV64-NEXT:    vsetivli zero, 8, e8, mf2, ta, mu
; RV64-NEXT:    vle8.v v8, (a0)
; RV64-NEXT:    vse8.v v8, (a1)
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    ret
  %a = load <8 x double>, <8 x double>* %x
  %d = call <8 x i8> @llvm.fptosi.sat.v8i8.v8f64(<8 x double> %a)
  store <8 x i8> %d, <8 x i8>* %y
  ret void
}
declare <8 x i8> @llvm.fptosi.sat.v8i8.v8f64(<8 x double>)

define void @fp2ui_v8f64_v8i8(<8 x double>* %x, <8 x i8>* %y) {
;
; RV32-LABEL: fp2ui_v8f64_v8i8:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV32-NEXT:    vle64.v v8, (a0)
; RV32-NEXT:    lui a0, %hi(.LCPI13_0)
; RV32-NEXT:    fld ft0, %lo(.LCPI13_0)(a0)
; RV32-NEXT:    vfmv.f.s ft1, v8
; RV32-NEXT:    fcvt.d.w ft2, zero
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 8(sp)
; RV32-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV32-NEXT:    vslidedown.vi v12, v8, 7
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 15(sp)
; RV32-NEXT:    vslidedown.vi v12, v8, 6
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 14(sp)
; RV32-NEXT:    vslidedown.vi v12, v8, 5
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 13(sp)
; RV32-NEXT:    vslidedown.vi v12, v8, 4
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 12(sp)
; RV32-NEXT:    vslidedown.vi v12, v8, 3
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 11(sp)
; RV32-NEXT:    vslidedown.vi v12, v8, 2
; RV32-NEXT:    vfmv.f.s ft1, v12
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft1, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft1, rtz
; RV32-NEXT:    sb a0, 10(sp)
; RV32-NEXT:    vslidedown.vi v8, v8, 1
; RV32-NEXT:    vfmv.f.s ft1, v8
; RV32-NEXT:    fmax.d ft1, ft1, ft2
; RV32-NEXT:    fmin.d ft0, ft1, ft0
; RV32-NEXT:    fcvt.wu.d a0, ft0, rtz
; RV32-NEXT:    sb a0, 9(sp)
; RV32-NEXT:    addi a0, sp, 8
; RV32-NEXT:    vsetivli zero, 8, e8, mf2, ta, mu
; RV32-NEXT:    vle8.v v8, (a0)
; RV32-NEXT:    vse8.v v8, (a1)
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    ret
;
; RV64-LABEL: fp2ui_v8f64_v8i8:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    vsetivli zero, 8, e64, m4, ta, mu
; RV64-NEXT:    vle64.v v8, (a0)
; RV64-NEXT:    lui a0, %hi(.LCPI13_0)
; RV64-NEXT:    fld ft0, %lo(.LCPI13_0)(a0)
; RV64-NEXT:    vfmv.f.s ft1, v8
; RV64-NEXT:    fmv.d.x ft2, zero
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 8(sp)
; RV64-NEXT:    vsetivli zero, 1, e64, m4, ta, mu
; RV64-NEXT:    vslidedown.vi v12, v8, 7
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 15(sp)
; RV64-NEXT:    vslidedown.vi v12, v8, 6
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 14(sp)
; RV64-NEXT:    vslidedown.vi v12, v8, 5
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 13(sp)
; RV64-NEXT:    vslidedown.vi v12, v8, 4
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 12(sp)
; RV64-NEXT:    vslidedown.vi v12, v8, 3
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 11(sp)
; RV64-NEXT:    vslidedown.vi v12, v8, 2
; RV64-NEXT:    vfmv.f.s ft1, v12
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft1, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft1, rtz
; RV64-NEXT:    sb a0, 10(sp)
; RV64-NEXT:    vslidedown.vi v8, v8, 1
; RV64-NEXT:    vfmv.f.s ft1, v8
; RV64-NEXT:    fmax.d ft1, ft1, ft2
; RV64-NEXT:    fmin.d ft0, ft1, ft0
; RV64-NEXT:    fcvt.lu.d a0, ft0, rtz
; RV64-NEXT:    sb a0, 9(sp)
; RV64-NEXT:    addi a0, sp, 8
; RV64-NEXT:    vsetivli zero, 8, e8, mf2, ta, mu
; RV64-NEXT:    vle8.v v8, (a0)
; RV64-NEXT:    vse8.v v8, (a1)
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    ret
  %a = load <8 x double>, <8 x double>* %x
  %d = call <8 x i8> @llvm.fptoui.sat.v8i8.v8f64(<8 x double> %a)
  store <8 x i8> %d, <8 x i8>* %y
  ret void
}
declare <8 x i8> @llvm.fptoui.sat.v8i8.v8f64(<8 x double> %a)
