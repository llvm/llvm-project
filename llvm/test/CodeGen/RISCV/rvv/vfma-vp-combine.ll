; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+d,+zvfh,+v,+m -target-abi=ilp32d \
; RUN:     -verify-machineinstrs < %s | FileCheck --check-prefixes=CHECK,RV32 %s
; RUN: llc -mtriple=riscv64 -mattr=+d,+zvfh,+v,+m -target-abi=lp64d \
; RUN:     -verify-machineinstrs < %s | FileCheck --check-prefixes=CHECK,RV64 %s

declare <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fneg.nxv1f64(<vscale x 1 x double>, <vscale x 1 x i1>, i32)
declare <vscale x 1 x double> @llvm.vp.fmul.nxv1f64(<vscale x 1 x double>, <vscale x 1 x double>, <vscale x 1 x i1>, i32)

; (-N0 * -N1) + N2 --> (N0 * N1) + N2
define <vscale x 1 x double> @test1(<vscale x 1 x double> %a, <vscale x 1 x double> %b, <vscale x 1 x double> %c, <vscale x 1 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: test1:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; CHECK-NEXT:    vfmadd.vv v9, v8, v10, v0.t
; CHECK-NEXT:    vmv.v.v v8, v9
; CHECK-NEXT:    ret
  %nega = call <vscale x 1 x double> @llvm.vp.fneg.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x i1> %m, i32 %evl)
  %negb = call <vscale x 1 x double> @llvm.vp.fneg.nxv1f64(<vscale x 1 x double> %b, <vscale x 1 x i1> %m, i32 %evl)
  %v = call <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double> %nega, <vscale x 1 x double> %negb, <vscale x 1 x double> %c, <vscale x 1 x i1> %m, i32 %evl)
  ret <vscale x 1 x double> %v
}

; (fma x, c1, (fmul x, c2)) -> (fmul x, c1+c2)
define <vscale x 1 x double> @test2(<vscale x 1 x double> %a, <vscale x 1 x i1> %m, i32 zeroext %evl) {
; RV32-LABEL: test2:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a1, %hi(.LCPI1_0)
; RV32-NEXT:    fld fa5, %lo(.LCPI1_0)(a1)
; RV32-NEXT:    lui a1, %hi(.LCPI1_1)
; RV32-NEXT:    fld fa4, %lo(.LCPI1_1)(a1)
; RV32-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; RV32-NEXT:    vfmv.v.f v9, fa5
; RV32-NEXT:    vfadd.vf v9, v9, fa4, v0.t
; RV32-NEXT:    vfmul.vv v8, v8, v9, v0.t
; RV32-NEXT:    ret
;
; RV64-LABEL: test2:
; RV64:       # %bb.0:
; RV64-NEXT:    li a1, 1025
; RV64-NEXT:    slli a1, a1, 52
; RV64-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; RV64-NEXT:    vmv.v.x v9, a1
; RV64-NEXT:    li a0, 1
; RV64-NEXT:    slli a0, a0, 62
; RV64-NEXT:    fmv.d.x fa5, a0
; RV64-NEXT:    vfadd.vf v9, v9, fa5, v0.t
; RV64-NEXT:    vfmul.vv v8, v8, v9, v0.t
; RV64-NEXT:    ret
  %t = call <vscale x 1 x double> @llvm.vp.fmul.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> splat (double 2.0), <vscale x 1 x i1> %m, i32 %evl)
  %v = call fast <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> splat (double 4.0), <vscale x 1 x double> %t, <vscale x 1 x i1> %m, i32 %evl)
  ret <vscale x 1 x double> %v
}

; (fma (fmul x, c1), c2, y) -> (fma x, c1*c2, y)
define <vscale x 1 x double> @test3(<vscale x 1 x double> %a, <vscale x 1 x double> %b, <vscale x 1 x i1> %m, i32 zeroext %evl) {
; RV32-LABEL: test3:
; RV32:       # %bb.0:
; RV32-NEXT:    lui a1, %hi(.LCPI2_0)
; RV32-NEXT:    fld fa5, %lo(.LCPI2_0)(a1)
; RV32-NEXT:    lui a1, %hi(.LCPI2_1)
; RV32-NEXT:    fld fa4, %lo(.LCPI2_1)(a1)
; RV32-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; RV32-NEXT:    vfmv.v.f v10, fa5
; RV32-NEXT:    vfmul.vf v10, v10, fa4, v0.t
; RV32-NEXT:    vfmadd.vv v10, v8, v9, v0.t
; RV32-NEXT:    vmv.v.v v8, v10
; RV32-NEXT:    ret
;
; RV64-LABEL: test3:
; RV64:       # %bb.0:
; RV64-NEXT:    li a1, 1025
; RV64-NEXT:    slli a1, a1, 52
; RV64-NEXT:    vsetvli zero, a0, e64, m1, ta, ma
; RV64-NEXT:    vmv.v.x v10, a1
; RV64-NEXT:    li a0, 1
; RV64-NEXT:    slli a0, a0, 62
; RV64-NEXT:    fmv.d.x fa5, a0
; RV64-NEXT:    vfmul.vf v10, v10, fa5, v0.t
; RV64-NEXT:    vfmadd.vv v10, v8, v9, v0.t
; RV64-NEXT:    vmv.v.v v8, v10
; RV64-NEXT:    ret
  %t = call <vscale x 1 x double> @llvm.vp.fmul.nxv1f64(<vscale x 1 x double> %a, <vscale x 1 x double> splat (double 2.0), <vscale x 1 x i1> %m, i32 %evl)
  %v = call fast <vscale x 1 x double> @llvm.vp.fma.nxv1f64(<vscale x 1 x double> %t, <vscale x 1 x double> splat (double 4.0), <vscale x 1 x double> %b, <vscale x 1 x i1> %m, i32 %evl)
  ret <vscale x 1 x double> %v
}
