; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+d,+zvfh,+v -target-abi=ilp32d \
; RUN:     -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV32
; RUN: llc -mtriple=riscv64 -mattr=+d,+zvfh,+v -target-abi=lp64d \
; RUN:     -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,RV64

; Test that we can remove trivially-poison VP operations of various kinds.

define <4 x i32> @vload_v4i32_zero_evl(ptr %ptr, <4 x i1> %m) {
; CHECK-LABEL: vload_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %v = call <4 x i32> @llvm.vp.load.v4i32.p0(ptr %ptr, <4 x i1> %m, i32 0)
  ret <4 x i32> %v
}

define <4 x i32> @vload_v4i32_false_mask(ptr %ptr, i32 %evl) {
; CHECK-LABEL: vload_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %v = call <4 x i32> @llvm.vp.load.v4i32.p0(ptr %ptr, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %v
}

define <4 x i32> @vgather_v4i32_v4i32_zero_evl(<4 x ptr> %ptrs, <4 x i1> %m) {
; CHECK-LABEL: vgather_v4i32_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %v = call <4 x i32> @llvm.vp.gather.v4i32.v4p0(<4 x ptr> %ptrs, <4 x i1> %m, i32 0)
  ret <4 x i32> %v
}

define <4 x i32> @vgather_v4i32_v4i32_false_mask(<4 x ptr> %ptrs, i32 %evl) {
; CHECK-LABEL: vgather_v4i32_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %v = call <4 x i32> @llvm.vp.gather.v4i32.v4p0(<4 x ptr> %ptrs, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %v
}

define void @vstore_v4i32_zero_evl(<4 x i32> %val, ptr %ptr, <4 x i1> %m) {
; CHECK-LABEL: vstore_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  call void @llvm.vp.store.v4i32.p0(<4 x i32> %val, ptr %ptr, <4 x i1> %m, i32 0)
  ret void
}

define void @vstore_v4i32_false_mask(<4 x i32> %val, ptr %ptr, i32 %evl) {
; CHECK-LABEL: vstore_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  call void @llvm.vp.store.v4i32.p0(<4 x i32> %val, ptr %ptr, <4 x i1> zeroinitializer, i32 %evl)
  ret void
}

define void @vscatter_v4i32_zero_evl(<4 x i32> %val, <4 x ptr> %ptrs, <4 x i1> %m) {
; CHECK-LABEL: vscatter_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  call void @llvm.vp.scatter.v4i32.v4p0(<4 x i32> %val, <4 x ptr> %ptrs, <4 x i1> %m, i32 0)
  ret void
}

define void @vscatter_v4i32_false_mask(<4 x i32> %val, <4 x ptr> %ptrs, i32 %evl) {
; CHECK-LABEL: vscatter_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  call void @llvm.vp.scatter.v4i32.v4p0(<4 x i32> %val, <4 x ptr> %ptrs, <4 x i1> zeroinitializer, i32 %evl)
  ret void
}

define <4 x i32> @vadd_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vadd_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vadd.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.add.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vadd_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vadd_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vadd.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.add.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vand_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vand_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vand.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.and.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vand_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vand_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vand.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.and.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vlshr_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vlshr_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vsrl.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.lshr.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vlshr_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vlshr_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vsrl.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.lshr.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vmul_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vmul_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vmul.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.mul.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vmul_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vmul_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vmul.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.mul.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vor_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vor_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vor.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.or.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vor_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vor_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vor.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.or.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vsdiv_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vsdiv_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.sdiv.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vsdiv_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vsdiv_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.sdiv.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vsrem_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vsrem_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.srem.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vsrem_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vsrem_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.srem.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vsub_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vsub_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vsub.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.sub.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vsub_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vsub_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vsub.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.sub.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vudiv_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vudiv_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.udiv.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vudiv_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vudiv_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.udiv.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vurem_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vurem_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.urem.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vurem_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vurem_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.urem.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x i32> @vxor_v4i32_zero_evl(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m) {
; CHECK-LABEL: vxor_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.xor.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> %m, i32 0)
  ret <4 x i32> %s
}

define <4 x i32> @vxor_v4i32_false_mask(<4 x i32> %va, <4 x i32> %vb, i32 %evl) {
; CHECK-LABEL: vxor_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vxor.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x i32> @llvm.vp.xor.v4i32(<4 x i32> %va, <4 x i32> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x i32> %s
}

define <4 x float> @vfadd_v4f32_zero_evl(<4 x float> %va, <4 x float> %vb, <4 x i1> %m) {
; CHECK-LABEL: vfadd_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfadd.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fadd.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> %m, i32 0)
  ret <4 x float> %s
}

define <4 x float> @vfadd_v4f32_false_mask(<4 x float> %va, <4 x float> %vb, i32 %evl) {
; CHECK-LABEL: vfadd_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfadd.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fadd.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x float> %s
}

define <4 x float> @vfsub_v4f32_zero_evl(<4 x float> %va, <4 x float> %vb, <4 x i1> %m) {
; CHECK-LABEL: vfsub_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfsub.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fsub.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> %m, i32 0)
  ret <4 x float> %s
}

define <4 x float> @vfsub_v4f32_false_mask(<4 x float> %va, <4 x float> %vb, i32 %evl) {
; CHECK-LABEL: vfsub_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfsub.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fsub.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x float> %s
}

define <4 x float> @vfmul_v4f32_zero_evl(<4 x float> %va, <4 x float> %vb, <4 x i1> %m) {
; CHECK-LABEL: vfmul_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfmul.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fmul.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> %m, i32 0)
  ret <4 x float> %s
}

define <4 x float> @vfmul_v4f32_false_mask(<4 x float> %va, <4 x float> %vb, i32 %evl) {
; CHECK-LABEL: vfmul_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfmul.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fmul.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x float> %s
}

define <4 x float> @vfdiv_v4f32_zero_evl(<4 x float> %va, <4 x float> %vb, <4 x i1> %m) {
; CHECK-LABEL: vfdiv_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfdiv.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fdiv.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> %m, i32 0)
  ret <4 x float> %s
}

define <4 x float> @vfdiv_v4f32_false_mask(<4 x float> %va, <4 x float> %vb, i32 %evl) {
; CHECK-LABEL: vfdiv_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vfdiv.vv v8, v8, v9
; CHECK-NEXT:    ret
  %s = call <4 x float> @llvm.vp.fdiv.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x float> %s
}

define <4 x float> @vfrem_v4f32_zero_evl(<4 x float> %va, <4 x float> %vb, <4 x i1> %m) {
; RV32-LABEL: vfrem_v4f32_zero_evl:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset fs0, -16
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    sub sp, sp, a0
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e32, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v10, v8, 1
; RV32-NEXT:    vfmv.f.s fa0, v10
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vfmv.f.s fa1, v8
; RV32-NEXT:    call fmodf
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    flw fa0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    flw fa1, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    call fmodf
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfmv.v.f v8, fa0
; RV32-NEXT:    vfslide1down.vf v8, v8, fs0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fa0, v8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fa1, v8
; RV32-NEXT:    call fmodf
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfslide1down.vf v8, v8, fa0
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 3
; RV32-NEXT:    vfmv.f.s fa0, v8
; RV32-NEXT:    vfmv.f.s fa1, v9
; RV32-NEXT:    call fmodf
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfslide1down.vf v8, v8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    .cfi_def_cfa sp, 48
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore fs0
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: vfrem_v4f32_zero_evl:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    .cfi_def_cfa_offset 32
; RV64-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset fs0, -16
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    sub sp, sp, a0
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x20, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 32 + 3 * vlenb
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v9, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e32, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v10, v8, 1
; RV64-NEXT:    vfmv.f.s fa0, v10
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vfmv.f.s fa1, v8
; RV64-NEXT:    call fmodf
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    flw fa0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    flw fa1, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    call fmodf
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfmv.v.f v8, fa0
; RV64-NEXT:    vfslide1down.vf v8, v8, fs0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vfmv.f.s fa0, v8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vfmv.f.s fa1, v8
; RV64-NEXT:    call fmodf
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfslide1down.vf v8, v8, fa0
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 3
; RV64-NEXT:    vfmv.f.s fa0, v8
; RV64-NEXT:    vfmv.f.s fa1, v9
; RV64-NEXT:    call fmodf
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfslide1down.vf v8, v8, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    .cfi_def_cfa sp, 32
; RV64-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    .cfi_restore fs0
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
  %s = call <4 x float> @llvm.vp.frem.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> %m, i32 0)
  ret <4 x float> %s
}

define <4 x float> @vfrem_v4f32_false_mask(<4 x float> %va, <4 x float> %vb, i32 %evl) {
; RV32-LABEL: vfrem_v4f32_false_mask:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -48
; RV32-NEXT:    .cfi_def_cfa_offset 48
; RV32-NEXT:    sw ra, 44(sp) # 4-byte Folded Spill
; RV32-NEXT:    fsd fs0, 32(sp) # 8-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset fs0, -16
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    sub sp, sp, a0
; RV32-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x30, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 48 + 3 * vlenb
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v9, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    vsetivli zero, 1, e32, m1, ta, ma
; RV32-NEXT:    vslidedown.vi v10, v8, 1
; RV32-NEXT:    vfmv.f.s fa0, v10
; RV32-NEXT:    vslidedown.vi v8, v9, 1
; RV32-NEXT:    vfmv.f.s fa1, v8
; RV32-NEXT:    call fmodf
; RV32-NEXT:    fmv.s fs0, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    flw fa0, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    flw fa1, 32(a0) # 8-byte Folded Reload
; RV32-NEXT:    call fmodf
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfmv.v.f v8, fa0
; RV32-NEXT:    vfslide1down.vf v8, v8, fs0
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fa0, v8
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 2
; RV32-NEXT:    vfmv.f.s fa1, v8
; RV32-NEXT:    call fmodf
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfslide1down.vf v8, v8, fa0
; RV32-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v8, v8, 3
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    add a0, sp, a0
; RV32-NEXT:    addi a0, a0, 32
; RV32-NEXT:    vl1r.v v9, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vslidedown.vi v9, v9, 3
; RV32-NEXT:    vfmv.f.s fa0, v8
; RV32-NEXT:    vfmv.f.s fa1, v9
; RV32-NEXT:    call fmodf
; RV32-NEXT:    addi a0, sp, 32
; RV32-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV32-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV32-NEXT:    vfslide1down.vf v8, v8, fa0
; RV32-NEXT:    csrr a0, vlenb
; RV32-NEXT:    slli a1, a0, 1
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add sp, sp, a0
; RV32-NEXT:    .cfi_def_cfa sp, 48
; RV32-NEXT:    lw ra, 44(sp) # 4-byte Folded Reload
; RV32-NEXT:    fld fs0, 32(sp) # 8-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore fs0
; RV32-NEXT:    addi sp, sp, 48
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: vfrem_v4f32_false_mask:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -32
; RV64-NEXT:    .cfi_def_cfa_offset 32
; RV64-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64-NEXT:    fsd fs0, 16(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    .cfi_offset fs0, -16
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    sub sp, sp, a0
; RV64-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x20, 0x22, 0x11, 0x03, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 32 + 3 * vlenb
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v9, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    vsetivli zero, 1, e32, m1, ta, ma
; RV64-NEXT:    vslidedown.vi v10, v8, 1
; RV64-NEXT:    vfmv.f.s fa0, v10
; RV64-NEXT:    vslidedown.vi v8, v9, 1
; RV64-NEXT:    vfmv.f.s fa1, v8
; RV64-NEXT:    call fmodf
; RV64-NEXT:    fmv.s fs0, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    flw fa0, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    flw fa1, 16(a0) # 8-byte Folded Reload
; RV64-NEXT:    call fmodf
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfmv.v.f v8, fa0
; RV64-NEXT:    vfslide1down.vf v8, v8, fs0
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vfmv.f.s fa0, v8
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 2
; RV64-NEXT:    vfmv.f.s fa1, v8
; RV64-NEXT:    call fmodf
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfslide1down.vf v8, v8, fa0
; RV64-NEXT:    vs1r.v v8, (a0) # vscale x 8-byte Folded Spill
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v8, v8, 3
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a0, a0, 1
; RV64-NEXT:    add a0, sp, a0
; RV64-NEXT:    addi a0, a0, 16
; RV64-NEXT:    vl1r.v v9, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vslidedown.vi v9, v9, 3
; RV64-NEXT:    vfmv.f.s fa0, v8
; RV64-NEXT:    vfmv.f.s fa1, v9
; RV64-NEXT:    call fmodf
; RV64-NEXT:    addi a0, sp, 16
; RV64-NEXT:    vl1r.v v8, (a0) # vscale x 8-byte Folded Reload
; RV64-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; RV64-NEXT:    vfslide1down.vf v8, v8, fa0
; RV64-NEXT:    csrr a0, vlenb
; RV64-NEXT:    slli a1, a0, 1
; RV64-NEXT:    add a0, a1, a0
; RV64-NEXT:    add sp, sp, a0
; RV64-NEXT:    .cfi_def_cfa sp, 32
; RV64-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64-NEXT:    fld fs0, 16(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    .cfi_restore fs0
; RV64-NEXT:    addi sp, sp, 32
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
  %s = call <4 x float> @llvm.vp.frem.v4f32(<4 x float> %va, <4 x float> %vb, <4 x i1> zeroinitializer, i32 %evl)
  ret <4 x float> %s
}

define i32 @vreduce_add_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_add_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.add.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_add_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_add_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.add.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_mul_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_mul_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.mul.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_mul_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_mul_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.mul.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_and_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_and_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.and.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_and_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_and_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.and.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_or_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_or_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.or.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_or_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_or_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.or.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_xor_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_xor_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.xor.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_xor_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_xor_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.xor.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_smax_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_smax_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.smax.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_smax_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_smax_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.smax.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_smin_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_smin_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.smin.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_smin_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_smin_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.smin.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_umax_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_umax_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.umax.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_umax_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_umax_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.umax.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define i32 @vreduce_umin_v4i32_zero_evl(i32 %start, <4 x i32> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_umin_v4i32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.umin.v4i32(i32 %start, <4 x i32> %val, <4 x i1> %m, i32 0)
  ret i32 %s
}

define i32 @vreduce_umin_v4i32_false_mask(i32 %start, <4 x i32> %val, i32 %evl) {
; CHECK-LABEL: vreduce_umin_v4i32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call i32 @llvm.vp.reduce.umin.v4i32(i32 %start, <4 x i32> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret i32 %s
}

define float @vreduce_seq_fadd_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_seq_fadd_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_seq_fadd_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_seq_fadd_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fadd_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fadd_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call reassoc float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fadd_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fadd_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call reassoc float @llvm.vp.reduce.fadd.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_seq_fmul_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_seq_fmul_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_seq_fmul_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_seq_fmul_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fmul_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fmul_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call reassoc float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fmul_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fmul_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call reassoc float @llvm.vp.reduce.fmul.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fmin_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fmin_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmin.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fmin_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fmin_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmin.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fmax_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fmax_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmax.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fmax_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fmax_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmax.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fminimum_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fminimum_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fminimum.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fminimum_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fminimum_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fminimum.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}

define float @vreduce_fmaximum_v4f32_zero_evl(float %start, <4 x float> %val, <4 x i1> %m) {
; CHECK-LABEL: vreduce_fmaximum_v4f32_zero_evl:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmaximum.v4f32(float %start, <4 x float> %val, <4 x i1> %m, i32 0)
  ret float %s
}

define float @vreduce_fmaximum_v4f32_false_mask(float %start, <4 x float> %val, i32 %evl) {
; CHECK-LABEL: vreduce_fmaximum_v4f32_false_mask:
; CHECK:       # %bb.0:
; CHECK-NEXT:    ret
  %s = call float @llvm.vp.reduce.fmaximum.v4f32(float %start, <4 x float> %val, <4 x i1> zeroinitializer, i32 %evl)
  ret float %s
}
