; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
; RUN: llc -mtriple=riscv64 -mattr=+v -stop-after=finalize-isel < %s \
; RUN:   -target-abi=lp64d | FileCheck %s
declare {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.nxv16i16(<vscale x 16 x i16>,<vscale x 16 x i16>, i16* , i64)
declare {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i16(<vscale x 16 x i16>,<vscale x 16 x i16>, i16*, <vscale x 16 x i1>, i64, i64)
declare {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.nxv4i32(<vscale x 4 x i32>,<vscale x 4 x i32>, i32* , i64)
declare {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i32(<vscale x 4 x i32>,<vscale x 4 x i32>, i32*, <vscale x 4 x i1>, i64, i64)
declare {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.nxv16i8(<vscale x 16 x i8>,<vscale x 16 x i8>, i8* , i64)
declare {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i8(<vscale x 16 x i8>,<vscale x 16 x i8>, i8*, <vscale x 16 x i1>, i64, i64)
declare {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.nxv1i64(<vscale x 1 x i64>,<vscale x 1 x i64>, i64* , i64)
declare {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i64(<vscale x 1 x i64>,<vscale x 1 x i64>, i64*, <vscale x 1 x i1>, i64, i64)
declare {<vscale x 1 x i32>,<vscale x 1 x i32>, i64} @llvm.riscv.vlseg2ff.nxv1i32(<vscale x 1 x i32>,<vscale x 1 x i32>, i32* , i64)
declare {<vscale x 1 x i32>,<vscale x 1 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i32(<vscale x 1 x i32>,<vscale x 1 x i32>, i32*, <vscale x 1 x i1>, i64, i64)
declare {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.nxv8i16(<vscale x 8 x i16>,<vscale x 8 x i16>, i16* , i64)
declare {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i16(<vscale x 8 x i16>,<vscale x 8 x i16>, i16*, <vscale x 8 x i1>, i64, i64)
declare {<vscale x 4 x i8>,<vscale x 4 x i8>, i64} @llvm.riscv.vlseg2ff.nxv4i8(<vscale x 4 x i8>,<vscale x 4 x i8>, i8* , i64)
declare {<vscale x 4 x i8>,<vscale x 4 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i8(<vscale x 4 x i8>,<vscale x 4 x i8>, i8*, <vscale x 4 x i1>, i64, i64)
declare {<vscale x 1 x i16>,<vscale x 1 x i16>, i64} @llvm.riscv.vlseg2ff.nxv1i16(<vscale x 1 x i16>,<vscale x 1 x i16>, i16* , i64)
declare {<vscale x 1 x i16>,<vscale x 1 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i16(<vscale x 1 x i16>,<vscale x 1 x i16>, i16*, <vscale x 1 x i1>, i64, i64)
declare {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.nxv2i32(<vscale x 2 x i32>,<vscale x 2 x i32>, i32* , i64)
declare {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i32(<vscale x 2 x i32>,<vscale x 2 x i32>, i32*, <vscale x 2 x i1>, i64, i64)
declare {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.nxv8i8(<vscale x 8 x i8>,<vscale x 8 x i8>, i8* , i64)
declare {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i8(<vscale x 8 x i8>,<vscale x 8 x i8>, i8*, <vscale x 8 x i1>, i64, i64)
declare {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.nxv4i64(<vscale x 4 x i64>,<vscale x 4 x i64>, i64* , i64)
declare {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i64(<vscale x 4 x i64>,<vscale x 4 x i64>, i64*, <vscale x 4 x i1>, i64, i64)
declare {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.nxv4i16(<vscale x 4 x i16>,<vscale x 4 x i16>, i16* , i64)
declare {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i16(<vscale x 4 x i16>,<vscale x 4 x i16>, i16*, <vscale x 4 x i1>, i64, i64)
declare {<vscale x 1 x i8>,<vscale x 1 x i8>, i64} @llvm.riscv.vlseg2ff.nxv1i8(<vscale x 1 x i8>,<vscale x 1 x i8>, i8* , i64)
declare {<vscale x 1 x i8>,<vscale x 1 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i8(<vscale x 1 x i8>,<vscale x 1 x i8>, i8*, <vscale x 1 x i1>, i64, i64)
declare {<vscale x 2 x i8>,<vscale x 2 x i8>, i64} @llvm.riscv.vlseg2ff.nxv2i8(<vscale x 2 x i8>,<vscale x 2 x i8>, i8* , i64)
declare {<vscale x 2 x i8>,<vscale x 2 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i8(<vscale x 2 x i8>,<vscale x 2 x i8>, i8*, <vscale x 2 x i1>, i64, i64)
declare {<vscale x 8 x i32>,<vscale x 8 x i32>, i64} @llvm.riscv.vlseg2ff.nxv8i32(<vscale x 8 x i32>,<vscale x 8 x i32>, i32* , i64)
declare {<vscale x 8 x i32>,<vscale x 8 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i32(<vscale x 8 x i32>,<vscale x 8 x i32>, i32*, <vscale x 8 x i1>, i64, i64)
declare {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.nxv32i8(<vscale x 32 x i8>,<vscale x 32 x i8>, i8* , i64)
declare {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv32i8(<vscale x 32 x i8>,<vscale x 32 x i8>, i8*, <vscale x 32 x i1>, i64, i64)
declare {<vscale x 2 x i16>,<vscale x 2 x i16>, i64} @llvm.riscv.vlseg2ff.nxv2i16(<vscale x 2 x i16>,<vscale x 2 x i16>, i16* , i64)
declare {<vscale x 2 x i16>,<vscale x 2 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i16(<vscale x 2 x i16>,<vscale x 2 x i16>, i16*, <vscale x 2 x i1>, i64, i64)
declare {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.nxv2i64(<vscale x 2 x i64>,<vscale x 2 x i64>, i64* , i64)
declare {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i64(<vscale x 2 x i64>,<vscale x 2 x i64>, i64*, <vscale x 2 x i1>, i64, i64)

define void @test_vlseg2ff_nxv8i8(i8* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv8i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M1_:%[0-9]+]]:vrn2m1 = PseudoVLSEG2E8FF_V_M1 [[COPY2]], [[COPY1]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 64 /* e8, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.nxv8i8(<vscale x 8 x i8> undef, <vscale x 8 x i8> undef, i8* %base, i64 %vl)
  %1 = extractvalue {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv8i8(<vscale x 8 x i8> %val, i8* %base, i64 %vl, <vscale x 8 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv8i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m1nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm1_0, [[COPY4]], %subreg.sub_vrm1_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M1_MASK:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E8FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 0 /* e8, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M1_MASK1:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E8FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 64 /* e8, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M1_MASK2:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E8FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 128 /* e8, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M1_MASK3:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E8FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 192 /* e8, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i8(<vscale x 8 x i8> %val,<vscale x 8 x i8> %val, i8* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i8(<vscale x 8 x i8> %val,<vscale x 8 x i8> %val, i8* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i8(<vscale x 8 x i8> %val,<vscale x 8 x i8> %val, i8* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i8(<vscale x 8 x i8> %val,<vscale x 8 x i8> %val, i8* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 8 x i8>,<vscale x 8 x i8>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv16i8(i8* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv16i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M2_:%[0-9]+]]:vrn2m2 = PseudoVLSEG2E8FF_V_M2 [[COPY2]], [[COPY1]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 65 /* e8, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.nxv16i8(<vscale x 16 x i8> undef, <vscale x 16 x i8> undef, i8* %base, i64 %vl)
  %1 = extractvalue {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv16i8(<vscale x 16 x i8> %val, i8* %base, i64 %vl, <vscale x 16 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv16i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m2nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm2_0, [[COPY4]], %subreg.sub_vrm2_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M2_MASK:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E8FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 1 /* e8, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M2_MASK1:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E8FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 65 /* e8, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M2_MASK2:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E8FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 129 /* e8, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M2_MASK3:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E8FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 193 /* e8, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i8(<vscale x 16 x i8> %val,<vscale x 16 x i8> %val, i8* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i8(<vscale x 16 x i8> %val,<vscale x 16 x i8> %val, i8* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} %0, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i8(<vscale x 16 x i8> %val,<vscale x 16 x i8> %val, i8* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} %0, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i8(<vscale x 16 x i8> %val,<vscale x 16 x i8> %val, i8* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 16 x i8>,<vscale x 16 x i8>, i64} %0, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv32i8(i8* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv32i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M4_:%[0-9]+]]:vrn2m4 = PseudoVLSEG2E8FF_V_M4 [[COPY2]], [[COPY1]], 3 /* e8 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 66 /* e8, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.nxv32i8(<vscale x 32 x i8> undef, <vscale x 32 x i8> undef, i8* %base, i64 %vl)
  %1 = extractvalue {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv32i8(<vscale x 32 x i8> %val, i8* %base, i64 %vl, <vscale x 32 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv32i8
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m4nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm4_0, [[COPY4]], %subreg.sub_vrm4_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M4_MASK:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E8FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 2 /* e8, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M4_MASK1:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E8FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 66 /* e8, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M4_MASK2:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E8FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 130 /* e8, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E8FF_V_M4_MASK3:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E8FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 3 /* e8 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 194 /* e8, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv32i8(<vscale x 32 x i8> %val,<vscale x 32 x i8> %val, i8* %base, <vscale x 32 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv32i8(<vscale x 32 x i8> %val,<vscale x 32 x i8> %val, i8* %base, <vscale x 32 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv32i8(<vscale x 32 x i8> %val,<vscale x 32 x i8> %val, i8* %base, <vscale x 32 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} @llvm.riscv.vlseg2ff.mask.nxv32i8(<vscale x 32 x i8> %val,<vscale x 32 x i8> %val, i8* %base, <vscale x 32 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 32 x i8>,<vscale x 32 x i8>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv4i16(i16* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv4i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M1_:%[0-9]+]]:vrn2m1 = PseudoVLSEG2E16FF_V_M1 [[COPY2]], [[COPY1]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 72 /* e16, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.nxv4i16(<vscale x 4 x i16> undef, <vscale x 4 x i16> undef, i16* %base, i64 %vl)
  %1 = extractvalue {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv4i16(<vscale x 4 x i16> %val, i16* %base, i64 %vl, <vscale x 4 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv4i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m1nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm1_0, [[COPY4]], %subreg.sub_vrm1_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M1_MASK:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E16FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 8 /* e16, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M1_MASK1:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E16FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 72 /* e16, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M1_MASK2:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E16FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 136 /* e16, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M1_MASK3:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E16FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 200 /* e16, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i16(<vscale x 4 x i16> %val,<vscale x 4 x i16> %val, i16* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i16(<vscale x 4 x i16> %val,<vscale x 4 x i16> %val, i16* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i16(<vscale x 4 x i16> %val,<vscale x 4 x i16> %val, i16* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i16(<vscale x 4 x i16> %val,<vscale x 4 x i16> %val, i16* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 4 x i16>,<vscale x 4 x i16>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv8i16(i16* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv8i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M2_:%[0-9]+]]:vrn2m2 = PseudoVLSEG2E16FF_V_M2 [[COPY2]], [[COPY1]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 73 /* e16, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.nxv8i16(<vscale x 8 x i16> undef, <vscale x 8 x i16> undef, i16* %base, i64 %vl)
  %1 = extractvalue {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv8i16(<vscale x 8 x i16> %val, i16* %base, i64 %vl, <vscale x 8 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv8i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m2nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm2_0, [[COPY4]], %subreg.sub_vrm2_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M2_MASK:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E16FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 9 /* e16, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M2_MASK1:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E16FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 73 /* e16, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M2_MASK2:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E16FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 137 /* e16, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M2_MASK3:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E16FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 201 /* e16, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i16(<vscale x 8 x i16> %val,<vscale x 8 x i16> %val, i16* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i16(<vscale x 8 x i16> %val,<vscale x 8 x i16> %val, i16* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i16(<vscale x 8 x i16> %val,<vscale x 8 x i16> %val, i16* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv8i16(<vscale x 8 x i16> %val,<vscale x 8 x i16> %val, i16* %base, <vscale x 8 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 8 x i16>,<vscale x 8 x i16>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv16i16(i16* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv16i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M4_:%[0-9]+]]:vrn2m4 = PseudoVLSEG2E16FF_V_M4 [[COPY2]], [[COPY1]], 4 /* e16 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 74 /* e16, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.nxv16i16(<vscale x 16 x i16> undef, <vscale x 16 x i16> undef, i16* %base, i64 %vl)
  %1 = extractvalue {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv16i16(<vscale x 16 x i16> %val, i16* %base, i64 %vl, <vscale x 16 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv16i16
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m4nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm4_0, [[COPY4]], %subreg.sub_vrm4_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M4_MASK:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E16FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 10 /* e16, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M4_MASK1:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E16FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 74 /* e16, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M4_MASK2:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E16FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 138 /* e16, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E16FF_V_M4_MASK3:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E16FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 4 /* e16 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 202 /* e16, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i16(<vscale x 16 x i16> %val,<vscale x 16 x i16> %val, i16* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i16(<vscale x 16 x i16> %val,<vscale x 16 x i16> %val, i16* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i16(<vscale x 16 x i16> %val,<vscale x 16 x i16> %val, i16* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} @llvm.riscv.vlseg2ff.mask.nxv16i16(<vscale x 16 x i16> %val,<vscale x 16 x i16> %val, i16* %base, <vscale x 16 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 16 x i16>,<vscale x 16 x i16>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv2i32(i32* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv2i32
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M1_:%[0-9]+]]:vrn2m1 = PseudoVLSEG2E32FF_V_M1 [[COPY2]], [[COPY1]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 80 /* e32, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.nxv2i32(<vscale x 2 x i32> undef, <vscale x 2 x i32> undef, i32* %base, i64 %vl)
  %1 = extractvalue {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv2i32(<vscale x 2 x i32> %val, i32* %base, i64 %vl, <vscale x 2 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv2i32
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m1nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm1_0, [[COPY4]], %subreg.sub_vrm1_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M1_MASK:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E32FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 16 /* e32, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M1_MASK1:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E32FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 80 /* e32, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M1_MASK2:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E32FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 144 /* e32, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M1_MASK3:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E32FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 208 /* e32, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i32(<vscale x 2 x i32> %val,<vscale x 2 x i32> %val, i32* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i32(<vscale x 2 x i32> %val,<vscale x 2 x i32> %val, i32* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i32(<vscale x 2 x i32> %val,<vscale x 2 x i32> %val, i32* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i32(<vscale x 2 x i32> %val,<vscale x 2 x i32> %val, i32* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 2 x i32>,<vscale x 2 x i32>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv4i32(i32* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv4i32
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M2_:%[0-9]+]]:vrn2m2 = PseudoVLSEG2E32FF_V_M2 [[COPY2]], [[COPY1]], 5 /* e32 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 81 /* e32, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.nxv4i32(<vscale x 4 x i32> undef, <vscale x 4 x i32> undef, i32* %base, i64 %vl)
  %1 = extractvalue {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv4i32(<vscale x 4 x i32> %val, i32* %base, i64 %vl, <vscale x 4 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv4i32
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m2nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm2_0, [[COPY4]], %subreg.sub_vrm2_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M2_MASK:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E32FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 17 /* e32, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M2_MASK1:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E32FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 81 /* e32, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M2_MASK2:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E32FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 145 /* e32, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E32FF_V_M2_MASK3:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E32FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 5 /* e32 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 209 /* e32, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i32(<vscale x 4 x i32> %val,<vscale x 4 x i32> %val, i32* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i32(<vscale x 4 x i32> %val,<vscale x 4 x i32> %val, i32* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} %2, 2
  store volatile i64 %1, i64* %outvl
  %4 = tail call {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i32(<vscale x 4 x i32> %val,<vscale x 4 x i32> %val, i32* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} %4, 2
  store volatile i64 %1, i64* %outvl
  %6 = tail call {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i32(<vscale x 4 x i32> %val,<vscale x 4 x i32> %val, i32* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 4 x i32>,<vscale x 4 x i32>, i64} %6, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv1i64(i64* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv1i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M1_:%[0-9]+]]:vrn2m1 = PseudoVLSEG2E64FF_V_M1 [[COPY2]], [[COPY1]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 88 /* e64, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.nxv1i64(<vscale x 1 x i64> undef, <vscale x 1 x i64> undef, i64* %base, i64 %vl)
  %1 = extractvalue {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv1i64(<vscale x 1 x i64> %val, i64* %base, i64 %vl, <vscale x 1 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv1i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vr = COPY $v8
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m1nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm1_0, [[COPY4]], %subreg.sub_vrm1_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M1_MASK:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E64FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 24 /* e64, m1, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M1_MASK1:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E64FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 88 /* e64, m1, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M1_MASK2:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E64FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 152 /* e64, m1, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M1_MASK3:%[0-9]+]]:vrn2m1nov0 = PseudoVLSEG2E64FF_V_M1_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 216 /* e64, m1, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i64(<vscale x 1 x i64> %val,<vscale x 1 x i64> %val, i64* %base, <vscale x 1 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i64(<vscale x 1 x i64> %val,<vscale x 1 x i64> %val, i64* %base, <vscale x 1 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i64(<vscale x 1 x i64> %val,<vscale x 1 x i64> %val, i64* %base, <vscale x 1 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv1i64(<vscale x 1 x i64> %val,<vscale x 1 x i64> %val, i64* %base, <vscale x 1 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 1 x i64>,<vscale x 1 x i64>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv2i64(i64* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv2i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M2_:%[0-9]+]]:vrn2m2 = PseudoVLSEG2E64FF_V_M2 [[COPY2]], [[COPY1]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 89 /* e64, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.nxv2i64(<vscale x 2 x i64> undef, <vscale x 2 x i64> undef, i64* %base, i64 %vl)
  %1 = extractvalue {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv2i64(<vscale x 2 x i64> %val, i64* %base, i64 %vl, <vscale x 2 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv2i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m2, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm2 = COPY $v8m2
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m2nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm2_0, [[COPY4]], %subreg.sub_vrm2_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M2_MASK:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E64FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 25 /* e64, m2, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M2_MASK1:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E64FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 89 /* e64, m2, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M2_MASK2:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E64FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 153 /* e64, m2, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M2_MASK3:%[0-9]+]]:vrn2m2nov0 = PseudoVLSEG2E64FF_V_M2_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 217 /* e64, m2, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i64(<vscale x 2 x i64> %val,<vscale x 2 x i64> %val, i64* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i64(<vscale x 2 x i64> %val,<vscale x 2 x i64> %val, i64* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i64(<vscale x 2 x i64> %val,<vscale x 2 x i64> %val, i64* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv2i64(<vscale x 2 x i64> %val,<vscale x 2 x i64> %val, i64* %base, <vscale x 2 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 2 x i64>,<vscale x 2 x i64>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}

define void @test_vlseg2ff_nxv4i64(i64* %base, i64 %vl, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_nxv4i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $x10, $x11, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M4_:%[0-9]+]]:vrn2m4 = PseudoVLSEG2E64FF_V_M4 [[COPY2]], [[COPY1]], 6 /* e64 */, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 90 /* e64, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.nxv4i64(<vscale x 4 x i64> undef, <vscale x 4 x i64> undef, i64* %base, i64 %vl)
  %1 = extractvalue {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  ret void
}

define void @test_vlseg2ff_mask_nxv4i64(<vscale x 4 x i64> %val, i64* %base, i64 %vl, <vscale x 4 x i1> %mask, i64* %outvl) {
  ; CHECK-LABEL: name: test_vlseg2ff_mask_nxv4i64
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   liveins: $v8m4, $x10, $x11, $v0, $x12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x12
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vr = COPY $v0
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:gprnox0 = COPY $x11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vrm4 = COPY $v8m4
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vrn2m4nov0 = REG_SEQUENCE [[COPY4]], %subreg.sub_vrm4_0, [[COPY4]], %subreg.sub_vrm4_1
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M4_MASK:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E64FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 0, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL:%[0-9]+]]:gpr = PseudoReadVL 26 /* e64, m4, tu, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M4_MASK1:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E64FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 1, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL1:%[0-9]+]]:gpr = PseudoReadVL 90 /* e64, m4, ta, mu */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL1]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M4_MASK2:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E64FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 2, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL2:%[0-9]+]]:gpr = PseudoReadVL 154 /* e64, m4, tu, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL2]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   $v0 = COPY [[COPY1]]
  ; CHECK-NEXT:   [[PseudoVLSEG2E64FF_V_M4_MASK3:%[0-9]+]]:vrn2m4nov0 = PseudoVLSEG2E64FF_V_M4_MASK [[REG_SEQUENCE]], [[COPY3]], $v0, [[COPY2]], 6 /* e64 */, 3, implicit-def $vl
  ; CHECK-NEXT:   [[PseudoReadVL3:%[0-9]+]]:gpr = PseudoReadVL 218 /* e64, m4, ta, ma */, implicit $vl
  ; CHECK-NEXT:   SD killed [[PseudoReadVL3]], [[COPY]], 0 :: (volatile store (s64) into %ir.outvl)
  ; CHECK-NEXT:   PseudoRET
entry:
  %0 = tail call {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i64(<vscale x 4 x i64> %val,<vscale x 4 x i64> %val, i64* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 0)
  %1 = extractvalue {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} %0, 2
  store volatile i64 %1, i64* %outvl
  %2 = tail call {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i64(<vscale x 4 x i64> %val,<vscale x 4 x i64> %val, i64* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 1)
  %3 = extractvalue {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} %2, 2
  store volatile i64 %3, i64* %outvl
  %4 = tail call {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i64(<vscale x 4 x i64> %val,<vscale x 4 x i64> %val, i64* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 2)
  %5 = extractvalue {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} %4, 2
  store volatile i64 %5, i64* %outvl
  %6 = tail call {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} @llvm.riscv.vlseg2ff.mask.nxv4i64(<vscale x 4 x i64> %val,<vscale x 4 x i64> %val, i64* %base, <vscale x 4 x i1> %mask, i64 %vl, i64 3)
  %7 = extractvalue {<vscale x 4 x i64>,<vscale x 4 x i64>, i64} %6, 2
  store volatile i64 %7, i64* %outvl
  ret void
}
