; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv32 -mattr=+v -verify-machineinstrs | FileCheck %s
; RUN: llc < %s -mtriple=riscv64 -mattr=+v -verify-machineinstrs | FileCheck %s

define <8 x float> @fpext_v8bf16(<8 x bfloat> %x) {
; CHECK-LABEL: fpext_v8bf16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmv.x.w a0, fa7
; CHECK-NEXT:    fmv.x.w a1, fa6
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; CHECK-NEXT:    vmv.s.x v9, a0
; CHECK-NEXT:    fmv.x.w a0, fa5
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v11, a1
; CHECK-NEXT:    fmv.x.w a1, fa4
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v12, a0
; CHECK-NEXT:    fmv.x.w a0, fa3
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v10, a1
; CHECK-NEXT:    fmv.x.w a1, fa2
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v13, a0
; CHECK-NEXT:    fmv.x.w a0, fa1
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v14, a1
; CHECK-NEXT:    fmv.x.w a1, fa0
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v15, a0
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v8, a1
; CHECK-NEXT:    vslideup.vi v14, v13, 1
; CHECK-NEXT:    vslideup.vi v8, v15, 1
; CHECK-NEXT:    vslideup.vi v11, v9, 1
; CHECK-NEXT:    vslideup.vi v10, v12, 1
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vslideup.vi v8, v14, 2
; CHECK-NEXT:    vslideup.vi v10, v11, 2
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vslideup.vi v8, v10, 4
; CHECK-NEXT:    ret
  %y = fpext <8 x bfloat> %x to <8 x float>
  ret <8 x float> %y
}

define <8 x float> @fpext_v8f16(<8 x bfloat> %x) {
; CHECK-LABEL: fpext_v8f16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    fmv.x.w a0, fa7
; CHECK-NEXT:    fmv.x.w a1, fa6
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vsetivli zero, 2, e32, mf2, ta, ma
; CHECK-NEXT:    vmv.s.x v9, a0
; CHECK-NEXT:    fmv.x.w a0, fa5
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v11, a1
; CHECK-NEXT:    fmv.x.w a1, fa4
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v12, a0
; CHECK-NEXT:    fmv.x.w a0, fa3
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v10, a1
; CHECK-NEXT:    fmv.x.w a1, fa2
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v13, a0
; CHECK-NEXT:    fmv.x.w a0, fa1
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v14, a1
; CHECK-NEXT:    fmv.x.w a1, fa0
; CHECK-NEXT:    slli a0, a0, 16
; CHECK-NEXT:    vmv.s.x v15, a0
; CHECK-NEXT:    slli a1, a1, 16
; CHECK-NEXT:    vmv.s.x v8, a1
; CHECK-NEXT:    vslideup.vi v14, v13, 1
; CHECK-NEXT:    vslideup.vi v8, v15, 1
; CHECK-NEXT:    vslideup.vi v11, v9, 1
; CHECK-NEXT:    vslideup.vi v10, v12, 1
; CHECK-NEXT:    vsetivli zero, 4, e32, m1, ta, ma
; CHECK-NEXT:    vslideup.vi v8, v14, 2
; CHECK-NEXT:    vslideup.vi v10, v11, 2
; CHECK-NEXT:    vsetivli zero, 8, e32, m2, ta, ma
; CHECK-NEXT:    vslideup.vi v8, v10, 4
; CHECK-NEXT:    ret
  %y = fpext <8 x bfloat> %x to <8 x float>
  ret <8 x float> %y
}

