; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+v,+m,+zfh,+zvfh -target-abi=ilp32d \
; RUN:   -verify-machineinstrs < %s | FileCheck %s \
; RUN:   --check-prefixes=CHECK,ZVFH,ZVFH32
; RUN: llc -mtriple=riscv64 -mattr=+v,+m,+zfh,+zvfh -target-abi=lp64d \
; RUN:   -verify-machineinstrs < %s | FileCheck %s \
; RUN:   --check-prefixes=CHECK,ZVFH,ZVFH64
; RUN: llc -mtriple=riscv32 -mattr=+v,+m,+zfh,+zvfhmin -target-abi=ilp32d \
; RUN:   -verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,ZVFHMIN,ZVFHMIN32
; RUN: llc -mtriple=riscv64 -mattr=+v,+m,+zfh,+zvfhmin -target-abi=lp64d \
; RUN:   -verify-machineinstrs < %s | FileCheck %s \
; RUN:   --check-prefixes=CHECK,ZVFHMIN,ZVFHMIN64

declare <7 x i1> @llvm.vp.fcmp.v7f16(<7 x half>, <7 x half>, metadata, <7 x i1>, i32)

define <7 x i1> @fcmp_oeq_vv_v7f16(<7 x half> %va, <7 x half> %vb, <7 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oeq_vv_v7f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vv v0, v8, v9, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oeq_vv_v7f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <7 x i1> @llvm.vp.fcmp.v7f16(<7 x half> %va, <7 x half> %vb, metadata !"oeq", <7 x i1> %m, i32 %evl)
  ret <7 x i1> %v
}

declare <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half>, <8 x half>, metadata, <8 x i1>, i32)

define <8 x i1> @fcmp_oeq_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oeq_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vv v0, v8, v9, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oeq_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oeq_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oeq_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oeq_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oeq_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oeq_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oeq_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ogt_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v0, v9, v8, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ogt_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ogt_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ogt_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ogt_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ogt_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oge_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vv v0, v9, v8, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oge_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oge_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfge.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oge_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oge_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_oge_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_olt_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v0, v8, v9, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_olt_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_olt_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_olt_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_olt_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_olt_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ole_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vv v0, v8, v9, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ole_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ole_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ole_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ole_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfge.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ole_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_one_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v10, v8, v9, v0.t
; ZVFH-NEXT:    vmflt.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vmor.mm v0, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_one_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_one_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v9, v8, fa0, v0.t
; ZVFH-NEXT:    vmfgt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_one_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_one_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v9, v8, fa0, v0.t
; ZVFH-NEXT:    vmflt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_one_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ord_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vv v9, v9, v9, v0.t
; ZVFH-NEXT:    vmfeq.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmand.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ord_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmand.mm v0, v8, v9
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ord_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmv.v.f v9, fa0
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vf v9, v9, fa0, v0.t
; ZVFH-NEXT:    vmfeq.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmand.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ord_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmand.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ord_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmv.v.f v9, fa0
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfeq.vf v9, v9, fa0, v0.t
; ZVFH-NEXT:    vmfeq.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmand.mm v0, v9, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ord_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfeq.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmand.mm v0, v8, v9
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ueq_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v10, v8, v9, v0.t
; ZVFH-NEXT:    vmflt.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vmnor.mm v0, v8, v10
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ueq_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ueq_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v9, v8, fa0, v0.t
; ZVFH-NEXT:    vmfgt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ueq_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ueq_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v9, v8, fa0, v0.t
; ZVFH-NEXT:    vmflt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ueq_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmflt.vv v9, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ugt_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vv v8, v8, v9, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ugt_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ugt_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ugt_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ugt_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfge.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ugt_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uge_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v8, v8, v9, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uge_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uge_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uge_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uge_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uge_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ult_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ult_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ult_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfge.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ult_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ult_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfle.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ult_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfle.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ule_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vv v8, v9, v8, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ule_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ule_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfgt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ule_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_ule_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmflt.vf v8, v8, fa0, v0.t
; ZVFH-NEXT:    vmnot.m v0, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_ule_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmflt.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmnot.m v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_une_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vv v0, v8, v9, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_une_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_une_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_une_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v10, v12, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_une_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vf v0, v8, fa0, v0.t
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_une_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vmv.v.x v9, a1
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v12, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v12, v10, v0.t
; ZVFHMIN-NEXT:    vmv1r.v v0, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vv_v8f16(<8 x half> %va, <8 x half> %vb, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uno_vv_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vv v9, v9, v9, v0.t
; ZVFH-NEXT:    vmfne.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uno_vv_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v9
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v8, v9
; ZVFHMIN-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vf_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uno_vf_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmv.v.f v9, fa0
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vf v9, v9, fa0, v0.t
; ZVFH-NEXT:    vmfne.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmor.mm v0, v8, v9
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uno_vf_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v9, v8
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %va, <8 x half> %vb, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vf_swap_v8f16(<8 x half> %va, half %b, <8 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_uno_vf_swap_v8f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFH-NEXT:    vfmv.v.f v9, fa0
; ZVFH-NEXT:    vsetvli zero, a0, e16, m1, ta, ma
; ZVFH-NEXT:    vmfne.vf v9, v9, fa0, v0.t
; ZVFH-NEXT:    vmfne.vv v8, v8, v8, v0.t
; ZVFH-NEXT:    vmor.mm v0, v9, v8
; ZVFH-NEXT:    ret
;
; ZVFHMIN-LABEL: fcmp_uno_vf_swap_v8f16:
; ZVFHMIN:       # %bb.0:
; ZVFHMIN-NEXT:    fmv.x.h a1, fa0
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vmv.v.x v8, a1
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v9, v10, v10, v0.t
; ZVFHMIN-NEXT:    vsetivli zero, 8, e16, m1, ta, ma
; ZVFHMIN-NEXT:    vfwcvt.f.f.v v10, v8
; ZVFHMIN-NEXT:    vsetvli zero, a0, e32, m2, ta, ma
; ZVFHMIN-NEXT:    vmfne.vv v8, v10, v10, v0.t
; ZVFHMIN-NEXT:    vmor.mm v0, v8, v9
; ZVFHMIN-NEXT:    ret
  %elt.head = insertelement <8 x half> poison, half %b, i32 0
  %vb = shufflevector <8 x half> %elt.head, <8 x half> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f16(<8 x half> %vb, <8 x half> %va, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

declare <128 x i1> @llvm.vp.fcmp.v128f16(<128 x half>, <128 x half>, metadata, <128 x i1>, i32)

define <128 x i1> @fcmp_oeq_vv_v128f16(<128 x half> %va, <128 x half> %vb, <128 x i1> %m, i32 zeroext %evl) {
; ZVFH-LABEL: fcmp_oeq_vv_v128f16:
; ZVFH:       # %bb.0:
; ZVFH-NEXT:    addi sp, sp, -16
; ZVFH-NEXT:    .cfi_def_cfa_offset 16
; ZVFH-NEXT:    csrr a1, vlenb
; ZVFH-NEXT:    slli a1, a1, 4
; ZVFH-NEXT:    sub sp, sp, a1
; ZVFH-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x10, 0x22, 0x11, 0x10, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 16 + 16 * vlenb
; ZVFH-NEXT:    csrr a1, vlenb
; ZVFH-NEXT:    slli a1, a1, 3
; ZVFH-NEXT:    add a1, sp, a1
; ZVFH-NEXT:    addi a1, a1, 16
; ZVFH-NEXT:    vs8r.v v16, (a1) # Unknown-size Folded Spill
; ZVFH-NEXT:    addi a1, a0, 128
; ZVFH-NEXT:    li a3, 64
; ZVFH-NEXT:    vsetivli zero, 8, e8, m1, ta, ma
; ZVFH-NEXT:    vslidedown.vi v24, v0, 8
; ZVFH-NEXT:    vsetvli zero, a3, e16, m8, ta, ma
; ZVFH-NEXT:    vle16.v v16, (a1)
; ZVFH-NEXT:    addi a1, sp, 16
; ZVFH-NEXT:    vs8r.v v16, (a1) # Unknown-size Folded Spill
; ZVFH-NEXT:    mv a1, a2
; ZVFH-NEXT:    vle16.v v16, (a0)
; ZVFH-NEXT:    bltu a2, a3, .LBB43_2
; ZVFH-NEXT:  # %bb.1:
; ZVFH-NEXT:    li a1, 64
; ZVFH-NEXT:  .LBB43_2:
; ZVFH-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFH-NEXT:    vmfeq.vv v7, v8, v16, v0.t
; ZVFH-NEXT:    addi a0, a2, -64
; ZVFH-NEXT:    sltu a1, a2, a0
; ZVFH-NEXT:    addi a1, a1, -1
; ZVFH-NEXT:    and a0, a1, a0
; ZVFH-NEXT:    vmv1r.v v0, v24
; ZVFH-NEXT:    csrr a1, vlenb
; ZVFH-NEXT:    slli a1, a1, 3
; ZVFH-NEXT:    add a1, sp, a1
; ZVFH-NEXT:    addi a1, a1, 16
; ZVFH-NEXT:    vl8r.v v16, (a1) # Unknown-size Folded Reload
; ZVFH-NEXT:    addi a1, sp, 16
; ZVFH-NEXT:    vl8r.v v24, (a1) # Unknown-size Folded Reload
; ZVFH-NEXT:    vsetvli zero, a0, e16, m8, ta, ma
; ZVFH-NEXT:    vmfeq.vv v8, v16, v24, v0.t
; ZVFH-NEXT:    vsetivli zero, 16, e8, m1, ta, ma
; ZVFH-NEXT:    vslideup.vi v7, v8, 8
; ZVFH-NEXT:    vmv.v.v v0, v7
; ZVFH-NEXT:    csrr a0, vlenb
; ZVFH-NEXT:    slli a0, a0, 4
; ZVFH-NEXT:    add sp, sp, a0
; ZVFH-NEXT:    .cfi_def_cfa sp, 16
; ZVFH-NEXT:    addi sp, sp, 16
; ZVFH-NEXT:    .cfi_def_cfa_offset 0
; ZVFH-NEXT:    ret
;
; ZVFHMIN32-LABEL: fcmp_oeq_vv_v128f16:
; ZVFHMIN32:       # %bb.0:
; ZVFHMIN32-NEXT:    addi sp, sp, -1024
; ZVFHMIN32-NEXT:    .cfi_def_cfa_offset 1024
; ZVFHMIN32-NEXT:    sw ra, 1020(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s0, 1016(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s2, 1012(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s3, 1008(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s4, 1004(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s5, 1000(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s6, 996(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s7, 992(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s8, 988(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s9, 984(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s10, 980(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    sw s11, 976(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs0, 968(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs1, 960(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs2, 952(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs3, 944(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs4, 936(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs5, 928(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs6, 920(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs7, 912(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs8, 904(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs9, 896(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs10, 888(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    fsd fs11, 880(sp) # 8-byte Folded Spill
; ZVFHMIN32-NEXT:    .cfi_offset ra, -4
; ZVFHMIN32-NEXT:    .cfi_offset s0, -8
; ZVFHMIN32-NEXT:    .cfi_offset s2, -12
; ZVFHMIN32-NEXT:    .cfi_offset s3, -16
; ZVFHMIN32-NEXT:    .cfi_offset s4, -20
; ZVFHMIN32-NEXT:    .cfi_offset s5, -24
; ZVFHMIN32-NEXT:    .cfi_offset s6, -28
; ZVFHMIN32-NEXT:    .cfi_offset s7, -32
; ZVFHMIN32-NEXT:    .cfi_offset s8, -36
; ZVFHMIN32-NEXT:    .cfi_offset s9, -40
; ZVFHMIN32-NEXT:    .cfi_offset s10, -44
; ZVFHMIN32-NEXT:    .cfi_offset s11, -48
; ZVFHMIN32-NEXT:    .cfi_offset fs0, -56
; ZVFHMIN32-NEXT:    .cfi_offset fs1, -64
; ZVFHMIN32-NEXT:    .cfi_offset fs2, -72
; ZVFHMIN32-NEXT:    .cfi_offset fs3, -80
; ZVFHMIN32-NEXT:    .cfi_offset fs4, -88
; ZVFHMIN32-NEXT:    .cfi_offset fs5, -96
; ZVFHMIN32-NEXT:    .cfi_offset fs6, -104
; ZVFHMIN32-NEXT:    .cfi_offset fs7, -112
; ZVFHMIN32-NEXT:    .cfi_offset fs8, -120
; ZVFHMIN32-NEXT:    .cfi_offset fs9, -128
; ZVFHMIN32-NEXT:    .cfi_offset fs10, -136
; ZVFHMIN32-NEXT:    .cfi_offset fs11, -144
; ZVFHMIN32-NEXT:    addi s0, sp, 1024
; ZVFHMIN32-NEXT:    .cfi_def_cfa s0, 0
; ZVFHMIN32-NEXT:    csrr a1, vlenb
; ZVFHMIN32-NEXT:    li a2, 41
; ZVFHMIN32-NEXT:    mul a1, a1, a2
; ZVFHMIN32-NEXT:    sub sp, sp, a1
; ZVFHMIN32-NEXT:    andi sp, sp, -128
; ZVFHMIN32-NEXT:    addi a3, a0, 128
; ZVFHMIN32-NEXT:    li a1, 64
; ZVFHMIN32-NEXT:    addi a4, sp, 640
; ZVFHMIN32-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN32-NEXT:    vmv.x.s a2, v8
; ZVFHMIN32-NEXT:    vle16.v v24, (a3)
; ZVFHMIN32-NEXT:    csrr a3, vlenb
; ZVFHMIN32-NEXT:    slli a5, a3, 5
; ZVFHMIN32-NEXT:    add a3, a5, a3
; ZVFHMIN32-NEXT:    add a3, sp, a3
; ZVFHMIN32-NEXT:    addi a3, a3, 880
; ZVFHMIN32-NEXT:    vs8r.v v24, (a3) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vle16.v v0, (a0)
; ZVFHMIN32-NEXT:    vse16.v v8, (a4)
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 7
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a0, a0, 5
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 6
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 5
; ZVFHMIN32-NEXT:    sub a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 5
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 30
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 4
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 29
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 3
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 28
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 2
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 27
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 1
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 26
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 15
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 24
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 14
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 22
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 13
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 20
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 12
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 18
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 11
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 3
; ZVFHMIN32-NEXT:    add a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 10
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 3
; ZVFHMIN32-NEXT:    sub a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v8, 9
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 2
; ZVFHMIN32-NEXT:    add a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v8, 8
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 1
; ZVFHMIN32-NEXT:    add a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    addi a0, sp, 384
; ZVFHMIN32-NEXT:    addi a3, sp, 512
; ZVFHMIN32-NEXT:    vmv.x.s a5, v16
; ZVFHMIN32-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN32-NEXT:    vse16.v v0, (a0)
; ZVFHMIN32-NEXT:    vse16.v v16, (a3)
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 7
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 11
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 6
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 12
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 5
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 13
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 4
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 14
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 3
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 4
; ZVFHMIN32-NEXT:    sub a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 2
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a0, a0, 4
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 1
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 4
; ZVFHMIN32-NEXT:    add a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v16, 15
; ZVFHMIN32-NEXT:    addi a0, sp, 880
; ZVFHMIN32-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vslidedown.vi v4, v16, 14
; ZVFHMIN32-NEXT:    vslidedown.vi v2, v16, 13
; ZVFHMIN32-NEXT:    vslidedown.vi v24, v16, 12
; ZVFHMIN32-NEXT:    vslidedown.vi v22, v16, 11
; ZVFHMIN32-NEXT:    vslidedown.vi v20, v16, 10
; ZVFHMIN32-NEXT:    vslidedown.vi v18, v16, 9
; ZVFHMIN32-NEXT:    vslidedown.vi v16, v16, 8
; ZVFHMIN32-NEXT:    vmv.x.s a6, v0
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v5, v0, 7
; ZVFHMIN32-NEXT:    vslidedown.vi v17, v0, 6
; ZVFHMIN32-NEXT:    vslidedown.vi v23, v0, 5
; ZVFHMIN32-NEXT:    vslidedown.vi v19, v0, 4
; ZVFHMIN32-NEXT:    vslidedown.vi v21, v0, 3
; ZVFHMIN32-NEXT:    vslidedown.vi v3, v0, 2
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v0, 1
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a0, a0, 1
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v0, 15
; ZVFHMIN32-NEXT:    vslidedown.vi v10, v0, 14
; ZVFHMIN32-NEXT:    vslidedown.vi v12, v0, 13
; ZVFHMIN32-NEXT:    vslidedown.vi v14, v0, 12
; ZVFHMIN32-NEXT:    vslidedown.vi v26, v0, 11
; ZVFHMIN32-NEXT:    vslidedown.vi v28, v0, 10
; ZVFHMIN32-NEXT:    vslidedown.vi v30, v0, 9
; ZVFHMIN32-NEXT:    vslidedown.vi v0, v0, 8
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    li a3, 24
; ZVFHMIN32-NEXT:    mul a0, a0, a3
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a0) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s a0, v6
; ZVFHMIN32-NEXT:    csrr a3, vlenb
; ZVFHMIN32-NEXT:    li a4, 22
; ZVFHMIN32-NEXT:    mul a3, a3, a4
; ZVFHMIN32-NEXT:    add a3, sp, a3
; ZVFHMIN32-NEXT:    addi a3, a3, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a3) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s a3, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    li a7, 20
; ZVFHMIN32-NEXT:    mul a4, a4, a7
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s a7, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    li t0, 18
; ZVFHMIN32-NEXT:    mul a4, a4, t0
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s3, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    slli t0, a4, 3
; ZVFHMIN32-NEXT:    add a4, t0, a4
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s10, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    slli t0, a4, 3
; ZVFHMIN32-NEXT:    sub a4, t0, a4
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s11, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    slli t0, a4, 2
; ZVFHMIN32-NEXT:    add a4, t0, a4
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s5, v6
; ZVFHMIN32-NEXT:    csrr a4, vlenb
; ZVFHMIN32-NEXT:    slli t0, a4, 1
; ZVFHMIN32-NEXT:    add a4, t0, a4
; ZVFHMIN32-NEXT:    add a4, sp, a4
; ZVFHMIN32-NEXT:    addi a4, a4, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s7, v6
; ZVFHMIN32-NEXT:    addi a4, sp, 880
; ZVFHMIN32-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vmv.x.s s9, v6
; ZVFHMIN32-NEXT:    vmv.x.s s8, v4
; ZVFHMIN32-NEXT:    vmv.x.s s6, v2
; ZVFHMIN32-NEXT:    vmv.x.s s4, v24
; ZVFHMIN32-NEXT:    vmv.x.s s2, v22
; ZVFHMIN32-NEXT:    vmv.x.s a4, v20
; ZVFHMIN32-NEXT:    vmv.x.s t0, v18
; ZVFHMIN32-NEXT:    sw t0, 120(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    vmv.x.s t0, v16
; ZVFHMIN32-NEXT:    sw t0, 124(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    vmv.x.s t6, v8
; ZVFHMIN32-NEXT:    vmv.x.s t0, v10
; ZVFHMIN32-NEXT:    vmv.x.s t1, v12
; ZVFHMIN32-NEXT:    vmv.x.s t2, v14
; ZVFHMIN32-NEXT:    vmv.x.s t3, v26
; ZVFHMIN32-NEXT:    vmv.x.s t4, v28
; ZVFHMIN32-NEXT:    vmv.x.s t5, v30
; ZVFHMIN32-NEXT:    fmv.h.x fs8, a2
; ZVFHMIN32-NEXT:    fmv.h.x fs7, a5
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    slli a2, a2, 5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs6, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    slli a5, a2, 5
; ZVFHMIN32-NEXT:    sub a2, a5, a2
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs5, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 30
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft10, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 29
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft8, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 28
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft2, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 27
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft3, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 26
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft4, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 11
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft5, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 12
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x ft6, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 13
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fa6, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    li a5, 14
; ZVFHMIN32-NEXT:    mul a2, a2, a5
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs0, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    slli a5, a2, 4
; ZVFHMIN32-NEXT:    sub a2, a5, a2
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs1, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    slli a2, a2, 4
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs2, a2
; ZVFHMIN32-NEXT:    csrr a2, vlenb
; ZVFHMIN32-NEXT:    slli a5, a2, 4
; ZVFHMIN32-NEXT:    add a2, a5, a2
; ZVFHMIN32-NEXT:    add a2, sp, a2
; ZVFHMIN32-NEXT:    lh a2, 880(a2) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fs3, a2
; ZVFHMIN32-NEXT:    addi a2, sp, 256
; ZVFHMIN32-NEXT:    fmv.h.x fs4, a0
; ZVFHMIN32-NEXT:    fmv.h.x ft7, a3
; ZVFHMIN32-NEXT:    fmv.h.x ft11, a7
; ZVFHMIN32-NEXT:    fmv.h.x ft9, s3
; ZVFHMIN32-NEXT:    fmv.h.x fa7, s10
; ZVFHMIN32-NEXT:    fmv.h.x fa5, s11
; ZVFHMIN32-NEXT:    fsh fa5, 114(sp) # 2-byte Folded Spill
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a3, a0, 5
; ZVFHMIN32-NEXT:    add a0, a3, a0
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    addi a0, a0, 880
; ZVFHMIN32-NEXT:    vl8r.v v24, (a0) # Unknown-size Folded Reload
; ZVFHMIN32-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN32-NEXT:    vse16.v v24, (a2)
; ZVFHMIN32-NEXT:    vmv.x.s a3, v0
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 15
; ZVFHMIN32-NEXT:    vmv.x.s a5, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 14
; ZVFHMIN32-NEXT:    vmv.x.s ra, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 13
; ZVFHMIN32-NEXT:    vmv.x.s a2, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 12
; ZVFHMIN32-NEXT:    vmv.x.s a1, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 11
; ZVFHMIN32-NEXT:    vmv.x.s s3, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 10
; ZVFHMIN32-NEXT:    vmv.x.s a7, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 9
; ZVFHMIN32-NEXT:    vmv.x.s a0, v8
; ZVFHMIN32-NEXT:    sw a0, 116(sp) # 4-byte Folded Spill
; ZVFHMIN32-NEXT:    fmv.h.x fa3, s5
; ZVFHMIN32-NEXT:    vmv.x.s s5, v5
; ZVFHMIN32-NEXT:    fmv.h.x fa2, s7
; ZVFHMIN32-NEXT:    vmv.x.s s7, v17
; ZVFHMIN32-NEXT:    fmv.h.x fa1, s9
; ZVFHMIN32-NEXT:    vmv.x.s s9, v23
; ZVFHMIN32-NEXT:    fmv.h.x fa0, s8
; ZVFHMIN32-NEXT:    vmv.x.s s8, v19
; ZVFHMIN32-NEXT:    fmv.h.x ft0, s6
; ZVFHMIN32-NEXT:    vmv.x.s s6, v21
; ZVFHMIN32-NEXT:    fmv.h.x ft1, s4
; ZVFHMIN32-NEXT:    vmv.x.s s10, v3
; ZVFHMIN32-NEXT:    fmv.h.x fa5, s2
; ZVFHMIN32-NEXT:    fsh fa5, 112(sp) # 2-byte Folded Spill
; ZVFHMIN32-NEXT:    vmv.x.s s2, v24
; ZVFHMIN32-NEXT:    fmv.h.x fs9, a6
; ZVFHMIN32-NEXT:    csrr a0, vlenb
; ZVFHMIN32-NEXT:    slli a0, a0, 1
; ZVFHMIN32-NEXT:    add a0, sp, a0
; ZVFHMIN32-NEXT:    lh a6, 880(a0) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 7
; ZVFHMIN32-NEXT:    fmv.h.x fs10, s2
; ZVFHMIN32-NEXT:    vmv.x.s a0, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 6
; ZVFHMIN32-NEXT:    fmv.h.x fs11, s5
; ZVFHMIN32-NEXT:    feq.h s2, fs8, fs9
; ZVFHMIN32-NEXT:    fmv.h.x fs8, s7
; ZVFHMIN32-NEXT:    vmv.x.s s7, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 5
; ZVFHMIN32-NEXT:    fmv.h.x fs9, s9
; ZVFHMIN32-NEXT:    feq.h s11, fs7, fs10
; ZVFHMIN32-NEXT:    fmv.h.x fs7, s8
; ZVFHMIN32-NEXT:    vmv.x.s s8, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 4
; ZVFHMIN32-NEXT:    fmv.h.x fs10, s6
; ZVFHMIN32-NEXT:    feq.h s4, fs6, fs11
; ZVFHMIN32-NEXT:    fmv.h.x fs6, s10
; ZVFHMIN32-NEXT:    vmv.x.s s9, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 3
; ZVFHMIN32-NEXT:    fmv.h.x fs11, a6
; ZVFHMIN32-NEXT:    feq.h s5, fs5, fs8
; ZVFHMIN32-NEXT:    fmv.h.x fs5, a0
; ZVFHMIN32-NEXT:    vmv.x.s a0, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 2
; ZVFHMIN32-NEXT:    fmv.h.x fs8, s7
; ZVFHMIN32-NEXT:    feq.h s6, ft10, fs9
; ZVFHMIN32-NEXT:    fmv.h.x fs9, s8
; ZVFHMIN32-NEXT:    vmv.x.s a6, v8
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 1
; ZVFHMIN32-NEXT:    fmv.h.x fa5, s9
; ZVFHMIN32-NEXT:    feq.h s7, ft8, fs7
; ZVFHMIN32-NEXT:    fmv.h.x fs7, a0
; ZVFHMIN32-NEXT:    vmv.x.s a0, v8
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a6
; ZVFHMIN32-NEXT:    feq.h s8, ft2, fs10
; ZVFHMIN32-NEXT:    fmv.h.x fs10, a0
; ZVFHMIN32-NEXT:    feq.h s9, ft3, fs6
; ZVFHMIN32-NEXT:    fmv.h.x fs6, t6
; ZVFHMIN32-NEXT:    feq.h s10, ft4, fs11
; ZVFHMIN32-NEXT:    fmv.h.x fs11, t0
; ZVFHMIN32-NEXT:    feq.h t0, ft5, fs5
; ZVFHMIN32-NEXT:    fmv.h.x fs5, t1
; ZVFHMIN32-NEXT:    feq.h t1, ft6, fs8
; ZVFHMIN32-NEXT:    fmv.h.x ft10, t2
; ZVFHMIN32-NEXT:    feq.h t2, fa6, fs9
; ZVFHMIN32-NEXT:    fmv.h.x ft8, t3
; ZVFHMIN32-NEXT:    feq.h t3, fs0, fa5
; ZVFHMIN32-NEXT:    fmv.h.x ft2, t4
; ZVFHMIN32-NEXT:    feq.h t4, fs1, fs7
; ZVFHMIN32-NEXT:    fmv.h.x ft3, t5
; ZVFHMIN32-NEXT:    feq.h t5, fs2, fa4
; ZVFHMIN32-NEXT:    fmv.h.x ft4, a3
; ZVFHMIN32-NEXT:    feq.h t6, fs3, fs10
; ZVFHMIN32-NEXT:    fmv.h.x ft5, a5
; ZVFHMIN32-NEXT:    feq.h a0, fs4, fs6
; ZVFHMIN32-NEXT:    fmv.h.x ft6, ra
; ZVFHMIN32-NEXT:    feq.h a5, ft7, fs11
; ZVFHMIN32-NEXT:    fmv.h.x ft7, a2
; ZVFHMIN32-NEXT:    lh a2, 704(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa6, a1
; ZVFHMIN32-NEXT:    feq.h a6, ft11, fs5
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN32-NEXT:    lh a1, 448(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 160(sp)
; ZVFHMIN32-NEXT:    lh a1, 702(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 446(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 159(sp)
; ZVFHMIN32-NEXT:    lh a1, 700(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 444(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 158(sp)
; ZVFHMIN32-NEXT:    lh a1, 698(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 442(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 157(sp)
; ZVFHMIN32-NEXT:    lh a1, 696(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 440(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 156(sp)
; ZVFHMIN32-NEXT:    lh a1, 694(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 438(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 155(sp)
; ZVFHMIN32-NEXT:    lh a1, 692(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 436(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 154(sp)
; ZVFHMIN32-NEXT:    lh a1, 690(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 434(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 153(sp)
; ZVFHMIN32-NEXT:    lh a1, 688(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 432(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 152(sp)
; ZVFHMIN32-NEXT:    lh a1, 686(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 430(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 151(sp)
; ZVFHMIN32-NEXT:    lh a1, 684(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 428(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 150(sp)
; ZVFHMIN32-NEXT:    lh a1, 682(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 426(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 149(sp)
; ZVFHMIN32-NEXT:    lh a1, 680(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 424(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 148(sp)
; ZVFHMIN32-NEXT:    lh a1, 678(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 422(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 147(sp)
; ZVFHMIN32-NEXT:    lh a1, 676(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 420(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 146(sp)
; ZVFHMIN32-NEXT:    lh a1, 674(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 418(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 145(sp)
; ZVFHMIN32-NEXT:    lh a1, 672(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 416(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb s2, 128(sp)
; ZVFHMIN32-NEXT:    feq.h s2, ft9, ft10
; ZVFHMIN32-NEXT:    sb a1, 144(sp)
; ZVFHMIN32-NEXT:    lh a1, 576(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 320(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 224(sp)
; ZVFHMIN32-NEXT:    lh a1, 574(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 318(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 223(sp)
; ZVFHMIN32-NEXT:    lh a1, 572(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 316(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 222(sp)
; ZVFHMIN32-NEXT:    lh a1, 570(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 314(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 221(sp)
; ZVFHMIN32-NEXT:    lh a1, 568(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 312(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 220(sp)
; ZVFHMIN32-NEXT:    lh a1, 566(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 310(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 219(sp)
; ZVFHMIN32-NEXT:    lh a1, 564(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 308(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 218(sp)
; ZVFHMIN32-NEXT:    lh a1, 562(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 306(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 217(sp)
; ZVFHMIN32-NEXT:    lh a1, 560(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 304(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 216(sp)
; ZVFHMIN32-NEXT:    lh a1, 558(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 302(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 215(sp)
; ZVFHMIN32-NEXT:    lh a1, 556(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 300(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 214(sp)
; ZVFHMIN32-NEXT:    lh a1, 554(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 298(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 213(sp)
; ZVFHMIN32-NEXT:    lh a1, 552(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 296(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 212(sp)
; ZVFHMIN32-NEXT:    lh a1, 550(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 294(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 211(sp)
; ZVFHMIN32-NEXT:    lh a1, 548(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 292(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 210(sp)
; ZVFHMIN32-NEXT:    lh a1, 546(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 290(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 209(sp)
; ZVFHMIN32-NEXT:    lh a1, 544(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 288(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb s11, 192(sp)
; ZVFHMIN32-NEXT:    feq.h s11, fa7, ft8
; ZVFHMIN32-NEXT:    sb a1, 208(sp)
; ZVFHMIN32-NEXT:    lh a1, 738(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 482(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 177(sp)
; ZVFHMIN32-NEXT:    lh a1, 736(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 480(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 176(sp)
; ZVFHMIN32-NEXT:    lh a1, 734(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 478(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 175(sp)
; ZVFHMIN32-NEXT:    lh a1, 732(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 476(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 174(sp)
; ZVFHMIN32-NEXT:    lh a1, 730(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 474(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 173(sp)
; ZVFHMIN32-NEXT:    lh a1, 728(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 472(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 172(sp)
; ZVFHMIN32-NEXT:    lh a1, 726(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 470(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 171(sp)
; ZVFHMIN32-NEXT:    lh a1, 724(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 468(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 170(sp)
; ZVFHMIN32-NEXT:    lh a1, 722(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 466(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 169(sp)
; ZVFHMIN32-NEXT:    lh a1, 720(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 464(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 168(sp)
; ZVFHMIN32-NEXT:    lh a1, 718(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 462(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 167(sp)
; ZVFHMIN32-NEXT:    lh a1, 716(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 460(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 166(sp)
; ZVFHMIN32-NEXT:    lh a1, 714(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 458(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 165(sp)
; ZVFHMIN32-NEXT:    lh a1, 712(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 456(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 164(sp)
; ZVFHMIN32-NEXT:    lh a1, 710(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 454(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 163(sp)
; ZVFHMIN32-NEXT:    lh a1, 708(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 452(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    fmv.h.x fa5, s3
; ZVFHMIN32-NEXT:    sb a1, 162(sp)
; ZVFHMIN32-NEXT:    lh a1, 706(sp)
; ZVFHMIN32-NEXT:    lh a2, 450(sp)
; ZVFHMIN32-NEXT:    sb s10, 129(sp)
; ZVFHMIN32-NEXT:    flh fa4, 114(sp) # 2-byte Folded Reload
; ZVFHMIN32-NEXT:    feq.h s10, fa4, ft2
; ZVFHMIN32-NEXT:    sb s9, 130(sp)
; ZVFHMIN32-NEXT:    feq.h s9, fa3, ft3
; ZVFHMIN32-NEXT:    sb s8, 131(sp)
; ZVFHMIN32-NEXT:    feq.h ra, fa2, ft4
; ZVFHMIN32-NEXT:    sb s7, 132(sp)
; ZVFHMIN32-NEXT:    feq.h s3, fa1, ft5
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h s7, fa0, ft6
; ZVFHMIN32-NEXT:    fmv.h.x fa3, a2
; ZVFHMIN32-NEXT:    feq.h s8, ft0, ft7
; ZVFHMIN32-NEXT:    sb s6, 133(sp)
; ZVFHMIN32-NEXT:    feq.h s6, ft1, fa6
; ZVFHMIN32-NEXT:    sb s5, 134(sp)
; ZVFHMIN32-NEXT:    feq.h a1, fa4, fa3
; ZVFHMIN32-NEXT:    sb s4, 135(sp)
; ZVFHMIN32-NEXT:    flh fa4, 112(sp) # 2-byte Folded Reload
; ZVFHMIN32-NEXT:    feq.h s4, fa4, fa5
; ZVFHMIN32-NEXT:    sb a1, 161(sp)
; ZVFHMIN32-NEXT:    lh a1, 610(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 354(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 241(sp)
; ZVFHMIN32-NEXT:    lh a1, 608(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 352(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 240(sp)
; ZVFHMIN32-NEXT:    lh a1, 606(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 350(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 239(sp)
; ZVFHMIN32-NEXT:    lh a1, 604(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 348(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 238(sp)
; ZVFHMIN32-NEXT:    lh a1, 602(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 346(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 237(sp)
; ZVFHMIN32-NEXT:    lh a1, 600(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 344(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 236(sp)
; ZVFHMIN32-NEXT:    lh a1, 598(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 342(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 235(sp)
; ZVFHMIN32-NEXT:    lh a1, 596(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 340(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 234(sp)
; ZVFHMIN32-NEXT:    lh a1, 594(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 338(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 233(sp)
; ZVFHMIN32-NEXT:    lh a1, 592(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 336(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 232(sp)
; ZVFHMIN32-NEXT:    lh a1, 590(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 334(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 231(sp)
; ZVFHMIN32-NEXT:    lh a1, 588(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 332(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 230(sp)
; ZVFHMIN32-NEXT:    lh a1, 586(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 330(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 229(sp)
; ZVFHMIN32-NEXT:    lh a1, 584(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 328(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 228(sp)
; ZVFHMIN32-NEXT:    lh a1, 582(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 326(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 227(sp)
; ZVFHMIN32-NEXT:    lh a1, 580(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 324(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a4
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a7
; ZVFHMIN32-NEXT:    sb a1, 226(sp)
; ZVFHMIN32-NEXT:    lh a1, 578(sp)
; ZVFHMIN32-NEXT:    lh a2, 322(sp)
; ZVFHMIN32-NEXT:    sb t6, 193(sp)
; ZVFHMIN32-NEXT:    sb t5, 194(sp)
; ZVFHMIN32-NEXT:    sb t4, 195(sp)
; ZVFHMIN32-NEXT:    sb t3, 196(sp)
; ZVFHMIN32-NEXT:    feq.h a4, fa5, fa4
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb t2, 197(sp)
; ZVFHMIN32-NEXT:    sb t1, 198(sp)
; ZVFHMIN32-NEXT:    sb t0, 199(sp)
; ZVFHMIN32-NEXT:    sb a1, 225(sp)
; ZVFHMIN32-NEXT:    lh a1, 766(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 510(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 191(sp)
; ZVFHMIN32-NEXT:    lh a1, 764(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 508(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 190(sp)
; ZVFHMIN32-NEXT:    lh a1, 762(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 506(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 189(sp)
; ZVFHMIN32-NEXT:    lh a1, 760(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 504(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 188(sp)
; ZVFHMIN32-NEXT:    lh a1, 758(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 502(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 187(sp)
; ZVFHMIN32-NEXT:    lh a1, 756(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 500(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 186(sp)
; ZVFHMIN32-NEXT:    lh a1, 754(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 498(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 185(sp)
; ZVFHMIN32-NEXT:    lh a1, 752(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 496(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 184(sp)
; ZVFHMIN32-NEXT:    lh a1, 750(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 494(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 183(sp)
; ZVFHMIN32-NEXT:    lh a1, 748(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 492(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 182(sp)
; ZVFHMIN32-NEXT:    lh a1, 746(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 490(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 181(sp)
; ZVFHMIN32-NEXT:    lh a1, 744(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 488(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    sb a1, 180(sp)
; ZVFHMIN32-NEXT:    lh a1, 742(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN32-NEXT:    lh a1, 486(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    lw a2, 120(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN32-NEXT:    lw a2, 116(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN32-NEXT:    sb a1, 179(sp)
; ZVFHMIN32-NEXT:    lh a2, 740(sp)
; ZVFHMIN32-NEXT:    lh a3, 484(sp)
; ZVFHMIN32-NEXT:    sb s2, 140(sp)
; ZVFHMIN32-NEXT:    sb a6, 141(sp)
; ZVFHMIN32-NEXT:    sb a5, 142(sp)
; ZVFHMIN32-NEXT:    sb a0, 143(sp)
; ZVFHMIN32-NEXT:    sb ra, 136(sp)
; ZVFHMIN32-NEXT:    sb s9, 137(sp)
; ZVFHMIN32-NEXT:    sb s10, 138(sp)
; ZVFHMIN32-NEXT:    sb s11, 139(sp)
; ZVFHMIN32-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a3
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 178(sp)
; ZVFHMIN32-NEXT:    lh a0, 638(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 382(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 255(sp)
; ZVFHMIN32-NEXT:    lh a0, 636(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 380(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 254(sp)
; ZVFHMIN32-NEXT:    lh a0, 634(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 378(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 253(sp)
; ZVFHMIN32-NEXT:    lh a0, 632(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 376(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 252(sp)
; ZVFHMIN32-NEXT:    lh a0, 630(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 374(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 251(sp)
; ZVFHMIN32-NEXT:    lh a0, 628(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 372(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 250(sp)
; ZVFHMIN32-NEXT:    lh a0, 626(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 370(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 249(sp)
; ZVFHMIN32-NEXT:    lh a0, 624(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 368(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 248(sp)
; ZVFHMIN32-NEXT:    lh a0, 622(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 366(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 247(sp)
; ZVFHMIN32-NEXT:    lh a0, 620(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 364(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 246(sp)
; ZVFHMIN32-NEXT:    lh a0, 618(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 362(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 245(sp)
; ZVFHMIN32-NEXT:    lh a0, 616(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 360(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 244(sp)
; ZVFHMIN32-NEXT:    lh a0, 614(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    lh a0, 358(sp)
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    lw a2, 124(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN32-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN32-NEXT:    vslidedown.vi v8, v24, 8
; ZVFHMIN32-NEXT:    vmv.x.s a2, v8
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN32-NEXT:    sb a0, 243(sp)
; ZVFHMIN32-NEXT:    lh a0, 612(sp)
; ZVFHMIN32-NEXT:    lh a2, 356(sp)
; ZVFHMIN32-NEXT:    sb s6, 204(sp)
; ZVFHMIN32-NEXT:    sb s8, 205(sp)
; ZVFHMIN32-NEXT:    sb s7, 206(sp)
; ZVFHMIN32-NEXT:    sb s3, 207(sp)
; ZVFHMIN32-NEXT:    feq.h a3, fa5, fa4
; ZVFHMIN32-NEXT:    sb a3, 200(sp)
; ZVFHMIN32-NEXT:    sb a1, 201(sp)
; ZVFHMIN32-NEXT:    sb a4, 202(sp)
; ZVFHMIN32-NEXT:    sb s4, 203(sp)
; ZVFHMIN32-NEXT:    li a1, 128
; ZVFHMIN32-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN32-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN32-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN32-NEXT:    sb a0, 242(sp)
; ZVFHMIN32-NEXT:    addi a0, sp, 128
; ZVFHMIN32-NEXT:    vsetvli zero, a1, e8, m8, ta, ma
; ZVFHMIN32-NEXT:    vle8.v v8, (a0)
; ZVFHMIN32-NEXT:    vand.vi v8, v8, 1
; ZVFHMIN32-NEXT:    vmsne.vi v0, v8, 0
; ZVFHMIN32-NEXT:    addi sp, s0, -1024
; ZVFHMIN32-NEXT:    .cfi_def_cfa sp, 1024
; ZVFHMIN32-NEXT:    lw ra, 1020(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s0, 1016(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s2, 1012(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s3, 1008(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s4, 1004(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s5, 1000(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s6, 996(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s7, 992(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s8, 988(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s9, 984(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s10, 980(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    lw s11, 976(sp) # 4-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs0, 968(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs1, 960(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs2, 952(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs3, 944(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs4, 936(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs5, 928(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs6, 920(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs7, 912(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs8, 904(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs9, 896(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs10, 888(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    fld fs11, 880(sp) # 8-byte Folded Reload
; ZVFHMIN32-NEXT:    .cfi_restore ra
; ZVFHMIN32-NEXT:    .cfi_restore s0
; ZVFHMIN32-NEXT:    .cfi_restore s2
; ZVFHMIN32-NEXT:    .cfi_restore s3
; ZVFHMIN32-NEXT:    .cfi_restore s4
; ZVFHMIN32-NEXT:    .cfi_restore s5
; ZVFHMIN32-NEXT:    .cfi_restore s6
; ZVFHMIN32-NEXT:    .cfi_restore s7
; ZVFHMIN32-NEXT:    .cfi_restore s8
; ZVFHMIN32-NEXT:    .cfi_restore s9
; ZVFHMIN32-NEXT:    .cfi_restore s10
; ZVFHMIN32-NEXT:    .cfi_restore s11
; ZVFHMIN32-NEXT:    .cfi_restore fs0
; ZVFHMIN32-NEXT:    .cfi_restore fs1
; ZVFHMIN32-NEXT:    .cfi_restore fs2
; ZVFHMIN32-NEXT:    .cfi_restore fs3
; ZVFHMIN32-NEXT:    .cfi_restore fs4
; ZVFHMIN32-NEXT:    .cfi_restore fs5
; ZVFHMIN32-NEXT:    .cfi_restore fs6
; ZVFHMIN32-NEXT:    .cfi_restore fs7
; ZVFHMIN32-NEXT:    .cfi_restore fs8
; ZVFHMIN32-NEXT:    .cfi_restore fs9
; ZVFHMIN32-NEXT:    .cfi_restore fs10
; ZVFHMIN32-NEXT:    .cfi_restore fs11
; ZVFHMIN32-NEXT:    addi sp, sp, 1024
; ZVFHMIN32-NEXT:    .cfi_def_cfa_offset 0
; ZVFHMIN32-NEXT:    ret
;
; ZVFHMIN64-LABEL: fcmp_oeq_vv_v128f16:
; ZVFHMIN64:       # %bb.0:
; ZVFHMIN64-NEXT:    addi sp, sp, -1024
; ZVFHMIN64-NEXT:    .cfi_def_cfa_offset 1024
; ZVFHMIN64-NEXT:    sd ra, 1016(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s0, 1008(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s2, 1000(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s3, 992(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s4, 984(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s5, 976(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s6, 968(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s7, 960(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s8, 952(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s9, 944(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s10, 936(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    sd s11, 928(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs0, 920(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs1, 912(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs2, 904(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs3, 896(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs4, 888(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs5, 880(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs6, 872(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs7, 864(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs8, 856(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs9, 848(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs10, 840(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fsd fs11, 832(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    .cfi_offset ra, -8
; ZVFHMIN64-NEXT:    .cfi_offset s0, -16
; ZVFHMIN64-NEXT:    .cfi_offset s2, -24
; ZVFHMIN64-NEXT:    .cfi_offset s3, -32
; ZVFHMIN64-NEXT:    .cfi_offset s4, -40
; ZVFHMIN64-NEXT:    .cfi_offset s5, -48
; ZVFHMIN64-NEXT:    .cfi_offset s6, -56
; ZVFHMIN64-NEXT:    .cfi_offset s7, -64
; ZVFHMIN64-NEXT:    .cfi_offset s8, -72
; ZVFHMIN64-NEXT:    .cfi_offset s9, -80
; ZVFHMIN64-NEXT:    .cfi_offset s10, -88
; ZVFHMIN64-NEXT:    .cfi_offset s11, -96
; ZVFHMIN64-NEXT:    .cfi_offset fs0, -104
; ZVFHMIN64-NEXT:    .cfi_offset fs1, -112
; ZVFHMIN64-NEXT:    .cfi_offset fs2, -120
; ZVFHMIN64-NEXT:    .cfi_offset fs3, -128
; ZVFHMIN64-NEXT:    .cfi_offset fs4, -136
; ZVFHMIN64-NEXT:    .cfi_offset fs5, -144
; ZVFHMIN64-NEXT:    .cfi_offset fs6, -152
; ZVFHMIN64-NEXT:    .cfi_offset fs7, -160
; ZVFHMIN64-NEXT:    .cfi_offset fs8, -168
; ZVFHMIN64-NEXT:    .cfi_offset fs9, -176
; ZVFHMIN64-NEXT:    .cfi_offset fs10, -184
; ZVFHMIN64-NEXT:    .cfi_offset fs11, -192
; ZVFHMIN64-NEXT:    addi s0, sp, 1024
; ZVFHMIN64-NEXT:    .cfi_def_cfa s0, 0
; ZVFHMIN64-NEXT:    csrr a1, vlenb
; ZVFHMIN64-NEXT:    li a2, 41
; ZVFHMIN64-NEXT:    mul a1, a1, a2
; ZVFHMIN64-NEXT:    sub sp, sp, a1
; ZVFHMIN64-NEXT:    andi sp, sp, -128
; ZVFHMIN64-NEXT:    addi a3, a0, 128
; ZVFHMIN64-NEXT:    li a1, 64
; ZVFHMIN64-NEXT:    addi a4, sp, 640
; ZVFHMIN64-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN64-NEXT:    vmv.x.s a2, v8
; ZVFHMIN64-NEXT:    vle16.v v24, (a3)
; ZVFHMIN64-NEXT:    csrr a3, vlenb
; ZVFHMIN64-NEXT:    slli a5, a3, 5
; ZVFHMIN64-NEXT:    add a3, a5, a3
; ZVFHMIN64-NEXT:    add a3, sp, a3
; ZVFHMIN64-NEXT:    addi a3, a3, 832
; ZVFHMIN64-NEXT:    vs8r.v v24, (a3) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vle16.v v0, (a0)
; ZVFHMIN64-NEXT:    vse16.v v8, (a4)
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 7
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a0, a0, 5
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 6
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 5
; ZVFHMIN64-NEXT:    sub a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 5
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 30
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 4
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 29
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 3
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 28
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 2
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 27
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 1
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 26
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 15
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 24
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 14
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 22
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 13
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 20
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 12
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 18
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 11
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 3
; ZVFHMIN64-NEXT:    add a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 10
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 3
; ZVFHMIN64-NEXT:    sub a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v8, 9
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 2
; ZVFHMIN64-NEXT:    add a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v10, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v8, 8
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 1
; ZVFHMIN64-NEXT:    add a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    addi a0, sp, 384
; ZVFHMIN64-NEXT:    addi a3, sp, 512
; ZVFHMIN64-NEXT:    vmv.x.s a5, v16
; ZVFHMIN64-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN64-NEXT:    vse16.v v0, (a0)
; ZVFHMIN64-NEXT:    vse16.v v16, (a3)
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 7
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 11
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 6
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 12
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 5
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 13
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 4
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 14
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 3
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 4
; ZVFHMIN64-NEXT:    sub a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 2
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a0, a0, 4
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 1
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 4
; ZVFHMIN64-NEXT:    add a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v16, 15
; ZVFHMIN64-NEXT:    addi a0, sp, 832
; ZVFHMIN64-NEXT:    vs2r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vslidedown.vi v4, v16, 14
; ZVFHMIN64-NEXT:    vslidedown.vi v2, v16, 13
; ZVFHMIN64-NEXT:    vslidedown.vi v24, v16, 12
; ZVFHMIN64-NEXT:    vslidedown.vi v22, v16, 11
; ZVFHMIN64-NEXT:    vslidedown.vi v20, v16, 10
; ZVFHMIN64-NEXT:    vslidedown.vi v18, v16, 9
; ZVFHMIN64-NEXT:    vslidedown.vi v16, v16, 8
; ZVFHMIN64-NEXT:    vmv.x.s a6, v0
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v5, v0, 7
; ZVFHMIN64-NEXT:    vslidedown.vi v17, v0, 6
; ZVFHMIN64-NEXT:    vslidedown.vi v23, v0, 5
; ZVFHMIN64-NEXT:    vslidedown.vi v19, v0, 4
; ZVFHMIN64-NEXT:    vslidedown.vi v21, v0, 3
; ZVFHMIN64-NEXT:    vslidedown.vi v3, v0, 2
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v0, 1
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a0, a0, 1
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vs1r.v v8, (a0) # Unknown-size Folded Spill
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v0, 15
; ZVFHMIN64-NEXT:    vslidedown.vi v10, v0, 14
; ZVFHMIN64-NEXT:    vslidedown.vi v12, v0, 13
; ZVFHMIN64-NEXT:    vslidedown.vi v14, v0, 12
; ZVFHMIN64-NEXT:    vslidedown.vi v26, v0, 11
; ZVFHMIN64-NEXT:    vslidedown.vi v28, v0, 10
; ZVFHMIN64-NEXT:    vslidedown.vi v30, v0, 9
; ZVFHMIN64-NEXT:    vslidedown.vi v0, v0, 8
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    li a3, 24
; ZVFHMIN64-NEXT:    mul a0, a0, a3
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a0) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s a0, v6
; ZVFHMIN64-NEXT:    csrr a3, vlenb
; ZVFHMIN64-NEXT:    li a4, 22
; ZVFHMIN64-NEXT:    mul a3, a3, a4
; ZVFHMIN64-NEXT:    add a3, sp, a3
; ZVFHMIN64-NEXT:    addi a3, a3, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a3) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s a3, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    li a7, 20
; ZVFHMIN64-NEXT:    mul a4, a4, a7
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s a7, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    li t0, 18
; ZVFHMIN64-NEXT:    mul a4, a4, t0
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s3, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    slli t0, a4, 3
; ZVFHMIN64-NEXT:    add a4, t0, a4
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s10, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    slli t0, a4, 3
; ZVFHMIN64-NEXT:    sub a4, t0, a4
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s11, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    slli t0, a4, 2
; ZVFHMIN64-NEXT:    add a4, t0, a4
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s5, v6
; ZVFHMIN64-NEXT:    csrr a4, vlenb
; ZVFHMIN64-NEXT:    slli t0, a4, 1
; ZVFHMIN64-NEXT:    add a4, t0, a4
; ZVFHMIN64-NEXT:    add a4, sp, a4
; ZVFHMIN64-NEXT:    addi a4, a4, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s7, v6
; ZVFHMIN64-NEXT:    addi a4, sp, 832
; ZVFHMIN64-NEXT:    vl2r.v v6, (a4) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vmv.x.s s9, v6
; ZVFHMIN64-NEXT:    vmv.x.s s8, v4
; ZVFHMIN64-NEXT:    vmv.x.s s6, v2
; ZVFHMIN64-NEXT:    vmv.x.s s4, v24
; ZVFHMIN64-NEXT:    vmv.x.s s2, v22
; ZVFHMIN64-NEXT:    vmv.x.s a4, v20
; ZVFHMIN64-NEXT:    vmv.x.s t0, v18
; ZVFHMIN64-NEXT:    sd t0, 112(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    vmv.x.s t0, v16
; ZVFHMIN64-NEXT:    sd t0, 120(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    vmv.x.s t6, v8
; ZVFHMIN64-NEXT:    vmv.x.s t0, v10
; ZVFHMIN64-NEXT:    vmv.x.s t1, v12
; ZVFHMIN64-NEXT:    vmv.x.s t2, v14
; ZVFHMIN64-NEXT:    vmv.x.s t3, v26
; ZVFHMIN64-NEXT:    vmv.x.s t4, v28
; ZVFHMIN64-NEXT:    vmv.x.s t5, v30
; ZVFHMIN64-NEXT:    fmv.h.x fs8, a2
; ZVFHMIN64-NEXT:    fmv.h.x fs7, a5
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    slli a2, a2, 5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs6, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    slli a5, a2, 5
; ZVFHMIN64-NEXT:    sub a2, a5, a2
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs5, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 30
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft10, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 29
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft8, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 28
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft2, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 27
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft3, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 26
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft4, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 11
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft5, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 12
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x ft6, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 13
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fa6, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    li a5, 14
; ZVFHMIN64-NEXT:    mul a2, a2, a5
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs0, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    slli a5, a2, 4
; ZVFHMIN64-NEXT:    sub a2, a5, a2
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs1, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    slli a2, a2, 4
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs2, a2
; ZVFHMIN64-NEXT:    csrr a2, vlenb
; ZVFHMIN64-NEXT:    slli a5, a2, 4
; ZVFHMIN64-NEXT:    add a2, a5, a2
; ZVFHMIN64-NEXT:    add a2, sp, a2
; ZVFHMIN64-NEXT:    lh a2, 832(a2) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fs3, a2
; ZVFHMIN64-NEXT:    addi a2, sp, 256
; ZVFHMIN64-NEXT:    fmv.h.x fs4, a0
; ZVFHMIN64-NEXT:    fmv.h.x ft7, a3
; ZVFHMIN64-NEXT:    fmv.h.x ft11, a7
; ZVFHMIN64-NEXT:    fmv.h.x ft9, s3
; ZVFHMIN64-NEXT:    fmv.h.x fa7, s10
; ZVFHMIN64-NEXT:    fmv.h.x fa5, s11
; ZVFHMIN64-NEXT:    fsh fa5, 102(sp) # 2-byte Folded Spill
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a3, a0, 5
; ZVFHMIN64-NEXT:    add a0, a3, a0
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    addi a0, a0, 832
; ZVFHMIN64-NEXT:    vl8r.v v24, (a0) # Unknown-size Folded Reload
; ZVFHMIN64-NEXT:    vsetvli zero, a1, e16, m8, ta, ma
; ZVFHMIN64-NEXT:    vse16.v v24, (a2)
; ZVFHMIN64-NEXT:    vmv.x.s a3, v0
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 15
; ZVFHMIN64-NEXT:    vmv.x.s a5, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 14
; ZVFHMIN64-NEXT:    vmv.x.s ra, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 13
; ZVFHMIN64-NEXT:    vmv.x.s a2, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 12
; ZVFHMIN64-NEXT:    vmv.x.s a1, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 11
; ZVFHMIN64-NEXT:    vmv.x.s s3, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 10
; ZVFHMIN64-NEXT:    vmv.x.s a7, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 9
; ZVFHMIN64-NEXT:    vmv.x.s a0, v8
; ZVFHMIN64-NEXT:    sd a0, 104(sp) # 8-byte Folded Spill
; ZVFHMIN64-NEXT:    fmv.h.x fa3, s5
; ZVFHMIN64-NEXT:    vmv.x.s s5, v5
; ZVFHMIN64-NEXT:    fmv.h.x fa2, s7
; ZVFHMIN64-NEXT:    vmv.x.s s7, v17
; ZVFHMIN64-NEXT:    fmv.h.x fa1, s9
; ZVFHMIN64-NEXT:    vmv.x.s s9, v23
; ZVFHMIN64-NEXT:    fmv.h.x fa0, s8
; ZVFHMIN64-NEXT:    vmv.x.s s8, v19
; ZVFHMIN64-NEXT:    fmv.h.x ft0, s6
; ZVFHMIN64-NEXT:    vmv.x.s s6, v21
; ZVFHMIN64-NEXT:    fmv.h.x ft1, s4
; ZVFHMIN64-NEXT:    vmv.x.s s10, v3
; ZVFHMIN64-NEXT:    fmv.h.x fa5, s2
; ZVFHMIN64-NEXT:    fsh fa5, 100(sp) # 2-byte Folded Spill
; ZVFHMIN64-NEXT:    vmv.x.s s2, v24
; ZVFHMIN64-NEXT:    fmv.h.x fs9, a6
; ZVFHMIN64-NEXT:    csrr a0, vlenb
; ZVFHMIN64-NEXT:    slli a0, a0, 1
; ZVFHMIN64-NEXT:    add a0, sp, a0
; ZVFHMIN64-NEXT:    lh a6, 832(a0) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m1, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 7
; ZVFHMIN64-NEXT:    fmv.h.x fs10, s2
; ZVFHMIN64-NEXT:    vmv.x.s a0, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 6
; ZVFHMIN64-NEXT:    fmv.h.x fs11, s5
; ZVFHMIN64-NEXT:    feq.h s2, fs8, fs9
; ZVFHMIN64-NEXT:    fmv.h.x fs8, s7
; ZVFHMIN64-NEXT:    vmv.x.s s7, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 5
; ZVFHMIN64-NEXT:    fmv.h.x fs9, s9
; ZVFHMIN64-NEXT:    feq.h s11, fs7, fs10
; ZVFHMIN64-NEXT:    fmv.h.x fs7, s8
; ZVFHMIN64-NEXT:    vmv.x.s s8, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 4
; ZVFHMIN64-NEXT:    fmv.h.x fs10, s6
; ZVFHMIN64-NEXT:    feq.h s4, fs6, fs11
; ZVFHMIN64-NEXT:    fmv.h.x fs6, s10
; ZVFHMIN64-NEXT:    vmv.x.s s9, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 3
; ZVFHMIN64-NEXT:    fmv.h.x fs11, a6
; ZVFHMIN64-NEXT:    feq.h s5, fs5, fs8
; ZVFHMIN64-NEXT:    fmv.h.x fs5, a0
; ZVFHMIN64-NEXT:    vmv.x.s a0, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 2
; ZVFHMIN64-NEXT:    fmv.h.x fs8, s7
; ZVFHMIN64-NEXT:    feq.h s6, ft10, fs9
; ZVFHMIN64-NEXT:    fmv.h.x fs9, s8
; ZVFHMIN64-NEXT:    vmv.x.s a6, v8
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 1
; ZVFHMIN64-NEXT:    fmv.h.x fa5, s9
; ZVFHMIN64-NEXT:    feq.h s7, ft8, fs7
; ZVFHMIN64-NEXT:    fmv.h.x fs7, a0
; ZVFHMIN64-NEXT:    vmv.x.s a0, v8
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a6
; ZVFHMIN64-NEXT:    feq.h s8, ft2, fs10
; ZVFHMIN64-NEXT:    fmv.h.x fs10, a0
; ZVFHMIN64-NEXT:    feq.h s9, ft3, fs6
; ZVFHMIN64-NEXT:    fmv.h.x fs6, t6
; ZVFHMIN64-NEXT:    feq.h s10, ft4, fs11
; ZVFHMIN64-NEXT:    fmv.h.x fs11, t0
; ZVFHMIN64-NEXT:    feq.h t0, ft5, fs5
; ZVFHMIN64-NEXT:    fmv.h.x fs5, t1
; ZVFHMIN64-NEXT:    feq.h t1, ft6, fs8
; ZVFHMIN64-NEXT:    fmv.h.x ft10, t2
; ZVFHMIN64-NEXT:    feq.h t2, fa6, fs9
; ZVFHMIN64-NEXT:    fmv.h.x ft8, t3
; ZVFHMIN64-NEXT:    feq.h t3, fs0, fa5
; ZVFHMIN64-NEXT:    fmv.h.x ft2, t4
; ZVFHMIN64-NEXT:    feq.h t4, fs1, fs7
; ZVFHMIN64-NEXT:    fmv.h.x ft3, t5
; ZVFHMIN64-NEXT:    feq.h t5, fs2, fa4
; ZVFHMIN64-NEXT:    fmv.h.x ft4, a3
; ZVFHMIN64-NEXT:    feq.h t6, fs3, fs10
; ZVFHMIN64-NEXT:    fmv.h.x ft5, a5
; ZVFHMIN64-NEXT:    feq.h a0, fs4, fs6
; ZVFHMIN64-NEXT:    fmv.h.x ft6, ra
; ZVFHMIN64-NEXT:    feq.h a5, ft7, fs11
; ZVFHMIN64-NEXT:    fmv.h.x ft7, a2
; ZVFHMIN64-NEXT:    lh a2, 704(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa6, a1
; ZVFHMIN64-NEXT:    feq.h a6, ft11, fs5
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN64-NEXT:    lh a1, 448(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 160(sp)
; ZVFHMIN64-NEXT:    lh a1, 702(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 446(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 159(sp)
; ZVFHMIN64-NEXT:    lh a1, 700(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 444(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 158(sp)
; ZVFHMIN64-NEXT:    lh a1, 698(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 442(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 157(sp)
; ZVFHMIN64-NEXT:    lh a1, 696(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 440(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 156(sp)
; ZVFHMIN64-NEXT:    lh a1, 694(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 438(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 155(sp)
; ZVFHMIN64-NEXT:    lh a1, 692(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 436(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 154(sp)
; ZVFHMIN64-NEXT:    lh a1, 690(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 434(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 153(sp)
; ZVFHMIN64-NEXT:    lh a1, 688(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 432(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 152(sp)
; ZVFHMIN64-NEXT:    lh a1, 686(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 430(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 151(sp)
; ZVFHMIN64-NEXT:    lh a1, 684(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 428(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 150(sp)
; ZVFHMIN64-NEXT:    lh a1, 682(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 426(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 149(sp)
; ZVFHMIN64-NEXT:    lh a1, 680(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 424(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 148(sp)
; ZVFHMIN64-NEXT:    lh a1, 678(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 422(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 147(sp)
; ZVFHMIN64-NEXT:    lh a1, 676(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 420(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 146(sp)
; ZVFHMIN64-NEXT:    lh a1, 674(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 418(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 145(sp)
; ZVFHMIN64-NEXT:    lh a1, 672(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 416(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb s2, 128(sp)
; ZVFHMIN64-NEXT:    feq.h s2, ft9, ft10
; ZVFHMIN64-NEXT:    sb a1, 144(sp)
; ZVFHMIN64-NEXT:    lh a1, 576(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 320(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 224(sp)
; ZVFHMIN64-NEXT:    lh a1, 574(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 318(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 223(sp)
; ZVFHMIN64-NEXT:    lh a1, 572(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 316(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 222(sp)
; ZVFHMIN64-NEXT:    lh a1, 570(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 314(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 221(sp)
; ZVFHMIN64-NEXT:    lh a1, 568(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 312(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 220(sp)
; ZVFHMIN64-NEXT:    lh a1, 566(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 310(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 219(sp)
; ZVFHMIN64-NEXT:    lh a1, 564(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 308(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 218(sp)
; ZVFHMIN64-NEXT:    lh a1, 562(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 306(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 217(sp)
; ZVFHMIN64-NEXT:    lh a1, 560(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 304(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 216(sp)
; ZVFHMIN64-NEXT:    lh a1, 558(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 302(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 215(sp)
; ZVFHMIN64-NEXT:    lh a1, 556(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 300(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 214(sp)
; ZVFHMIN64-NEXT:    lh a1, 554(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 298(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 213(sp)
; ZVFHMIN64-NEXT:    lh a1, 552(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 296(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 212(sp)
; ZVFHMIN64-NEXT:    lh a1, 550(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 294(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 211(sp)
; ZVFHMIN64-NEXT:    lh a1, 548(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 292(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 210(sp)
; ZVFHMIN64-NEXT:    lh a1, 546(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 290(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 209(sp)
; ZVFHMIN64-NEXT:    lh a1, 544(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 288(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb s11, 192(sp)
; ZVFHMIN64-NEXT:    feq.h s11, fa7, ft8
; ZVFHMIN64-NEXT:    sb a1, 208(sp)
; ZVFHMIN64-NEXT:    lh a1, 738(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 482(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 177(sp)
; ZVFHMIN64-NEXT:    lh a1, 736(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 480(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 176(sp)
; ZVFHMIN64-NEXT:    lh a1, 734(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 478(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 175(sp)
; ZVFHMIN64-NEXT:    lh a1, 732(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 476(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 174(sp)
; ZVFHMIN64-NEXT:    lh a1, 730(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 474(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 173(sp)
; ZVFHMIN64-NEXT:    lh a1, 728(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 472(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 172(sp)
; ZVFHMIN64-NEXT:    lh a1, 726(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 470(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 171(sp)
; ZVFHMIN64-NEXT:    lh a1, 724(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 468(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 170(sp)
; ZVFHMIN64-NEXT:    lh a1, 722(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 466(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 169(sp)
; ZVFHMIN64-NEXT:    lh a1, 720(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 464(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 168(sp)
; ZVFHMIN64-NEXT:    lh a1, 718(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 462(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 167(sp)
; ZVFHMIN64-NEXT:    lh a1, 716(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 460(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 166(sp)
; ZVFHMIN64-NEXT:    lh a1, 714(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 458(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 165(sp)
; ZVFHMIN64-NEXT:    lh a1, 712(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 456(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 164(sp)
; ZVFHMIN64-NEXT:    lh a1, 710(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 454(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 163(sp)
; ZVFHMIN64-NEXT:    lh a1, 708(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 452(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    fmv.h.x fa5, s3
; ZVFHMIN64-NEXT:    sb a1, 162(sp)
; ZVFHMIN64-NEXT:    lh a1, 706(sp)
; ZVFHMIN64-NEXT:    lh a2, 450(sp)
; ZVFHMIN64-NEXT:    sb s10, 129(sp)
; ZVFHMIN64-NEXT:    flh fa4, 102(sp) # 2-byte Folded Reload
; ZVFHMIN64-NEXT:    feq.h s10, fa4, ft2
; ZVFHMIN64-NEXT:    sb s9, 130(sp)
; ZVFHMIN64-NEXT:    feq.h s9, fa3, ft3
; ZVFHMIN64-NEXT:    sb s8, 131(sp)
; ZVFHMIN64-NEXT:    feq.h ra, fa2, ft4
; ZVFHMIN64-NEXT:    sb s7, 132(sp)
; ZVFHMIN64-NEXT:    feq.h s3, fa1, ft5
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h s7, fa0, ft6
; ZVFHMIN64-NEXT:    fmv.h.x fa3, a2
; ZVFHMIN64-NEXT:    feq.h s8, ft0, ft7
; ZVFHMIN64-NEXT:    sb s6, 133(sp)
; ZVFHMIN64-NEXT:    feq.h s6, ft1, fa6
; ZVFHMIN64-NEXT:    sb s5, 134(sp)
; ZVFHMIN64-NEXT:    feq.h a1, fa4, fa3
; ZVFHMIN64-NEXT:    sb s4, 135(sp)
; ZVFHMIN64-NEXT:    flh fa4, 100(sp) # 2-byte Folded Reload
; ZVFHMIN64-NEXT:    feq.h s4, fa4, fa5
; ZVFHMIN64-NEXT:    sb a1, 161(sp)
; ZVFHMIN64-NEXT:    lh a1, 610(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 354(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 241(sp)
; ZVFHMIN64-NEXT:    lh a1, 608(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 352(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 240(sp)
; ZVFHMIN64-NEXT:    lh a1, 606(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 350(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 239(sp)
; ZVFHMIN64-NEXT:    lh a1, 604(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 348(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 238(sp)
; ZVFHMIN64-NEXT:    lh a1, 602(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 346(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 237(sp)
; ZVFHMIN64-NEXT:    lh a1, 600(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 344(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 236(sp)
; ZVFHMIN64-NEXT:    lh a1, 598(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 342(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 235(sp)
; ZVFHMIN64-NEXT:    lh a1, 596(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 340(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 234(sp)
; ZVFHMIN64-NEXT:    lh a1, 594(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 338(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 233(sp)
; ZVFHMIN64-NEXT:    lh a1, 592(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 336(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 232(sp)
; ZVFHMIN64-NEXT:    lh a1, 590(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 334(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 231(sp)
; ZVFHMIN64-NEXT:    lh a1, 588(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 332(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 230(sp)
; ZVFHMIN64-NEXT:    lh a1, 586(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 330(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 229(sp)
; ZVFHMIN64-NEXT:    lh a1, 584(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 328(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 228(sp)
; ZVFHMIN64-NEXT:    lh a1, 582(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 326(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 227(sp)
; ZVFHMIN64-NEXT:    lh a1, 580(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 324(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a4
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a7
; ZVFHMIN64-NEXT:    sb a1, 226(sp)
; ZVFHMIN64-NEXT:    lh a1, 578(sp)
; ZVFHMIN64-NEXT:    lh a2, 322(sp)
; ZVFHMIN64-NEXT:    sb t6, 193(sp)
; ZVFHMIN64-NEXT:    sb t5, 194(sp)
; ZVFHMIN64-NEXT:    sb t4, 195(sp)
; ZVFHMIN64-NEXT:    sb t3, 196(sp)
; ZVFHMIN64-NEXT:    feq.h a4, fa5, fa4
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb t2, 197(sp)
; ZVFHMIN64-NEXT:    sb t1, 198(sp)
; ZVFHMIN64-NEXT:    sb t0, 199(sp)
; ZVFHMIN64-NEXT:    sb a1, 225(sp)
; ZVFHMIN64-NEXT:    lh a1, 766(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 510(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 191(sp)
; ZVFHMIN64-NEXT:    lh a1, 764(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 508(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 190(sp)
; ZVFHMIN64-NEXT:    lh a1, 762(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 506(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 189(sp)
; ZVFHMIN64-NEXT:    lh a1, 760(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 504(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 188(sp)
; ZVFHMIN64-NEXT:    lh a1, 758(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 502(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 187(sp)
; ZVFHMIN64-NEXT:    lh a1, 756(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 500(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 186(sp)
; ZVFHMIN64-NEXT:    lh a1, 754(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 498(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 185(sp)
; ZVFHMIN64-NEXT:    lh a1, 752(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 496(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 184(sp)
; ZVFHMIN64-NEXT:    lh a1, 750(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 494(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 183(sp)
; ZVFHMIN64-NEXT:    lh a1, 748(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 492(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 182(sp)
; ZVFHMIN64-NEXT:    lh a1, 746(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 490(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 181(sp)
; ZVFHMIN64-NEXT:    lh a1, 744(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 488(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    sb a1, 180(sp)
; ZVFHMIN64-NEXT:    lh a1, 742(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a1
; ZVFHMIN64-NEXT:    lh a1, 486(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a1
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    ld a2, 112(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN64-NEXT:    ld a2, 104(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN64-NEXT:    sb a1, 179(sp)
; ZVFHMIN64-NEXT:    lh a2, 740(sp)
; ZVFHMIN64-NEXT:    lh a3, 484(sp)
; ZVFHMIN64-NEXT:    sb s2, 140(sp)
; ZVFHMIN64-NEXT:    sb a6, 141(sp)
; ZVFHMIN64-NEXT:    sb a5, 142(sp)
; ZVFHMIN64-NEXT:    sb a0, 143(sp)
; ZVFHMIN64-NEXT:    sb ra, 136(sp)
; ZVFHMIN64-NEXT:    sb s9, 137(sp)
; ZVFHMIN64-NEXT:    sb s10, 138(sp)
; ZVFHMIN64-NEXT:    sb s11, 139(sp)
; ZVFHMIN64-NEXT:    feq.h a1, fa5, fa4
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a3
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 178(sp)
; ZVFHMIN64-NEXT:    lh a0, 638(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 382(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 255(sp)
; ZVFHMIN64-NEXT:    lh a0, 636(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 380(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 254(sp)
; ZVFHMIN64-NEXT:    lh a0, 634(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 378(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 253(sp)
; ZVFHMIN64-NEXT:    lh a0, 632(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 376(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 252(sp)
; ZVFHMIN64-NEXT:    lh a0, 630(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 374(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 251(sp)
; ZVFHMIN64-NEXT:    lh a0, 628(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 372(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 250(sp)
; ZVFHMIN64-NEXT:    lh a0, 626(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 370(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 249(sp)
; ZVFHMIN64-NEXT:    lh a0, 624(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 368(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 248(sp)
; ZVFHMIN64-NEXT:    lh a0, 622(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 366(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 247(sp)
; ZVFHMIN64-NEXT:    lh a0, 620(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 364(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 246(sp)
; ZVFHMIN64-NEXT:    lh a0, 618(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 362(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 245(sp)
; ZVFHMIN64-NEXT:    lh a0, 616(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 360(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 244(sp)
; ZVFHMIN64-NEXT:    lh a0, 614(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    lh a0, 358(sp)
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a0
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    ld a2, 120(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a2
; ZVFHMIN64-NEXT:    vsetivli zero, 1, e16, m2, ta, ma
; ZVFHMIN64-NEXT:    vslidedown.vi v8, v24, 8
; ZVFHMIN64-NEXT:    vmv.x.s a2, v8
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN64-NEXT:    sb a0, 243(sp)
; ZVFHMIN64-NEXT:    lh a0, 612(sp)
; ZVFHMIN64-NEXT:    lh a2, 356(sp)
; ZVFHMIN64-NEXT:    sb s6, 204(sp)
; ZVFHMIN64-NEXT:    sb s8, 205(sp)
; ZVFHMIN64-NEXT:    sb s7, 206(sp)
; ZVFHMIN64-NEXT:    sb s3, 207(sp)
; ZVFHMIN64-NEXT:    feq.h a3, fa5, fa4
; ZVFHMIN64-NEXT:    sb a3, 200(sp)
; ZVFHMIN64-NEXT:    sb a1, 201(sp)
; ZVFHMIN64-NEXT:    sb a4, 202(sp)
; ZVFHMIN64-NEXT:    sb s4, 203(sp)
; ZVFHMIN64-NEXT:    li a1, 128
; ZVFHMIN64-NEXT:    fmv.h.x fa5, a0
; ZVFHMIN64-NEXT:    fmv.h.x fa4, a2
; ZVFHMIN64-NEXT:    feq.h a0, fa5, fa4
; ZVFHMIN64-NEXT:    sb a0, 242(sp)
; ZVFHMIN64-NEXT:    addi a0, sp, 128
; ZVFHMIN64-NEXT:    vsetvli zero, a1, e8, m8, ta, ma
; ZVFHMIN64-NEXT:    vle8.v v8, (a0)
; ZVFHMIN64-NEXT:    vand.vi v8, v8, 1
; ZVFHMIN64-NEXT:    vmsne.vi v0, v8, 0
; ZVFHMIN64-NEXT:    addi sp, s0, -1024
; ZVFHMIN64-NEXT:    .cfi_def_cfa sp, 1024
; ZVFHMIN64-NEXT:    ld ra, 1016(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s0, 1008(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s2, 1000(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s3, 992(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s4, 984(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s5, 976(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s6, 968(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s7, 960(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s8, 952(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s9, 944(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s10, 936(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    ld s11, 928(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs0, 920(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs1, 912(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs2, 904(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs3, 896(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs4, 888(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs5, 880(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs6, 872(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs7, 864(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs8, 856(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs9, 848(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs10, 840(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    fld fs11, 832(sp) # 8-byte Folded Reload
; ZVFHMIN64-NEXT:    .cfi_restore ra
; ZVFHMIN64-NEXT:    .cfi_restore s0
; ZVFHMIN64-NEXT:    .cfi_restore s2
; ZVFHMIN64-NEXT:    .cfi_restore s3
; ZVFHMIN64-NEXT:    .cfi_restore s4
; ZVFHMIN64-NEXT:    .cfi_restore s5
; ZVFHMIN64-NEXT:    .cfi_restore s6
; ZVFHMIN64-NEXT:    .cfi_restore s7
; ZVFHMIN64-NEXT:    .cfi_restore s8
; ZVFHMIN64-NEXT:    .cfi_restore s9
; ZVFHMIN64-NEXT:    .cfi_restore s10
; ZVFHMIN64-NEXT:    .cfi_restore s11
; ZVFHMIN64-NEXT:    .cfi_restore fs0
; ZVFHMIN64-NEXT:    .cfi_restore fs1
; ZVFHMIN64-NEXT:    .cfi_restore fs2
; ZVFHMIN64-NEXT:    .cfi_restore fs3
; ZVFHMIN64-NEXT:    .cfi_restore fs4
; ZVFHMIN64-NEXT:    .cfi_restore fs5
; ZVFHMIN64-NEXT:    .cfi_restore fs6
; ZVFHMIN64-NEXT:    .cfi_restore fs7
; ZVFHMIN64-NEXT:    .cfi_restore fs8
; ZVFHMIN64-NEXT:    .cfi_restore fs9
; ZVFHMIN64-NEXT:    .cfi_restore fs10
; ZVFHMIN64-NEXT:    .cfi_restore fs11
; ZVFHMIN64-NEXT:    addi sp, sp, 1024
; ZVFHMIN64-NEXT:    .cfi_def_cfa_offset 0
; ZVFHMIN64-NEXT:    ret
  %v = call <128 x i1> @llvm.vp.fcmp.v128f16(<128 x half> %va, <128 x half> %vb, metadata !"oeq", <128 x i1> %m, i32 %evl)
  ret <128 x i1> %v
}

declare <7 x i1> @llvm.vp.fcmp.v7f64(<7 x double>, <7 x double>, metadata, <7 x i1>, i32)

define <7 x i1> @fcmp_oeq_vv_v7f64(<7 x double> %va, <7 x double> %vb, <7 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oeq_vv_v7f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <7 x i1> @llvm.vp.fcmp.v7f64(<7 x double> %va, <7 x double> %vb, metadata !"oeq", <7 x i1> %m, i32 %evl)
  ret <7 x i1> %v
}

declare <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double>, <8 x double>, metadata, <8 x i1>, i32)

define <8 x i1> @fcmp_oeq_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oeq_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oeq_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oeq_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oeq_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oeq_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"oeq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ogt_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v12, v8, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ogt_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ogt_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ogt_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ogt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oge_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vv v16, v12, v8, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oge_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfge.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_oge_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oge_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"oge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_olt_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_olt_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_olt_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_olt_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"olt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ole_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ole_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ole_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ole_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfge.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ole", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_one_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmflt.vv v17, v12, v8, v0.t
; CHECK-NEXT:    vmor.mm v0, v17, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_one_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmfgt.vf v13, v8, fa0, v0.t
; CHECK-NEXT:    vmor.mm v0, v13, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_one_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_one_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmflt.vf v13, v8, fa0, v0.t
; CHECK-NEXT:    vmor.mm v0, v13, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"one", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ord_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vv v16, v12, v12, v0.t
; CHECK-NEXT:    vmfeq.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmand.mm v0, v12, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ord_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e64, m4, ta, ma
; CHECK-NEXT:    vfmv.v.f v12, fa0
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vf v16, v12, fa0, v0.t
; CHECK-NEXT:    vmfeq.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmand.mm v0, v12, v16
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ord_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ord_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e64, m4, ta, ma
; CHECK-NEXT:    vfmv.v.f v12, fa0
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfeq.vf v16, v12, fa0, v0.t
; CHECK-NEXT:    vmfeq.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmand.mm v0, v16, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ord", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ueq_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmflt.vv v17, v12, v8, v0.t
; CHECK-NEXT:    vmnor.mm v0, v17, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ueq_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmfgt.vf v13, v8, fa0, v0.t
; CHECK-NEXT:    vmnor.mm v0, v13, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ueq_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ueq_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmflt.vf v13, v8, fa0, v0.t
; CHECK-NEXT:    vmnor.mm v0, v13, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ueq", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ugt_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmnot.m v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ugt_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ugt_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ugt_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfge.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ugt", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uge_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmnot.m v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uge_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uge_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uge_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"uge", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ult_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vv v16, v12, v8, v0.t
; CHECK-NEXT:    vmnot.m v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ult_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfge.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ult_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ult_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfle.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ult", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ule_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vv v16, v12, v8, v0.t
; CHECK-NEXT:    vmnot.m v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ule_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfgt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_ule_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_ule_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmflt.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmnot.m v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"ule", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_une_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vv v16, v8, v12, v0.t
; CHECK-NEXT:    vmv1r.v v0, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_une_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_une_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_une_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vf v12, v8, fa0, v0.t
; CHECK-NEXT:    vmv1r.v v0, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"une", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vv_v8f64(<8 x double> %va, <8 x double> %vb, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uno_vv_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vv v16, v12, v12, v0.t
; CHECK-NEXT:    vmfne.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmor.mm v0, v12, v16
; CHECK-NEXT:    ret
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vf_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uno_vf_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e64, m4, ta, ma
; CHECK-NEXT:    vfmv.v.f v12, fa0
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vf v16, v12, fa0, v0.t
; CHECK-NEXT:    vmfne.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmor.mm v0, v12, v16
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %va, <8 x double> %vb, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

define <8 x i1> @fcmp_uno_vf_swap_v8f64(<8 x double> %va, double %b, <8 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_uno_vf_swap_v8f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vsetivli zero, 8, e64, m4, ta, ma
; CHECK-NEXT:    vfmv.v.f v12, fa0
; CHECK-NEXT:    vsetvli zero, a0, e64, m4, ta, ma
; CHECK-NEXT:    vmfne.vf v16, v12, fa0, v0.t
; CHECK-NEXT:    vmfne.vv v12, v8, v8, v0.t
; CHECK-NEXT:    vmor.mm v0, v16, v12
; CHECK-NEXT:    ret
  %elt.head = insertelement <8 x double> poison, double %b, i32 0
  %vb = shufflevector <8 x double> %elt.head, <8 x double> poison, <8 x i32> zeroinitializer
  %v = call <8 x i1> @llvm.vp.fcmp.v8f64(<8 x double> %vb, <8 x double> %va, metadata !"uno", <8 x i1> %m, i32 %evl)
  ret <8 x i1> %v
}

declare <32 x i1> @llvm.vp.fcmp.v32f64(<32 x double>, <32 x double>, metadata, <32 x i1>, i32)

define <32 x i1> @fcmp_oeq_vv_v32f64(<32 x double> %va, <32 x double> %vb, <32 x i1> %m, i32 zeroext %evl) {
; CHECK-LABEL: fcmp_oeq_vv_v32f64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addi sp, sp, -16
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 4
; CHECK-NEXT:    sub sp, sp, a1
; CHECK-NEXT:    .cfi_escape 0x0f, 0x0d, 0x72, 0x00, 0x11, 0x10, 0x22, 0x11, 0x10, 0x92, 0xa2, 0x38, 0x00, 0x1e, 0x22 # sp + 16 + 16 * vlenb
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vs8r.v v16, (a1) # Unknown-size Folded Spill
; CHECK-NEXT:    addi a1, a0, 128
; CHECK-NEXT:    vsetivli zero, 2, e8, mf4, ta, ma
; CHECK-NEXT:    vslidedown.vi v24, v0, 2
; CHECK-NEXT:    li a3, 16
; CHECK-NEXT:    vsetivli zero, 16, e64, m8, ta, ma
; CHECK-NEXT:    vle64.v v16, (a1)
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vs8r.v v16, (a1) # Unknown-size Folded Spill
; CHECK-NEXT:    mv a1, a2
; CHECK-NEXT:    vle64.v v16, (a0)
; CHECK-NEXT:    bltu a2, a3, .LBB87_2
; CHECK-NEXT:  # %bb.1:
; CHECK-NEXT:    li a1, 16
; CHECK-NEXT:  .LBB87_2:
; CHECK-NEXT:    vsetvli zero, a1, e64, m8, ta, ma
; CHECK-NEXT:    vmfeq.vv v7, v8, v16, v0.t
; CHECK-NEXT:    addi a0, a2, -16
; CHECK-NEXT:    sltu a1, a2, a0
; CHECK-NEXT:    addi a1, a1, -1
; CHECK-NEXT:    and a0, a1, a0
; CHECK-NEXT:    vmv1r.v v0, v24
; CHECK-NEXT:    csrr a1, vlenb
; CHECK-NEXT:    slli a1, a1, 3
; CHECK-NEXT:    add a1, sp, a1
; CHECK-NEXT:    addi a1, a1, 16
; CHECK-NEXT:    vl8r.v v8, (a1) # Unknown-size Folded Reload
; CHECK-NEXT:    addi a1, sp, 16
; CHECK-NEXT:    vl8r.v v24, (a1) # Unknown-size Folded Reload
; CHECK-NEXT:    vsetvli zero, a0, e64, m8, ta, ma
; CHECK-NEXT:    vmfeq.vv v16, v8, v24, v0.t
; CHECK-NEXT:    vsetivli zero, 4, e8, mf4, ta, ma
; CHECK-NEXT:    vslideup.vi v7, v16, 2
; CHECK-NEXT:    vmv1r.v v0, v7
; CHECK-NEXT:    csrr a0, vlenb
; CHECK-NEXT:    slli a0, a0, 4
; CHECK-NEXT:    add sp, sp, a0
; CHECK-NEXT:    .cfi_def_cfa sp, 16
; CHECK-NEXT:    addi sp, sp, 16
; CHECK-NEXT:    .cfi_def_cfa_offset 0
; CHECK-NEXT:    ret
  %v = call <32 x i1> @llvm.vp.fcmp.v32f64(<32 x double> %va, <32 x double> %vb, metadata !"oeq", <32 x i1> %m, i32 %evl)
  ret <32 x i1> %v
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; ZVFH32: {{.*}}
; ZVFH64: {{.*}}
