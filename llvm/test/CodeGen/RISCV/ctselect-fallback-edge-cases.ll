; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv64 -O3 | FileCheck %s --check-prefix=RV64
; RUN: llc < %s -mtriple=riscv32 -O3 | FileCheck %s --check-prefix=RV32

; Test with small integer types
define i1 @test_ctselect_i1(i1 %cond, i1 %a, i1 %b) {
; RV64-LABEL: test_ctselect_i1:
; RV64:       # %bb.0:
; RV64-NEXT:    and a1, a0, a1
; RV64-NEXT:    xori a0, a0, 1
; RV64-NEXT:    and a0, a0, a2
; RV64-NEXT:    or a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_i1:
; RV32:       # %bb.0:
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    xori a0, a0, 1
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    ret
  %result = call i1 @llvm.ct.select.i1(i1 %cond, i1 %a, i1 %b)
  ret i1 %result
}

; Test with extremal values
define i32 @test_ctselect_extremal_values(i1 %cond) {
; RV64-LABEL: test_ctselect_extremal_values:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    lui a1, 524288
; RV64-NEXT:    addi a2, a0, -1
; RV64-NEXT:    negw a0, a0
; RV64-NEXT:    and a1, a2, a1
; RV64-NEXT:    slli a0, a0, 33
; RV64-NEXT:    srli a0, a0, 33
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_extremal_values:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    not a2, a0
; RV32-NEXT:    slli a0, a0, 1
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    srli a0, a0, 1
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 2147483647, i32 -2147483648)
  ret i32 %result
}

; Test with null pointers
define ptr @test_ctselect_null_ptr(i1 %cond, ptr %ptr) {
; RV64-LABEL: test_ctselect_null_ptr:
; RV64:       # %bb.0:
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_null_ptr:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a0, a0, a1
; RV32-NEXT:    ret
  %result = call ptr @llvm.ct.select.p0(i1 %cond, ptr %ptr, ptr null)
  ret ptr %result
}

; Test with function pointers
define ptr @test_ctselect_function_ptr(i1 %cond, ptr %func1, ptr %func2) {
; RV64-LABEL: test_ctselect_function_ptr:
; RV64:       # %bb.0:
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a1, a0, a1
; RV64-NEXT:    not a0, a0
; RV64-NEXT:    and a0, a0, a2
; RV64-NEXT:    or a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_function_ptr:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    ret
  %result = call ptr @llvm.ct.select.p0(i1 %cond, ptr %func1, ptr %func2)
  ret ptr %result
}

; Test with condition from icmp on pointers
define ptr @test_ctselect_ptr_cmp(ptr %p1, ptr %p2, ptr %a, ptr %b) {
; RV64-LABEL: test_ctselect_ptr_cmp:
; RV64:       # %bb.0:
; RV64-NEXT:    xor a0, a0, a1
; RV64-NEXT:    snez a0, a0
; RV64-NEXT:    addi a0, a0, -1
; RV64-NEXT:    and a2, a0, a2
; RV64-NEXT:    not a0, a0
; RV64-NEXT:    and a0, a0, a3
; RV64-NEXT:    or a0, a2, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_ptr_cmp:
; RV32:       # %bb.0:
; RV32-NEXT:    xor a0, a0, a1
; RV32-NEXT:    snez a0, a0
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a2, a0, a2
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    or a0, a2, a0
; RV32-NEXT:    ret
  %cmp = icmp eq ptr %p1, %p2
  %result = call ptr @llvm.ct.select.p0(i1 %cmp, ptr %a, ptr %b)
  ret ptr %result
}

; Test with struct pointer types
%struct.pair = type { i32, i32 }

define ptr @test_ctselect_struct_ptr(i1 %cond, ptr %a, ptr %b) {
; RV64-LABEL: test_ctselect_struct_ptr:
; RV64:       # %bb.0:
; RV64-NEXT:    slli a0, a0, 63
; RV64-NEXT:    srai a0, a0, 63
; RV64-NEXT:    and a1, a0, a1
; RV64-NEXT:    not a0, a0
; RV64-NEXT:    and a0, a0, a2
; RV64-NEXT:    or a0, a1, a0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_struct_ptr:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    ret
  %result = call ptr @llvm.ct.select.p0(i1 %cond, ptr %a, ptr %b)
  ret ptr %result
}

; Test with deeply nested conditions
define i32 @test_ctselect_deeply_nested(i1 %c1, i1 %c2, i1 %c3, i1 %c4, i32 %a, i32 %b, i32 %c, i32 %d, i32 %e) {
; RV64-LABEL: test_ctselect_deeply_nested:
; RV64:       # %bb.0:
; RV64-NEXT:    lw t0, 0(sp)
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    andi a1, a1, 1
; RV64-NEXT:    andi a2, a2, 1
; RV64-NEXT:    andi a3, a3, 1
; RV64-NEXT:    addi t1, a0, -1
; RV64-NEXT:    neg a0, a0
; RV64-NEXT:    and a5, t1, a5
; RV64-NEXT:    neg t1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a0, a4
; RV64-NEXT:    neg a4, a2
; RV64-NEXT:    addi a2, a2, -1
; RV64-NEXT:    and a1, a1, a6
; RV64-NEXT:    neg a6, a3
; RV64-NEXT:    addi a3, a3, -1
; RV64-NEXT:    and a2, a2, a7
; RV64-NEXT:    or a0, a0, a5
; RV64-NEXT:    and a0, t1, a0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    and a0, a4, a0
; RV64-NEXT:    or a0, a0, a2
; RV64-NEXT:    and a0, a6, a0
; RV64-NEXT:    and a1, a3, t0
; RV64-NEXT:    or a0, a0, a1
; RV64-NEXT:    ret
;
; RV32-LABEL: test_ctselect_deeply_nested:
; RV32:       # %bb.0:
; RV32-NEXT:    lw t0, 0(sp)
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    slli a1, a1, 31
; RV32-NEXT:    slli a2, a2, 31
; RV32-NEXT:    slli a3, a3, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    srai a1, a1, 31
; RV32-NEXT:    srai a2, a2, 31
; RV32-NEXT:    srai a3, a3, 31
; RV32-NEXT:    and a4, a0, a4
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a5
; RV32-NEXT:    not a5, a1
; RV32-NEXT:    and a5, a5, a6
; RV32-NEXT:    not a6, a2
; RV32-NEXT:    and a6, a6, a7
; RV32-NEXT:    not a7, a3
; RV32-NEXT:    or a0, a4, a0
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    or a0, a0, a5
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    or a0, a0, a6
; RV32-NEXT:    and a0, a3, a0
; RV32-NEXT:    and a1, a7, t0
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    ret
  %sel1 = call i32 @llvm.ct.select.i32(i1 %c1, i32 %a, i32 %b)
  %sel2 = call i32 @llvm.ct.select.i32(i1 %c2, i32 %sel1, i32 %c)
  %sel3 = call i32 @llvm.ct.select.i32(i1 %c3, i32 %sel2, i32 %d)
  %sel4 = call i32 @llvm.ct.select.i32(i1 %c4, i32 %sel3, i32 %e)
  ret i32 %sel4
}

; This test demonstrates the FStar cmovznz4 pattern using ct.select
; Based on https://godbolt.org/z/6Kb71Ks7z
; Shows that NoMerge flag prevents DAG optimization from introducing branches
define void @cmovznz4_fstar_original(i64 %cin, ptr %x, ptr %y, ptr %r) {
; RV64-LABEL: cmovznz4_fstar_original:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    mv a4, a1
; RV64-NEXT:    beqz a0, .LBB7_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a4, a2
; RV64-NEXT:  .LBB7_2: # %entry
; RV64-NEXT:    beqz a0, .LBB7_6
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    addi a5, a2, 8
; RV64-NEXT:    bnez a0, .LBB7_7
; RV64-NEXT:  .LBB7_4:
; RV64-NEXT:    addi a6, a1, 16
; RV64-NEXT:    ld a4, 0(a4)
; RV64-NEXT:    ld a5, 0(a5)
; RV64-NEXT:    ld a6, 0(a6)
; RV64-NEXT:    bnez a0, .LBB7_8
; RV64-NEXT:  .LBB7_5:
; RV64-NEXT:    addi a1, a1, 24
; RV64-NEXT:    ld a0, 0(a1)
; RV64-NEXT:    sd a4, 0(a3)
; RV64-NEXT:    sd a5, 8(a3)
; RV64-NEXT:    sd a6, 16(a3)
; RV64-NEXT:    sd a0, 24(a3)
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB7_6:
; RV64-NEXT:    addi a5, a1, 8
; RV64-NEXT:    beqz a0, .LBB7_4
; RV64-NEXT:  .LBB7_7: # %entry
; RV64-NEXT:    addi a6, a2, 16
; RV64-NEXT:    ld a4, 0(a4)
; RV64-NEXT:    ld a5, 0(a5)
; RV64-NEXT:    ld a6, 0(a6)
; RV64-NEXT:    beqz a0, .LBB7_5
; RV64-NEXT:  .LBB7_8: # %entry
; RV64-NEXT:    addi a1, a2, 24
; RV64-NEXT:    ld a0, 0(a1)
; RV64-NEXT:    sd a4, 0(a3)
; RV64-NEXT:    sd a5, 8(a3)
; RV64-NEXT:    sd a6, 16(a3)
; RV64-NEXT:    sd a0, 24(a3)
; RV64-NEXT:    ret
;
; RV32-LABEL: cmovznz4_fstar_original:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    mv a1, a2
; RV32-NEXT:    beqz a0, .LBB7_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a1, a3
; RV32-NEXT:  .LBB7_2: # %entry
; RV32-NEXT:    beqz a0, .LBB7_5
; RV32-NEXT:  # %bb.3: # %entry
; RV32-NEXT:    addi a5, a3, 8
; RV32-NEXT:    bnez a0, .LBB7_6
; RV32-NEXT:  .LBB7_4:
; RV32-NEXT:    addi t0, a2, 16
; RV32-NEXT:    j .LBB7_7
; RV32-NEXT:  .LBB7_5:
; RV32-NEXT:    addi a5, a2, 8
; RV32-NEXT:    beqz a0, .LBB7_4
; RV32-NEXT:  .LBB7_6: # %entry
; RV32-NEXT:    addi t0, a3, 16
; RV32-NEXT:  .LBB7_7: # %entry
; RV32-NEXT:    lw a6, 0(a1)
; RV32-NEXT:    lw a1, 4(a1)
; RV32-NEXT:    lw a7, 0(a5)
; RV32-NEXT:    lw a5, 4(a5)
; RV32-NEXT:    lw t1, 0(t0)
; RV32-NEXT:    lw t0, 4(t0)
; RV32-NEXT:    beqz a0, .LBB7_9
; RV32-NEXT:  # %bb.8: # %entry
; RV32-NEXT:    addi a2, a3, 24
; RV32-NEXT:    j .LBB7_10
; RV32-NEXT:  .LBB7_9:
; RV32-NEXT:    addi a2, a2, 24
; RV32-NEXT:  .LBB7_10: # %entry
; RV32-NEXT:    lw a0, 0(a2)
; RV32-NEXT:    lw a2, 4(a2)
; RV32-NEXT:    sw a6, 0(a4)
; RV32-NEXT:    sw a1, 4(a4)
; RV32-NEXT:    sw a7, 8(a4)
; RV32-NEXT:    sw a5, 12(a4)
; RV32-NEXT:    sw t1, 16(a4)
; RV32-NEXT:    sw t0, 20(a4)
; RV32-NEXT:    sw a0, 24(a4)
; RV32-NEXT:    sw a2, 28(a4)
; RV32-NEXT:    ret
entry:
  %.not.i = icmp eq i64 %cin, 0
  %0 = load i64, ptr %y, align 8
  %1 = load i64, ptr %x, align 8
  %or = select i1 %.not.i, i64 %1, i64 %0
  %arrayidx4 = getelementptr inbounds nuw i8, ptr %y, i64 8
  %2 = load i64, ptr %arrayidx4, align 8
  %arrayidx6 = getelementptr inbounds nuw i8, ptr %x, i64 8
  %3 = load i64, ptr %arrayidx6, align 8
  %or9 = select i1 %.not.i, i64 %3, i64 %2
  %arrayidx10 = getelementptr inbounds nuw i8, ptr %y, i64 16
  %4 = load i64, ptr %arrayidx10, align 8
  %arrayidx12 = getelementptr inbounds nuw i8, ptr %x, i64 16
  %5 = load i64, ptr %arrayidx12, align 8
  %or15 = select i1 %.not.i, i64 %5, i64 %4
  %arrayidx16 = getelementptr inbounds nuw i8, ptr %y, i64 24
  %6 = load i64, ptr %arrayidx16, align 8
  %arrayidx18 = getelementptr inbounds nuw i8, ptr %x, i64 24
  %7 = load i64, ptr %arrayidx18, align 8
  %or21 = select i1 %.not.i, i64 %7, i64 %6
  store i64 %or, ptr %r, align 8
  %arrayidx23 = getelementptr inbounds nuw i8, ptr %r, i64 8
  store i64 %or9, ptr %arrayidx23, align 8
  %arrayidx24 = getelementptr inbounds nuw i8, ptr %r, i64 16
  store i64 %or15, ptr %arrayidx24, align 8
  %arrayidx25 = getelementptr inbounds nuw i8, ptr %r, i64 24
  store i64 %or21, ptr %arrayidx25, align 8
  ret void
}

define void @cmovznz4_builtin_ctselect(i64 %cin, ptr %x, ptr %y, ptr %r) {
; RV64-LABEL: cmovznz4_builtin_ctselect:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    snez a0, a0
; RV64-NEXT:    ld a4, 0(a1)
; RV64-NEXT:    ld a5, 0(a2)
; RV64-NEXT:    addi a0, a0, -1
; RV64-NEXT:    not a6, a0
; RV64-NEXT:    and a4, a0, a4
; RV64-NEXT:    and a5, a6, a5
; RV64-NEXT:    or a4, a4, a5
; RV64-NEXT:    sd a4, 0(a3)
; RV64-NEXT:    ld a4, 8(a2)
; RV64-NEXT:    ld a5, 8(a1)
; RV64-NEXT:    and a4, a6, a4
; RV64-NEXT:    and a5, a0, a5
; RV64-NEXT:    or a4, a5, a4
; RV64-NEXT:    sd a4, 8(a3)
; RV64-NEXT:    ld a4, 16(a2)
; RV64-NEXT:    ld a5, 16(a1)
; RV64-NEXT:    and a4, a6, a4
; RV64-NEXT:    and a5, a0, a5
; RV64-NEXT:    or a4, a5, a4
; RV64-NEXT:    sd a4, 16(a3)
; RV64-NEXT:    ld a2, 24(a2)
; RV64-NEXT:    ld a1, 24(a1)
; RV64-NEXT:    and a2, a6, a2
; RV64-NEXT:    and a0, a0, a1
; RV64-NEXT:    or a0, a0, a2
; RV64-NEXT:    sd a0, 24(a3)
; RV64-NEXT:    ret
;
; RV32-LABEL: cmovznz4_builtin_ctselect:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    lw a1, 0(a2)
; RV32-NEXT:    lw a5, 4(a2)
; RV32-NEXT:    lw a6, 0(a3)
; RV32-NEXT:    lw a7, 4(a3)
; RV32-NEXT:    seqz t0, a0
; RV32-NEXT:    addi a0, t0, -1
; RV32-NEXT:    neg t0, t0
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    and a1, t0, a1
; RV32-NEXT:    and a7, a0, a7
; RV32-NEXT:    and a5, t0, a5
; RV32-NEXT:    or a1, a1, a6
; RV32-NEXT:    or a5, a5, a7
; RV32-NEXT:    sw a1, 0(a4)
; RV32-NEXT:    sw a5, 4(a4)
; RV32-NEXT:    lw a1, 8(a3)
; RV32-NEXT:    lw a5, 8(a2)
; RV32-NEXT:    lw a6, 12(a3)
; RV32-NEXT:    lw a7, 12(a2)
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    and a5, t0, a5
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    and a7, t0, a7
; RV32-NEXT:    or a1, a5, a1
; RV32-NEXT:    or a5, a7, a6
; RV32-NEXT:    sw a1, 8(a4)
; RV32-NEXT:    sw a5, 12(a4)
; RV32-NEXT:    lw a1, 16(a3)
; RV32-NEXT:    lw a5, 16(a2)
; RV32-NEXT:    lw a6, 20(a3)
; RV32-NEXT:    lw a7, 20(a2)
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    and a5, t0, a5
; RV32-NEXT:    and a6, a0, a6
; RV32-NEXT:    and a7, t0, a7
; RV32-NEXT:    or a1, a5, a1
; RV32-NEXT:    or a5, a7, a6
; RV32-NEXT:    sw a1, 16(a4)
; RV32-NEXT:    sw a5, 20(a4)
; RV32-NEXT:    lw a1, 24(a3)
; RV32-NEXT:    lw a5, 24(a2)
; RV32-NEXT:    lw a3, 28(a3)
; RV32-NEXT:    lw a2, 28(a2)
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    and a5, t0, a5
; RV32-NEXT:    and a0, a0, a3
; RV32-NEXT:    and a2, t0, a2
; RV32-NEXT:    or a1, a5, a1
; RV32-NEXT:    or a0, a2, a0
; RV32-NEXT:    sw a1, 24(a4)
; RV32-NEXT:    sw a0, 28(a4)
; RV32-NEXT:    ret
entry:
  %cmp = icmp eq i64 %cin, 0
  %0 = load i64, ptr %x, align 8
  %1 = load i64, ptr %y, align 8
  %2 = tail call i64 @llvm.ct.select.i64(i1 %cmp, i64 %0, i64 %1)
  store i64 %2, ptr %r, align 8
  %arrayidx4 = getelementptr inbounds nuw i8, ptr %x, i64 8
  %3 = load i64, ptr %arrayidx4, align 8
  %arrayidx5 = getelementptr inbounds nuw i8, ptr %y, i64 8
  %4 = load i64, ptr %arrayidx5, align 8
  %5 = tail call i64 @llvm.ct.select.i64(i1 %cmp, i64 %3, i64 %4)
  %arrayidx6 = getelementptr inbounds nuw i8, ptr %r, i64 8
  store i64 %5, ptr %arrayidx6, align 8
  %arrayidx8 = getelementptr inbounds nuw i8, ptr %x, i64 16
  %6 = load i64, ptr %arrayidx8, align 8
  %arrayidx9 = getelementptr inbounds nuw i8, ptr %y, i64 16
  %7 = load i64, ptr %arrayidx9, align 8
  %8 = tail call i64 @llvm.ct.select.i64(i1 %cmp, i64 %6, i64 %7)
  %arrayidx10 = getelementptr inbounds nuw i8, ptr %r, i64 16
  store i64 %8, ptr %arrayidx10, align 8
  %arrayidx12 = getelementptr inbounds nuw i8, ptr %x, i64 24
  %9 = load i64, ptr %arrayidx12, align 8
  %arrayidx13 = getelementptr inbounds nuw i8, ptr %y, i64 24
  %10 = load i64, ptr %arrayidx13, align 8
  %11 = tail call i64 @llvm.ct.select.i64(i1 %cmp, i64 %9, i64 %10)
  %arrayidx14 = getelementptr inbounds nuw i8, ptr %r, i64 24
  store i64 %11, ptr %arrayidx14, align 8
  ret void
}

; Declare the intrinsics
declare i1 @llvm.ct.select.i1(i1, i1, i1)
declare i32 @llvm.ct.select.i32(i1, i32, i32)
declare ptr @llvm.ct.select.p0(i1, ptr, ptr)
