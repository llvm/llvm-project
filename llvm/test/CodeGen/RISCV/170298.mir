# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=riscv64 -x mir -mattr=+v -start-after=twoaddressinstruction -stop-after=greedy -simplify-mir -verify-machineinstrs < %s | FileCheck %s

--- |
  define void @subregliveness_greedy_split(i1 %cond) {
  entry:
    %index.entry = tail call <vscale x 16 x i16> @llvm.riscv.vmv.s.x.nxv16i16.i64(<vscale x 16 x i16> poison, i16 9, i64 0)
    call void @llvm.riscv.vsoxseg3.mask.triscv.vector.tuple_nxv16i8_3t.p0.nxv16i16.nxv16i1.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 3) zeroinitializer, ptr null, <vscale x 16 x i16> %index.entry, <vscale x 16 x i1> zeroinitializer, i64 0, i64 3)
    %call = tail call i32 null()
    br i1 %cond, label %if.then, label %if.else

  if.then:
    %index.then = tail call <vscale x 16 x i16> @llvm.riscv.vmv.s.x.nxv16i16.i64(<vscale x 16 x i16> poison, i16 9, i64 0)
    br label %if.end

  if.else:
    %index.else = tail call <vscale x 16 x i16> @llvm.riscv.vmv.s.x.nxv16i16.i64(<vscale x 16 x i16> poison, i16 9, i64 0)
    call void @llvm.riscv.vsoxseg3.mask.triscv.vector.tuple_nxv16i8_3t.p0.nxv16i16.nxv16i1.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 3) zeroinitializer, ptr null, <vscale x 16 x i16> %index.else, <vscale x 16 x i1> zeroinitializer, i64 0, i64 3)
    br label %if.end

  if.end:
    %index = phi <vscale x 16 x i16> [ %index.then, %if.then ], [ %index.else, %if.else ]
    call void @llvm.memset.p0.i64(ptr align 1 poison, i8 0, i64 9, i1 false)
    call void @llvm.riscv.vsoxseg3.mask.triscv.vector.tuple_nxv16i8_3t.p0.nxv16i16.nxv16i1.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 3) zeroinitializer, ptr null, <vscale x 16 x i16> %index, <vscale x 16 x i1> zeroinitializer, i64 0, i64 3)
    ret void
  }

  declare void @llvm.memset.p0.i64(ptr writeonly captures(none), i8, i64, i1 immarg)
  declare <vscale x 16 x i16> @llvm.riscv.vmv.s.x.nxv16i16.i64(<vscale x 16 x i16>, i16, i64)
  declare void @llvm.riscv.vsoxseg3.mask.triscv.vector.tuple_nxv16i8_3t.p0.nxv16i16.nxv16i1.i64(target("riscv.vector.tuple", <vscale x 16 x i8>, 3), ptr captures(none), <vscale x 16 x i16>, <vscale x 16 x i1>, i64, i64 immarg)
...
---
name:            subregliveness_greedy_split
alignment:       4
tracksRegLiveness: true
noPhis:          true
registers:
  - { id: 0, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 1, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 2, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 3, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 4, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 5, class: vr, preferred-register: '', flags: [  ] }
  - { id: 6, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 7, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 8, class: vrm2, preferred-register: '', flags: [  ] }
  - { id: 9, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 10, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 11, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 12, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 13, class: vr, preferred-register: '', flags: [  ] }
  - { id: 14, class: gprjalr, preferred-register: '', flags: [  ] }
  - { id: 15, class: vmv0, preferred-register: '', flags: [  ] }
  - { id: 16, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 17, class: vr, preferred-register: '', flags: [  ] }
  - { id: 18, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 19, class: vrm2, preferred-register: '', flags: [  ] }
  - { id: 20, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 21, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 22, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 23, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 24, class: vr, preferred-register: '', flags: [  ] }
  - { id: 25, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 26, class: vmv0, preferred-register: '', flags: [  ] }
  - { id: 27, class: vr, preferred-register: '', flags: [  ] }
  - { id: 28, class: vrm4, preferred-register: '', flags: [  ] }
  - { id: 29, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 30, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 31, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 32, class: gpr, preferred-register: '', flags: [  ] }
  - { id: 33, class: vrm2, preferred-register: '', flags: [  ] }
  - { id: 34, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 35, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 36, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 37, class: vrn3m2, preferred-register: '', flags: [  ] }
  - { id: 38, class: vr, preferred-register: '', flags: [  ] }
  - { id: 39, class: vmv0, preferred-register: '', flags: [  ] }
  - { id: 40, class: vr, preferred-register: '', flags: [  ] }
  - { id: 41, class: vrm2, preferred-register: '', flags: [  ] }
  - { id: 42, class: vrm4, preferred-register: '', flags: [  ] }
liveins:
  - { reg: '$x10', virtual-reg: '%3' }
frameInfo:
  adjustsStack:    true
  hasCalls:        true
  maxCallFrameSize: 4294967295
body:             |
  ; CHECK-LABEL: name: subregliveness_greedy_split
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1, %bb.2
  ; CHECK-NEXT:   liveins: $x10
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:gpr = COPY $x10
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:gprjalr = COPY $x0
  ; CHECK-NEXT:   [[ANDI:%[0-9]+]]:gpr = ANDI [[COPY]], 1
  ; CHECK-NEXT:   undef [[PseudoVMV_V_I_M2_:%[0-9]+]].sub_vrm2_0:vrn3m2 = PseudoVMV_V_I_M2 undef [[PseudoVMV_V_I_M2_]].sub_vrm2_0, 0, 0, 3 /* e8 */, 0 /* tu, mu */
  ; CHECK-NEXT:   [[PseudoVMCLR_M_B4_:%[0-9]+]]:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
  ; CHECK-NEXT:   [[PseudoVMV_V_I_M2_:%[0-9]+]].sub_vrm2_1:vrn3m2 = COPY [[PseudoVMV_V_I_M2_]].sub_vrm2_0
  ; CHECK-NEXT:   undef [[PseudoVMV_V_I_M1_:%[0-9]+]].sub_vrm1_0:vrm4 = PseudoVMV_V_I_M1 undef [[PseudoVMV_V_I_M1_]].sub_vrm1_0, 9, 0, 4 /* e16 */, 0 /* tu, mu */
  ; CHECK-NEXT:   [[PseudoVMV_V_I_M2_:%[0-9]+]].sub_vrm2_2:vrn3m2 = COPY [[PseudoVMV_V_I_M2_]].sub_vrm2_0
  ; CHECK-NEXT:   $v0 = COPY [[PseudoVMCLR_M_B4_]]
  ; CHECK-NEXT:   undef [[COPY2:%[0-9]+]].sub_vrm2_0:vrn3m2 = COPY [[PseudoVMV_V_I_M2_]].sub_vrm2_0
  ; CHECK-NEXT:   PseudoVSPILL3_M2 [[COPY2]], %stack.0 :: (store (<vscale x 1 x s384>) into %stack.0, align 8)
  ; CHECK-NEXT:   undef [[COPY3:%[0-9]+]].sub_vrm1_0:vrm4 = COPY [[PseudoVMV_V_I_M1_]].sub_vrm1_0
  ; CHECK-NEXT:   VS4R_V [[COPY3]], %stack.1 :: (store (<vscale x 1 x s256>) into %stack.1, align 8)
  ; CHECK-NEXT:   PseudoVSOXSEG3EI16_V_M4_M2_MASK [[PseudoVMV_V_I_M2_]], [[COPY1]], [[PseudoVMV_V_I_M1_]], $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
  ; CHECK-NEXT:   ADJCALLSTACKDOWN 0, 0, implicit-def dead $x2, implicit $x2
  ; CHECK-NEXT:   PseudoCALLIndirect [[COPY1]], csr_ilp32d_lp64d, implicit-def dead $x1, implicit-def $x2, implicit-def dead $x10
  ; CHECK-NEXT:   ADJCALLSTACKUP 0, 0, implicit-def dead $x2, implicit $x2
  ; CHECK-NEXT:   BEQ [[ANDI]], $x0, %bb.2
  ; CHECK-NEXT:   PseudoBR %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.if.then:
  ; CHECK-NEXT:   PseudoBR %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.if.else:
  ; CHECK-NEXT:   [[PseudoVRELOAD3_M2_:%[0-9]+]]:vrn3m2 = PseudoVRELOAD3_M2 %stack.0 :: (load (<vscale x 1 x s384>) from %stack.0, align 8)
  ; CHECK-NEXT:   undef [[COPY4:%[0-9]+]].sub_vrm2_0:vrn3m2 = COPY [[PseudoVRELOAD3_M2_]].sub_vrm2_0
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]].sub_vrm2_1:vrn3m2 = COPY [[COPY4]].sub_vrm2_0
  ; CHECK-NEXT:   [[PseudoVMCLR_M_B4_1:%[0-9]+]]:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]].sub_vrm2_2:vrn3m2 = COPY [[COPY4]].sub_vrm2_0
  ; CHECK-NEXT:   $v0 = COPY [[PseudoVMCLR_M_B4_1]]
  ; CHECK-NEXT:   undef [[COPY5:%[0-9]+]].sub_vrm2_0:vrn3m2 = COPY [[COPY4]].sub_vrm2_0
  ; CHECK-NEXT:   PseudoVSPILL3_M2 [[COPY5]], %stack.0 :: (store (<vscale x 1 x s384>) into %stack.0, align 8)
  ; CHECK-NEXT:   undef [[PseudoVMV_V_I_M1_1:%[0-9]+]].sub_vrm1_0:vrm4 = PseudoVMV_V_I_M1 undef [[PseudoVMV_V_I_M1_1]].sub_vrm1_0, 9, 0, 4 /* e16 */, 0 /* tu, mu */
  ; CHECK-NEXT:   PseudoVSOXSEG3EI16_V_M4_M2_MASK [[COPY4]], $x0, [[PseudoVMV_V_I_M1_1]], $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.if.end:
  ; CHECK-NEXT:   ADJCALLSTACKDOWN 0, 0, implicit-def dead $x2, implicit $x2
  ; CHECK-NEXT:   $x12 = ADDI $x0, 9
  ; CHECK-NEXT:   $x11 = COPY $x0
  ; CHECK-NEXT:   PseudoCALL target-flags(riscv-call) &memset, csr_ilp32d_lp64d, implicit-def dead $x1, implicit undef $x10, implicit $x11, implicit killed $x12, implicit-def $x2, implicit-def dead $x10
  ; CHECK-NEXT:   ADJCALLSTACKUP 0, 0, implicit-def dead $x2, implicit $x2
  ; CHECK-NEXT:   [[PseudoVRELOAD3_M2_1:%[0-9]+]]:vrn3m2 = PseudoVRELOAD3_M2 %stack.0 :: (load (<vscale x 1 x s384>) from %stack.0, align 8)
  ; CHECK-NEXT:   undef [[COPY6:%[0-9]+]].sub_vrm2_0:vrn3m2 = COPY [[PseudoVRELOAD3_M2_1]].sub_vrm2_0
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]].sub_vrm2_1:vrn3m2 = COPY [[COPY6]].sub_vrm2_0
  ; CHECK-NEXT:   [[PseudoVMCLR_M_B4_2:%[0-9]+]]:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]].sub_vrm2_2:vrn3m2 = COPY [[COPY6]].sub_vrm2_0
  ; CHECK-NEXT:   $v0 = COPY [[PseudoVMCLR_M_B4_2]]
  ; CHECK-NEXT:   [[VL4RE8_V:%[0-9]+]]:vrm4 = VL4RE8_V %stack.1 :: (load (<vscale x 1 x s256>) from %stack.1, align 8)
  ; CHECK-NEXT:   PseudoVSOXSEG3EI16_V_M4_M2_MASK [[COPY6]], $x0, [[VL4RE8_V]], $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
  ; CHECK-NEXT:   PseudoRET
  bb.0.entry:
    successors: %bb.1(0x40000000), %bb.2(0x40000000)
    liveins: $x10

    %3:gpr = COPY killed $x10
    %4:gpr = ANDI killed %3, 1
    %5:vr = PseudoVMV_V_I_M1 undef %5, 9, 0, 4 /* e16 */, 0 /* tu, mu */
    undef %6.sub_vrm1_0:vrm4 = COPY %5
    %8:vrm2 = PseudoVMV_V_I_M2 undef %8, 0, 0, 3 /* e8 */, 0 /* tu, mu */
    undef %9.sub_vrm2_0:vrn3m2 = COPY %8
    %11:vrn3m2 = COPY killed %9
    %11.sub_vrm2_1:vrn3m2 = COPY %8
    %12:vrn3m2 = COPY killed %11
    %12.sub_vrm2_2:vrn3m2 = COPY %8
    %13:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
    %14:gprjalr = COPY $x0
    $v0 = COPY killed %13
    PseudoVSOXSEG3EI16_V_M4_M2_MASK killed %12, %14, killed %6, killed $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $x2, implicit $x2
    PseudoCALLIndirect killed %14, csr_ilp32d_lp64d, implicit-def dead $x1, implicit-def $x2, implicit-def dead $x10
    ADJCALLSTACKUP 0, 0, implicit-def dead $x2, implicit $x2
    BEQ killed %4, $x0, %bb.2
    PseudoBR %bb.1

  bb.1.if.then:
    successors: %bb.3(0x80000000)

    undef %0.sub_vrm1_0:vrm4 = COPY killed %5
    %42:vrm4 = COPY killed %0
    PseudoBR %bb.3

  bb.2.if.else:
    successors: %bb.3(0x80000000)

    undef %1.sub_vrm1_0:vrm4 = COPY killed %5
    undef %20.sub_vrm2_0:vrn3m2 = COPY %8
    %22:vrn3m2 = COPY killed %20
    %22.sub_vrm2_1:vrn3m2 = COPY %8
    %23:vrn3m2 = COPY killed %22
    %23.sub_vrm2_2:vrn3m2 = COPY %8
    %24:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
    %25:gpr = COPY $x0
    $v0 = COPY killed %24
    PseudoVSOXSEG3EI16_V_M4_M2_MASK killed %23, killed %25, %1, killed $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
    %42:vrm4 = COPY killed %1

  bb.3.if.end:
    %2:vrm4 = COPY killed %42
    ADJCALLSTACKDOWN 0, 0, implicit-def dead $x2, implicit $x2
    %29:gpr = COPY $x0
    $x11 = COPY %29
    $x12 = ADDI $x0, 9
    PseudoCALL target-flags(riscv-call) &memset, csr_ilp32d_lp64d, implicit-def dead $x1, implicit undef $x10, implicit killed $x11, implicit killed $x12, implicit-def $x2, implicit-def dead $x10
    ADJCALLSTACKUP 0, 0, implicit-def dead $x2, implicit $x2
    undef %34.sub_vrm2_0:vrn3m2 = COPY %8
    %36:vrn3m2 = COPY killed %34
    %36.sub_vrm2_1:vrn3m2 = COPY %8
    %37:vrn3m2 = COPY killed %36
    %37.sub_vrm2_2:vrn3m2 = COPY killed %8
    %38:vr = PseudoVMCLR_M_B4 0, 0 /* e8 */
    $v0 = COPY killed %38
    PseudoVSOXSEG3EI16_V_M4_M2_MASK killed %37, killed %29, killed %2, killed $v0, 0, 3 /* e8 */ :: (store unknown-size, align 1)
    PseudoRET
...
