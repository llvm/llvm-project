; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=riscv64 < %s | FileCheck -check-prefix=RV64I %s
; RUN: llc -mtriple=riscv64 -mattr=+d < %s | FileCheck -check-prefix=RV64D %s

define double @max(double, double) unnamed_addr #0 {
; RV64I-LABEL: max:
; RV64I:       # %bb.0: # %start
; RV64I-NEXT:    addi sp, sp, -32
; RV64I-NEXT:    .cfi_def_cfa_offset 32
; RV64I-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s0, 16(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s1, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    sd s2, 0(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    .cfi_offset s0, -16
; RV64I-NEXT:    .cfi_offset s1, -24
; RV64I-NEXT:    .cfi_offset s2, -32
; RV64I-NEXT:    mv s0, a1
; RV64I-NEXT:    mv s1, a0
; RV64I-NEXT:    call __ltdf2
; RV64I-NEXT:    srli s2, a0, 63
; RV64I-NEXT:    mv a0, s1
; RV64I-NEXT:    mv a1, s1
; RV64I-NEXT:    call __unorddf2
; RV64I-NEXT:    snez a0, a0
; RV64I-NEXT:    or a0, a0, s2
; RV64I-NEXT:    bnez a0, .LBB0_2
; RV64I-NEXT:  # %bb.1: # %start
; RV64I-NEXT:    mv s0, s1
; RV64I-NEXT:  .LBB0_2: # %start
; RV64I-NEXT:    mv a0, s0
; RV64I-NEXT:    call fmin
; RV64I-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s0, 16(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s1, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    ld s2, 0(sp) # 8-byte Folded Reload
; RV64I-NEXT:    .cfi_restore ra
; RV64I-NEXT:    .cfi_restore s0
; RV64I-NEXT:    .cfi_restore s1
; RV64I-NEXT:    .cfi_restore s2
; RV64I-NEXT:    addi sp, sp, 32
; RV64I-NEXT:    .cfi_def_cfa_offset 0
; RV64I-NEXT:    ret
;
; RV64D-LABEL: max:
; RV64D:       # %bb.0: # %start
; RV64D-NEXT:    flt.d a0, fa0, fa1
; RV64D-NEXT:    feq.d a1, fa0, fa0
; RV64D-NEXT:    xori a1, a1, 1
; RV64D-NEXT:    or a0, a1, a0
; RV64D-NEXT:    bnez a0, .LBB0_2
; RV64D-NEXT:  # %bb.1: # %start
; RV64D-NEXT:    fmv.d fa1, fa0
; RV64D-NEXT:  .LBB0_2: # %start
; RV64D-NEXT:    fmin.d fa0, fa1, fa1
; RV64D-NEXT:    ret
start:
  %2 = fcmp olt double %0, %1
  %3 = fcmp uno double %0, 0.000000e+00
  %or.cond.i.i = or i1 %3, %2
  %4 = select i1 %or.cond.i.i, double %1, double %0
  %5 = tail call double @llvm.canonicalize.f64(double %4) #2
  ret double %5
}
