; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -mattr=+f -verify-machineinstrs < %s \
; RUN:   -target-abi=ilp32f | FileCheck -check-prefixes=RV32,RV32IF %s
; RUN: llc -mtriple=riscv64 -mattr=+f -verify-machineinstrs < %s \
; RUN:   -target-abi=lp64f | FileCheck -check-prefixes=RV64,RV64IF %s
; RUN: llc -mtriple=riscv32 -mattr=+f,+d -verify-machineinstrs < %s \
; RUN:   -target-abi=ilp32d | FileCheck -check-prefixes=RV32,RV32IFD %s
; RUN: llc -mtriple=riscv64 -mattr=+f,+d -verify-machineinstrs < %s \
; RUN:   -target-abi=lp64d | FileCheck -check-prefixes=RV64,RV64IFD %s

; i32 saturate

define i32 @stest_f64i32(double %x) {
; RV32IF-LABEL: stest_f64i32:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfdi
; RV32IF-NEXT:    lui a2, 524288
; RV32IF-NEXT:    addi a3, a2, -1
; RV32IF-NEXT:    beqz a1, .LBB0_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    srli a4, a1, 31
; RV32IF-NEXT:    j .LBB0_3
; RV32IF-NEXT:  .LBB0_2:
; RV32IF-NEXT:    sltu a4, a0, a3
; RV32IF-NEXT:  .LBB0_3: # %entry
; RV32IF-NEXT:    neg a5, a4
; RV32IF-NEXT:    and a1, a5, a1
; RV32IF-NEXT:    bnez a4, .LBB0_5
; RV32IF-NEXT:  # %bb.4: # %entry
; RV32IF-NEXT:    mv a0, a3
; RV32IF-NEXT:  .LBB0_5: # %entry
; RV32IF-NEXT:    li a3, -1
; RV32IF-NEXT:    beq a1, a3, .LBB0_7
; RV32IF-NEXT:  # %bb.6: # %entry
; RV32IF-NEXT:    srli a1, a1, 31
; RV32IF-NEXT:    xori a1, a1, 1
; RV32IF-NEXT:    beqz a1, .LBB0_8
; RV32IF-NEXT:    j .LBB0_9
; RV32IF-NEXT:  .LBB0_7:
; RV32IF-NEXT:    sltu a1, a2, a0
; RV32IF-NEXT:    bnez a1, .LBB0_9
; RV32IF-NEXT:  .LBB0_8: # %entry
; RV32IF-NEXT:    lui a0, 524288
; RV32IF-NEXT:  .LBB0_9: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i32:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfdi
; RV64IF-NEXT:    lui a1, 524288
; RV64IF-NEXT:    addiw a2, a1, -1
; RV64IF-NEXT:    blt a0, a2, .LBB0_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a2
; RV64IF-NEXT:  .LBB0_2: # %entry
; RV64IF-NEXT:    blt a1, a0, .LBB0_4
; RV64IF-NEXT:  # %bb.3: # %entry
; RV64IF-NEXT:    lui a0, 524288
; RV64IF-NEXT:  .LBB0_4: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i32:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i32:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 524288
; RV64IFD-NEXT:    addiw a2, a1, -1
; RV64IFD-NEXT:    bge a0, a2, .LBB0_3
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    bge a1, a0, .LBB0_4
; RV64IFD-NEXT:  .LBB0_2: # %entry
; RV64IFD-NEXT:    ret
; RV64IFD-NEXT:  .LBB0_3: # %entry
; RV64IFD-NEXT:    mv a0, a2
; RV64IFD-NEXT:    blt a1, a2, .LBB0_2
; RV64IFD-NEXT:  .LBB0_4: # %entry
; RV64IFD-NEXT:    lui a0, 524288
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i64
  %0 = icmp slt i64 %conv, 2147483647
  %spec.store.select = select i1 %0, i64 %conv, i64 2147483647
  %1 = icmp sgt i64 %spec.store.select, -2147483648
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 -2147483648
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utest_f64i32(double %x) {
; RV32IF-LABEL: utest_f64i32:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixunsdfdi
; RV32IF-NEXT:    sltiu a2, a0, -1
; RV32IF-NEXT:    seqz a1, a1
; RV32IF-NEXT:    and a1, a1, a2
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    or a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: utest_f64i32:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixunsdfdi
; RV64IF-NEXT:    li a1, -1
; RV64IF-NEXT:    srli a1, a1, 32
; RV64IF-NEXT:    bltu a0, a1, .LBB1_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB1_2: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i32:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: utest_f64i32:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.lu.d a0, fa0, rtz
; RV64IFD-NEXT:    li a1, -1
; RV64IFD-NEXT:    srli a1, a1, 32
; RV64IFD-NEXT:    bltu a0, a1, .LBB1_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB1_2: # %entry
; RV64IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i64
  %0 = icmp ult i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f64i32(double %x) {
; RV32IF-LABEL: ustest_f64i32:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfdi
; RV32IF-NEXT:    beqz a1, .LBB2_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    srli a2, a1, 31
; RV32IF-NEXT:    j .LBB2_3
; RV32IF-NEXT:  .LBB2_2:
; RV32IF-NEXT:    sltiu a2, a0, -1
; RV32IF-NEXT:  .LBB2_3: # %entry
; RV32IF-NEXT:    addi a3, a2, -1
; RV32IF-NEXT:    neg a2, a2
; RV32IF-NEXT:    and a1, a2, a1
; RV32IF-NEXT:    or a0, a3, a0
; RV32IF-NEXT:    beqz a1, .LBB2_5
; RV32IF-NEXT:  # %bb.4: # %entry
; RV32IF-NEXT:    sgtz a1, a1
; RV32IF-NEXT:    j .LBB2_6
; RV32IF-NEXT:  .LBB2_5:
; RV32IF-NEXT:    snez a1, a0
; RV32IF-NEXT:  .LBB2_6: # %entry
; RV32IF-NEXT:    neg a1, a1
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: ustest_f64i32:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfdi
; RV64IF-NEXT:    li a1, -1
; RV64IF-NEXT:    srli a1, a1, 32
; RV64IF-NEXT:    blt a0, a1, .LBB2_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB2_2: # %entry
; RV64IF-NEXT:    sgtz a1, a0
; RV64IF-NEXT:    neg a1, a1
; RV64IF-NEXT:    and a0, a1, a0
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i32:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: ustest_f64i32:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    li a1, -1
; RV64IFD-NEXT:    srli a1, a1, 32
; RV64IFD-NEXT:    blt a0, a1, .LBB2_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB2_2: # %entry
; RV64IFD-NEXT:    sgtz a1, a0
; RV64IFD-NEXT:    neg a1, a1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i64
  %0 = icmp slt i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %1 = icmp sgt i64 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 0
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @stest_f32i32(float %x) {
; RV32-LABEL: stest_f32i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 524288
; RV64-NEXT:    addiw a2, a1, -1
; RV64-NEXT:    bge a0, a2, .LBB3_3
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    bge a1, a0, .LBB3_4
; RV64-NEXT:  .LBB3_2: # %entry
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB3_3: # %entry
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:    blt a1, a2, .LBB3_2
; RV64-NEXT:  .LBB3_4: # %entry
; RV64-NEXT:    lui a0, 524288
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i64
  %0 = icmp slt i64 %conv, 2147483647
  %spec.store.select = select i1 %0, i64 %conv, i64 2147483647
  %1 = icmp sgt i64 %spec.store.select, -2147483648
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 -2147483648
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utest_f32i32(float %x) {
; RV32-LABEL: utest_f32i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    bltu a0, a1, .LBB4_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB4_2: # %entry
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i64
  %0 = icmp ult i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f32i32(float %x) {
; RV32-LABEL: ustest_f32i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    blt a0, a1, .LBB5_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB5_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i64
  %0 = icmp slt i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %1 = icmp sgt i64 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 0
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @stest_f16i32(half %x) {
; RV32-LABEL: stest_f16i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixsfdi
; RV32-NEXT:    lui a2, 524288
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    beqz a1, .LBB6_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    srli a4, a1, 31
; RV32-NEXT:    j .LBB6_3
; RV32-NEXT:  .LBB6_2:
; RV32-NEXT:    sltu a4, a0, a3
; RV32-NEXT:  .LBB6_3: # %entry
; RV32-NEXT:    neg a5, a4
; RV32-NEXT:    and a1, a5, a1
; RV32-NEXT:    bnez a4, .LBB6_5
; RV32-NEXT:  # %bb.4: # %entry
; RV32-NEXT:    mv a0, a3
; RV32-NEXT:  .LBB6_5: # %entry
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beq a1, a3, .LBB6_7
; RV32-NEXT:  # %bb.6: # %entry
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    xori a1, a1, 1
; RV32-NEXT:    beqz a1, .LBB6_8
; RV32-NEXT:    j .LBB6_9
; RV32-NEXT:  .LBB6_7:
; RV32-NEXT:    sltu a1, a2, a0
; RV32-NEXT:    bnez a1, .LBB6_9
; RV32-NEXT:  .LBB6_8: # %entry
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:  .LBB6_9: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 524288
; RV64-NEXT:    addiw a2, a1, -1
; RV64-NEXT:    blt a0, a2, .LBB6_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:  .LBB6_2: # %entry
; RV64-NEXT:    blt a1, a0, .LBB6_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    lui a0, 524288
; RV64-NEXT:  .LBB6_4: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i64
  %0 = icmp slt i64 %conv, 2147483647
  %spec.store.select = select i1 %0, i64 %conv, i64 2147483647
  %1 = icmp sgt i64 %spec.store.select, -2147483648
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 -2147483648
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utest_f16i32(half %x) {
; RV32-LABEL: utest_f16i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixunssfdi
; RV32-NEXT:    sltiu a2, a0, -1
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    and a1, a1, a2
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f16i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    bltu a0, a1, .LBB7_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB7_2: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i64
  %0 = icmp ult i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f16i32(half %x) {
; RV32-LABEL: ustest_f16i32:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixsfdi
; RV32-NEXT:    beqz a1, .LBB8_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    srli a2, a1, 31
; RV32-NEXT:    j .LBB8_3
; RV32-NEXT:  .LBB8_2:
; RV32-NEXT:    sltiu a2, a0, -1
; RV32-NEXT:  .LBB8_3: # %entry
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    or a0, a3, a0
; RV32-NEXT:    beqz a1, .LBB8_5
; RV32-NEXT:  # %bb.4: # %entry
; RV32-NEXT:    sgtz a1, a1
; RV32-NEXT:    j .LBB8_6
; RV32-NEXT:  .LBB8_5:
; RV32-NEXT:    snez a1, a0
; RV32-NEXT:  .LBB8_6: # %entry
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i32:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    blt a0, a1, .LBB8_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB8_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i64
  %0 = icmp slt i64 %conv, 4294967295
  %spec.store.select = select i1 %0, i64 %conv, i64 4294967295
  %1 = icmp sgt i64 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i64 %spec.store.select, i64 0
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

; i16 saturate

define i16 @stest_f64i16(double %x) {
; RV32IF-LABEL: stest_f64i16:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfsi
; RV32IF-NEXT:    lui a1, 8
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    blt a0, a1, .LBB9_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB9_2: # %entry
; RV32IF-NEXT:    lui a1, 1048568
; RV32IF-NEXT:    blt a1, a0, .LBB9_4
; RV32IF-NEXT:  # %bb.3: # %entry
; RV32IF-NEXT:    lui a0, 1048568
; RV32IF-NEXT:  .LBB9_4: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i16:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfsi
; RV64IF-NEXT:    lui a1, 8
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    blt a0, a1, .LBB9_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB9_2: # %entry
; RV64IF-NEXT:    lui a1, 1048568
; RV64IF-NEXT:    blt a1, a0, .LBB9_4
; RV64IF-NEXT:  # %bb.3: # %entry
; RV64IF-NEXT:    lui a0, 1048568
; RV64IF-NEXT:  .LBB9_4: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i16:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 8
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    bge a0, a1, .LBB9_3
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    lui a1, 1048568
; RV32IFD-NEXT:    bge a1, a0, .LBB9_4
; RV32IFD-NEXT:  .LBB9_2: # %entry
; RV32IFD-NEXT:    ret
; RV32IFD-NEXT:  .LBB9_3: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:    lui a1, 1048568
; RV32IFD-NEXT:    blt a1, a0, .LBB9_2
; RV32IFD-NEXT:  .LBB9_4: # %entry
; RV32IFD-NEXT:    lui a0, 1048568
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i16:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 8
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    bge a0, a1, .LBB9_3
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    lui a1, 1048568
; RV64IFD-NEXT:    bge a1, a0, .LBB9_4
; RV64IFD-NEXT:  .LBB9_2: # %entry
; RV64IFD-NEXT:    ret
; RV64IFD-NEXT:  .LBB9_3: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:    lui a1, 1048568
; RV64IFD-NEXT:    blt a1, a0, .LBB9_2
; RV64IFD-NEXT:  .LBB9_4: # %entry
; RV64IFD-NEXT:    lui a0, 1048568
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i32
  %0 = icmp slt i32 %conv, 32767
  %spec.store.select = select i1 %0, i32 %conv, i32 32767
  %1 = icmp sgt i32 %spec.store.select, -32768
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 -32768
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utest_f64i16(double %x) {
; RV32IF-LABEL: utest_f64i16:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixunsdfsi
; RV32IF-NEXT:    lui a1, 16
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    bltu a0, a1, .LBB10_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB10_2: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: utest_f64i16:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixunsdfsi
; RV64IF-NEXT:    lui a1, 16
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    bltu a0, a1, .LBB10_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB10_2: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i16:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 16
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    bltu a0, a1, .LBB10_2
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:  .LBB10_2: # %entry
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: utest_f64i16:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 16
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    bltu a0, a1, .LBB10_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB10_2: # %entry
; RV64IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i32
  %0 = icmp ult i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f64i16(double %x) {
; RV32IF-LABEL: ustest_f64i16:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfsi
; RV32IF-NEXT:    lui a1, 16
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    blt a0, a1, .LBB11_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB11_2: # %entry
; RV32IF-NEXT:    sgtz a1, a0
; RV32IF-NEXT:    neg a1, a1
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: ustest_f64i16:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfsi
; RV64IF-NEXT:    lui a1, 16
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    blt a0, a1, .LBB11_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB11_2: # %entry
; RV64IF-NEXT:    sgtz a1, a0
; RV64IF-NEXT:    neg a1, a1
; RV64IF-NEXT:    and a0, a1, a0
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i16:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 16
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    blt a0, a1, .LBB11_2
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:  .LBB11_2: # %entry
; RV32IFD-NEXT:    sgtz a1, a0
; RV32IFD-NEXT:    neg a1, a1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: ustest_f64i16:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 16
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    blt a0, a1, .LBB11_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB11_2: # %entry
; RV64IFD-NEXT:    sgtz a1, a0
; RV64IFD-NEXT:    neg a1, a1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i32
  %0 = icmp slt i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %1 = icmp sgt i32 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 0
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @stest_f32i16(float %x) {
; RV32-LABEL: stest_f32i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 8
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bge a0, a1, .LBB12_3
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    bge a1, a0, .LBB12_4
; RV32-NEXT:  .LBB12_2: # %entry
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB12_3: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    blt a1, a0, .LBB12_2
; RV32-NEXT:  .LBB12_4: # %entry
; RV32-NEXT:    lui a0, 1048568
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 8
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bge a0, a1, .LBB12_3
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    bge a1, a0, .LBB12_4
; RV64-NEXT:  .LBB12_2: # %entry
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB12_3: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    blt a1, a0, .LBB12_2
; RV64-NEXT:  .LBB12_4: # %entry
; RV64-NEXT:    lui a0, 1048568
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i32
  %0 = icmp slt i32 %conv, 32767
  %spec.store.select = select i1 %0, i32 %conv, i32 32767
  %1 = icmp sgt i32 %spec.store.select, -32768
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 -32768
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utest_f32i16(float %x) {
; RV32-LABEL: utest_f32i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bltu a0, a1, .LBB13_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB13_2: # %entry
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bltu a0, a1, .LBB13_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB13_2: # %entry
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i32
  %0 = icmp ult i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f32i16(float %x) {
; RV32-LABEL: ustest_f32i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB14_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB14_2: # %entry
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB14_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB14_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i32
  %0 = icmp slt i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %1 = icmp sgt i32 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 0
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @stest_f16i16(half %x) {
; RV32-LABEL: stest_f16i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 8
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB15_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB15_2: # %entry
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    blt a1, a0, .LBB15_4
; RV32-NEXT:  # %bb.3: # %entry
; RV32-NEXT:    lui a0, 1048568
; RV32-NEXT:  .LBB15_4: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 8
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB15_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB15_2: # %entry
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    blt a1, a0, .LBB15_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    lui a0, 1048568
; RV64-NEXT:  .LBB15_4: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i32
  %0 = icmp slt i32 %conv, 32767
  %spec.store.select = select i1 %0, i32 %conv, i32 32767
  %1 = icmp sgt i32 %spec.store.select, -32768
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 -32768
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utest_f16i16(half %x) {
; RV32-LABEL: utest_f16i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bltu a0, a1, .LBB16_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB16_2: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f16i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bltu a0, a1, .LBB16_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB16_2: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i32
  %0 = icmp ult i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f16i16(half %x) {
; RV32-LABEL: ustest_f16i16:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB17_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB17_2: # %entry
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i16:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB17_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB17_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i32
  %0 = icmp slt i32 %conv, 65535
  %spec.store.select = select i1 %0, i32 %conv, i32 65535
  %1 = icmp sgt i32 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i32 %spec.store.select, i32 0
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

; i64 saturate

define i64 @stest_f64i64(double %x) {
; RV32IF-LABEL: stest_f64i64:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB18_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    li a2, 0
; RV32IF-NEXT:    j .LBB18_7
; RV32IF-NEXT:  .LBB18_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB18_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a2, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a2, a2, a0
; RV32IF-NEXT:    mv a1, s2
; RV32IF-NEXT:    mv a0, s2
; RV32IF-NEXT:    j .LBB18_7
; RV32IF-NEXT:  .LBB18_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB18_6
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s9, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s10, a0, s7
; RV32IF-NEXT:    sltu a0, s10, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s9, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv a2, a1
; RV32IF-NEXT:    mv a1, s10
; RV32IF-NEXT:    add a3, s5, a0
; RV32IF-NEXT:    sltu a4, s7, s1
; RV32IF-NEXT:    sltu a5, s11, s9
; RV32IF-NEXT:    add a2, s6, a2
; RV32IF-NEXT:    add a0, s7, a3
; RV32IF-NEXT:    add a5, s8, a5
; RV32IF-NEXT:    sltu a3, a3, s5
; RV32IF-NEXT:    sltu a6, a0, s7
; RV32IF-NEXT:    add a4, a5, a4
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:    add a2, a4, a2
; RV32IF-NEXT:    add a2, a2, a6
; RV32IF-NEXT:    j .LBB18_7
; RV32IF-NEXT:  .LBB18_6: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s8, s5, a1
; RV32IF-NEXT:    sltu a0, s8, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s8, a0, s8
; RV32IF-NEXT:    sltu a0, s8, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv a2, a1
; RV32IF-NEXT:    mv a1, s8
; RV32IF-NEXT:    add a2, a2, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a3, s4, a0
; RV32IF-NEXT:    add a0, s4, s1
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:    add a2, a2, s7
; RV32IF-NEXT:    sltu a3, a0, s4
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:  .LBB18_7: # %fp-to-i-cleanup
; RV32IF-NEXT:    lui a3, 524288
; RV32IF-NEXT:    addi a4, a3, -1
; RV32IF-NEXT:    beq a1, a4, .LBB18_9
; RV32IF-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32IF-NEXT:    sltu a5, a1, a4
; RV32IF-NEXT:    or a6, a0, a2
; RV32IF-NEXT:    bnez a6, .LBB18_10
; RV32IF-NEXT:    j .LBB18_11
; RV32IF-NEXT:  .LBB18_9:
; RV32IF-NEXT:    sltiu a5, s2, -1
; RV32IF-NEXT:    or a6, a0, a2
; RV32IF-NEXT:    beqz a6, .LBB18_11
; RV32IF-NEXT:  .LBB18_10: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a5, a2, 31
; RV32IF-NEXT:  .LBB18_11: # %fp-to-i-cleanup
; RV32IF-NEXT:    neg a6, a5
; RV32IF-NEXT:    addi a7, a5, -1
; RV32IF-NEXT:    bnez a5, .LBB18_13
; RV32IF-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IF-NEXT:    mv a1, a4
; RV32IF-NEXT:  .LBB18_13: # %fp-to-i-cleanup
; RV32IF-NEXT:    or a4, a7, s2
; RV32IF-NEXT:    and a2, a6, a2
; RV32IF-NEXT:    and a5, a6, a0
; RV32IF-NEXT:    beq a1, a3, .LBB18_15
; RV32IF-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32IF-NEXT:    sltu a0, a3, a1
; RV32IF-NEXT:    j .LBB18_16
; RV32IF-NEXT:  .LBB18_15:
; RV32IF-NEXT:    snez a0, a4
; RV32IF-NEXT:  .LBB18_16: # %fp-to-i-cleanup
; RV32IF-NEXT:    and a5, a5, a2
; RV32IF-NEXT:    li a3, -1
; RV32IF-NEXT:    beq a5, a3, .LBB18_18
; RV32IF-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a2, a2, 31
; RV32IF-NEXT:    xori a0, a2, 1
; RV32IF-NEXT:  .LBB18_18: # %fp-to-i-cleanup
; RV32IF-NEXT:    bnez a0, .LBB18_20
; RV32IF-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:  .LBB18_20: # %fp-to-i-cleanup
; RV32IF-NEXT:    neg a0, a0
; RV32IF-NEXT:    and a0, a0, a4
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i64:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfti
; RV64IF-NEXT:    li a2, -1
; RV64IF-NEXT:    srli a3, a2, 1
; RV64IF-NEXT:    beqz a1, .LBB18_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    srli a4, a1, 63
; RV64IF-NEXT:    j .LBB18_3
; RV64IF-NEXT:  .LBB18_2:
; RV64IF-NEXT:    sltu a4, a0, a3
; RV64IF-NEXT:  .LBB18_3: # %entry
; RV64IF-NEXT:    neg a5, a4
; RV64IF-NEXT:    and a5, a5, a1
; RV64IF-NEXT:    bnez a4, .LBB18_5
; RV64IF-NEXT:  # %bb.4: # %entry
; RV64IF-NEXT:    mv a0, a3
; RV64IF-NEXT:  .LBB18_5: # %entry
; RV64IF-NEXT:    slli a1, a2, 63
; RV64IF-NEXT:    beq a5, a2, .LBB18_7
; RV64IF-NEXT:  # %bb.6: # %entry
; RV64IF-NEXT:    srli a5, a5, 63
; RV64IF-NEXT:    xori a2, a5, 1
; RV64IF-NEXT:    beqz a2, .LBB18_8
; RV64IF-NEXT:    j .LBB18_9
; RV64IF-NEXT:  .LBB18_7:
; RV64IF-NEXT:    sltu a2, a1, a0
; RV64IF-NEXT:    bnez a2, .LBB18_9
; RV64IF-NEXT:  .LBB18_8: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB18_9: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i64:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB18_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    li a2, 0
; RV32IFD-NEXT:    j .LBB18_7
; RV32IFD-NEXT:  .LBB18_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB18_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a2, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a2, a2, a0
; RV32IFD-NEXT:    mv a1, s2
; RV32IFD-NEXT:    mv a0, s2
; RV32IFD-NEXT:    j .LBB18_7
; RV32IFD-NEXT:  .LBB18_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB18_6
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s9, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s10, a0, s7
; RV32IFD-NEXT:    sltu a0, s10, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s9, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv a2, a1
; RV32IFD-NEXT:    mv a1, s10
; RV32IFD-NEXT:    add a3, s5, a0
; RV32IFD-NEXT:    sltu a4, s7, s1
; RV32IFD-NEXT:    sltu a5, s11, s9
; RV32IFD-NEXT:    add a2, s6, a2
; RV32IFD-NEXT:    add a0, s7, a3
; RV32IFD-NEXT:    add a5, s8, a5
; RV32IFD-NEXT:    sltu a3, a3, s5
; RV32IFD-NEXT:    sltu a6, a0, s7
; RV32IFD-NEXT:    add a4, a5, a4
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:    add a2, a4, a2
; RV32IFD-NEXT:    add a2, a2, a6
; RV32IFD-NEXT:    j .LBB18_7
; RV32IFD-NEXT:  .LBB18_6: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s8, s5, a1
; RV32IFD-NEXT:    sltu a0, s8, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s8, a0, s8
; RV32IFD-NEXT:    sltu a0, s8, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv a2, a1
; RV32IFD-NEXT:    mv a1, s8
; RV32IFD-NEXT:    add a2, a2, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a3, s4, a0
; RV32IFD-NEXT:    add a0, s4, s1
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:    add a2, a2, s7
; RV32IFD-NEXT:    sltu a3, a0, s4
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:  .LBB18_7: # %fp-to-i-cleanup
; RV32IFD-NEXT:    lui a3, 524288
; RV32IFD-NEXT:    addi a4, a3, -1
; RV32IFD-NEXT:    beq a1, a4, .LBB18_9
; RV32IFD-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32IFD-NEXT:    sltu a5, a1, a4
; RV32IFD-NEXT:    or a6, a0, a2
; RV32IFD-NEXT:    bnez a6, .LBB18_10
; RV32IFD-NEXT:    j .LBB18_11
; RV32IFD-NEXT:  .LBB18_9:
; RV32IFD-NEXT:    sltiu a5, s2, -1
; RV32IFD-NEXT:    or a6, a0, a2
; RV32IFD-NEXT:    beqz a6, .LBB18_11
; RV32IFD-NEXT:  .LBB18_10: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a5, a2, 31
; RV32IFD-NEXT:  .LBB18_11: # %fp-to-i-cleanup
; RV32IFD-NEXT:    neg a6, a5
; RV32IFD-NEXT:    addi a7, a5, -1
; RV32IFD-NEXT:    bnez a5, .LBB18_13
; RV32IFD-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IFD-NEXT:    mv a1, a4
; RV32IFD-NEXT:  .LBB18_13: # %fp-to-i-cleanup
; RV32IFD-NEXT:    or a4, a7, s2
; RV32IFD-NEXT:    and a2, a6, a2
; RV32IFD-NEXT:    and a5, a6, a0
; RV32IFD-NEXT:    beq a1, a3, .LBB18_15
; RV32IFD-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32IFD-NEXT:    sltu a0, a3, a1
; RV32IFD-NEXT:    j .LBB18_16
; RV32IFD-NEXT:  .LBB18_15:
; RV32IFD-NEXT:    snez a0, a4
; RV32IFD-NEXT:  .LBB18_16: # %fp-to-i-cleanup
; RV32IFD-NEXT:    and a5, a5, a2
; RV32IFD-NEXT:    li a3, -1
; RV32IFD-NEXT:    beq a5, a3, .LBB18_18
; RV32IFD-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a2, a2, 31
; RV32IFD-NEXT:    xori a0, a2, 1
; RV32IFD-NEXT:  .LBB18_18: # %fp-to-i-cleanup
; RV32IFD-NEXT:    bnez a0, .LBB18_20
; RV32IFD-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:  .LBB18_20: # %fp-to-i-cleanup
; RV32IFD-NEXT:    neg a0, a0
; RV32IFD-NEXT:    and a0, a0, a4
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i64:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    feq.d a1, fa0, fa0
; RV64IFD-NEXT:    seqz a1, a1
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i128
  %0 = icmp slt i128 %conv, 9223372036854775807
  %spec.store.select = select i1 %0, i128 %conv, i128 9223372036854775807
  %1 = icmp sgt i128 %spec.store.select, -9223372036854775808
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 -9223372036854775808
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utest_f64i64(double %x) {
; RV32IF-LABEL: utest_f64i64:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB19_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li s9, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    j .LBB19_7
; RV32IF-NEXT:  .LBB19_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB19_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a1, a1, a0
; RV32IF-NEXT:    mv s9, s2
; RV32IF-NEXT:    mv a0, s2
; RV32IF-NEXT:    j .LBB19_7
; RV32IF-NEXT:  .LBB19_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB19_6
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s10, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s7
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s10, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a2, s5, a0
; RV32IF-NEXT:    sltu a3, s7, s1
; RV32IF-NEXT:    sltu a4, s11, s10
; RV32IF-NEXT:    add a1, s6, a1
; RV32IF-NEXT:    add a0, s7, a2
; RV32IF-NEXT:    add a4, s8, a4
; RV32IF-NEXT:    sltu a2, a2, s5
; RV32IF-NEXT:    sltu a5, a0, s7
; RV32IF-NEXT:    add a3, a4, a3
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a3, a1
; RV32IF-NEXT:    add a1, a1, a5
; RV32IF-NEXT:    j .LBB19_7
; RV32IF-NEXT:  .LBB19_6: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s9, s5, a1
; RV32IF-NEXT:    sltu a0, s9, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s9
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a2, s4, a0
; RV32IF-NEXT:    add a0, s4, s1
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a1, s7
; RV32IF-NEXT:    sltu a2, a0, s4
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:  .LBB19_7: # %fp-to-i-cleanup
; RV32IF-NEXT:    or a2, a1, a0
; RV32IF-NEXT:    xori a0, a0, 1
; RV32IF-NEXT:    seqz a2, a2
; RV32IF-NEXT:    or a0, a0, a1
; RV32IF-NEXT:    seqz a0, a0
; RV32IF-NEXT:    addi a0, a0, -1
; RV32IF-NEXT:    and a0, a0, a2
; RV32IF-NEXT:    neg a1, a0
; RV32IF-NEXT:    and a0, a1, s2
; RV32IF-NEXT:    and a1, a1, s9
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64-LABEL: utest_f64i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixunsdfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i64:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB19_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li s9, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    j .LBB19_7
; RV32IFD-NEXT:  .LBB19_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB19_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    mv s9, s2
; RV32IFD-NEXT:    mv a0, s2
; RV32IFD-NEXT:    j .LBB19_7
; RV32IFD-NEXT:  .LBB19_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB19_6
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s10, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s7
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s10, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a2, s5, a0
; RV32IFD-NEXT:    sltu a3, s7, s1
; RV32IFD-NEXT:    sltu a4, s11, s10
; RV32IFD-NEXT:    add a1, s6, a1
; RV32IFD-NEXT:    add a0, s7, a2
; RV32IFD-NEXT:    add a4, s8, a4
; RV32IFD-NEXT:    sltu a2, a2, s5
; RV32IFD-NEXT:    sltu a5, a0, s7
; RV32IFD-NEXT:    add a3, a4, a3
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a3, a1
; RV32IFD-NEXT:    add a1, a1, a5
; RV32IFD-NEXT:    j .LBB19_7
; RV32IFD-NEXT:  .LBB19_6: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s9, s5, a1
; RV32IFD-NEXT:    sltu a0, s9, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s9
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a2, s4, a0
; RV32IFD-NEXT:    add a0, s4, s1
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a1, s7
; RV32IFD-NEXT:    sltu a2, a0, s4
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:  .LBB19_7: # %fp-to-i-cleanup
; RV32IFD-NEXT:    or a2, a1, a0
; RV32IFD-NEXT:    xori a0, a0, 1
; RV32IFD-NEXT:    seqz a2, a2
; RV32IFD-NEXT:    or a0, a0, a1
; RV32IFD-NEXT:    seqz a0, a0
; RV32IFD-NEXT:    addi a0, a0, -1
; RV32IFD-NEXT:    and a0, a0, a2
; RV32IFD-NEXT:    neg a1, a0
; RV32IFD-NEXT:    and a0, a1, s2
; RV32IFD-NEXT:    and a1, a1, s9
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i128
  %0 = icmp ult i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f64i64(double %x) {
; RV32IF-LABEL: ustest_f64i64:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB20_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li s9, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    j .LBB20_6
; RV32IF-NEXT:  .LBB20_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB20_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a1, a1, a0
; RV32IF-NEXT:    mv s9, s2
; RV32IF-NEXT:    mv a0, s2
; RV32IF-NEXT:    beqz a1, .LBB20_6
; RV32IF-NEXT:    j .LBB20_8
; RV32IF-NEXT:  .LBB20_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB20_7
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s10, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s7
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s10, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a2, s5, a0
; RV32IF-NEXT:    sltu a3, s7, s1
; RV32IF-NEXT:    sltu a4, s11, s10
; RV32IF-NEXT:    add a1, s6, a1
; RV32IF-NEXT:    add a0, s7, a2
; RV32IF-NEXT:    add a4, s8, a4
; RV32IF-NEXT:    sltu a2, a2, s5
; RV32IF-NEXT:    sltu a5, a0, s7
; RV32IF-NEXT:    add a3, a4, a3
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a3, a1
; RV32IF-NEXT:    add a1, a1, a5
; RV32IF-NEXT:    bnez a1, .LBB20_8
; RV32IF-NEXT:  .LBB20_6:
; RV32IF-NEXT:    seqz a2, a0
; RV32IF-NEXT:    j .LBB20_9
; RV32IF-NEXT:  .LBB20_7: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s9, s5, a1
; RV32IF-NEXT:    sltu a0, s9, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s9
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a2, s4, a0
; RV32IF-NEXT:    add a0, s4, s1
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a1, s7
; RV32IF-NEXT:    sltu a2, a0, s4
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    beqz a1, .LBB20_6
; RV32IF-NEXT:  .LBB20_8: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a2, a1, 31
; RV32IF-NEXT:  .LBB20_9: # %fp-to-i-cleanup
; RV32IF-NEXT:    xori a3, a0, 1
; RV32IF-NEXT:    or a3, a3, a1
; RV32IF-NEXT:    seqz a3, a3
; RV32IF-NEXT:    addi a3, a3, -1
; RV32IF-NEXT:    and a3, a3, a2
; RV32IF-NEXT:    neg a2, a3
; RV32IF-NEXT:    bnez a3, .LBB20_11
; RV32IF-NEXT:  # %bb.10: # %fp-to-i-cleanup
; RV32IF-NEXT:    li a0, 1
; RV32IF-NEXT:  .LBB20_11: # %fp-to-i-cleanup
; RV32IF-NEXT:    and a4, a2, a1
; RV32IF-NEXT:    beqz a4, .LBB20_13
; RV32IF-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IF-NEXT:    sgtz a1, a4
; RV32IF-NEXT:    j .LBB20_14
; RV32IF-NEXT:  .LBB20_13:
; RV32IF-NEXT:    snez a1, a0
; RV32IF-NEXT:  .LBB20_14: # %fp-to-i-cleanup
; RV32IF-NEXT:    and a3, a2, s9
; RV32IF-NEXT:    or a4, a0, a4
; RV32IF-NEXT:    and a0, a2, s2
; RV32IF-NEXT:    bnez a4, .LBB20_16
; RV32IF-NEXT:  # %bb.15:
; RV32IF-NEXT:    or a1, a0, a3
; RV32IF-NEXT:    snez a1, a1
; RV32IF-NEXT:  .LBB20_16: # %fp-to-i-cleanup
; RV32IF-NEXT:    neg a1, a1
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    and a1, a1, a3
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64-LABEL: ustest_f64i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixdfti
; RV64-NEXT:    slti a2, a1, 1
; RV64-NEXT:    blez a1, .LBB20_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a1, 1
; RV64-NEXT:  .LBB20_2: # %entry
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    beqz a1, .LBB20_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    sgtz a1, a1
; RV64-NEXT:    j .LBB20_5
; RV64-NEXT:  .LBB20_4:
; RV64-NEXT:    snez a1, a0
; RV64-NEXT:  .LBB20_5: # %entry
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i64:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB20_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li s9, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    j .LBB20_6
; RV32IFD-NEXT:  .LBB20_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB20_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    mv s9, s2
; RV32IFD-NEXT:    mv a0, s2
; RV32IFD-NEXT:    beqz a1, .LBB20_6
; RV32IFD-NEXT:    j .LBB20_8
; RV32IFD-NEXT:  .LBB20_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB20_7
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s10, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s7
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s10, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a2, s5, a0
; RV32IFD-NEXT:    sltu a3, s7, s1
; RV32IFD-NEXT:    sltu a4, s11, s10
; RV32IFD-NEXT:    add a1, s6, a1
; RV32IFD-NEXT:    add a0, s7, a2
; RV32IFD-NEXT:    add a4, s8, a4
; RV32IFD-NEXT:    sltu a2, a2, s5
; RV32IFD-NEXT:    sltu a5, a0, s7
; RV32IFD-NEXT:    add a3, a4, a3
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a3, a1
; RV32IFD-NEXT:    add a1, a1, a5
; RV32IFD-NEXT:    bnez a1, .LBB20_8
; RV32IFD-NEXT:  .LBB20_6:
; RV32IFD-NEXT:    seqz a2, a0
; RV32IFD-NEXT:    j .LBB20_9
; RV32IFD-NEXT:  .LBB20_7: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s9, s5, a1
; RV32IFD-NEXT:    sltu a0, s9, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s9
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a2, s4, a0
; RV32IFD-NEXT:    add a0, s4, s1
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a1, s7
; RV32IFD-NEXT:    sltu a2, a0, s4
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    beqz a1, .LBB20_6
; RV32IFD-NEXT:  .LBB20_8: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a2, a1, 31
; RV32IFD-NEXT:  .LBB20_9: # %fp-to-i-cleanup
; RV32IFD-NEXT:    xori a3, a0, 1
; RV32IFD-NEXT:    or a3, a3, a1
; RV32IFD-NEXT:    seqz a3, a3
; RV32IFD-NEXT:    addi a3, a3, -1
; RV32IFD-NEXT:    and a3, a3, a2
; RV32IFD-NEXT:    neg a2, a3
; RV32IFD-NEXT:    bnez a3, .LBB20_11
; RV32IFD-NEXT:  # %bb.10: # %fp-to-i-cleanup
; RV32IFD-NEXT:    li a0, 1
; RV32IFD-NEXT:  .LBB20_11: # %fp-to-i-cleanup
; RV32IFD-NEXT:    and a4, a2, a1
; RV32IFD-NEXT:    beqz a4, .LBB20_13
; RV32IFD-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IFD-NEXT:    sgtz a1, a4
; RV32IFD-NEXT:    j .LBB20_14
; RV32IFD-NEXT:  .LBB20_13:
; RV32IFD-NEXT:    snez a1, a0
; RV32IFD-NEXT:  .LBB20_14: # %fp-to-i-cleanup
; RV32IFD-NEXT:    and a3, a2, s9
; RV32IFD-NEXT:    or a4, a0, a4
; RV32IFD-NEXT:    and a0, a2, s2
; RV32IFD-NEXT:    bnez a4, .LBB20_16
; RV32IFD-NEXT:  # %bb.15:
; RV32IFD-NEXT:    or a1, a0, a3
; RV32IFD-NEXT:    snez a1, a1
; RV32IFD-NEXT:  .LBB20_16: # %fp-to-i-cleanup
; RV32IFD-NEXT:    neg a1, a1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    and a1, a1, a3
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i128
  %0 = icmp slt i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %1 = icmp sgt i128 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 0
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @stest_f32i64(float %x) {
; RV32-LABEL: stest_f32i64:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB21_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a2, 0
; RV32-NEXT:    j .LBB21_7
; RV32-NEXT:  .LBB21_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB21_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a2, a0, a2
; RV32-NEXT:    mv a1, s2
; RV32-NEXT:    mv a0, s2
; RV32-NEXT:    j .LBB21_7
; RV32-NEXT:  .LBB21_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB21_6
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s9, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s10, a0, s7
; RV32-NEXT:    sltu a0, s10, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s9, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv a2, a1
; RV32-NEXT:    mv a1, s10
; RV32-NEXT:    add a3, s5, a0
; RV32-NEXT:    sltu a4, s7, s1
; RV32-NEXT:    sltu a5, s11, s9
; RV32-NEXT:    add a2, s6, a2
; RV32-NEXT:    add a0, s7, a3
; RV32-NEXT:    add a5, s8, a5
; RV32-NEXT:    sltu a3, a3, s5
; RV32-NEXT:    sltu a6, a0, s7
; RV32-NEXT:    add a4, a5, a4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    add a2, a4, a2
; RV32-NEXT:    add a2, a2, a6
; RV32-NEXT:    j .LBB21_7
; RV32-NEXT:  .LBB21_6: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s8, s5, a1
; RV32-NEXT:    sltu a0, s8, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s8, a0, s8
; RV32-NEXT:    sltu a0, s8, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv a2, a1
; RV32-NEXT:    mv a1, s8
; RV32-NEXT:    add a2, a2, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a3, s4, a0
; RV32-NEXT:    add a0, s4, s1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    add a2, a2, s7
; RV32-NEXT:    sltu a3, a0, s4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:  .LBB21_7: # %fp-to-i-cleanup
; RV32-NEXT:    lui a3, 524288
; RV32-NEXT:    addi a4, a3, -1
; RV32-NEXT:    beq a1, a4, .LBB21_9
; RV32-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32-NEXT:    sltu a5, a1, a4
; RV32-NEXT:    or a6, a0, a2
; RV32-NEXT:    bnez a6, .LBB21_10
; RV32-NEXT:    j .LBB21_11
; RV32-NEXT:  .LBB21_9:
; RV32-NEXT:    sltiu a5, s2, -1
; RV32-NEXT:    or a6, a0, a2
; RV32-NEXT:    beqz a6, .LBB21_11
; RV32-NEXT:  .LBB21_10: # %fp-to-i-cleanup
; RV32-NEXT:    srli a5, a2, 31
; RV32-NEXT:  .LBB21_11: # %fp-to-i-cleanup
; RV32-NEXT:    neg a6, a5
; RV32-NEXT:    addi a7, a5, -1
; RV32-NEXT:    bnez a5, .LBB21_13
; RV32-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32-NEXT:    mv a1, a4
; RV32-NEXT:  .LBB21_13: # %fp-to-i-cleanup
; RV32-NEXT:    or a4, a7, s2
; RV32-NEXT:    and a2, a6, a2
; RV32-NEXT:    and a5, a6, a0
; RV32-NEXT:    beq a1, a3, .LBB21_15
; RV32-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32-NEXT:    sltu a0, a3, a1
; RV32-NEXT:    j .LBB21_16
; RV32-NEXT:  .LBB21_15:
; RV32-NEXT:    snez a0, a4
; RV32-NEXT:  .LBB21_16: # %fp-to-i-cleanup
; RV32-NEXT:    and a5, a5, a2
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beq a5, a3, .LBB21_18
; RV32-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32-NEXT:    srli a2, a2, 31
; RV32-NEXT:    xori a0, a2, 1
; RV32-NEXT:  .LBB21_18: # %fp-to-i-cleanup
; RV32-NEXT:    bnez a0, .LBB21_20
; RV32-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB21_20: # %fp-to-i-cleanup
; RV32-NEXT:    neg a0, a0
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    feq.s a1, fa0, fa0
; RV64-NEXT:    seqz a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i128
  %0 = icmp slt i128 %conv, 9223372036854775807
  %spec.store.select = select i1 %0, i128 %conv, i128 9223372036854775807
  %1 = icmp sgt i128 %spec.store.select, -9223372036854775808
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 -9223372036854775808
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utest_f32i64(float %x) {
; RV32-LABEL: utest_f32i64:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB22_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    j .LBB22_7
; RV32-NEXT:  .LBB22_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB22_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a1, a0, a2
; RV32-NEXT:    mv s9, s2
; RV32-NEXT:    mv a0, s2
; RV32-NEXT:    j .LBB22_7
; RV32-NEXT:  .LBB22_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB22_6
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s10, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s7
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s10, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a2, s5, a0
; RV32-NEXT:    sltu a3, s7, s1
; RV32-NEXT:    sltu a4, s11, s10
; RV32-NEXT:    add a1, s6, a1
; RV32-NEXT:    add a0, s7, a2
; RV32-NEXT:    add a4, s8, a4
; RV32-NEXT:    sltu a2, a2, s5
; RV32-NEXT:    sltu a5, a0, s7
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a3, a1
; RV32-NEXT:    add a1, a1, a5
; RV32-NEXT:    j .LBB22_7
; RV32-NEXT:  .LBB22_6: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s9, s5, a1
; RV32-NEXT:    sltu a0, s9, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s9
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a2, s4, a0
; RV32-NEXT:    add a0, s4, s1
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a1, s7
; RV32-NEXT:    sltu a2, a0, s4
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:  .LBB22_7: # %fp-to-i-cleanup
; RV32-NEXT:    or a2, a1, a0
; RV32-NEXT:    xori a0, a0, 1
; RV32-NEXT:    seqz a2, a2
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    seqz a0, a0
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    neg a1, a0
; RV32-NEXT:    and a0, a1, s2
; RV32-NEXT:    and a1, a1, s9
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixunssfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i128
  %0 = icmp ult i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f32i64(float %x) {
; RV32-LABEL: ustest_f32i64:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB23_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    j .LBB23_6
; RV32-NEXT:  .LBB23_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB23_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a1, a0, a2
; RV32-NEXT:    mv s9, s2
; RV32-NEXT:    mv a0, s2
; RV32-NEXT:    beqz a1, .LBB23_6
; RV32-NEXT:    j .LBB23_8
; RV32-NEXT:  .LBB23_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB23_7
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s10, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s7
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s10, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a2, s5, a0
; RV32-NEXT:    sltu a3, s7, s1
; RV32-NEXT:    sltu a4, s11, s10
; RV32-NEXT:    add a1, s6, a1
; RV32-NEXT:    add a0, s7, a2
; RV32-NEXT:    add a4, s8, a4
; RV32-NEXT:    sltu a2, a2, s5
; RV32-NEXT:    sltu a5, a0, s7
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a3, a1
; RV32-NEXT:    add a1, a1, a5
; RV32-NEXT:    bnez a1, .LBB23_8
; RV32-NEXT:  .LBB23_6:
; RV32-NEXT:    seqz a2, a0
; RV32-NEXT:    j .LBB23_9
; RV32-NEXT:  .LBB23_7: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s9, s5, a1
; RV32-NEXT:    sltu a0, s9, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s9
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a2, s4, a0
; RV32-NEXT:    add a0, s4, s1
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a1, s7
; RV32-NEXT:    sltu a2, a0, s4
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    beqz a1, .LBB23_6
; RV32-NEXT:  .LBB23_8: # %fp-to-i-cleanup
; RV32-NEXT:    srli a2, a1, 31
; RV32-NEXT:  .LBB23_9: # %fp-to-i-cleanup
; RV32-NEXT:    xori a3, a0, 1
; RV32-NEXT:    or a3, a3, a1
; RV32-NEXT:    seqz a3, a3
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and a3, a3, a2
; RV32-NEXT:    neg a2, a3
; RV32-NEXT:    bnez a3, .LBB23_11
; RV32-NEXT:  # %bb.10: # %fp-to-i-cleanup
; RV32-NEXT:    li a0, 1
; RV32-NEXT:  .LBB23_11: # %fp-to-i-cleanup
; RV32-NEXT:    and a4, a2, a1
; RV32-NEXT:    beqz a4, .LBB23_13
; RV32-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32-NEXT:    sgtz a1, a4
; RV32-NEXT:    j .LBB23_14
; RV32-NEXT:  .LBB23_13:
; RV32-NEXT:    snez a1, a0
; RV32-NEXT:  .LBB23_14: # %fp-to-i-cleanup
; RV32-NEXT:    and a3, a2, s9
; RV32-NEXT:    or a4, a0, a4
; RV32-NEXT:    and a0, a2, s2
; RV32-NEXT:    bnez a4, .LBB23_16
; RV32-NEXT:  # %bb.15:
; RV32-NEXT:    or a1, a0, a3
; RV32-NEXT:    snez a1, a1
; RV32-NEXT:  .LBB23_16: # %fp-to-i-cleanup
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    and a1, a1, a3
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    slti a2, a1, 1
; RV64-NEXT:    blez a1, .LBB23_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a1, 1
; RV64-NEXT:  .LBB23_2: # %entry
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    beqz a1, .LBB23_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    sgtz a1, a1
; RV64-NEXT:    j .LBB23_5
; RV64-NEXT:  .LBB23_4:
; RV64-NEXT:    snez a1, a0
; RV64-NEXT:  .LBB23_5: # %entry
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i128
  %0 = icmp slt i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %1 = icmp sgt i128 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 0
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @stest_f16i64(half %x) {
; RV32-LABEL: stest_f16i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a2, fa0, rtz
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    srai a3, a2, 31
; RV32-NEXT:    addi a4, a0, -1
; RV32-NEXT:    beq a3, a4, .LBB24_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    sltu a5, a3, a4
; RV32-NEXT:    bnez a3, .LBB24_3
; RV32-NEXT:    j .LBB24_4
; RV32-NEXT:  .LBB24_2:
; RV32-NEXT:    sltiu a5, a2, -1
; RV32-NEXT:    beqz a3, .LBB24_4
; RV32-NEXT:  .LBB24_3: # %entry
; RV32-NEXT:    srli a5, a3, 31
; RV32-NEXT:  .LBB24_4: # %entry
; RV32-NEXT:    neg a6, a5
; RV32-NEXT:    addi a7, a5, -1
; RV32-NEXT:    mv a1, a3
; RV32-NEXT:    bnez a5, .LBB24_6
; RV32-NEXT:  # %bb.5: # %entry
; RV32-NEXT:    mv a1, a4
; RV32-NEXT:  .LBB24_6: # %entry
; RV32-NEXT:    or a2, a7, a2
; RV32-NEXT:    and a3, a6, a3
; RV32-NEXT:    beq a1, a0, .LBB24_8
; RV32-NEXT:  # %bb.7: # %entry
; RV32-NEXT:    sltu a0, a0, a1
; RV32-NEXT:    li a4, -1
; RV32-NEXT:    bne a3, a4, .LBB24_9
; RV32-NEXT:    j .LBB24_10
; RV32-NEXT:  .LBB24_8:
; RV32-NEXT:    snez a0, a2
; RV32-NEXT:    li a4, -1
; RV32-NEXT:    beq a3, a4, .LBB24_10
; RV32-NEXT:  .LBB24_9: # %entry
; RV32-NEXT:    srli a3, a3, 31
; RV32-NEXT:    xori a0, a3, 1
; RV32-NEXT:  .LBB24_10: # %entry
; RV32-NEXT:    bnez a0, .LBB24_12
; RV32-NEXT:  # %bb.11: # %entry
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB24_12: # %entry
; RV32-NEXT:    neg a0, a0
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    li a2, -1
; RV64-NEXT:    srli a3, a2, 1
; RV64-NEXT:    beqz a1, .LBB24_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    srli a4, a1, 63
; RV64-NEXT:    j .LBB24_3
; RV64-NEXT:  .LBB24_2:
; RV64-NEXT:    sltu a4, a0, a3
; RV64-NEXT:  .LBB24_3: # %entry
; RV64-NEXT:    neg a5, a4
; RV64-NEXT:    and a5, a5, a1
; RV64-NEXT:    bnez a4, .LBB24_5
; RV64-NEXT:  # %bb.4: # %entry
; RV64-NEXT:    mv a0, a3
; RV64-NEXT:  .LBB24_5: # %entry
; RV64-NEXT:    slli a1, a2, 63
; RV64-NEXT:    beq a5, a2, .LBB24_7
; RV64-NEXT:  # %bb.6: # %entry
; RV64-NEXT:    srli a5, a5, 63
; RV64-NEXT:    xori a2, a5, 1
; RV64-NEXT:    beqz a2, .LBB24_8
; RV64-NEXT:    j .LBB24_9
; RV64-NEXT:  .LBB24_7:
; RV64-NEXT:    sltu a2, a1, a0
; RV64-NEXT:    bnez a2, .LBB24_9
; RV64-NEXT:  .LBB24_8: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB24_9: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i128
  %0 = icmp slt i128 %conv, 9223372036854775807
  %spec.store.select = select i1 %0, i128 %conv, i128 9223372036854775807
  %1 = icmp sgt i128 %spec.store.select, -9223372036854775808
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 -9223372036854775808
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utesth_f16i64(half %x) {
; RV32-LABEL: utesth_f16i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utesth_f16i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixunssfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i128
  %0 = icmp ult i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f16i64(half %x) {
; RV32-LABEL: ustest_f16i64:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    srai a1, a0, 31
; RV32-NEXT:    srli a2, a1, 31
; RV32-NEXT:    seqz a3, a1
; RV32-NEXT:    ori a4, a1, 1
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    seqz a3, a4
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and a4, a3, a2
; RV32-NEXT:    neg a2, a4
; RV32-NEXT:    mv a3, a1
; RV32-NEXT:    bnez a4, .LBB26_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    li a3, 1
; RV32-NEXT:  .LBB26_2: # %entry
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    beqz a1, .LBB26_4
; RV32-NEXT:  # %bb.3: # %entry
; RV32-NEXT:    sgtz a4, a1
; RV32-NEXT:    j .LBB26_5
; RV32-NEXT:  .LBB26_4:
; RV32-NEXT:    snez a4, a3
; RV32-NEXT:  .LBB26_5: # %entry
; RV32-NEXT:    or a3, a3, a1
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    bnez a3, .LBB26_7
; RV32-NEXT:  # %bb.6:
; RV32-NEXT:    or a2, a0, a1
; RV32-NEXT:    snez a4, a2
; RV32-NEXT:  .LBB26_7: # %entry
; RV32-NEXT:    neg a2, a4
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i64:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    slti a2, a1, 1
; RV64-NEXT:    blez a1, .LBB26_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a1, 1
; RV64-NEXT:  .LBB26_2: # %entry
; RV64-NEXT:    neg a2, a2
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    beqz a1, .LBB26_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    sgtz a1, a1
; RV64-NEXT:    j .LBB26_5
; RV64-NEXT:  .LBB26_4:
; RV64-NEXT:    snez a1, a0
; RV64-NEXT:  .LBB26_5: # %entry
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i128
  %0 = icmp slt i128 %conv, 18446744073709551616
  %spec.store.select = select i1 %0, i128 %conv, i128 18446744073709551616
  %1 = icmp sgt i128 %spec.store.select, 0
  %spec.store.select7 = select i1 %1, i128 %spec.store.select, i128 0
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

; i32 saturate

define i32 @stest_f64i32_mm(double %x) {
; RV32IF-LABEL: stest_f64i32_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfdi
; RV32IF-NEXT:    lui a2, 524288
; RV32IF-NEXT:    addi a3, a2, -1
; RV32IF-NEXT:    beqz a1, .LBB27_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    srli a4, a1, 31
; RV32IF-NEXT:    j .LBB27_3
; RV32IF-NEXT:  .LBB27_2:
; RV32IF-NEXT:    sltu a4, a0, a3
; RV32IF-NEXT:  .LBB27_3: # %entry
; RV32IF-NEXT:    neg a5, a4
; RV32IF-NEXT:    and a1, a5, a1
; RV32IF-NEXT:    bnez a4, .LBB27_5
; RV32IF-NEXT:  # %bb.4: # %entry
; RV32IF-NEXT:    mv a0, a3
; RV32IF-NEXT:  .LBB27_5: # %entry
; RV32IF-NEXT:    li a3, -1
; RV32IF-NEXT:    beq a1, a3, .LBB27_7
; RV32IF-NEXT:  # %bb.6: # %entry
; RV32IF-NEXT:    srli a1, a1, 31
; RV32IF-NEXT:    xori a1, a1, 1
; RV32IF-NEXT:    beqz a1, .LBB27_8
; RV32IF-NEXT:    j .LBB27_9
; RV32IF-NEXT:  .LBB27_7:
; RV32IF-NEXT:    sltu a1, a2, a0
; RV32IF-NEXT:    bnez a1, .LBB27_9
; RV32IF-NEXT:  .LBB27_8: # %entry
; RV32IF-NEXT:    lui a0, 524288
; RV32IF-NEXT:  .LBB27_9: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i32_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfdi
; RV64IF-NEXT:    lui a1, 524288
; RV64IF-NEXT:    addiw a2, a1, -1
; RV64IF-NEXT:    blt a0, a2, .LBB27_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a2
; RV64IF-NEXT:  .LBB27_2: # %entry
; RV64IF-NEXT:    blt a1, a0, .LBB27_4
; RV64IF-NEXT:  # %bb.3: # %entry
; RV64IF-NEXT:    lui a0, 524288
; RV64IF-NEXT:  .LBB27_4: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i32_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i32_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 524288
; RV64IFD-NEXT:    addiw a2, a1, -1
; RV64IFD-NEXT:    bge a0, a2, .LBB27_3
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    bge a1, a0, .LBB27_4
; RV64IFD-NEXT:  .LBB27_2: # %entry
; RV64IFD-NEXT:    ret
; RV64IFD-NEXT:  .LBB27_3: # %entry
; RV64IFD-NEXT:    mv a0, a2
; RV64IFD-NEXT:    blt a1, a2, .LBB27_2
; RV64IFD-NEXT:  .LBB27_4: # %entry
; RV64IFD-NEXT:    lui a0, 524288
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 2147483647)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 -2147483648)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utest_f64i32_mm(double %x) {
; RV32IF-LABEL: utest_f64i32_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixunsdfdi
; RV32IF-NEXT:    seqz a1, a1
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    or a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: utest_f64i32_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixunsdfdi
; RV64IF-NEXT:    li a1, -1
; RV64IF-NEXT:    srli a1, a1, 32
; RV64IF-NEXT:    bltu a0, a1, .LBB28_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB28_2: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i32_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: utest_f64i32_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.lu.d a0, fa0, rtz
; RV64IFD-NEXT:    li a1, -1
; RV64IFD-NEXT:    srli a1, a1, 32
; RV64IFD-NEXT:    bltu a0, a1, .LBB28_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB28_2: # %entry
; RV64IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i64
  %spec.store.select = call i64 @llvm.umin.i64(i64 %conv, i64 4294967295)
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f64i32_mm(double %x) {
; RV32IF-LABEL: ustest_f64i32_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfdi
; RV32IF-NEXT:    bnez a1, .LBB29_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    li a2, 1
; RV32IF-NEXT:    j .LBB29_3
; RV32IF-NEXT:  .LBB29_2:
; RV32IF-NEXT:    slti a2, a1, 1
; RV32IF-NEXT:  .LBB29_3: # %entry
; RV32IF-NEXT:    addi a3, a2, -1
; RV32IF-NEXT:    neg a2, a2
; RV32IF-NEXT:    or a0, a3, a0
; RV32IF-NEXT:    and a1, a2, a1
; RV32IF-NEXT:    srli a1, a1, 31
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: ustest_f64i32_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfdi
; RV64IF-NEXT:    li a1, -1
; RV64IF-NEXT:    srli a1, a1, 32
; RV64IF-NEXT:    blt a0, a1, .LBB29_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB29_2: # %entry
; RV64IF-NEXT:    sgtz a1, a0
; RV64IF-NEXT:    neg a1, a1
; RV64IF-NEXT:    and a0, a1, a0
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i32_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    feq.d a1, fa0, fa0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: ustest_f64i32_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    li a1, -1
; RV64IFD-NEXT:    srli a1, a1, 32
; RV64IFD-NEXT:    blt a0, a1, .LBB29_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB29_2: # %entry
; RV64IFD-NEXT:    sgtz a1, a0
; RV64IFD-NEXT:    neg a1, a1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 4294967295)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 0)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @stest_f32i32_mm(float %x) {
; RV32-LABEL: stest_f32i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 524288
; RV64-NEXT:    addiw a2, a1, -1
; RV64-NEXT:    bge a0, a2, .LBB30_3
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    bge a1, a0, .LBB30_4
; RV64-NEXT:  .LBB30_2: # %entry
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB30_3: # %entry
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:    blt a1, a2, .LBB30_2
; RV64-NEXT:  .LBB30_4: # %entry
; RV64-NEXT:    lui a0, 524288
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 2147483647)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 -2147483648)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utest_f32i32_mm(float %x) {
; RV32-LABEL: utest_f32i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    bltu a0, a1, .LBB31_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB31_2: # %entry
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i64
  %spec.store.select = call i64 @llvm.umin.i64(i64 %conv, i64 4294967295)
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f32i32_mm(float %x) {
; RV32-LABEL: ustest_f32i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    feq.s a1, fa0, fa0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    blt a0, a1, .LBB32_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB32_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 4294967295)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 0)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @stest_f16i32_mm(half %x) {
; RV32-LABEL: stest_f16i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixsfdi
; RV32-NEXT:    lui a2, 524288
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    beqz a1, .LBB33_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    srli a4, a1, 31
; RV32-NEXT:    j .LBB33_3
; RV32-NEXT:  .LBB33_2:
; RV32-NEXT:    sltu a4, a0, a3
; RV32-NEXT:  .LBB33_3: # %entry
; RV32-NEXT:    neg a5, a4
; RV32-NEXT:    and a1, a5, a1
; RV32-NEXT:    bnez a4, .LBB33_5
; RV32-NEXT:  # %bb.4: # %entry
; RV32-NEXT:    mv a0, a3
; RV32-NEXT:  .LBB33_5: # %entry
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beq a1, a3, .LBB33_7
; RV32-NEXT:  # %bb.6: # %entry
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    xori a1, a1, 1
; RV32-NEXT:    beqz a1, .LBB33_8
; RV32-NEXT:    j .LBB33_9
; RV32-NEXT:  .LBB33_7:
; RV32-NEXT:    sltu a1, a2, a0
; RV32-NEXT:    bnez a1, .LBB33_9
; RV32-NEXT:  .LBB33_8: # %entry
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:  .LBB33_9: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 524288
; RV64-NEXT:    addiw a2, a1, -1
; RV64-NEXT:    blt a0, a2, .LBB33_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:  .LBB33_2: # %entry
; RV64-NEXT:    blt a1, a0, .LBB33_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    lui a0, 524288
; RV64-NEXT:  .LBB33_4: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 2147483647)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 -2147483648)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

define i32 @utesth_f16i32_mm(half %x) {
; RV32-LABEL: utesth_f16i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixunssfdi
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utesth_f16i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    bltu a0, a1, .LBB34_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB34_2: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i64
  %spec.store.select = call i64 @llvm.umin.i64(i64 %conv, i64 4294967295)
  %conv6 = trunc i64 %spec.store.select to i32
  ret i32 %conv6
}

define i32 @ustest_f16i32_mm(half %x) {
; RV32-LABEL: ustest_f16i32_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    call __fixsfdi
; RV32-NEXT:    bnez a1, .LBB35_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    li a2, 1
; RV32-NEXT:    j .LBB35_3
; RV32-NEXT:  .LBB35_2:
; RV32-NEXT:    slti a2, a1, 1
; RV32-NEXT:  .LBB35_3: # %entry
; RV32-NEXT:    addi a3, a2, -1
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    or a0, a3, a0
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i32_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    li a1, -1
; RV64-NEXT:    srli a1, a1, 32
; RV64-NEXT:    blt a0, a1, .LBB35_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB35_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i64
  %spec.store.select = call i64 @llvm.smin.i64(i64 %conv, i64 4294967295)
  %spec.store.select7 = call i64 @llvm.smax.i64(i64 %spec.store.select, i64 0)
  %conv6 = trunc i64 %spec.store.select7 to i32
  ret i32 %conv6
}

; i16 saturate

define i16 @stest_f64i16_mm(double %x) {
; RV32IF-LABEL: stest_f64i16_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfsi
; RV32IF-NEXT:    lui a1, 8
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    blt a0, a1, .LBB36_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB36_2: # %entry
; RV32IF-NEXT:    lui a1, 1048568
; RV32IF-NEXT:    blt a1, a0, .LBB36_4
; RV32IF-NEXT:  # %bb.3: # %entry
; RV32IF-NEXT:    lui a0, 1048568
; RV32IF-NEXT:  .LBB36_4: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i16_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfsi
; RV64IF-NEXT:    lui a1, 8
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    blt a0, a1, .LBB36_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB36_2: # %entry
; RV64IF-NEXT:    lui a1, 1048568
; RV64IF-NEXT:    blt a1, a0, .LBB36_4
; RV64IF-NEXT:  # %bb.3: # %entry
; RV64IF-NEXT:    lui a0, 1048568
; RV64IF-NEXT:  .LBB36_4: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i16_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 8
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    bge a0, a1, .LBB36_3
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    lui a1, 1048568
; RV32IFD-NEXT:    bge a1, a0, .LBB36_4
; RV32IFD-NEXT:  .LBB36_2: # %entry
; RV32IFD-NEXT:    ret
; RV32IFD-NEXT:  .LBB36_3: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:    lui a1, 1048568
; RV32IFD-NEXT:    blt a1, a0, .LBB36_2
; RV32IFD-NEXT:  .LBB36_4: # %entry
; RV32IFD-NEXT:    lui a0, 1048568
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i16_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 8
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    bge a0, a1, .LBB36_3
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    lui a1, 1048568
; RV64IFD-NEXT:    bge a1, a0, .LBB36_4
; RV64IFD-NEXT:  .LBB36_2: # %entry
; RV64IFD-NEXT:    ret
; RV64IFD-NEXT:  .LBB36_3: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:    lui a1, 1048568
; RV64IFD-NEXT:    blt a1, a0, .LBB36_2
; RV64IFD-NEXT:  .LBB36_4: # %entry
; RV64IFD-NEXT:    lui a0, 1048568
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 32767)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 -32768)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utest_f64i16_mm(double %x) {
; RV32IF-LABEL: utest_f64i16_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixunsdfsi
; RV32IF-NEXT:    lui a1, 16
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    bltu a0, a1, .LBB37_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB37_2: # %entry
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: utest_f64i16_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixunsdfsi
; RV64IF-NEXT:    lui a1, 16
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    bltu a0, a1, .LBB37_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB37_2: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i16_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 16
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    bltu a0, a1, .LBB37_2
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:  .LBB37_2: # %entry
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: utest_f64i16_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.wu.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 16
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    bltu a0, a1, .LBB37_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB37_2: # %entry
; RV64IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i32
  %spec.store.select = call i32 @llvm.umin.i32(i32 %conv, i32 65535)
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f64i16_mm(double %x) {
; RV32IF-LABEL: ustest_f64i16_mm:
; RV32IF:       # %bb.0: # %entry
; RV32IF-NEXT:    addi sp, sp, -16
; RV32IF-NEXT:    .cfi_def_cfa_offset 16
; RV32IF-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    call __fixdfsi
; RV32IF-NEXT:    lui a1, 16
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    blt a0, a1, .LBB38_2
; RV32IF-NEXT:  # %bb.1: # %entry
; RV32IF-NEXT:    mv a0, a1
; RV32IF-NEXT:  .LBB38_2: # %entry
; RV32IF-NEXT:    sgtz a1, a0
; RV32IF-NEXT:    neg a1, a1
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    addi sp, sp, 16
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: ustest_f64i16_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfsi
; RV64IF-NEXT:    lui a1, 16
; RV64IF-NEXT:    addi a1, a1, -1
; RV64IF-NEXT:    blt a0, a1, .LBB38_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB38_2: # %entry
; RV64IF-NEXT:    sgtz a1, a0
; RV64IF-NEXT:    neg a1, a1
; RV64IF-NEXT:    and a0, a1, a0
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i16_mm:
; RV32IFD:       # %bb.0: # %entry
; RV32IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV32IFD-NEXT:    lui a1, 16
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    blt a0, a1, .LBB38_2
; RV32IFD-NEXT:  # %bb.1: # %entry
; RV32IFD-NEXT:    mv a0, a1
; RV32IFD-NEXT:  .LBB38_2: # %entry
; RV32IFD-NEXT:    sgtz a1, a0
; RV32IFD-NEXT:    neg a1, a1
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: ustest_f64i16_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.w.d a0, fa0, rtz
; RV64IFD-NEXT:    lui a1, 16
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    blt a0, a1, .LBB38_2
; RV64IFD-NEXT:  # %bb.1: # %entry
; RV64IFD-NEXT:    mv a0, a1
; RV64IFD-NEXT:  .LBB38_2: # %entry
; RV64IFD-NEXT:    sgtz a1, a0
; RV64IFD-NEXT:    neg a1, a1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 65535)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 0)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @stest_f32i16_mm(float %x) {
; RV32-LABEL: stest_f32i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 8
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bge a0, a1, .LBB39_3
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    bge a1, a0, .LBB39_4
; RV32-NEXT:  .LBB39_2: # %entry
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB39_3: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    blt a1, a0, .LBB39_2
; RV32-NEXT:  .LBB39_4: # %entry
; RV32-NEXT:    lui a0, 1048568
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 8
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bge a0, a1, .LBB39_3
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    bge a1, a0, .LBB39_4
; RV64-NEXT:  .LBB39_2: # %entry
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB39_3: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    blt a1, a0, .LBB39_2
; RV64-NEXT:  .LBB39_4: # %entry
; RV64-NEXT:    lui a0, 1048568
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 32767)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 -32768)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utest_f32i16_mm(float %x) {
; RV32-LABEL: utest_f32i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bltu a0, a1, .LBB40_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB40_2: # %entry
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bltu a0, a1, .LBB40_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB40_2: # %entry
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i32
  %spec.store.select = call i32 @llvm.umin.i32(i32 %conv, i32 65535)
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f32i16_mm(float %x) {
; RV32-LABEL: ustest_f32i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB41_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB41_2: # %entry
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.w.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB41_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB41_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 65535)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 0)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @stest_f16i16_mm(half %x) {
; RV32-LABEL: stest_f16i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 8
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB42_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB42_2: # %entry
; RV32-NEXT:    lui a1, 1048568
; RV32-NEXT:    blt a1, a0, .LBB42_4
; RV32-NEXT:  # %bb.3: # %entry
; RV32-NEXT:    lui a0, 1048568
; RV32-NEXT:  .LBB42_4: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 8
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB42_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB42_2: # %entry
; RV64-NEXT:    lui a1, 1048568
; RV64-NEXT:    blt a1, a0, .LBB42_4
; RV64-NEXT:  # %bb.3: # %entry
; RV64-NEXT:    lui a0, 1048568
; RV64-NEXT:  .LBB42_4: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 32767)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 -32768)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

define i16 @utesth_f16i16_mm(half %x) {
; RV32-LABEL: utesth_f16i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    bltu a0, a1, .LBB43_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB43_2: # %entry
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utesth_f16i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.lu.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    bltu a0, a1, .LBB43_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB43_2: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i32
  %spec.store.select = call i32 @llvm.umin.i32(i32 %conv, i32 65535)
  %conv6 = trunc i32 %spec.store.select to i16
  ret i16 %conv6
}

define i16 @ustest_f16i16_mm(half %x) {
; RV32-LABEL: ustest_f16i16_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    lui a1, 16
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    blt a0, a1, .LBB44_2
; RV32-NEXT:  # %bb.1: # %entry
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:  .LBB44_2: # %entry
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i16_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    lui a1, 16
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    blt a0, a1, .LBB44_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB44_2: # %entry
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 %conv, i32 65535)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 0)
  %conv6 = trunc i32 %spec.store.select7 to i16
  ret i16 %conv6
}

; i64 saturate

define i64 @stest_f64i64_mm(double %x) {
; RV32IF-LABEL: stest_f64i64_mm:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB45_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    li a2, 0
; RV32IF-NEXT:    j .LBB45_7
; RV32IF-NEXT:  .LBB45_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB45_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a2, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a2, a2, a0
; RV32IF-NEXT:    mv a1, s2
; RV32IF-NEXT:    mv a0, s2
; RV32IF-NEXT:    j .LBB45_7
; RV32IF-NEXT:  .LBB45_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB45_6
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s9, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s10, a0, s7
; RV32IF-NEXT:    sltu a0, s10, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s9, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv a2, a1
; RV32IF-NEXT:    mv a1, s10
; RV32IF-NEXT:    add a3, s5, a0
; RV32IF-NEXT:    sltu a4, s7, s1
; RV32IF-NEXT:    sltu a5, s11, s9
; RV32IF-NEXT:    add a2, s6, a2
; RV32IF-NEXT:    add a0, s7, a3
; RV32IF-NEXT:    add a5, s8, a5
; RV32IF-NEXT:    sltu a3, a3, s5
; RV32IF-NEXT:    sltu a6, a0, s7
; RV32IF-NEXT:    add a4, a5, a4
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:    add a2, a4, a2
; RV32IF-NEXT:    add a2, a2, a6
; RV32IF-NEXT:    j .LBB45_7
; RV32IF-NEXT:  .LBB45_6: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s8, s5, a1
; RV32IF-NEXT:    sltu a0, s8, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s8, a0, s8
; RV32IF-NEXT:    sltu a0, s8, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv a2, a1
; RV32IF-NEXT:    mv a1, s8
; RV32IF-NEXT:    add a2, a2, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a3, s4, a0
; RV32IF-NEXT:    add a0, s4, s1
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:    add a2, a2, s7
; RV32IF-NEXT:    sltu a3, a0, s4
; RV32IF-NEXT:    add a2, a2, a3
; RV32IF-NEXT:  .LBB45_7: # %fp-to-i-cleanup
; RV32IF-NEXT:    lui a3, 524288
; RV32IF-NEXT:    addi a4, a3, -1
; RV32IF-NEXT:    beq a1, a4, .LBB45_9
; RV32IF-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32IF-NEXT:    sltu a5, a1, a4
; RV32IF-NEXT:    or a6, a0, a2
; RV32IF-NEXT:    bnez a6, .LBB45_10
; RV32IF-NEXT:    j .LBB45_11
; RV32IF-NEXT:  .LBB45_9:
; RV32IF-NEXT:    sltiu a5, s2, -1
; RV32IF-NEXT:    or a6, a0, a2
; RV32IF-NEXT:    beqz a6, .LBB45_11
; RV32IF-NEXT:  .LBB45_10: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a5, a2, 31
; RV32IF-NEXT:  .LBB45_11: # %fp-to-i-cleanup
; RV32IF-NEXT:    neg a6, a5
; RV32IF-NEXT:    addi a7, a5, -1
; RV32IF-NEXT:    bnez a5, .LBB45_13
; RV32IF-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IF-NEXT:    mv a1, a4
; RV32IF-NEXT:  .LBB45_13: # %fp-to-i-cleanup
; RV32IF-NEXT:    or a4, a7, s2
; RV32IF-NEXT:    and a2, a6, a2
; RV32IF-NEXT:    and a5, a6, a0
; RV32IF-NEXT:    beq a1, a3, .LBB45_15
; RV32IF-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32IF-NEXT:    sltu a0, a3, a1
; RV32IF-NEXT:    j .LBB45_16
; RV32IF-NEXT:  .LBB45_15:
; RV32IF-NEXT:    snez a0, a4
; RV32IF-NEXT:  .LBB45_16: # %fp-to-i-cleanup
; RV32IF-NEXT:    and a5, a5, a2
; RV32IF-NEXT:    li a3, -1
; RV32IF-NEXT:    beq a5, a3, .LBB45_18
; RV32IF-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a2, a2, 31
; RV32IF-NEXT:    xori a0, a2, 1
; RV32IF-NEXT:  .LBB45_18: # %fp-to-i-cleanup
; RV32IF-NEXT:    bnez a0, .LBB45_20
; RV32IF-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:  .LBB45_20: # %fp-to-i-cleanup
; RV32IF-NEXT:    neg a0, a0
; RV32IF-NEXT:    and a0, a0, a4
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64IF-LABEL: stest_f64i64_mm:
; RV64IF:       # %bb.0: # %entry
; RV64IF-NEXT:    addi sp, sp, -16
; RV64IF-NEXT:    .cfi_def_cfa_offset 16
; RV64IF-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64IF-NEXT:    .cfi_offset ra, -8
; RV64IF-NEXT:    call __fixdfti
; RV64IF-NEXT:    li a2, -1
; RV64IF-NEXT:    srli a3, a2, 1
; RV64IF-NEXT:    beqz a1, .LBB45_2
; RV64IF-NEXT:  # %bb.1: # %entry
; RV64IF-NEXT:    srli a4, a1, 63
; RV64IF-NEXT:    j .LBB45_3
; RV64IF-NEXT:  .LBB45_2:
; RV64IF-NEXT:    sltu a4, a0, a3
; RV64IF-NEXT:  .LBB45_3: # %entry
; RV64IF-NEXT:    neg a5, a4
; RV64IF-NEXT:    and a5, a5, a1
; RV64IF-NEXT:    bnez a4, .LBB45_5
; RV64IF-NEXT:  # %bb.4: # %entry
; RV64IF-NEXT:    mv a0, a3
; RV64IF-NEXT:  .LBB45_5: # %entry
; RV64IF-NEXT:    slli a1, a2, 63
; RV64IF-NEXT:    beq a5, a2, .LBB45_7
; RV64IF-NEXT:  # %bb.6: # %entry
; RV64IF-NEXT:    srli a5, a5, 63
; RV64IF-NEXT:    xori a2, a5, 1
; RV64IF-NEXT:    beqz a2, .LBB45_8
; RV64IF-NEXT:    j .LBB45_9
; RV64IF-NEXT:  .LBB45_7:
; RV64IF-NEXT:    sltu a2, a1, a0
; RV64IF-NEXT:    bnez a2, .LBB45_9
; RV64IF-NEXT:  .LBB45_8: # %entry
; RV64IF-NEXT:    mv a0, a1
; RV64IF-NEXT:  .LBB45_9: # %entry
; RV64IF-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64IF-NEXT:    .cfi_restore ra
; RV64IF-NEXT:    addi sp, sp, 16
; RV64IF-NEXT:    .cfi_def_cfa_offset 0
; RV64IF-NEXT:    ret
;
; RV32IFD-LABEL: stest_f64i64_mm:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB45_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    li a2, 0
; RV32IFD-NEXT:    j .LBB45_7
; RV32IFD-NEXT:  .LBB45_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB45_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a2, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a2, a2, a0
; RV32IFD-NEXT:    mv a1, s2
; RV32IFD-NEXT:    mv a0, s2
; RV32IFD-NEXT:    j .LBB45_7
; RV32IFD-NEXT:  .LBB45_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB45_6
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s9, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s10, a0, s7
; RV32IFD-NEXT:    sltu a0, s10, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s9, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv a2, a1
; RV32IFD-NEXT:    mv a1, s10
; RV32IFD-NEXT:    add a3, s5, a0
; RV32IFD-NEXT:    sltu a4, s7, s1
; RV32IFD-NEXT:    sltu a5, s11, s9
; RV32IFD-NEXT:    add a2, s6, a2
; RV32IFD-NEXT:    add a0, s7, a3
; RV32IFD-NEXT:    add a5, s8, a5
; RV32IFD-NEXT:    sltu a3, a3, s5
; RV32IFD-NEXT:    sltu a6, a0, s7
; RV32IFD-NEXT:    add a4, a5, a4
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:    add a2, a4, a2
; RV32IFD-NEXT:    add a2, a2, a6
; RV32IFD-NEXT:    j .LBB45_7
; RV32IFD-NEXT:  .LBB45_6: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s8, s5, a1
; RV32IFD-NEXT:    sltu a0, s8, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s8, a0, s8
; RV32IFD-NEXT:    sltu a0, s8, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv a2, a1
; RV32IFD-NEXT:    mv a1, s8
; RV32IFD-NEXT:    add a2, a2, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a3, s4, a0
; RV32IFD-NEXT:    add a0, s4, s1
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:    add a2, a2, s7
; RV32IFD-NEXT:    sltu a3, a0, s4
; RV32IFD-NEXT:    add a2, a2, a3
; RV32IFD-NEXT:  .LBB45_7: # %fp-to-i-cleanup
; RV32IFD-NEXT:    lui a3, 524288
; RV32IFD-NEXT:    addi a4, a3, -1
; RV32IFD-NEXT:    beq a1, a4, .LBB45_9
; RV32IFD-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32IFD-NEXT:    sltu a5, a1, a4
; RV32IFD-NEXT:    or a6, a0, a2
; RV32IFD-NEXT:    bnez a6, .LBB45_10
; RV32IFD-NEXT:    j .LBB45_11
; RV32IFD-NEXT:  .LBB45_9:
; RV32IFD-NEXT:    sltiu a5, s2, -1
; RV32IFD-NEXT:    or a6, a0, a2
; RV32IFD-NEXT:    beqz a6, .LBB45_11
; RV32IFD-NEXT:  .LBB45_10: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a5, a2, 31
; RV32IFD-NEXT:  .LBB45_11: # %fp-to-i-cleanup
; RV32IFD-NEXT:    neg a6, a5
; RV32IFD-NEXT:    addi a7, a5, -1
; RV32IFD-NEXT:    bnez a5, .LBB45_13
; RV32IFD-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32IFD-NEXT:    mv a1, a4
; RV32IFD-NEXT:  .LBB45_13: # %fp-to-i-cleanup
; RV32IFD-NEXT:    or a4, a7, s2
; RV32IFD-NEXT:    and a2, a6, a2
; RV32IFD-NEXT:    and a5, a6, a0
; RV32IFD-NEXT:    beq a1, a3, .LBB45_15
; RV32IFD-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32IFD-NEXT:    sltu a0, a3, a1
; RV32IFD-NEXT:    j .LBB45_16
; RV32IFD-NEXT:  .LBB45_15:
; RV32IFD-NEXT:    snez a0, a4
; RV32IFD-NEXT:  .LBB45_16: # %fp-to-i-cleanup
; RV32IFD-NEXT:    and a5, a5, a2
; RV32IFD-NEXT:    li a3, -1
; RV32IFD-NEXT:    beq a5, a3, .LBB45_18
; RV32IFD-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a2, a2, 31
; RV32IFD-NEXT:    xori a0, a2, 1
; RV32IFD-NEXT:  .LBB45_18: # %fp-to-i-cleanup
; RV32IFD-NEXT:    bnez a0, .LBB45_20
; RV32IFD-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:  .LBB45_20: # %fp-to-i-cleanup
; RV32IFD-NEXT:    neg a0, a0
; RV32IFD-NEXT:    and a0, a0, a4
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
;
; RV64IFD-LABEL: stest_f64i64_mm:
; RV64IFD:       # %bb.0: # %entry
; RV64IFD-NEXT:    fcvt.l.d a0, fa0, rtz
; RV64IFD-NEXT:    feq.d a1, fa0, fa0
; RV64IFD-NEXT:    seqz a1, a1
; RV64IFD-NEXT:    addi a1, a1, -1
; RV64IFD-NEXT:    and a0, a1, a0
; RV64IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 9223372036854775807)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 -9223372036854775808)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utest_f64i64_mm(double %x) {
; RV32IF-LABEL: utest_f64i64_mm:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB46_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li s9, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    j .LBB46_7
; RV32IF-NEXT:  .LBB46_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB46_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a1, a1, a0
; RV32IF-NEXT:    mv s9, s2
; RV32IF-NEXT:    mv a0, s2
; RV32IF-NEXT:    j .LBB46_7
; RV32IF-NEXT:  .LBB46_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB46_6
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s10, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s7
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s10, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a2, s5, a0
; RV32IF-NEXT:    sltu a3, s7, s1
; RV32IF-NEXT:    sltu a4, s11, s10
; RV32IF-NEXT:    add a1, s6, a1
; RV32IF-NEXT:    add a0, s7, a2
; RV32IF-NEXT:    add a4, s8, a4
; RV32IF-NEXT:    sltu a2, a2, s5
; RV32IF-NEXT:    sltu a5, a0, s7
; RV32IF-NEXT:    add a3, a4, a3
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a3, a1
; RV32IF-NEXT:    add a1, a1, a5
; RV32IF-NEXT:    j .LBB46_7
; RV32IF-NEXT:  .LBB46_6: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s9, s5, a1
; RV32IF-NEXT:    sltu a0, s9, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s9
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a2, s4, a0
; RV32IF-NEXT:    add a0, s4, s1
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    add a1, a1, s7
; RV32IF-NEXT:    sltu a2, a0, s4
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:  .LBB46_7: # %fp-to-i-cleanup
; RV32IF-NEXT:    or a2, a1, a0
; RV32IF-NEXT:    xori a0, a0, 1
; RV32IF-NEXT:    seqz a2, a2
; RV32IF-NEXT:    or a0, a0, a1
; RV32IF-NEXT:    seqz a0, a0
; RV32IF-NEXT:    addi a0, a0, -1
; RV32IF-NEXT:    and a0, a0, a2
; RV32IF-NEXT:    neg a1, a0
; RV32IF-NEXT:    and a0, a1, s2
; RV32IF-NEXT:    and a1, a1, s9
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64-LABEL: utest_f64i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixunsdfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
;
; RV32IFD-LABEL: utest_f64i64_mm:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB46_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li s9, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    j .LBB46_7
; RV32IFD-NEXT:  .LBB46_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB46_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    mv s9, s2
; RV32IFD-NEXT:    mv a0, s2
; RV32IFD-NEXT:    j .LBB46_7
; RV32IFD-NEXT:  .LBB46_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB46_6
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s10, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s7
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s10, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a2, s5, a0
; RV32IFD-NEXT:    sltu a3, s7, s1
; RV32IFD-NEXT:    sltu a4, s11, s10
; RV32IFD-NEXT:    add a1, s6, a1
; RV32IFD-NEXT:    add a0, s7, a2
; RV32IFD-NEXT:    add a4, s8, a4
; RV32IFD-NEXT:    sltu a2, a2, s5
; RV32IFD-NEXT:    sltu a5, a0, s7
; RV32IFD-NEXT:    add a3, a4, a3
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a3, a1
; RV32IFD-NEXT:    add a1, a1, a5
; RV32IFD-NEXT:    j .LBB46_7
; RV32IFD-NEXT:  .LBB46_6: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s9, s5, a1
; RV32IFD-NEXT:    sltu a0, s9, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s9
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a2, s4, a0
; RV32IFD-NEXT:    add a0, s4, s1
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:    add a1, a1, s7
; RV32IFD-NEXT:    sltu a2, a0, s4
; RV32IFD-NEXT:    add a1, a1, a2
; RV32IFD-NEXT:  .LBB46_7: # %fp-to-i-cleanup
; RV32IFD-NEXT:    or a2, a1, a0
; RV32IFD-NEXT:    xori a0, a0, 1
; RV32IFD-NEXT:    seqz a2, a2
; RV32IFD-NEXT:    or a0, a0, a1
; RV32IFD-NEXT:    seqz a0, a0
; RV32IFD-NEXT:    addi a0, a0, -1
; RV32IFD-NEXT:    and a0, a0, a2
; RV32IFD-NEXT:    neg a1, a0
; RV32IFD-NEXT:    and a0, a1, s2
; RV32IFD-NEXT:    and a1, a1, s9
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
entry:
  %conv = fptoui double %x to i128
  %spec.store.select = call i128 @llvm.umin.i128(i128 %conv, i128 18446744073709551616)
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f64i64_mm(double %x) {
; RV32IF-LABEL: ustest_f64i64_mm:
; RV32IF:       # %bb.0: # %entryfp-to-i-entry
; RV32IF-NEXT:    addi sp, sp, -128
; RV32IF-NEXT:    .cfi_def_cfa_offset 128
; RV32IF-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    .cfi_offset ra, -4
; RV32IF-NEXT:    .cfi_offset s0, -8
; RV32IF-NEXT:    .cfi_offset s1, -12
; RV32IF-NEXT:    .cfi_offset s2, -16
; RV32IF-NEXT:    .cfi_offset s3, -20
; RV32IF-NEXT:    .cfi_offset s4, -24
; RV32IF-NEXT:    .cfi_offset s5, -28
; RV32IF-NEXT:    .cfi_offset s6, -32
; RV32IF-NEXT:    .cfi_offset s7, -36
; RV32IF-NEXT:    .cfi_offset s8, -40
; RV32IF-NEXT:    .cfi_offset s9, -44
; RV32IF-NEXT:    .cfi_offset s10, -48
; RV32IF-NEXT:    .cfi_offset s11, -52
; RV32IF-NEXT:    slli a2, a1, 1
; RV32IF-NEXT:    srli a2, a2, 21
; RV32IF-NEXT:    li a3, 1023
; RV32IF-NEXT:    bgeu a2, a3, .LBB47_2
; RV32IF-NEXT:  # %bb.1:
; RV32IF-NEXT:    li s2, 0
; RV32IF-NEXT:    li s9, 0
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    li a0, 0
; RV32IF-NEXT:    j .LBB47_6
; RV32IF-NEXT:  .LBB47_2: # %fp-to-i-if-end
; RV32IF-NEXT:    addi a3, a2, -1151
; RV32IF-NEXT:    sltu a4, a3, a2
; RV32IF-NEXT:    sltiu a3, a3, -128
; RV32IF-NEXT:    or a4, a4, a3
; RV32IF-NEXT:    srli a3, a1, 31
; RV32IF-NEXT:    beqz a4, .LBB47_4
; RV32IF-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IF-NEXT:    xori a0, a3, 1
; RV32IF-NEXT:    lui a1, 524288
; RV32IF-NEXT:    neg s2, a0
; RV32IF-NEXT:    sub a0, a1, a0
; RV32IF-NEXT:    mv s9, s2
; RV32IF-NEXT:    mv a1, s2
; RV32IF-NEXT:    beqz a0, .LBB47_6
; RV32IF-NEXT:    j .LBB47_8
; RV32IF-NEXT:  .LBB47_4: # %fp-to-i-if-end9
; RV32IF-NEXT:    neg s0, a3
; RV32IF-NEXT:    slli a1, a1, 12
; RV32IF-NEXT:    lui a3, 256
; RV32IF-NEXT:    li a4, 1074
; RV32IF-NEXT:    srli a1, a1, 12
; RV32IF-NEXT:    or a1, a1, a3
; RV32IF-NEXT:    ori s8, s0, 1
; RV32IF-NEXT:    bltu a4, a2, .LBB47_7
; RV32IF-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IF-NEXT:    sw zero, 24(sp)
; RV32IF-NEXT:    sw zero, 28(sp)
; RV32IF-NEXT:    sw zero, 32(sp)
; RV32IF-NEXT:    sw zero, 36(sp)
; RV32IF-NEXT:    sw a0, 8(sp)
; RV32IF-NEXT:    sw a1, 12(sp)
; RV32IF-NEXT:    sw zero, 16(sp)
; RV32IF-NEXT:    sw zero, 20(sp)
; RV32IF-NEXT:    li a0, 1075
; RV32IF-NEXT:    addi a1, sp, 8
; RV32IF-NEXT:    sub a0, a0, a2
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    add a1, a1, a2
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a3, 8(a1)
; RV32IF-NEXT:    lw a4, 12(a1)
; RV32IF-NEXT:    lw a1, 0(a1)
; RV32IF-NEXT:    andi a5, a0, 31
; RV32IF-NEXT:    xori a5, a5, 31
; RV32IF-NEXT:    slli a6, a4, 1
; RV32IF-NEXT:    slli a7, a3, 1
; RV32IF-NEXT:    slli t0, a2, 1
; RV32IF-NEXT:    sll a6, a6, a5
; RV32IF-NEXT:    sll a7, a7, a5
; RV32IF-NEXT:    sll a5, t0, a5
; RV32IF-NEXT:    srl a3, a3, a0
; RV32IF-NEXT:    srl a2, a2, a0
; RV32IF-NEXT:    srl a1, a1, a0
; RV32IF-NEXT:    or a3, a3, a6
; RV32IF-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32IF-NEXT:    or s5, a2, a7
; RV32IF-NEXT:    or s6, a1, a5
; RV32IF-NEXT:    srl s4, a4, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s7, a1
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s7, a0, s7
; RV32IF-NEXT:    sltu a0, s7, a0
; RV32IF-NEXT:    add s10, a1, a0
; RV32IF-NEXT:    mv a0, s6
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s7
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s11, s10, a0
; RV32IF-NEXT:    mv a0, s5
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s1, a0
; RV32IF-NEXT:    mv s3, s8
; RV32IF-NEXT:    mv s8, a1
; RV32IF-NEXT:    add s7, a0, s11
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s6
; RV32IF-NEXT:    mv a3, s5
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    mv a3, s4
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a0, s5, a0
; RV32IF-NEXT:    sltu a2, s7, s1
; RV32IF-NEXT:    sltu a3, s11, s10
; RV32IF-NEXT:    add s6, s6, a1
; RV32IF-NEXT:    add a1, s7, a0
; RV32IF-NEXT:    add a3, s8, a3
; RV32IF-NEXT:    sltu a0, a0, s5
; RV32IF-NEXT:    sltu a4, a1, s7
; RV32IF-NEXT:    add a2, a3, a2
; RV32IF-NEXT:    add a0, s6, a0
; RV32IF-NEXT:    add a0, a2, a0
; RV32IF-NEXT:    add a0, a0, a4
; RV32IF-NEXT:    bnez a0, .LBB47_8
; RV32IF-NEXT:  .LBB47_6:
; RV32IF-NEXT:    seqz a2, a1
; RV32IF-NEXT:    j .LBB47_9
; RV32IF-NEXT:  .LBB47_7: # %fp-to-i-if-else
; RV32IF-NEXT:    sw a0, 56(sp)
; RV32IF-NEXT:    sw a1, 60(sp)
; RV32IF-NEXT:    sw zero, 64(sp)
; RV32IF-NEXT:    sw zero, 68(sp)
; RV32IF-NEXT:    sw zero, 40(sp)
; RV32IF-NEXT:    sw zero, 44(sp)
; RV32IF-NEXT:    sw zero, 48(sp)
; RV32IF-NEXT:    sw zero, 52(sp)
; RV32IF-NEXT:    addi a0, a2, -1075
; RV32IF-NEXT:    addi a1, sp, 56
; RV32IF-NEXT:    srli a2, a0, 3
; RV32IF-NEXT:    andi a3, a0, 31
; RV32IF-NEXT:    andi a2, a2, 12
; RV32IF-NEXT:    xori a3, a3, 31
; RV32IF-NEXT:    sub a1, a1, a2
; RV32IF-NEXT:    lw a4, 0(a1)
; RV32IF-NEXT:    lw a2, 4(a1)
; RV32IF-NEXT:    lw a5, 8(a1)
; RV32IF-NEXT:    lw a1, 12(a1)
; RV32IF-NEXT:    srli a6, a4, 1
; RV32IF-NEXT:    srli a7, a2, 1
; RV32IF-NEXT:    srli t0, a5, 1
; RV32IF-NEXT:    srl a6, a6, a3
; RV32IF-NEXT:    srl a7, a7, a3
; RV32IF-NEXT:    srl a3, t0, a3
; RV32IF-NEXT:    sll a2, a2, a0
; RV32IF-NEXT:    sll a5, a5, a0
; RV32IF-NEXT:    sll a1, a1, a0
; RV32IF-NEXT:    or s3, a2, a6
; RV32IF-NEXT:    or a2, a5, a7
; RV32IF-NEXT:    or a3, a1, a3
; RV32IF-NEXT:    sll s4, a4, a0
; RV32IF-NEXT:    mv a0, s8
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    mv s5, a1
; RV32IF-NEXT:    mv a0, s0
; RV32IF-NEXT:    mv a1, s0
; RV32IF-NEXT:    mv a2, s4
; RV32IF-NEXT:    mv a3, s3
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add a1, a1, s5
; RV32IF-NEXT:    add s1, a0, s2
; RV32IF-NEXT:    sltu a0, s1, a0
; RV32IF-NEXT:    add s7, a1, a0
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s5, a0
; RV32IF-NEXT:    mv s6, a1
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s8
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    mv s2, a0
; RV32IF-NEXT:    add s9, s5, a1
; RV32IF-NEXT:    sltu a0, s9, s5
; RV32IF-NEXT:    add s6, s6, a0
; RV32IF-NEXT:    mv a0, s4
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s9, a0, s9
; RV32IF-NEXT:    sltu a0, s9, a0
; RV32IF-NEXT:    add a0, a1, a0
; RV32IF-NEXT:    add s4, s6, a0
; RV32IF-NEXT:    sltu s5, s4, s6
; RV32IF-NEXT:    mv a0, s3
; RV32IF-NEXT:    li a1, 0
; RV32IF-NEXT:    mv a2, s0
; RV32IF-NEXT:    li a3, 0
; RV32IF-NEXT:    call __muldi3
; RV32IF-NEXT:    add s5, a1, s5
; RV32IF-NEXT:    add s4, a0, s4
; RV32IF-NEXT:    sltu a0, s4, a0
; RV32IF-NEXT:    add a1, s4, s1
; RV32IF-NEXT:    add a0, s5, a0
; RV32IF-NEXT:    add a0, a0, s7
; RV32IF-NEXT:    sltu a2, a1, s4
; RV32IF-NEXT:    add a0, a0, a2
; RV32IF-NEXT:    beqz a0, .LBB47_6
; RV32IF-NEXT:  .LBB47_8: # %fp-to-i-cleanup
; RV32IF-NEXT:    srli a2, a0, 31
; RV32IF-NEXT:  .LBB47_9: # %fp-to-i-cleanup
; RV32IF-NEXT:    xori a1, a1, 1
; RV32IF-NEXT:    or a1, a1, a0
; RV32IF-NEXT:    seqz a1, a1
; RV32IF-NEXT:    addi a1, a1, -1
; RV32IF-NEXT:    and a1, a1, a2
; RV32IF-NEXT:    neg a1, a1
; RV32IF-NEXT:    and a2, a1, s9
; RV32IF-NEXT:    and a3, a1, s2
; RV32IF-NEXT:    and a0, a1, a0
; RV32IF-NEXT:    srli a0, a0, 31
; RV32IF-NEXT:    addi a1, a0, -1
; RV32IF-NEXT:    and a0, a1, a3
; RV32IF-NEXT:    and a1, a1, a2
; RV32IF-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32IF-NEXT:    .cfi_restore ra
; RV32IF-NEXT:    .cfi_restore s0
; RV32IF-NEXT:    .cfi_restore s1
; RV32IF-NEXT:    .cfi_restore s2
; RV32IF-NEXT:    .cfi_restore s3
; RV32IF-NEXT:    .cfi_restore s4
; RV32IF-NEXT:    .cfi_restore s5
; RV32IF-NEXT:    .cfi_restore s6
; RV32IF-NEXT:    .cfi_restore s7
; RV32IF-NEXT:    .cfi_restore s8
; RV32IF-NEXT:    .cfi_restore s9
; RV32IF-NEXT:    .cfi_restore s10
; RV32IF-NEXT:    .cfi_restore s11
; RV32IF-NEXT:    addi sp, sp, 128
; RV32IF-NEXT:    .cfi_def_cfa_offset 0
; RV32IF-NEXT:    ret
;
; RV64-LABEL: ustest_f64i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixdfti
; RV64-NEXT:    mv a2, a1
; RV64-NEXT:    blez a1, .LBB47_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a2, 1
; RV64-NEXT:  .LBB47_2: # %entry
; RV64-NEXT:    slti a1, a1, 1
; RV64-NEXT:    srli a2, a2, 63
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    addi a2, a2, -1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
;
; RV32IFD-LABEL: ustest_f64i64_mm:
; RV32IFD:       # %bb.0: # %entryfp-to-i-entry
; RV32IFD-NEXT:    addi sp, sp, -144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 144
; RV32IFD-NEXT:    sw ra, 140(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s0, 136(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s1, 132(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s2, 128(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s3, 124(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s4, 120(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s5, 116(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s6, 112(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s7, 108(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s8, 104(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s9, 100(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s10, 96(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    sw s11, 92(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    .cfi_offset ra, -4
; RV32IFD-NEXT:    .cfi_offset s0, -8
; RV32IFD-NEXT:    .cfi_offset s1, -12
; RV32IFD-NEXT:    .cfi_offset s2, -16
; RV32IFD-NEXT:    .cfi_offset s3, -20
; RV32IFD-NEXT:    .cfi_offset s4, -24
; RV32IFD-NEXT:    .cfi_offset s5, -28
; RV32IFD-NEXT:    .cfi_offset s6, -32
; RV32IFD-NEXT:    .cfi_offset s7, -36
; RV32IFD-NEXT:    .cfi_offset s8, -40
; RV32IFD-NEXT:    .cfi_offset s9, -44
; RV32IFD-NEXT:    .cfi_offset s10, -48
; RV32IFD-NEXT:    .cfi_offset s11, -52
; RV32IFD-NEXT:    fsd fa0, 16(sp)
; RV32IFD-NEXT:    lw a2, 20(sp)
; RV32IFD-NEXT:    slli a0, a2, 1
; RV32IFD-NEXT:    srli a0, a0, 21
; RV32IFD-NEXT:    li a1, 1023
; RV32IFD-NEXT:    bgeu a0, a1, .LBB47_2
; RV32IFD-NEXT:  # %bb.1:
; RV32IFD-NEXT:    li s2, 0
; RV32IFD-NEXT:    li s9, 0
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    li a0, 0
; RV32IFD-NEXT:    j .LBB47_6
; RV32IFD-NEXT:  .LBB47_2: # %fp-to-i-if-end
; RV32IFD-NEXT:    addi a1, a0, -1151
; RV32IFD-NEXT:    sltu a3, a1, a0
; RV32IFD-NEXT:    sltiu a1, a1, -128
; RV32IFD-NEXT:    or a1, a3, a1
; RV32IFD-NEXT:    srli a3, a2, 31
; RV32IFD-NEXT:    beqz a1, .LBB47_4
; RV32IFD-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32IFD-NEXT:    xori a0, a3, 1
; RV32IFD-NEXT:    lui a1, 524288
; RV32IFD-NEXT:    neg s2, a0
; RV32IFD-NEXT:    sub a0, a1, a0
; RV32IFD-NEXT:    mv s9, s2
; RV32IFD-NEXT:    mv a1, s2
; RV32IFD-NEXT:    beqz a0, .LBB47_6
; RV32IFD-NEXT:    j .LBB47_8
; RV32IFD-NEXT:  .LBB47_4: # %fp-to-i-if-end9
; RV32IFD-NEXT:    lw a1, 16(sp)
; RV32IFD-NEXT:    neg s0, a3
; RV32IFD-NEXT:    slli a2, a2, 12
; RV32IFD-NEXT:    lui a3, 256
; RV32IFD-NEXT:    srli a2, a2, 12
; RV32IFD-NEXT:    or a2, a2, a3
; RV32IFD-NEXT:    li a3, 1074
; RV32IFD-NEXT:    ori s8, s0, 1
; RV32IFD-NEXT:    bltu a3, a0, .LBB47_7
; RV32IFD-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32IFD-NEXT:    sw zero, 40(sp)
; RV32IFD-NEXT:    sw zero, 44(sp)
; RV32IFD-NEXT:    sw zero, 48(sp)
; RV32IFD-NEXT:    sw zero, 52(sp)
; RV32IFD-NEXT:    sw a1, 24(sp)
; RV32IFD-NEXT:    sw a2, 28(sp)
; RV32IFD-NEXT:    sw zero, 32(sp)
; RV32IFD-NEXT:    sw zero, 36(sp)
; RV32IFD-NEXT:    li a1, 1075
; RV32IFD-NEXT:    addi a2, sp, 24
; RV32IFD-NEXT:    sub a1, a1, a0
; RV32IFD-NEXT:    srli a0, a1, 3
; RV32IFD-NEXT:    andi a0, a0, 12
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    lw a2, 4(a0)
; RV32IFD-NEXT:    lw a3, 8(a0)
; RV32IFD-NEXT:    lw a4, 12(a0)
; RV32IFD-NEXT:    lw a0, 0(a0)
; RV32IFD-NEXT:    andi a5, a1, 31
; RV32IFD-NEXT:    xori a5, a5, 31
; RV32IFD-NEXT:    slli a6, a4, 1
; RV32IFD-NEXT:    slli a7, a3, 1
; RV32IFD-NEXT:    slli t0, a2, 1
; RV32IFD-NEXT:    sll a6, a6, a5
; RV32IFD-NEXT:    sll a7, a7, a5
; RV32IFD-NEXT:    sll a5, t0, a5
; RV32IFD-NEXT:    srl a3, a3, a1
; RV32IFD-NEXT:    srl a2, a2, a1
; RV32IFD-NEXT:    srl a0, a0, a1
; RV32IFD-NEXT:    or a3, a3, a6
; RV32IFD-NEXT:    sw a3, 12(sp) # 4-byte Folded Spill
; RV32IFD-NEXT:    or s5, a2, a7
; RV32IFD-NEXT:    or s6, a0, a5
; RV32IFD-NEXT:    srl s4, a4, a1
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s7, a1
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s7, a0, s7
; RV32IFD-NEXT:    sltu a0, s7, a0
; RV32IFD-NEXT:    add s10, a1, a0
; RV32IFD-NEXT:    mv a0, s6
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s7
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s11, s10, a0
; RV32IFD-NEXT:    mv a0, s5
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s1, a0
; RV32IFD-NEXT:    mv s3, s8
; RV32IFD-NEXT:    mv s8, a1
; RV32IFD-NEXT:    add s7, a0, s11
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s6
; RV32IFD-NEXT:    mv a3, s5
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    lw a2, 12(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    mv a3, s4
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a0, s5, a0
; RV32IFD-NEXT:    sltu a2, s7, s1
; RV32IFD-NEXT:    sltu a3, s11, s10
; RV32IFD-NEXT:    add s6, s6, a1
; RV32IFD-NEXT:    add a1, s7, a0
; RV32IFD-NEXT:    add a3, s8, a3
; RV32IFD-NEXT:    sltu a0, a0, s5
; RV32IFD-NEXT:    sltu a4, a1, s7
; RV32IFD-NEXT:    add a2, a3, a2
; RV32IFD-NEXT:    add a0, s6, a0
; RV32IFD-NEXT:    add a0, a2, a0
; RV32IFD-NEXT:    add a0, a0, a4
; RV32IFD-NEXT:    bnez a0, .LBB47_8
; RV32IFD-NEXT:  .LBB47_6:
; RV32IFD-NEXT:    seqz a2, a1
; RV32IFD-NEXT:    j .LBB47_9
; RV32IFD-NEXT:  .LBB47_7: # %fp-to-i-if-else
; RV32IFD-NEXT:    sw a1, 72(sp)
; RV32IFD-NEXT:    sw a2, 76(sp)
; RV32IFD-NEXT:    sw zero, 80(sp)
; RV32IFD-NEXT:    sw zero, 84(sp)
; RV32IFD-NEXT:    sw zero, 56(sp)
; RV32IFD-NEXT:    sw zero, 60(sp)
; RV32IFD-NEXT:    sw zero, 64(sp)
; RV32IFD-NEXT:    sw zero, 68(sp)
; RV32IFD-NEXT:    addi a0, a0, -1075
; RV32IFD-NEXT:    addi a1, sp, 72
; RV32IFD-NEXT:    srli a2, a0, 3
; RV32IFD-NEXT:    andi a3, a0, 31
; RV32IFD-NEXT:    andi a2, a2, 12
; RV32IFD-NEXT:    xori a3, a3, 31
; RV32IFD-NEXT:    sub a1, a1, a2
; RV32IFD-NEXT:    lw a4, 0(a1)
; RV32IFD-NEXT:    lw a2, 4(a1)
; RV32IFD-NEXT:    lw a5, 8(a1)
; RV32IFD-NEXT:    lw a1, 12(a1)
; RV32IFD-NEXT:    srli a6, a4, 1
; RV32IFD-NEXT:    srli a7, a2, 1
; RV32IFD-NEXT:    srli t0, a5, 1
; RV32IFD-NEXT:    srl a6, a6, a3
; RV32IFD-NEXT:    srl a7, a7, a3
; RV32IFD-NEXT:    srl a3, t0, a3
; RV32IFD-NEXT:    sll a2, a2, a0
; RV32IFD-NEXT:    sll a5, a5, a0
; RV32IFD-NEXT:    sll a1, a1, a0
; RV32IFD-NEXT:    or s3, a2, a6
; RV32IFD-NEXT:    or a2, a5, a7
; RV32IFD-NEXT:    or a3, a1, a3
; RV32IFD-NEXT:    sll s4, a4, a0
; RV32IFD-NEXT:    mv a0, s8
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    mv s5, a1
; RV32IFD-NEXT:    mv a0, s0
; RV32IFD-NEXT:    mv a1, s0
; RV32IFD-NEXT:    mv a2, s4
; RV32IFD-NEXT:    mv a3, s3
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add a1, a1, s5
; RV32IFD-NEXT:    add s1, a0, s2
; RV32IFD-NEXT:    sltu a0, s1, a0
; RV32IFD-NEXT:    add s7, a1, a0
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s5, a0
; RV32IFD-NEXT:    mv s6, a1
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s8
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    mv s2, a0
; RV32IFD-NEXT:    add s9, s5, a1
; RV32IFD-NEXT:    sltu a0, s9, s5
; RV32IFD-NEXT:    add s6, s6, a0
; RV32IFD-NEXT:    mv a0, s4
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s9, a0, s9
; RV32IFD-NEXT:    sltu a0, s9, a0
; RV32IFD-NEXT:    add a0, a1, a0
; RV32IFD-NEXT:    add s4, s6, a0
; RV32IFD-NEXT:    sltu s5, s4, s6
; RV32IFD-NEXT:    mv a0, s3
; RV32IFD-NEXT:    li a1, 0
; RV32IFD-NEXT:    mv a2, s0
; RV32IFD-NEXT:    li a3, 0
; RV32IFD-NEXT:    call __muldi3
; RV32IFD-NEXT:    add s5, a1, s5
; RV32IFD-NEXT:    add s4, a0, s4
; RV32IFD-NEXT:    sltu a0, s4, a0
; RV32IFD-NEXT:    add a1, s4, s1
; RV32IFD-NEXT:    add a0, s5, a0
; RV32IFD-NEXT:    add a0, a0, s7
; RV32IFD-NEXT:    sltu a2, a1, s4
; RV32IFD-NEXT:    add a0, a0, a2
; RV32IFD-NEXT:    beqz a0, .LBB47_6
; RV32IFD-NEXT:  .LBB47_8: # %fp-to-i-cleanup
; RV32IFD-NEXT:    srli a2, a0, 31
; RV32IFD-NEXT:  .LBB47_9: # %fp-to-i-cleanup
; RV32IFD-NEXT:    xori a1, a1, 1
; RV32IFD-NEXT:    or a1, a1, a0
; RV32IFD-NEXT:    seqz a1, a1
; RV32IFD-NEXT:    addi a1, a1, -1
; RV32IFD-NEXT:    and a1, a1, a2
; RV32IFD-NEXT:    neg a1, a1
; RV32IFD-NEXT:    and a2, a1, s9
; RV32IFD-NEXT:    and a3, a1, s2
; RV32IFD-NEXT:    and a0, a1, a0
; RV32IFD-NEXT:    srli a0, a0, 31
; RV32IFD-NEXT:    addi a1, a0, -1
; RV32IFD-NEXT:    and a0, a1, a3
; RV32IFD-NEXT:    and a1, a1, a2
; RV32IFD-NEXT:    lw ra, 140(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s0, 136(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s1, 132(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s2, 128(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s3, 124(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s4, 120(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s5, 116(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s6, 112(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s7, 108(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s8, 104(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s9, 100(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s10, 96(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    lw s11, 92(sp) # 4-byte Folded Reload
; RV32IFD-NEXT:    .cfi_restore ra
; RV32IFD-NEXT:    .cfi_restore s0
; RV32IFD-NEXT:    .cfi_restore s1
; RV32IFD-NEXT:    .cfi_restore s2
; RV32IFD-NEXT:    .cfi_restore s3
; RV32IFD-NEXT:    .cfi_restore s4
; RV32IFD-NEXT:    .cfi_restore s5
; RV32IFD-NEXT:    .cfi_restore s6
; RV32IFD-NEXT:    .cfi_restore s7
; RV32IFD-NEXT:    .cfi_restore s8
; RV32IFD-NEXT:    .cfi_restore s9
; RV32IFD-NEXT:    .cfi_restore s10
; RV32IFD-NEXT:    .cfi_restore s11
; RV32IFD-NEXT:    addi sp, sp, 144
; RV32IFD-NEXT:    .cfi_def_cfa_offset 0
; RV32IFD-NEXT:    ret
entry:
  %conv = fptosi double %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 18446744073709551616)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 0)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @stest_f32i64_mm(float %x) {
; RV32-LABEL: stest_f32i64_mm:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB48_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a2, 0
; RV32-NEXT:    j .LBB48_7
; RV32-NEXT:  .LBB48_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB48_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a2, a0, a2
; RV32-NEXT:    mv a1, s2
; RV32-NEXT:    mv a0, s2
; RV32-NEXT:    j .LBB48_7
; RV32-NEXT:  .LBB48_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB48_6
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s9, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s10, a0, s7
; RV32-NEXT:    sltu a0, s10, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s9, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv a2, a1
; RV32-NEXT:    mv a1, s10
; RV32-NEXT:    add a3, s5, a0
; RV32-NEXT:    sltu a4, s7, s1
; RV32-NEXT:    sltu a5, s11, s9
; RV32-NEXT:    add a2, s6, a2
; RV32-NEXT:    add a0, s7, a3
; RV32-NEXT:    add a5, s8, a5
; RV32-NEXT:    sltu a3, a3, s5
; RV32-NEXT:    sltu a6, a0, s7
; RV32-NEXT:    add a4, a5, a4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    add a2, a4, a2
; RV32-NEXT:    add a2, a2, a6
; RV32-NEXT:    j .LBB48_7
; RV32-NEXT:  .LBB48_6: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s8, s5, a1
; RV32-NEXT:    sltu a0, s8, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s8, a0, s8
; RV32-NEXT:    sltu a0, s8, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv a2, a1
; RV32-NEXT:    mv a1, s8
; RV32-NEXT:    add a2, a2, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a3, s4, a0
; RV32-NEXT:    add a0, s4, s1
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:    add a2, a2, s7
; RV32-NEXT:    sltu a3, a0, s4
; RV32-NEXT:    add a2, a2, a3
; RV32-NEXT:  .LBB48_7: # %fp-to-i-cleanup
; RV32-NEXT:    lui a3, 524288
; RV32-NEXT:    addi a4, a3, -1
; RV32-NEXT:    beq a1, a4, .LBB48_9
; RV32-NEXT:  # %bb.8: # %fp-to-i-cleanup
; RV32-NEXT:    sltu a5, a1, a4
; RV32-NEXT:    or a6, a0, a2
; RV32-NEXT:    bnez a6, .LBB48_10
; RV32-NEXT:    j .LBB48_11
; RV32-NEXT:  .LBB48_9:
; RV32-NEXT:    sltiu a5, s2, -1
; RV32-NEXT:    or a6, a0, a2
; RV32-NEXT:    beqz a6, .LBB48_11
; RV32-NEXT:  .LBB48_10: # %fp-to-i-cleanup
; RV32-NEXT:    srli a5, a2, 31
; RV32-NEXT:  .LBB48_11: # %fp-to-i-cleanup
; RV32-NEXT:    neg a6, a5
; RV32-NEXT:    addi a7, a5, -1
; RV32-NEXT:    bnez a5, .LBB48_13
; RV32-NEXT:  # %bb.12: # %fp-to-i-cleanup
; RV32-NEXT:    mv a1, a4
; RV32-NEXT:  .LBB48_13: # %fp-to-i-cleanup
; RV32-NEXT:    or a4, a7, s2
; RV32-NEXT:    and a2, a6, a2
; RV32-NEXT:    and a5, a6, a0
; RV32-NEXT:    beq a1, a3, .LBB48_15
; RV32-NEXT:  # %bb.14: # %fp-to-i-cleanup
; RV32-NEXT:    sltu a0, a3, a1
; RV32-NEXT:    j .LBB48_16
; RV32-NEXT:  .LBB48_15:
; RV32-NEXT:    snez a0, a4
; RV32-NEXT:  .LBB48_16: # %fp-to-i-cleanup
; RV32-NEXT:    and a5, a5, a2
; RV32-NEXT:    li a3, -1
; RV32-NEXT:    beq a5, a3, .LBB48_18
; RV32-NEXT:  # %bb.17: # %fp-to-i-cleanup
; RV32-NEXT:    srli a2, a2, 31
; RV32-NEXT:    xori a0, a2, 1
; RV32-NEXT:  .LBB48_18: # %fp-to-i-cleanup
; RV32-NEXT:    bnez a0, .LBB48_20
; RV32-NEXT:  # %bb.19: # %fp-to-i-cleanup
; RV32-NEXT:    lui a1, 524288
; RV32-NEXT:  .LBB48_20: # %fp-to-i-cleanup
; RV32-NEXT:    neg a0, a0
; RV32-NEXT:    and a0, a0, a4
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f32i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    feq.s a1, fa0, fa0
; RV64-NEXT:    seqz a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 9223372036854775807)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 -9223372036854775808)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utest_f32i64_mm(float %x) {
; RV32-LABEL: utest_f32i64_mm:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB49_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    j .LBB49_7
; RV32-NEXT:  .LBB49_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB49_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a1, a0, a2
; RV32-NEXT:    mv s9, s2
; RV32-NEXT:    mv a0, s2
; RV32-NEXT:    j .LBB49_7
; RV32-NEXT:  .LBB49_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB49_6
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s10, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s7
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s10, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a2, s5, a0
; RV32-NEXT:    sltu a3, s7, s1
; RV32-NEXT:    sltu a4, s11, s10
; RV32-NEXT:    add a1, s6, a1
; RV32-NEXT:    add a0, s7, a2
; RV32-NEXT:    add a4, s8, a4
; RV32-NEXT:    sltu a2, a2, s5
; RV32-NEXT:    sltu a5, a0, s7
; RV32-NEXT:    add a3, a4, a3
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a3, a1
; RV32-NEXT:    add a1, a1, a5
; RV32-NEXT:    j .LBB49_7
; RV32-NEXT:  .LBB49_6: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s9, s5, a1
; RV32-NEXT:    sltu a0, s9, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s9
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a2, s4, a0
; RV32-NEXT:    add a0, s4, s1
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:    add a1, a1, s7
; RV32-NEXT:    sltu a2, a0, s4
; RV32-NEXT:    add a1, a1, a2
; RV32-NEXT:  .LBB49_7: # %fp-to-i-cleanup
; RV32-NEXT:    or a2, a1, a0
; RV32-NEXT:    xori a0, a0, 1
; RV32-NEXT:    seqz a2, a2
; RV32-NEXT:    or a0, a0, a1
; RV32-NEXT:    seqz a0, a0
; RV32-NEXT:    addi a0, a0, -1
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    neg a1, a0
; RV32-NEXT:    and a0, a1, s2
; RV32-NEXT:    and a1, a1, s9
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utest_f32i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixunssfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui float %x to i128
  %spec.store.select = call i128 @llvm.umin.i128(i128 %conv, i128 18446744073709551616)
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f32i64_mm(float %x) {
; RV32-LABEL: ustest_f32i64_mm:
; RV32:       # %bb.0: # %entryfp-to-i-entry
; RV32-NEXT:    addi sp, sp, -128
; RV32-NEXT:    .cfi_def_cfa_offset 128
; RV32-NEXT:    sw ra, 124(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s0, 120(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s1, 116(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s2, 112(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s3, 108(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s4, 104(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s5, 100(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s6, 96(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s7, 92(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s8, 88(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s9, 84(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s10, 80(sp) # 4-byte Folded Spill
; RV32-NEXT:    sw s11, 76(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    .cfi_offset s0, -8
; RV32-NEXT:    .cfi_offset s1, -12
; RV32-NEXT:    .cfi_offset s2, -16
; RV32-NEXT:    .cfi_offset s3, -20
; RV32-NEXT:    .cfi_offset s4, -24
; RV32-NEXT:    .cfi_offset s5, -28
; RV32-NEXT:    .cfi_offset s6, -32
; RV32-NEXT:    .cfi_offset s7, -36
; RV32-NEXT:    .cfi_offset s8, -40
; RV32-NEXT:    .cfi_offset s9, -44
; RV32-NEXT:    .cfi_offset s10, -48
; RV32-NEXT:    .cfi_offset s11, -52
; RV32-NEXT:    fmv.x.w a1, fa0
; RV32-NEXT:    slli a0, a1, 1
; RV32-NEXT:    srli a0, a0, 24
; RV32-NEXT:    li a2, 127
; RV32-NEXT:    bgeu a0, a2, .LBB50_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    li s2, 0
; RV32-NEXT:    li s9, 0
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    j .LBB50_6
; RV32-NEXT:  .LBB50_2: # %fp-to-i-if-end
; RV32-NEXT:    addi a2, a0, -255
; RV32-NEXT:    sltu a3, a2, a0
; RV32-NEXT:    sltiu a2, a2, -128
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    beqz a2, .LBB50_4
; RV32-NEXT:  # %bb.3: # %fp-to-i-if-then5
; RV32-NEXT:    srli a1, a1, 31
; RV32-NEXT:    lui a0, 524288
; RV32-NEXT:    xori a2, a1, 1
; RV32-NEXT:    addi s2, a1, -1
; RV32-NEXT:    sub a0, a0, a2
; RV32-NEXT:    mv s9, s2
; RV32-NEXT:    mv a1, s2
; RV32-NEXT:    beqz a0, .LBB50_6
; RV32-NEXT:    j .LBB50_8
; RV32-NEXT:  .LBB50_4: # %fp-to-i-if-end9
; RV32-NEXT:    srai s0, a1, 31
; RV32-NEXT:    slli a1, a1, 9
; RV32-NEXT:    lui a2, 2048
; RV32-NEXT:    li a3, 149
; RV32-NEXT:    srli a1, a1, 9
; RV32-NEXT:    or a1, a1, a2
; RV32-NEXT:    ori s8, s0, 1
; RV32-NEXT:    bltu a3, a0, .LBB50_7
; RV32-NEXT:  # %bb.5: # %fp-to-i-if-then12
; RV32-NEXT:    sw zero, 24(sp)
; RV32-NEXT:    sw zero, 28(sp)
; RV32-NEXT:    sw zero, 32(sp)
; RV32-NEXT:    sw zero, 36(sp)
; RV32-NEXT:    sw a1, 8(sp)
; RV32-NEXT:    sw zero, 12(sp)
; RV32-NEXT:    sw zero, 16(sp)
; RV32-NEXT:    sw zero, 20(sp)
; RV32-NEXT:    li a1, 150
; RV32-NEXT:    addi a2, sp, 8
; RV32-NEXT:    sub a1, a1, a0
; RV32-NEXT:    srli a0, a1, 3
; RV32-NEXT:    andi a0, a0, 12
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    lw a2, 4(a0)
; RV32-NEXT:    lw a3, 8(a0)
; RV32-NEXT:    lw a4, 12(a0)
; RV32-NEXT:    lw a0, 0(a0)
; RV32-NEXT:    andi a5, a1, 31
; RV32-NEXT:    xori a5, a5, 31
; RV32-NEXT:    slli a6, a4, 1
; RV32-NEXT:    slli a7, a3, 1
; RV32-NEXT:    slli t0, a2, 1
; RV32-NEXT:    sll a6, a6, a5
; RV32-NEXT:    sll a7, a7, a5
; RV32-NEXT:    sll a5, t0, a5
; RV32-NEXT:    srl a3, a3, a1
; RV32-NEXT:    srl a2, a2, a1
; RV32-NEXT:    srl a0, a0, a1
; RV32-NEXT:    or a3, a3, a6
; RV32-NEXT:    sw a3, 4(sp) # 4-byte Folded Spill
; RV32-NEXT:    or s5, a2, a7
; RV32-NEXT:    or s6, a0, a5
; RV32-NEXT:    srl s4, a4, a1
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s7, a1
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s7, a0, s7
; RV32-NEXT:    sltu a0, s7, a0
; RV32-NEXT:    add s10, a1, a0
; RV32-NEXT:    mv a0, s6
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s7
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s11, s10, a0
; RV32-NEXT:    mv a0, s5
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s1, a0
; RV32-NEXT:    mv s3, s8
; RV32-NEXT:    mv s8, a1
; RV32-NEXT:    add s7, a0, s11
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s6
; RV32-NEXT:    mv a3, s5
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    lw a2, 4(sp) # 4-byte Folded Reload
; RV32-NEXT:    mv a3, s4
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a0, s5, a0
; RV32-NEXT:    sltu a2, s7, s1
; RV32-NEXT:    sltu a3, s11, s10
; RV32-NEXT:    add s6, s6, a1
; RV32-NEXT:    add a1, s7, a0
; RV32-NEXT:    add a3, s8, a3
; RV32-NEXT:    sltu a0, a0, s5
; RV32-NEXT:    sltu a4, a1, s7
; RV32-NEXT:    add a2, a3, a2
; RV32-NEXT:    add a0, s6, a0
; RV32-NEXT:    add a0, a2, a0
; RV32-NEXT:    add a0, a0, a4
; RV32-NEXT:    bnez a0, .LBB50_8
; RV32-NEXT:  .LBB50_6:
; RV32-NEXT:    seqz a2, a1
; RV32-NEXT:    j .LBB50_9
; RV32-NEXT:  .LBB50_7: # %fp-to-i-if-else
; RV32-NEXT:    sw a1, 56(sp)
; RV32-NEXT:    sw zero, 60(sp)
; RV32-NEXT:    sw zero, 64(sp)
; RV32-NEXT:    sw zero, 68(sp)
; RV32-NEXT:    sw zero, 40(sp)
; RV32-NEXT:    sw zero, 44(sp)
; RV32-NEXT:    sw zero, 48(sp)
; RV32-NEXT:    sw zero, 52(sp)
; RV32-NEXT:    addi a0, a0, -150
; RV32-NEXT:    addi a1, sp, 56
; RV32-NEXT:    srli a2, a0, 3
; RV32-NEXT:    andi a3, a0, 31
; RV32-NEXT:    andi a2, a2, 12
; RV32-NEXT:    xori a3, a3, 31
; RV32-NEXT:    sub a1, a1, a2
; RV32-NEXT:    lw a4, 0(a1)
; RV32-NEXT:    lw a2, 4(a1)
; RV32-NEXT:    lw a5, 8(a1)
; RV32-NEXT:    lw a1, 12(a1)
; RV32-NEXT:    srli a6, a4, 1
; RV32-NEXT:    srli a7, a2, 1
; RV32-NEXT:    srli t0, a5, 1
; RV32-NEXT:    srl a6, a6, a3
; RV32-NEXT:    srl a7, a7, a3
; RV32-NEXT:    srl a3, t0, a3
; RV32-NEXT:    sll a2, a2, a0
; RV32-NEXT:    sll a5, a5, a0
; RV32-NEXT:    sll a1, a1, a0
; RV32-NEXT:    or s3, a2, a6
; RV32-NEXT:    or a2, a5, a7
; RV32-NEXT:    or a3, a1, a3
; RV32-NEXT:    sll s4, a4, a0
; RV32-NEXT:    mv a0, s8
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    mv s5, a1
; RV32-NEXT:    mv a0, s0
; RV32-NEXT:    mv a1, s0
; RV32-NEXT:    mv a2, s4
; RV32-NEXT:    mv a3, s3
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add a1, a1, s5
; RV32-NEXT:    add s1, a0, s2
; RV32-NEXT:    sltu a0, s1, a0
; RV32-NEXT:    add s7, a1, a0
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s5, a0
; RV32-NEXT:    mv s6, a1
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s8
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    mv s2, a0
; RV32-NEXT:    add s9, s5, a1
; RV32-NEXT:    sltu a0, s9, s5
; RV32-NEXT:    add s6, s6, a0
; RV32-NEXT:    mv a0, s4
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s9, a0, s9
; RV32-NEXT:    sltu a0, s9, a0
; RV32-NEXT:    add a0, a1, a0
; RV32-NEXT:    add s4, s6, a0
; RV32-NEXT:    sltu s5, s4, s6
; RV32-NEXT:    mv a0, s3
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    mv a2, s0
; RV32-NEXT:    li a3, 0
; RV32-NEXT:    call __muldi3
; RV32-NEXT:    add s5, a1, s5
; RV32-NEXT:    add s4, a0, s4
; RV32-NEXT:    sltu a0, s4, a0
; RV32-NEXT:    add a1, s4, s1
; RV32-NEXT:    add a0, s5, a0
; RV32-NEXT:    add a0, a0, s7
; RV32-NEXT:    sltu a2, a1, s4
; RV32-NEXT:    add a0, a0, a2
; RV32-NEXT:    beqz a0, .LBB50_6
; RV32-NEXT:  .LBB50_8: # %fp-to-i-cleanup
; RV32-NEXT:    srli a2, a0, 31
; RV32-NEXT:  .LBB50_9: # %fp-to-i-cleanup
; RV32-NEXT:    xori a1, a1, 1
; RV32-NEXT:    or a1, a1, a0
; RV32-NEXT:    seqz a1, a1
; RV32-NEXT:    addi a1, a1, -1
; RV32-NEXT:    and a1, a1, a2
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a2, a1, s9
; RV32-NEXT:    and a3, a1, s2
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    srli a0, a0, 31
; RV32-NEXT:    addi a1, a0, -1
; RV32-NEXT:    and a0, a1, a3
; RV32-NEXT:    and a1, a1, a2
; RV32-NEXT:    lw ra, 124(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s0, 120(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s1, 116(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s2, 112(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s3, 108(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s4, 104(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s5, 100(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s6, 96(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s7, 92(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s8, 88(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s9, 84(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s10, 80(sp) # 4-byte Folded Reload
; RV32-NEXT:    lw s11, 76(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    .cfi_restore s0
; RV32-NEXT:    .cfi_restore s1
; RV32-NEXT:    .cfi_restore s2
; RV32-NEXT:    .cfi_restore s3
; RV32-NEXT:    .cfi_restore s4
; RV32-NEXT:    .cfi_restore s5
; RV32-NEXT:    .cfi_restore s6
; RV32-NEXT:    .cfi_restore s7
; RV32-NEXT:    .cfi_restore s8
; RV32-NEXT:    .cfi_restore s9
; RV32-NEXT:    .cfi_restore s10
; RV32-NEXT:    .cfi_restore s11
; RV32-NEXT:    addi sp, sp, 128
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f32i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    mv a2, a1
; RV64-NEXT:    blez a1, .LBB50_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a2, 1
; RV64-NEXT:  .LBB50_2: # %entry
; RV64-NEXT:    slti a1, a1, 1
; RV64-NEXT:    srli a2, a2, 63
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    addi a2, a2, -1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi float %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 18446744073709551616)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 0)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @stest_f16i64_mm(half %x) {
; RV32-LABEL: stest_f16i64_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    srai a1, a0, 31
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: stest_f16i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    li a2, -1
; RV64-NEXT:    srli a3, a2, 1
; RV64-NEXT:    beqz a1, .LBB51_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    srli a4, a1, 63
; RV64-NEXT:    j .LBB51_3
; RV64-NEXT:  .LBB51_2:
; RV64-NEXT:    sltu a4, a0, a3
; RV64-NEXT:  .LBB51_3: # %entry
; RV64-NEXT:    neg a5, a4
; RV64-NEXT:    and a5, a5, a1
; RV64-NEXT:    bnez a4, .LBB51_5
; RV64-NEXT:  # %bb.4: # %entry
; RV64-NEXT:    mv a0, a3
; RV64-NEXT:  .LBB51_5: # %entry
; RV64-NEXT:    slli a1, a2, 63
; RV64-NEXT:    beq a5, a2, .LBB51_7
; RV64-NEXT:  # %bb.6: # %entry
; RV64-NEXT:    srli a5, a5, 63
; RV64-NEXT:    xori a2, a5, 1
; RV64-NEXT:    beqz a2, .LBB51_8
; RV64-NEXT:    j .LBB51_9
; RV64-NEXT:  .LBB51_7:
; RV64-NEXT:    sltu a2, a1, a0
; RV64-NEXT:    bnez a2, .LBB51_9
; RV64-NEXT:  .LBB51_8: # %entry
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:  .LBB51_9: # %entry
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 9223372036854775807)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 -9223372036854775808)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

define i64 @utesth_f16i64_mm(half %x) {
; RV32-LABEL: utesth_f16i64_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.wu.s a0, fa0, rtz
; RV32-NEXT:    li a1, 0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: utesth_f16i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixunssfti
; RV64-NEXT:    snez a1, a1
; RV64-NEXT:    addi a1, a1, -1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptoui half %x to i128
  %spec.store.select = call i128 @llvm.umin.i128(i128 %conv, i128 18446744073709551616)
  %conv6 = trunc i128 %spec.store.select to i64
  ret i64 %conv6
}

define i64 @ustest_f16i64_mm(half %x) {
; RV32-LABEL: ustest_f16i64_mm:
; RV32:       # %bb.0: # %entry
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    srai a1, a0, 31
; RV32-NEXT:    srli a2, a1, 31
; RV32-NEXT:    seqz a3, a1
; RV32-NEXT:    or a2, a3, a2
; RV32-NEXT:    ori a3, a1, 1
; RV32-NEXT:    seqz a3, a3
; RV32-NEXT:    addi a3, a3, -1
; RV32-NEXT:    and a2, a3, a2
; RV32-NEXT:    neg a2, a2
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    srli a2, a1, 31
; RV32-NEXT:    addi a2, a2, -1
; RV32-NEXT:    and a0, a2, a0
; RV32-NEXT:    and a1, a2, a1
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i64_mm:
; RV64:       # %bb.0: # %entry
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    call __fixsfti
; RV64-NEXT:    mv a2, a1
; RV64-NEXT:    blez a1, .LBB53_2
; RV64-NEXT:  # %bb.1: # %entry
; RV64-NEXT:    li a2, 1
; RV64-NEXT:  .LBB53_2: # %entry
; RV64-NEXT:    slti a1, a1, 1
; RV64-NEXT:    srli a2, a2, 63
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    addi a2, a2, -1
; RV64-NEXT:    and a0, a2, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
entry:
  %conv = fptosi half %x to i128
  %spec.store.select = call i128 @llvm.smin.i128(i128 %conv, i128 18446744073709551616)
  %spec.store.select7 = call i128 @llvm.smax.i128(i128 %spec.store.select, i128 0)
  %conv6 = trunc i128 %spec.store.select7 to i64
  ret i64 %conv6
}

; i32 non saturate

define i32 @ustest_f16i32_nsat(half %x) {
; RV32-LABEL: ustest_f16i32_nsat:
; RV32:       # %bb.0:
; RV32-NEXT:    addi sp, sp, -16
; RV32-NEXT:    .cfi_def_cfa_offset 16
; RV32-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32-NEXT:    .cfi_offset ra, -4
; RV32-NEXT:    call __extendhfsf2
; RV32-NEXT:    fcvt.w.s a0, fa0, rtz
; RV32-NEXT:    srai a1, a0, 31
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    sgtz a1, a0
; RV32-NEXT:    neg a1, a1
; RV32-NEXT:    and a0, a1, a0
; RV32-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32-NEXT:    .cfi_restore ra
; RV32-NEXT:    addi sp, sp, 16
; RV32-NEXT:    .cfi_def_cfa_offset 0
; RV32-NEXT:    ret
;
; RV64-LABEL: ustest_f16i32_nsat:
; RV64:       # %bb.0:
; RV64-NEXT:    addi sp, sp, -16
; RV64-NEXT:    .cfi_def_cfa_offset 16
; RV64-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64-NEXT:    .cfi_offset ra, -8
; RV64-NEXT:    call __extendhfsf2
; RV64-NEXT:    fcvt.l.s a0, fa0, rtz
; RV64-NEXT:    srai a1, a0, 63
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    sgtz a1, a0
; RV64-NEXT:    neg a1, a1
; RV64-NEXT:    and a0, a1, a0
; RV64-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64-NEXT:    .cfi_restore ra
; RV64-NEXT:    addi sp, sp, 16
; RV64-NEXT:    .cfi_def_cfa_offset 0
; RV64-NEXT:    ret
  %conv = fptosi half %x to i32
  %spec.store.select = call i32 @llvm.smin.i32(i32 0, i32 %conv)
  %spec.store.select7 = call i32 @llvm.smax.i32(i32 %spec.store.select, i32 0)
  ret i32 %spec.store.select7
}

