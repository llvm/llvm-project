; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=riscv32 -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV32I %s
; RUN: llc -mtriple=riscv64 -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64I %s
; RUN: llc -code-model=large -mtriple=riscv64 -verify-machineinstrs < %s \
; RUN:   | FileCheck -check-prefix=RV64I-LARGE %s

; Test call generation with cf-protection-branch enabled.
; This tests landing pad generation and NonX7 variants for indirect calls.

declare i32 @external_function(i32)

define i32 @test_call_external(i32 %a) nounwind {
; RV32I-LABEL: test_call_external:
; RV32I:       # %bb.0:
; RV32I-NEXT:    lpad 0
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    call external_function
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: test_call_external:
; RV64I:       # %bb.0:
; RV64I-NEXT:    lpad 0
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    call external_function
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64I-LARGE-LABEL: test_call_external:
; RV64I-LARGE:       # %bb.0:
; RV64I-LARGE-NEXT:    lpad 0
; RV64I-LARGE-NEXT:    addi sp, sp, -16
; RV64I-LARGE-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-LARGE-NEXT:  .Lpcrel_hi0:
; RV64I-LARGE-NEXT:    auipc a1, %pcrel_hi(.LCPI0_0)
; RV64I-LARGE-NEXT:    ld t2, %pcrel_lo(.Lpcrel_hi0)(a1)
; RV64I-LARGE-NEXT:    jalr t2
; RV64I-LARGE-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-LARGE-NEXT:    addi sp, sp, 16
; RV64I-LARGE-NEXT:    ret
  %1 = call i32 @external_function(i32 %a)
  ret i32 %1
}

define i32 @test_call_indirect(ptr %a, i32 %b) nounwind {
; RV32I-LABEL: test_call_indirect:
; RV32I:       # %bb.0:
; RV32I-NEXT:    lpad 0
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    mv a2, a0
; RV32I-NEXT:    mv a0, a1
; RV32I-NEXT:    jalr a2
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: test_call_indirect:
; RV64I:       # %bb.0:
; RV64I-NEXT:    lpad 0
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    mv a2, a0
; RV64I-NEXT:    mv a0, a1
; RV64I-NEXT:    jalr a2
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64I-LARGE-LABEL: test_call_indirect:
; RV64I-LARGE:       # %bb.0:
; RV64I-LARGE-NEXT:    lpad 0
; RV64I-LARGE-NEXT:    addi sp, sp, -16
; RV64I-LARGE-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-LARGE-NEXT:    mv a2, a0
; RV64I-LARGE-NEXT:    mv a0, a1
; RV64I-LARGE-NEXT:    jalr a2
; RV64I-LARGE-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-LARGE-NEXT:    addi sp, sp, 16
; RV64I-LARGE-NEXT:    ret
  %1 = call i32 %a(i32 %b)
  ret i32 %1
}

; Make sure we don't use t0 as the source for jalr as that is a hint to pop the
; return address stack on some microarchitectures.
define i32 @test_call_indirect_no_t0(ptr %a, i32 %b, i32 %c, i32 %d, i32 %e, i32 %f, i32 %g, i32 %h) nounwind {
; RV32I-LABEL: test_call_indirect_no_t0:
; RV32I:       # %bb.0:
; RV32I-NEXT:    lpad 0
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    mv t1, a0
; RV32I-NEXT:    mv a0, a1
; RV32I-NEXT:    mv a1, a2
; RV32I-NEXT:    mv a2, a3
; RV32I-NEXT:    mv a3, a4
; RV32I-NEXT:    mv a4, a5
; RV32I-NEXT:    mv a5, a6
; RV32I-NEXT:    mv a6, a7
; RV32I-NEXT:    jalr t1
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    ret
;
; RV64I-LABEL: test_call_indirect_no_t0:
; RV64I:       # %bb.0:
; RV64I-NEXT:    lpad 0
; RV64I-NEXT:    addi sp, sp, -16
; RV64I-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-NEXT:    mv t1, a0
; RV64I-NEXT:    mv a0, a1
; RV64I-NEXT:    mv a1, a2
; RV64I-NEXT:    mv a2, a3
; RV64I-NEXT:    mv a3, a4
; RV64I-NEXT:    mv a4, a5
; RV64I-NEXT:    mv a5, a6
; RV64I-NEXT:    mv a6, a7
; RV64I-NEXT:    jalr t1
; RV64I-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-NEXT:    addi sp, sp, 16
; RV64I-NEXT:    ret
;
; RV64I-LARGE-LABEL: test_call_indirect_no_t0:
; RV64I-LARGE:       # %bb.0:
; RV64I-LARGE-NEXT:    lpad 0
; RV64I-LARGE-NEXT:    addi sp, sp, -16
; RV64I-LARGE-NEXT:    sd ra, 8(sp) # 8-byte Folded Spill
; RV64I-LARGE-NEXT:    mv t1, a0
; RV64I-LARGE-NEXT:    mv a0, a1
; RV64I-LARGE-NEXT:    mv a1, a2
; RV64I-LARGE-NEXT:    mv a2, a3
; RV64I-LARGE-NEXT:    mv a3, a4
; RV64I-LARGE-NEXT:    mv a4, a5
; RV64I-LARGE-NEXT:    mv a5, a6
; RV64I-LARGE-NEXT:    mv a6, a7
; RV64I-LARGE-NEXT:    jalr t1
; RV64I-LARGE-NEXT:    ld ra, 8(sp) # 8-byte Folded Reload
; RV64I-LARGE-NEXT:    addi sp, sp, 16
; RV64I-LARGE-NEXT:    ret
  %1 = call i32 %a(i32 %b, i32 %c, i32 %d, i32 %e, i32 %f, i32 %g, i32 %h)
  ret i32 %1
}

declare i32 @external_many_args(i32, i32, i32, i32, i32, i32, i32, i32, i32, i32) nounwind

define fastcc void @fastcc_call_nonfastcc(){
; RV32I-LABEL: fastcc_call_nonfastcc:
; RV32I:       # %bb.0:
; RV32I-NEXT:    lpad 0
; RV32I-NEXT:    addi sp, sp, -16
; RV32I-NEXT:    .cfi_def_cfa_offset 16
; RV32I-NEXT:    sw ra, 12(sp) # 4-byte Folded Spill
; RV32I-NEXT:    .cfi_offset ra, -4
; RV32I-NEXT:    li t0, 10
; RV32I-NEXT:    li t1, 9
; RV32I-NEXT:    li a0, 1
; RV32I-NEXT:    li a1, 2
; RV32I-NEXT:    li a2, 3
; RV32I-NEXT:    li a3, 4
; RV32I-NEXT:    li a4, 5
; RV32I-NEXT:    li a5, 6
; RV32I-NEXT:    li a6, 7
; RV32I-NEXT:    li a7, 8
; RV32I-NEXT:    sw t1, 0(sp)
; RV32I-NEXT:    sw t0, 4(sp)
; RV32I-NEXT:    call external_many_args
; RV32I-NEXT:    lw ra, 12(sp) # 4-byte Folded Reload
; RV32I-NEXT:    .cfi_restore ra
; RV32I-NEXT:    addi sp, sp, 16
; RV32I-NEXT:    .cfi_def_cfa_offset 0
; RV32I-NEXT:    ret
;
; RV64I-LABEL: fastcc_call_nonfastcc:
; RV64I:       # %bb.0:
; RV64I-NEXT:    lpad 0
; RV64I-NEXT:    addi sp, sp, -32
; RV64I-NEXT:    .cfi_def_cfa_offset 32
; RV64I-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64I-NEXT:    .cfi_offset ra, -8
; RV64I-NEXT:    li t0, 10
; RV64I-NEXT:    li t1, 9
; RV64I-NEXT:    li a0, 1
; RV64I-NEXT:    li a1, 2
; RV64I-NEXT:    li a2, 3
; RV64I-NEXT:    li a3, 4
; RV64I-NEXT:    li a4, 5
; RV64I-NEXT:    li a5, 6
; RV64I-NEXT:    li a6, 7
; RV64I-NEXT:    li a7, 8
; RV64I-NEXT:    sd t1, 0(sp)
; RV64I-NEXT:    sd t0, 8(sp)
; RV64I-NEXT:    call external_many_args
; RV64I-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64I-NEXT:    .cfi_restore ra
; RV64I-NEXT:    addi sp, sp, 32
; RV64I-NEXT:    .cfi_def_cfa_offset 0
; RV64I-NEXT:    ret
;
; RV64I-LARGE-LABEL: fastcc_call_nonfastcc:
; RV64I-LARGE:       # %bb.0:
; RV64I-LARGE-NEXT:    lpad 0
; RV64I-LARGE-NEXT:    addi sp, sp, -32
; RV64I-LARGE-NEXT:    .cfi_def_cfa_offset 32
; RV64I-LARGE-NEXT:    sd ra, 24(sp) # 8-byte Folded Spill
; RV64I-LARGE-NEXT:    .cfi_offset ra, -8
; RV64I-LARGE-NEXT:    li t0, 10
; RV64I-LARGE-NEXT:    li t1, 9
; RV64I-LARGE-NEXT:  .Lpcrel_hi1:
; RV64I-LARGE-NEXT:    auipc a5, %pcrel_hi(.LCPI3_0)
; RV64I-LARGE-NEXT:    li a0, 1
; RV64I-LARGE-NEXT:    li a1, 2
; RV64I-LARGE-NEXT:    li a2, 3
; RV64I-LARGE-NEXT:    li a3, 4
; RV64I-LARGE-NEXT:    li a4, 5
; RV64I-LARGE-NEXT:    ld t2, %pcrel_lo(.Lpcrel_hi1)(a5)
; RV64I-LARGE-NEXT:    li a5, 6
; RV64I-LARGE-NEXT:    li a6, 7
; RV64I-LARGE-NEXT:    li a7, 8
; RV64I-LARGE-NEXT:    sd t1, 0(sp)
; RV64I-LARGE-NEXT:    sd t0, 8(sp)
; RV64I-LARGE-NEXT:    jalr t2
; RV64I-LARGE-NEXT:    ld ra, 24(sp) # 8-byte Folded Reload
; RV64I-LARGE-NEXT:    .cfi_restore ra
; RV64I-LARGE-NEXT:    addi sp, sp, 32
; RV64I-LARGE-NEXT:    .cfi_def_cfa_offset 0
; RV64I-LARGE-NEXT:    ret
  call void @external_many_args(i32 1, i32 2,i32 3,i32 4,i32 5,i32 6,i32 7,i32 8,i32 9,i32 10)
  ret void
}

!llvm.module.flags = !{!0}
!0 = !{i32 8, !"cf-protection-branch", i32 1}
