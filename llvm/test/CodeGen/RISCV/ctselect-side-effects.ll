; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=riscv64 -O3 -filetype=asm | FileCheck %s --check-prefix=RV64
; RUN: llc < %s -mtriple=riscv32 -O3 -filetype=asm | FileCheck %s --check-prefix=RV32

; Test 1: Basic optimizations should still work
define i32 @test_basic_opts(i32 %x) {
; RV64-LABEL: test_basic_opts:
; RV64:       # %bb.0:
; RV64-NEXT:    ret
;
; RV32-LABEL: test_basic_opts:
; RV32:       # %bb.0:
; RV32-NEXT:    ret
  %a = or i32 %x, 0      ; Should eliminate
  %b = and i32 %a, -1    ; Should eliminate
  %c = xor i32 %b, 0     ; Should eliminate
  ret i32 %c
}

; Test 2: Constant folding should work
define i32 @test_constant_fold() {
; RV64-LABEL: test_constant_fold:
; RV64:       # %bb.0:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_constant_fold:
; RV32:       # %bb.0:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    ret
  %a = xor i32 -1, -1    ; Should fold to 0
  ret i32 %a
}

; Test 3: Protected pattern should NOT have branches
define i32 @test_protected_no_branch(i1 %cond, i32 %a, i32 %b) {
; RV64-LABEL: test_protected_no_branch:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    addi a3, a0, -1
; RV64-NEXT:    neg a0, a0
; RV64-NEXT:    and a2, a3, a2
; RV64-NEXT:    and a0, a0, a1
; RV64-NEXT:    or a0, a0, a2
; RV64-NEXT:    ret
;
; RV32-LABEL: test_protected_no_branch:
; RV32:       # %bb.0:
; RV32-NEXT:    slli a0, a0, 31
; RV32-NEXT:    srai a0, a0, 31
; RV32-NEXT:    and a1, a0, a1
; RV32-NEXT:    not a0, a0
; RV32-NEXT:    and a0, a0, a2
; RV32-NEXT:    or a0, a1, a0
; RV32-NEXT:    ret
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}

; Test 4: Explicit branch should still generate branches
define i32 @test_explicit_branch(i1 %cond, i32 %a, i32 %b) {
; RV64-LABEL: test_explicit_branch:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a0, a0, 1
; RV64-NEXT:    beqz a0, .LBB3_2
; RV64-NEXT:  # %bb.1: # %true
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    ret
; RV64-NEXT:  .LBB3_2: # %false
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:    ret
;
; RV32-LABEL: test_explicit_branch:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a0, a0, 1
; RV32-NEXT:    beqz a0, .LBB3_2
; RV32-NEXT:  # %bb.1: # %true
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    ret
; RV32-NEXT:  .LBB3_2: # %false
; RV32-NEXT:    mv a0, a2
; RV32-NEXT:    ret
  br i1 %cond, label %true, label %false
true:
  ret i32 %a
false:
  ret i32 %b
}

; Test 5: Regular select (not ct.select) - whatever wasm wants to do
define i32 @test_regular_select(i1 %cond, i32 %a, i32 %b) {
; RV64-LABEL: test_regular_select:
; RV64:       # %bb.0:
; RV64-NEXT:    andi a3, a0, 1
; RV64-NEXT:    mv a0, a1
; RV64-NEXT:    bnez a3, .LBB4_2
; RV64-NEXT:  # %bb.1:
; RV64-NEXT:    mv a0, a2
; RV64-NEXT:  .LBB4_2:
; RV64-NEXT:    ret
;
; RV32-LABEL: test_regular_select:
; RV32:       # %bb.0:
; RV32-NEXT:    andi a3, a0, 1
; RV32-NEXT:    mv a0, a1
; RV32-NEXT:    bnez a3, .LBB4_2
; RV32-NEXT:  # %bb.1:
; RV32-NEXT:    mv a0, a2
; RV32-NEXT:  .LBB4_2:
; RV32-NEXT:    ret
  %result = select i1 %cond, i32 %a, i32 %b
  ret i32 %result
}

; Test if XOR with all-ones still gets optimized
define i32 @test_xor_all_ones() {
; RV64-LABEL: test_xor_all_ones:
; RV64:       # %bb.0:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_xor_all_ones:
; RV32:       # %bb.0:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    ret
  %xor1 = xor i32 -1, -1  ; Should optimize to 0
  ret i32 %xor1
}

define i32 @test_xor_same_value(i32 %x) {
; RV64-LABEL: test_xor_same_value:
; RV64:       # %bb.0:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_xor_same_value:
; RV32:       # %bb.0:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    ret
  %xor2 = xor i32 %x, %x  ; Should optimize to 0
  ret i32 %xor2
}

define i32 @test_normal_ops(i32 %x) {
; RV64-LABEL: test_normal_ops:
; RV64:       # %bb.0:
; RV64-NEXT:    ret
;
; RV32-LABEL: test_normal_ops:
; RV32:       # %bb.0:
; RV32-NEXT:    ret
  %or1 = or i32 %x, 0      ; Should optimize to %x
  %and1 = and i32 %or1, -1  ; Should optimize to %x
  %xor1 = xor i32 %and1, 0  ; Should optimize to %x
  ret i32 %xor1
}

; This simulates what the reviewer is worried about
define i32 @test_xor_with_const_operands() {
; RV64-LABEL: test_xor_with_const_operands:
; RV64:       # %bb.0:
; RV64-NEXT:    li a0, 0
; RV64-NEXT:    ret
;
; RV32-LABEL: test_xor_with_const_operands:
; RV32:       # %bb.0:
; RV32-NEXT:    li a0, 0
; RV32-NEXT:    ret
  %a = xor i32 -1, -1      ; -1 ^ -1 should become 0
  %b = xor i32 0, 0        ; 0 ^ 0 should become 0
  %c = xor i32 42, 42      ; 42 ^ 42 should become 0
  %result = or i32 %a, %b
  %final = or i32 %result, %c
  ret i32 %final  ; Should optimize to 0
}

declare i32 @llvm.ct.select.i32(i1, i32, i32)
