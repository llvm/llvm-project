; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx900 < %s | FileCheck %s

define hidden void @void_func_i32_inreg(i32 inreg) {
; CHECK-LABEL: void_func_i32_inreg:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CHECK-NEXT:    s_setpc_b64 s[30:31]
  ret void
}

define void @tail_call_i32_inreg_divergent(i32 %vgpr) {
; CHECK-LABEL: tail_call_i32_inreg_divergent:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CHECK-NEXT:    s_mov_b32 s19, s33
; CHECK-NEXT:    s_mov_b32 s33, s32
; CHECK-NEXT:    s_xor_saveexec_b64 s[16:17], -1
; CHECK-NEXT:    buffer_store_dword v1, off, s[0:3], s33 ; 4-byte Folded Spill
; CHECK-NEXT:    s_mov_b64 exec, s[16:17]
; CHECK-NEXT:    v_writelane_b32 v1, s30, 0
; CHECK-NEXT:    s_mov_b64 s[16:17], exec
; CHECK-NEXT:    s_addk_i32 s32, 0x400
; CHECK-NEXT:    v_writelane_b32 v1, s31, 1
; CHECK-NEXT:  .LBB1_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s18, v0
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s18, v0
; CHECK-NEXT:    s_and_saveexec_b64 vcc, vcc
; CHECK-NEXT:    s_getpc_b64 s[20:21]
; CHECK-NEXT:    s_add_u32 s20, s20, void_func_i32_inreg@rel32@lo+4
; CHECK-NEXT:    s_addc_u32 s21, s21, void_func_i32_inreg@rel32@hi+12
; CHECK-NEXT:    s_mov_b32 s0, s18
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[20:21]
; CHECK-NEXT:    ; implicit-def: $vgpr0
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, vcc
; CHECK-NEXT:    s_cbranch_execnz .LBB1_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_mov_b64 exec, s[16:17]
; CHECK-NEXT:    v_readlane_b32 s31, v1, 1
; CHECK-NEXT:    v_readlane_b32 s30, v1, 0
; CHECK-NEXT:    s_mov_b32 s32, s33
; CHECK-NEXT:    s_xor_saveexec_b64 s[4:5], -1
; CHECK-NEXT:    buffer_load_dword v1, off, s[0:3], s33 ; 4-byte Folded Reload
; CHECK-NEXT:    s_mov_b64 exec, s[4:5]
; CHECK-NEXT:    s_mov_b32 s33, s19
; CHECK-NEXT:    s_waitcnt vmcnt(0)
; CHECK-NEXT:    s_setpc_b64 s[30:31]
  tail call void @void_func_i32_inreg(i32 inreg %vgpr)
  ret void
}

@constant = external hidden addrspace(4) constant ptr

define void @indirect_tail_call_i32_inreg_divergent(i32 %vgpr) {
; CHECK-LABEL: indirect_tail_call_i32_inreg_divergent:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CHECK-NEXT:    s_mov_b32 s16, s33
; CHECK-NEXT:    s_mov_b32 s33, s32
; CHECK-NEXT:    s_or_saveexec_b64 s[18:19], -1
; CHECK-NEXT:    buffer_store_dword v40, off, s[0:3], s33 ; 4-byte Folded Spill
; CHECK-NEXT:    s_mov_b64 exec, s[18:19]
; CHECK-NEXT:    v_writelane_b32 v40, s16, 20
; CHECK-NEXT:    v_writelane_b32 v40, s30, 0
; CHECK-NEXT:    v_writelane_b32 v40, s31, 1
; CHECK-NEXT:    v_writelane_b32 v40, s34, 2
; CHECK-NEXT:    v_writelane_b32 v40, s35, 3
; CHECK-NEXT:    v_writelane_b32 v40, s36, 4
; CHECK-NEXT:    v_writelane_b32 v40, s37, 5
; CHECK-NEXT:    v_writelane_b32 v40, s38, 6
; CHECK-NEXT:    v_writelane_b32 v40, s39, 7
; CHECK-NEXT:    v_writelane_b32 v40, s48, 8
; CHECK-NEXT:    v_writelane_b32 v40, s49, 9
; CHECK-NEXT:    v_writelane_b32 v40, s50, 10
; CHECK-NEXT:    v_writelane_b32 v40, s51, 11
; CHECK-NEXT:    v_writelane_b32 v40, s52, 12
; CHECK-NEXT:    v_writelane_b32 v40, s53, 13
; CHECK-NEXT:    v_writelane_b32 v40, s54, 14
; CHECK-NEXT:    s_addk_i32 s32, 0x400
; CHECK-NEXT:    v_writelane_b32 v40, s55, 15
; CHECK-NEXT:    v_writelane_b32 v40, s64, 16
; CHECK-NEXT:    s_mov_b64 s[48:49], s[4:5]
; CHECK-NEXT:    s_getpc_b64 s[4:5]
; CHECK-NEXT:    s_add_u32 s4, s4, constant@rel32@lo+4
; CHECK-NEXT:    s_addc_u32 s5, s5, constant@rel32@hi+12
; CHECK-NEXT:    v_writelane_b32 v40, s65, 17
; CHECK-NEXT:    s_load_dwordx2 s[64:65], s[4:5], 0x0
; CHECK-NEXT:    v_writelane_b32 v40, s66, 18
; CHECK-NEXT:    s_mov_b32 s50, s15
; CHECK-NEXT:    s_mov_b32 s51, s14
; CHECK-NEXT:    s_mov_b32 s52, s13
; CHECK-NEXT:    s_mov_b32 s53, s12
; CHECK-NEXT:    s_mov_b64 s[34:35], s[10:11]
; CHECK-NEXT:    s_mov_b64 s[36:37], s[8:9]
; CHECK-NEXT:    s_mov_b64 s[38:39], s[6:7]
; CHECK-NEXT:    s_mov_b64 s[54:55], exec
; CHECK-NEXT:    v_writelane_b32 v40, s67, 19
; CHECK-NEXT:  .LBB2_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s16, v0
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s16, v0
; CHECK-NEXT:    s_and_saveexec_b64 s[66:67], vcc
; CHECK-NEXT:    s_mov_b64 s[4:5], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_mov_b32 s12, s53
; CHECK-NEXT:    s_mov_b32 s13, s52
; CHECK-NEXT:    s_mov_b32 s14, s51
; CHECK-NEXT:    s_mov_b32 s15, s50
; CHECK-NEXT:    s_mov_b32 s0, s16
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[64:65]
; CHECK-NEXT:    ; implicit-def: $vgpr0
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, s[66:67]
; CHECK-NEXT:    s_cbranch_execnz .LBB2_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_mov_b64 exec, s[54:55]
; CHECK-NEXT:    v_readlane_b32 s67, v40, 19
; CHECK-NEXT:    v_readlane_b32 s66, v40, 18
; CHECK-NEXT:    v_readlane_b32 s65, v40, 17
; CHECK-NEXT:    v_readlane_b32 s64, v40, 16
; CHECK-NEXT:    v_readlane_b32 s55, v40, 15
; CHECK-NEXT:    v_readlane_b32 s54, v40, 14
; CHECK-NEXT:    v_readlane_b32 s53, v40, 13
; CHECK-NEXT:    v_readlane_b32 s52, v40, 12
; CHECK-NEXT:    v_readlane_b32 s51, v40, 11
; CHECK-NEXT:    v_readlane_b32 s50, v40, 10
; CHECK-NEXT:    v_readlane_b32 s49, v40, 9
; CHECK-NEXT:    v_readlane_b32 s48, v40, 8
; CHECK-NEXT:    v_readlane_b32 s39, v40, 7
; CHECK-NEXT:    v_readlane_b32 s38, v40, 6
; CHECK-NEXT:    v_readlane_b32 s37, v40, 5
; CHECK-NEXT:    v_readlane_b32 s36, v40, 4
; CHECK-NEXT:    v_readlane_b32 s35, v40, 3
; CHECK-NEXT:    v_readlane_b32 s34, v40, 2
; CHECK-NEXT:    v_readlane_b32 s31, v40, 1
; CHECK-NEXT:    v_readlane_b32 s30, v40, 0
; CHECK-NEXT:    s_mov_b32 s32, s33
; CHECK-NEXT:    v_readlane_b32 s4, v40, 20
; CHECK-NEXT:    s_or_saveexec_b64 s[6:7], -1
; CHECK-NEXT:    buffer_load_dword v40, off, s[0:3], s33 ; 4-byte Folded Reload
; CHECK-NEXT:    s_mov_b64 exec, s[6:7]
; CHECK-NEXT:    s_mov_b32 s33, s4
; CHECK-NEXT:    s_waitcnt vmcnt(0)
; CHECK-NEXT:    s_setpc_b64 s[30:31]
  %fptr = load ptr, ptr addrspace(4) @constant, align 8
  tail call void %fptr(i32 inreg %vgpr)
  ret void
}

declare void @user(ptr addrspace(5))

define amdgpu_kernel void @v_multiple_frame_indexes_literal_offsets() #0 {
; CHECK-LABEL: v_multiple_frame_indexes_literal_offsets:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; CHECK-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; CHECK-NEXT:    s_add_u32 s0, s0, s17
; CHECK-NEXT:    v_mov_b32_e32 v3, 8
; CHECK-NEXT:    v_mov_b32_e32 v4, 0
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, 0, v0
; CHECK-NEXT:    v_lshlrev_b32_e32 v2, 20, v2
; CHECK-NEXT:    v_lshlrev_b32_e32 v1, 10, v1
; CHECK-NEXT:    s_addc_u32 s1, s1, 0
; CHECK-NEXT:    s_mov_b32 s33, s16
; CHECK-NEXT:    s_mov_b32 s50, s15
; CHECK-NEXT:    s_mov_b32 s51, s14
; CHECK-NEXT:    s_mov_b64 s[34:35], s[10:11]
; CHECK-NEXT:    s_mov_b64 s[36:37], s[8:9]
; CHECK-NEXT:    s_mov_b64 s[38:39], s[6:7]
; CHECK-NEXT:    s_mov_b64 s[48:49], s[4:5]
; CHECK-NEXT:    v_cndmask_b32_e32 v3, v3, v4, vcc
; CHECK-NEXT:    v_or3_b32 v31, v0, v1, v2
; CHECK-NEXT:    s_movk_i32 s32, 0x400
; CHECK-NEXT:    s_mov_b64 s[4:5], exec
; CHECK-NEXT:  .LBB3_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s15, v3
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v3
; CHECK-NEXT:    s_and_saveexec_b64 s[52:53], vcc
; CHECK-NEXT:    s_getpc_b64 s[4:5]
; CHECK-NEXT:    s_add_u32 s4, s4, user@gotpcrel32@lo+4
; CHECK-NEXT:    s_addc_u32 s5, s5, user@gotpcrel32@hi+12
; CHECK-NEXT:    s_load_dwordx2 s[16:17], s[4:5], 0x0
; CHECK-NEXT:    s_mov_b64 s[4:5], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_mov_b32 s12, s51
; CHECK-NEXT:    s_mov_b32 s13, s50
; CHECK-NEXT:    s_mov_b32 s14, s33
; CHECK-NEXT:    s_mov_b32 s0, s15
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[16:17]
; CHECK-NEXT:    ; implicit-def: $vgpr3
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, s[52:53]
; CHECK-NEXT:    s_cbranch_execnz .LBB3_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_endpgm
  %vgpr = call i32 @llvm.amdgcn.workitem.id.x()
  %alloca0 = alloca [2 x i32], align 8, addrspace(5)
  %alloca1 = alloca i32, align 4, addrspace(5)
  %cmp = icmp eq i32 %vgpr, 0
  %select = select i1 %cmp, ptr addrspace(5) %alloca0, ptr addrspace(5) %alloca1
  call void @user(ptr addrspace(5) inreg %select)
  ret void
}

declare void @user_i32_inreg_i32_i32_inreg(i32 inreg, i32, i32 inreg)
define amdgpu_kernel void @call_user_i32_inreg_i32_i32_inreg(i32 %a, i32 %a1, i32 %a2, i32 %b, i32 %b1, i32 %b2, i32 %c) #0 {
; CHECK-LABEL: call_user_i32_inreg_i32_i32_inreg:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_load_dwordx8 s[64:71], s[8:9], 0x0
; CHECK-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; CHECK-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; CHECK-NEXT:    s_add_u32 s0, s0, s17
; CHECK-NEXT:    s_addc_u32 s1, s1, 0
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    v_mov_b32_e32 v3, s66
; CHECK-NEXT:    v_mov_b32_e32 v4, s65
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s64, v0
; CHECK-NEXT:    v_cndmask_b32_e32 v3, v3, v4, vcc
; CHECK-NEXT:    v_mov_b32_e32 v4, s69
; CHECK-NEXT:    v_mov_b32_e32 v5, s68
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s67, v0
; CHECK-NEXT:    s_add_u32 s48, s8, 32
; CHECK-NEXT:    v_lshlrev_b32_e32 v2, 20, v2
; CHECK-NEXT:    v_lshlrev_b32_e32 v1, 10, v1
; CHECK-NEXT:    s_mov_b32 s33, s16
; CHECK-NEXT:    s_mov_b32 s50, s15
; CHECK-NEXT:    s_mov_b32 s51, s14
; CHECK-NEXT:    s_mov_b64 s[34:35], s[10:11]
; CHECK-NEXT:    s_mov_b64 s[36:37], s[6:7]
; CHECK-NEXT:    s_mov_b64 s[38:39], s[4:5]
; CHECK-NEXT:    v_cndmask_b32_e32 v4, v4, v5, vcc
; CHECK-NEXT:    s_addc_u32 s49, s9, 0
; CHECK-NEXT:    v_or3_b32 v31, v0, v1, v2
; CHECK-NEXT:    s_mov_b32 s32, 0
; CHECK-NEXT:    s_mov_b64 s[4:5], exec
; CHECK-NEXT:  .LBB4_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s15, v4
; CHECK-NEXT:    v_readfirstlane_b32 s16, v3
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v4
; CHECK-NEXT:    v_cmp_eq_u32_e64 s[4:5], s16, v3
; CHECK-NEXT:    s_and_b64 s[4:5], vcc, s[4:5]
; CHECK-NEXT:    s_and_saveexec_b64 s[52:53], s[4:5]
; CHECK-NEXT:    s_getpc_b64 s[4:5]
; CHECK-NEXT:    s_add_u32 s4, s4, user_i32_inreg_i32_i32_inreg@gotpcrel32@lo+4
; CHECK-NEXT:    s_addc_u32 s5, s5, user_i32_inreg_i32_i32_inreg@gotpcrel32@hi+12
; CHECK-NEXT:    s_load_dwordx2 s[18:19], s[4:5], 0x0
; CHECK-NEXT:    s_mov_b64 s[4:5], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_mov_b32 s12, s51
; CHECK-NEXT:    s_mov_b32 s13, s50
; CHECK-NEXT:    s_mov_b32 s14, s33
; CHECK-NEXT:    v_mov_b32_e32 v0, s70
; CHECK-NEXT:    s_mov_b32 s1, s15
; CHECK-NEXT:    s_mov_b32 s0, s16
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; CHECK-NEXT:    ; implicit-def: $vgpr4
; CHECK-NEXT:    ; implicit-def: $vgpr3
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, s[52:53]
; CHECK-NEXT:    s_cbranch_execnz .LBB4_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_endpgm
  %vgpr = call i32 @llvm.amdgcn.workitem.id.x()
  %cmp.a = icmp sgt i32 %vgpr, %a
  %cmp.b = icmp sgt i32 %vgpr, %b
  %sel.a = select i1 %cmp.a, i32 %a1, i32 %a2
  %sel.b = select i1 %cmp.b, i32 %b1, i32 %b2
  call void @user_i32_inreg_i32_i32_inreg(i32 inreg %sel.a, i32 %c, i32 inreg %sel.b)
  ret void
}

declare void @user_ft_inreg_ft_ft_inreg(float inreg, float, float inreg)
define amdgpu_kernel void @call_user_ft_inreg_ft_ft_inreg(i32 %a, float %a1, float %a2, i32 %b, float %b1, float %b2, float %c) #0 {
; CHECK-LABEL: call_user_ft_inreg_ft_ft_inreg:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_load_dwordx8 s[64:71], s[8:9], 0x0
; CHECK-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; CHECK-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; CHECK-NEXT:    s_add_u32 s0, s0, s17
; CHECK-NEXT:    s_addc_u32 s1, s1, 0
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    v_mov_b32_e32 v3, s66
; CHECK-NEXT:    v_mov_b32_e32 v4, s65
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s64, v0
; CHECK-NEXT:    v_cndmask_b32_e32 v3, v3, v4, vcc
; CHECK-NEXT:    v_mov_b32_e32 v4, s69
; CHECK-NEXT:    v_mov_b32_e32 v5, s68
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s67, v0
; CHECK-NEXT:    s_add_u32 s48, s8, 32
; CHECK-NEXT:    v_lshlrev_b32_e32 v2, 20, v2
; CHECK-NEXT:    v_lshlrev_b32_e32 v1, 10, v1
; CHECK-NEXT:    s_mov_b32 s33, s16
; CHECK-NEXT:    s_mov_b32 s50, s15
; CHECK-NEXT:    s_mov_b32 s51, s14
; CHECK-NEXT:    s_mov_b64 s[34:35], s[10:11]
; CHECK-NEXT:    s_mov_b64 s[36:37], s[6:7]
; CHECK-NEXT:    s_mov_b64 s[38:39], s[4:5]
; CHECK-NEXT:    v_cndmask_b32_e32 v4, v4, v5, vcc
; CHECK-NEXT:    s_addc_u32 s49, s9, 0
; CHECK-NEXT:    v_or3_b32 v31, v0, v1, v2
; CHECK-NEXT:    s_mov_b32 s32, 0
; CHECK-NEXT:    s_mov_b64 s[4:5], exec
; CHECK-NEXT:  .LBB5_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s15, v4
; CHECK-NEXT:    v_readfirstlane_b32 s16, v3
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v4
; CHECK-NEXT:    v_cmp_eq_u32_e64 s[4:5], s16, v3
; CHECK-NEXT:    s_and_b64 s[4:5], vcc, s[4:5]
; CHECK-NEXT:    s_and_saveexec_b64 s[52:53], s[4:5]
; CHECK-NEXT:    s_getpc_b64 s[4:5]
; CHECK-NEXT:    s_add_u32 s4, s4, user_ft_inreg_ft_ft_inreg@gotpcrel32@lo+4
; CHECK-NEXT:    s_addc_u32 s5, s5, user_ft_inreg_ft_ft_inreg@gotpcrel32@hi+12
; CHECK-NEXT:    s_load_dwordx2 s[18:19], s[4:5], 0x0
; CHECK-NEXT:    s_mov_b64 s[4:5], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_mov_b32 s12, s51
; CHECK-NEXT:    s_mov_b32 s13, s50
; CHECK-NEXT:    s_mov_b32 s14, s33
; CHECK-NEXT:    v_mov_b32_e32 v0, s70
; CHECK-NEXT:    s_mov_b32 s1, s15
; CHECK-NEXT:    s_mov_b32 s0, s16
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[18:19]
; CHECK-NEXT:    ; implicit-def: $vgpr4
; CHECK-NEXT:    ; implicit-def: $vgpr3
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, s[52:53]
; CHECK-NEXT:    s_cbranch_execnz .LBB5_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_endpgm
  %vgpr = call i32 @llvm.amdgcn.workitem.id.x()
  %cmp.a = icmp sgt i32 %vgpr, %a
  %cmp.b = icmp sgt i32 %vgpr, %b
  %sel.a = select i1 %cmp.a, float %a1, float %a2
  %sel.b = select i1 %cmp.b, float %b1, float %b2
  call void @user_ft_inreg_ft_ft_inreg(float inreg %sel.a, float %c, float inreg %sel.b)
  ret void
}

declare void @user_2xft_inreg_ft_2xft_inreg(<2 x float> inreg, float, <2 x float> inreg)
define amdgpu_kernel void @call_user_2xft_inreg_ft_2xft_inreg(i32 %a, <2 x float> %a1, <2 x float> %a2, i32 %b, <2 x float> %b1, <2 x float> %b2, float %c) #0 {
; CHECK-LABEL: call_user_2xft_inreg_ft_2xft_inreg:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_add_u32 flat_scratch_lo, s12, s17
; CHECK-NEXT:    s_addc_u32 flat_scratch_hi, s13, 0
; CHECK-NEXT:    s_mov_b32 s50, s15
; CHECK-NEXT:    s_mov_b32 s51, s14
; CHECK-NEXT:    s_mov_b64 s[34:35], s[10:11]
; CHECK-NEXT:    s_mov_b64 s[36:37], s[6:7]
; CHECK-NEXT:    s_mov_b64 s[38:39], s[4:5]
; CHECK-NEXT:    s_load_dwordx4 s[4:7], s[8:9], 0x8
; CHECK-NEXT:    s_load_dword s10, s[8:9], 0x0
; CHECK-NEXT:    s_load_dword s11, s[8:9], 0x18
; CHECK-NEXT:    s_load_dwordx4 s[12:15], s[8:9], 0x20
; CHECK-NEXT:    s_load_dword s54, s[8:9], 0x30
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    v_mov_b32_e32 v3, s6
; CHECK-NEXT:    v_mov_b32_e32 v4, s4
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s10, v0
; CHECK-NEXT:    s_add_u32 s0, s0, s17
; CHECK-NEXT:    v_cndmask_b32_e32 v3, v3, v4, vcc
; CHECK-NEXT:    v_mov_b32_e32 v4, s7
; CHECK-NEXT:    v_mov_b32_e32 v5, s5
; CHECK-NEXT:    s_addc_u32 s1, s1, 0
; CHECK-NEXT:    v_cndmask_b32_e32 v4, v4, v5, vcc
; CHECK-NEXT:    v_mov_b32_e32 v5, s14
; CHECK-NEXT:    v_mov_b32_e32 v6, s12
; CHECK-NEXT:    v_cmp_lt_i32_e32 vcc, s11, v0
; CHECK-NEXT:    v_cndmask_b32_e32 v5, v5, v6, vcc
; CHECK-NEXT:    v_mov_b32_e32 v6, s15
; CHECK-NEXT:    v_mov_b32_e32 v7, s13
; CHECK-NEXT:    s_add_u32 s48, s8, 56
; CHECK-NEXT:    v_lshlrev_b32_e32 v2, 20, v2
; CHECK-NEXT:    v_lshlrev_b32_e32 v1, 10, v1
; CHECK-NEXT:    s_mov_b32 s33, s16
; CHECK-NEXT:    v_cndmask_b32_e32 v6, v6, v7, vcc
; CHECK-NEXT:    s_addc_u32 s49, s9, 0
; CHECK-NEXT:    v_or3_b32 v31, v0, v1, v2
; CHECK-NEXT:    s_mov_b32 s32, 0
; CHECK-NEXT:    s_mov_b64 s[4:5], exec
; CHECK-NEXT:  .LBB6_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    v_readfirstlane_b32 s15, v6
; CHECK-NEXT:    v_readfirstlane_b32 s16, v5
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v6
; CHECK-NEXT:    v_cmp_eq_u32_e64 s[4:5], s16, v5
; CHECK-NEXT:    v_readfirstlane_b32 s17, v4
; CHECK-NEXT:    s_and_b64 s[4:5], vcc, s[4:5]
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s17, v4
; CHECK-NEXT:    v_readfirstlane_b32 s18, v3
; CHECK-NEXT:    s_and_b64 s[4:5], s[4:5], vcc
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, s18, v3
; CHECK-NEXT:    s_and_b64 s[4:5], s[4:5], vcc
; CHECK-NEXT:    s_and_saveexec_b64 s[52:53], s[4:5]
; CHECK-NEXT:    s_getpc_b64 s[4:5]
; CHECK-NEXT:    s_add_u32 s4, s4, user_2xft_inreg_ft_2xft_inreg@gotpcrel32@lo+4
; CHECK-NEXT:    s_addc_u32 s5, s5, user_2xft_inreg_ft_2xft_inreg@gotpcrel32@hi+12
; CHECK-NEXT:    s_load_dwordx2 s[20:21], s[4:5], 0x0
; CHECK-NEXT:    s_mov_b64 s[4:5], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_mov_b32 s12, s51
; CHECK-NEXT:    s_mov_b32 s13, s50
; CHECK-NEXT:    s_mov_b32 s14, s33
; CHECK-NEXT:    v_mov_b32_e32 v0, s54
; CHECK-NEXT:    s_mov_b32 s3, s15
; CHECK-NEXT:    s_mov_b32 s2, s16
; CHECK-NEXT:    s_mov_b32 s1, s17
; CHECK-NEXT:    s_mov_b32 s0, s18
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_swappc_b64 s[30:31], s[20:21]
; CHECK-NEXT:    ; implicit-def: $vgpr6
; CHECK-NEXT:    ; implicit-def: $vgpr5
; CHECK-NEXT:    ; implicit-def: $vgpr4
; CHECK-NEXT:    ; implicit-def: $vgpr3
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b64 exec, exec, s[52:53]
; CHECK-NEXT:    s_cbranch_execnz .LBB6_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_endpgm
  %vgpr = call i32 @llvm.amdgcn.workitem.id.x()
  %cmp.a = icmp sgt i32 %vgpr, %a
  %cmp.b = icmp sgt i32 %vgpr, %b
  %sel.a = select i1 %cmp.a, <2 x float> %a1, <2 x float> %a2
  %sel.b = select i1 %cmp.b, <2 x float> %b1, <2 x float> %b2
  call void @user_2xft_inreg_ft_2xft_inreg(<2 x float> inreg %sel.a, float %c, <2 x float> inreg %sel.b)
  ret void
}

declare noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x() #1

attributes #0 = { nounwind }
attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
