; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_permute_pair_2src_interleave_b64(i32 %src0, i32 %src1, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_permute_pair_2src_interleave_b64:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_permute_pair_2src_interleave_b64 v0, v1, v0, v1 aux_data:2
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.permute.pair.2src.interleave.b64(i32 %src0, i32 %src1, i32 2)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_permute_pack_tensor_2src_b64(i32 %src0, i32 %src1, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_permute_pack_tensor_2src_b64:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_permute_pack_tensor_2src_b64 v0, v1, v0, v1 aux_data:2
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.permute.pack.tensor.2src.b64(i32 %src0, i32 %src1, i32 2)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

;
; scale_bias_activate sequential
;

define amdgpu_ps void @test_scale_bias_activate_f32_inreg(float inreg %ssrc, <4 x float> %acc_in, float %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f32_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_f32 v[0:3], v[0:3], s0, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.scale.bias.activate.f32(<4 x float> %acc_in, float %ssrc, float %bias, i32 2, i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_f32_ssrc_null(<4 x float> %acc_in, float %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f32_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_f32 v[0:3], v[0:3], null, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.scale.bias.activate.f32(<4 x float> %acc_in, float 0.0, float %bias, i32 67108866, i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_f32(float %ssrc, <4 x float> %acc_in, float %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f32:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v4 :: v_dual_mov_b32 v10, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_f32 v[0:3], v[8:11], s0, v5 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.scale.bias.activate.f32(<4 x float> %acc_in, float %ssrc, float %bias, i32 2, i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_f16_inreg(half inreg %ssrc, <8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_f16 v[0:3], v[0:3], s0, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.scale.bias.activate.f16(<8 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_f16_ssrc_null(<8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_f16 v[0:3], v[0:3], null, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.scale.bias.activate.f16(<8 x half> %acc_in, half 0xH0000, <2 x half> %bias, i32 67108866, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_f16(half %ssrc, <8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v4 :: v_dual_mov_b32 v10, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_f16 v[0:3], v[8:11], s0, v5 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.scale.bias.activate.f16(<8 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_bf16_inreg(bfloat inreg %ssrc, <8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_bf16 v[0:3], v[0:3], s0, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.scale.bias.activate.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_bf16_ssrc_null(<8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_bf16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_bf16 v[0:3], v[0:3], null, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.scale.bias.activate.bf16(<8 x bfloat> %acc_in, bfloat 0xR0000, <2 x bfloat> %bias, i32 67108866, i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_bf16(bfloat %ssrc, <8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out) {
; GFX13-LABEL: test_scale_bias_activate_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v4 :: v_dual_mov_b32 v10, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_bf16 v[0:3], v[8:11], s0, v5 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[6:7], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.scale.bias.activate.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}


;
; scale_bias_activate scatter 2
;

define amdgpu_ps void @test_scale_bias_activate_scatter2_f16_inreg(half inreg %ssrc, <4 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_scale_bias_activate_scatter2_f16 v0, v1, v[0:1], s0, v2 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter2.f16(<4 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter2_f16_ssrc_null(<4 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_f16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_scale_bias_activate_scatter2_f16 v0, v1, v[0:1], null, v2 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter2.f16(<4 x half> %acc_in, half 0xH0000, <2 x half> %bias, i32 67108866, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter2_f16(half %ssrc, <4 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_scatter2_f16 v0, v1, v[8:9], s0, v3 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter2.f16(<4 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter2_bf16_inreg(bfloat inreg %ssrc, <4 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_scale_bias_activate_scatter2_bf16 v0, v1, v[0:1], s0, v2 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter2.bf16(<4 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter2_bf16_ssrc_null(<4 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_bf16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_scale_bias_activate_scatter2_bf16 v0, v1, v[0:1], null, v2 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter2.bf16(<4 x bfloat> %acc_in, bfloat 0xR0000, <2 x bfloat> %bias, i32 67108866, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter2_bf16(bfloat %ssrc, <4 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_scale_bias_activate_scatter2_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_scatter2_bf16 v0, v1, v[8:9], s0, v3 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter2.bf16(<4 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

;
; scale_bias_activate scatter 4 (VOP5M)
;

define amdgpu_ps void @test_scale_bias_activate_scatter4_f16_inreg(half inreg %ssrc, <8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_scatter4_f16 v0, v1, v2, v3, v[0:3], s0, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter4.f16(<8 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter4_f16_ssrc_null(<8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_f16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_scatter4_f16 v0, v1, v2, v3, v[0:3], null, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter4.f16(<8 x half> %acc_in, half 0xH0000, <2 x half> %bias, i32 67108866, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter4_f16(half %ssrc, <8 x half> %acc_in, <2 x half> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v4 :: v_dual_mov_b32 v16, v3
; GFX13-NEXT:    v_dual_mov_b32 v15, v2 :: v_dual_mov_b32 v14, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_scatter4_f16 v0, v1, v2, v3, v[14:17], s0, v5 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.scale.bias.activate.scatter4.f16(<8 x half> %acc_in, half %ssrc, <2 x half> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter4_bf16_inreg(bfloat inreg %ssrc, <8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_scatter4_bf16 v0, v1, v2, v3, v[0:3], s0, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter4.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter4_bf16_ssrc_null(<8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_bf16_ssrc_null:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_scale_bias_activate_scatter4_bf16 v0, v1, v2, v3, v[0:3], null, v4 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter4.bf16(<8 x bfloat> %acc_in, bfloat 0xR0000, <2 x bfloat> %bias, i32 67108866, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_scale_bias_activate_scatter4_bf16(bfloat %ssrc, <8 x bfloat> %acc_in, <2 x bfloat> %bias, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_scale_bias_activate_scatter4_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v4 :: v_dual_mov_b32 v16, v3
; GFX13-NEXT:    v_dual_mov_b32 v15, v2 :: v_dual_mov_b32 v14, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_scale_bias_activate_scatter4_bf16 v0, v1, v2, v3, v[14:17], s0, v5 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.scale.bias.activate.scatter4.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, <2 x bfloat> %bias, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

;
; uniform_scale_activate sequential
;

define amdgpu_ps void @test_uniform_scale_activate_f32_inreg(float inreg %ssrc, <4 x float> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_f32_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_f32 v[0:3], v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[4:5], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.uniform.scale.activate.f32(<4 x float> %acc_in, float %ssrc, i32 2, i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_f32(float %ssrc, <4 x float> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_f32:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v6 :: v_dual_mov_b32 v8, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v4 :: v_dual_mov_b32 v6, v1
; GFX13-NEXT:    v_dual_mov_b32 v7, v2 :: v_dual_mov_b32 v10, v5
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_uniform_scale_activate_f32 v[0:3], v[6:9], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[10:11], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.uniform.scale.activate.f32(<4 x float> %acc_in, float %ssrc, i32 2, i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_f16_inreg(half inreg %ssrc, <8 x half> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_f16 v[0:3], v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[4:5], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.uniform.scale.activate.f16(<8 x half> %acc_in, half %ssrc, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_f16(half %ssrc, <8 x half> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v6 :: v_dual_mov_b32 v8, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v4 :: v_dual_mov_b32 v6, v1
; GFX13-NEXT:    v_dual_mov_b32 v7, v2 :: v_dual_mov_b32 v10, v5
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_uniform_scale_activate_f16 v[0:3], v[6:9], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[10:11], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.uniform.scale.activate.f16(<8 x half> %acc_in, half %ssrc, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_bf16_inreg(bfloat inreg %ssrc, <8 x bfloat> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_bf16 v[0:3], v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[4:5], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.uniform.scale.activate.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_bf16(bfloat %ssrc, <8 x bfloat> %acc_in, ptr addrspace(1) %out) {
; GFX13-LABEL: test_uniform_scale_activate_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v11, v6 :: v_dual_mov_b32 v8, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v4 :: v_dual_mov_b32 v6, v1
; GFX13-NEXT:    v_dual_mov_b32 v7, v2 :: v_dual_mov_b32 v10, v5
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_uniform_scale_activate_bf16 v[0:3], v[6:9], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[10:11], v[0:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.uniform.scale.activate.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

;
; uniform_scale_activate scatter
;

define amdgpu_ps void @test_uniform_scale_activate_scatter2_f16_inreg(half inreg %ssrc, <4 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_uniform_scale_activate_scatter2_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_scatter2_f16 v0, v1, v[0:1], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.uniform.scale.activate.scatter2.f16(<4 x half> %acc_in, half %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter2_f16(half %ssrc, <4 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_uniform_scale_activate_scatter2_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_uniform_scale_activate_scatter2_f16 v0, v1, v[8:9], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.uniform.scale.activate.scatter2.f16(<4 x half> %acc_in, half %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter2_bf16_inreg(bfloat inreg %ssrc, <4 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_uniform_scale_activate_scatter2_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_scatter2_bf16 v0, v1, v[0:1], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.uniform.scale.activate.scatter2.bf16(<4 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter2_bf16(bfloat %ssrc, <4 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_uniform_scale_activate_scatter2_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    v_uniform_scale_activate_scatter2_bf16 v0, v1, v[8:9], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.uniform.scale.activate.scatter2.bf16(<4 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

;
; uniform_scale_activate scatter 4 (VOP5M)
;

define amdgpu_ps void @test_uniform_scale_activate_scatter4_f16_inreg(half inreg %ssrc, <8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_uniform_scale_activate_scatter4_f16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_scatter4_f16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.uniform.scale.activate.scatter4.f16(<8 x half> %acc_in, half %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter4_f16(half %ssrc, <8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_uniform_scale_activate_scatter4_f16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v15, v10 :: v_dual_mov_b32 v14, v9
; GFX13-NEXT:    v_dual_mov_b32 v17, v8 :: v_dual_mov_b32 v16, v7
; GFX13-NEXT:    v_dual_mov_b32 v11, v4 :: v_dual_mov_b32 v10, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_dual_mov_b32 v3, v6 :: v_dual_mov_b32 v2, v5
; GFX13-NEXT:    v_uniform_scale_activate_scatter4_f16 v0, v1, v4, v5, v[8:11], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[16:17], v1, off
; GFX13-NEXT:    global_store_b32 v[14:15], v4, off
; GFX13-NEXT:    global_store_b32 v[12:13], v5, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.uniform.scale.activate.scatter4.f16(<8 x half> %acc_in, half %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter4_bf16_inreg(bfloat inreg %ssrc, <8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_uniform_scale_activate_scatter4_bf16_inreg:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_uniform_scale_activate_scatter4_bf16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.uniform.scale.activate.scatter4.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_uniform_scale_activate_scatter4_bf16(bfloat %ssrc, <8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_uniform_scale_activate_scatter4_bf16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v15, v10 :: v_dual_mov_b32 v14, v9
; GFX13-NEXT:    v_dual_mov_b32 v17, v8 :: v_dual_mov_b32 v16, v7
; GFX13-NEXT:    v_dual_mov_b32 v11, v4 :: v_dual_mov_b32 v10, v3
; GFX13-NEXT:    v_dual_mov_b32 v9, v2 :: v_dual_mov_b32 v8, v1
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_dual_mov_b32 v3, v6 :: v_dual_mov_b32 v2, v5
; GFX13-NEXT:    v_uniform_scale_activate_scatter4_bf16 v0, v1, v4, v5, v[8:11], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[16:17], v1, off
; GFX13-NEXT:    global_store_b32 v[14:15], v4, off
; GFX13-NEXT:    global_store_b32 v[12:13], v5, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.uniform.scale.activate.scatter4.bf16(<8 x bfloat> %acc_in, bfloat %ssrc, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}
