; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_cvt_to_tensor_i4_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i4_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.i4.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f16.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.i4.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.bf16.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u4_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.u4.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f16.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.u4.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.bf16.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i8_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.i8.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.f16.scatter2.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.i8.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.bf16.scatter2.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u8_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.u8.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.f16.scatter2.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.u8.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.bf16.scatter2.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.fp8.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.f16.scatter2.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.fp8.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.bf16.scatter2.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f32 v0, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.f32.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.bf8.f16.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.f16.scatter2.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.f16.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v[0:1], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call <2 x i32> @llvm.amdgcn.cvt.to.tensor.bf8.bf16.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <2 x i32> %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v1, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.bf16.scatter2.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.bf16.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_f16_f32 v0, v1, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f32.scatter2.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v[0:1], v[2:3], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    global_store_b64 v[8:9], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <4 x half>, <4 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter2.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <4 x half>, <4 x half> } %pair, 0
  %dst1 = extractvalue { <4 x half>, <4 x half> } %pair, 1
  store <4 x half> %dst0, ptr addrspace(1) %out0
  store <4 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter2.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter4.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter2.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v[0:1], v[2:3], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    global_store_b64 v[8:9], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <4 x half>, <4 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter2.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <4 x half>, <4 x half> } %pair, 0
  %dst1 = extractvalue { <4 x half>, <4 x half> } %pair, 1
  store <4 x half> %dst0, ptr addrspace(1) %out0
  store <4 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter2.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter4.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter2.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f32_4x2x16(<4 x float> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f32_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f32 v0, v1, v[0:3], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f32.scatter2.v4f32(<4 x float> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_8x4x8(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v[0:1], v[2:3], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    global_store_b64 v[8:9], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <4 x bfloat>, <4 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter2.double.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <4 x bfloat>, <4 x bfloat> } %pair, 0
  %dst1 = extractvalue { <4 x bfloat>, <4 x bfloat> } %pair, 1
  store <4 x bfloat> %dst0, ptr addrspace(1) %out0
  store <4 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_4x4x8(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter2.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_4x4x16(<8 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter4.v8f16(<8 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_4x2x16(<4 x half> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter2.v4f16(<4 x half> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_8x4x8(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_8x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v[0:1], v[2:3], v[0:3], s0 clamp
; GFX13-NEXT:    global_store_b64 v[6:7], v[0:1], off
; GFX13-NEXT:    global_store_b64 v[8:9], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <4 x bfloat>, <4 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter2.double.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_8X4X8 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <4 x bfloat>, <4 x bfloat> } %pair, 0
  %dst1 = extractvalue { <4 x bfloat>, <4 x bfloat> } %pair, 1
  store <4 x bfloat> %dst0, ptr addrspace(1) %out0
  store <4 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_4x4x8(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_4x4x8:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v[0:1], s0 aux_data:1 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter2.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X8 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_4x4x16(<8 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_4x4x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v4, v4, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v4
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v2, v3, v[0:3], s0 aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[6:7], v0, off
; GFX13-NEXT:    global_store_b32 v[8:9], v1, off
; GFX13-NEXT:    global_store_b32 v[10:11], v2, off
; GFX13-NEXT:    global_store_b32 v[12:13], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter4.v8bf16(<8 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X4X16 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_4x2x16(<4 x bfloat> %acc_in, i8 %scale, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_4x2x16:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_bfe_i32 v2, v2, 0, 8
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_dual_mov_b32 v5, v4 :: v_dual_mov_b32 v4, v3
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v2
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v[0:1], s0 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter2.v4bf16(<4 x bfloat> %acc_in, i8 %scale,
              ;   AUX_DATA: PIXEL_SHAPE_4X2X16 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

