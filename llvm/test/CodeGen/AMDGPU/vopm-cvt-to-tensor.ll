; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_cvt_to_tensor_i4_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i4.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f32 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f32(<4 x float> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i4.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f16.v8f16(<8 x half> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.f16.v4f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i4.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.bf16.v8bf16(<8 x bfloat> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i4_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i4_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i4_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i4.bf16.v4bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u4.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f32 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f32(<4 x float> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u4.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f16.v8f16(<8 x half> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.f16.v4f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u4.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.bf16.v8bf16(<8 x bfloat> %acc_in, i32 2, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u4_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u4_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u4_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u4.bf16.v4bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f32 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.f32.scatter2(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.f16.scatter2(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.i8.bf16.scatter2(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_i8_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_i8_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_i8_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.i8.bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f32 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.f32.scatter2(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.f16.scatter2(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.u8.bf16.scatter2(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_u8_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_u8_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_u8_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.u8.bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f32 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.f32.scatter2(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.f16.scatter2(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.fp8.bf16.scatter2(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_fp8_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_fp8_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_fp8_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.fp8.bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f32 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.f32.scatter2(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f32 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.f32.scatter2(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f32 v0, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.f32(<4 x float> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.f16.scatter2(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.f16.scatter2(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_f16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.f16(<4 x half> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v1, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.bf16.scatter2(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v1, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { i32, i32 } @llvm.amdgcn.cvt.to.tensor.bf8.bf16.scatter2(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { i32, i32 } %pair, 0
  %dst1 = extractvalue { i32, i32 } %pair, 1
  store i32 %dst0, ptr addrspace(1) %out0
  store i32 %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf8_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0) {
; GFX13-LABEL: test_cvt_to_tensor_bf8_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf8_bf16 v0, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    s_endpgm
bb:
  %dest = call i32 @llvm.amdgcn.cvt.to.tensor.bf8.bf16(<4 x bfloat> %acc_in, i32 0, i1 1)
  store i32 %dest, ptr addrspace(1) %out0
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f32 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f32.scatter4(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f32 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f32.scatter4(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f32 v0, v1, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f32.scatter2(<4 x float> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter4(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter4(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_f16 v0, v1, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.f16.scatter2(<4 x half> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter4(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x half>, <2 x half>, <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter4(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 0
  %dst1 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 1
  %dst2 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 2
  %dst3 = extractvalue { <2 x half>, <2 x half>, <2 x half>, <2 x half> } %quad, 3
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  store <2 x half> %dst2, ptr addrspace(1) %out2
  store <2 x half> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_f16_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_f16_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_f16_bf16 v0, v1, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x half>, <2 x half> } @llvm.amdgcn.cvt.to.tensor.f16.bf16.scatter2(<4 x bfloat> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x half>, <2 x half> } %pair, 0
  %dst1 = extractvalue { <2 x half>, <2 x half> } %pair, 1
  store <2 x half> %dst0, ptr addrspace(1) %out0
  store <2 x half> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f32_8x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f32_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f32 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f32.scatter4(<4 x float> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f32_4x4(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f32_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f32 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f32.scatter4(<4 x float> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f32_4x2(<4 x float> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f32_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f32 v0, v1, v[0:3] clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f32.scatter2(<4 x float> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_8x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter4(<8 x half> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_4x4(<8 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter4(<8 x half> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_f16_4x2(<4 x half> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_f16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_f16 v0, v1, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.f16.scatter2(<4 x half> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_8x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v2, v3, v[0:3] aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter4(<8 x bfloat> %acc_in, i32 4, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_4x4(<8 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1, ptr addrspace(1) %out2, ptr addrspace(1) %out3) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v2, v3, v[0:3] aux_data:2 clamp
; GFX13-NEXT:    global_store_b32 v[4:5], v0, off
; GFX13-NEXT:    global_store_b32 v[6:7], v1, off
; GFX13-NEXT:    global_store_b32 v[8:9], v2, off
; GFX13-NEXT:    global_store_b32 v[10:11], v3, off
; GFX13-NEXT:    s_endpgm
bb:
  %quad = call { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter4(<8 x bfloat> %acc_in, i32 2, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 1
  %dst2 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 2
  %dst3 = extractvalue { <2 x bfloat>, <2 x bfloat>, <2 x bfloat>, <2 x bfloat> } %quad, 3
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  store <2 x bfloat> %dst2, ptr addrspace(1) %out2
  store <2 x bfloat> %dst3, ptr addrspace(1) %out3
  ret void
}

define amdgpu_ps void @test_cvt_to_tensor_bf16_bf16_4x2(<4 x bfloat> %acc_in, ptr addrspace(1) %out0, ptr addrspace(1) %out1) {
; GFX13-LABEL: test_cvt_to_tensor_bf16_bf16_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_cvt_to_tensor_bf16_bf16 v0, v1, v[0:1] clamp
; GFX13-NEXT:    global_store_b32 v[2:3], v0, off
; GFX13-NEXT:    global_store_b32 v[4:5], v1, off
; GFX13-NEXT:    s_endpgm
bb:
  %pair = call { <2 x bfloat>, <2 x bfloat> } @llvm.amdgcn.cvt.to.tensor.bf16.bf16.scatter2(<4 x bfloat> %acc_in, i32 0, i1 1)
  %dst0 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 0
  %dst1 = extractvalue { <2 x bfloat>, <2 x bfloat> } %pair, 1
  store <2 x bfloat> %dst0, ptr addrspace(1) %out0
  store <2 x bfloat> %dst1, ptr addrspace(1) %out1
  ret void
}

