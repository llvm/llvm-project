; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx90a -mattr=+wavefrontsize64 -verify-machineinstrs -stop-after=finalize-isel -amdgpu-late-wave-transform=1 < %s | FileCheck -check-prefixes=GFX90A %s
; RUN: llc -O2 -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1200 -verify-machineinstrs -stop-after=phi-node-elimination -amdgpu-late-wave-transform=1 < %s | FileCheck -check-prefixes=GFX1200 %s
; RUN: llc -global-isel -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1200 -verify-machineinstrs -stop-after=finalize-isel -amdgpu-late-wave-transform=0 < %s | FileCheck -check-prefixes=GISEL %s

; Mixing uniform and divergent control-flow
; Mixing unifrom-i1 with divergent-i1
; __global__ void loop_mix_i1(int *filter,  int *out) {
;   int id = __builtin_amdgcn_workitem_id_x();
;   bool sel = false;
;   if (id > 5) {
;     int i = 0;
;     int x;
;     do {
;       x = filter[i];
;       sel = sel ^ (x < 7);
;       i += 16;
;     } while (x > 11);
;   } else
;     sel = (filter[id] > 2);
;   if (sel)
;     out[id] = id;
; }
define amdgpu_kernel void @loop_mix_i1(ptr addrspace(1) %filter.coerce, ptr addrspace(1) %out.coerce) {
  ; GFX90A-LABEL: name: loop_mix_i1
  ; GFX90A: bb.0.entry:
  ; GFX90A-NEXT:   successors: %bb.1(0x40000000), %bb.3(0x40000000)
  ; GFX90A-NEXT:   liveins: $vgpr0, $sgpr8_sgpr9
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[COPY:%[0-9]+]]:sgpr_64(p4) = COPY $sgpr8_sgpr9
  ; GFX90A-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32(s32) = COPY $vgpr0
  ; GFX90A-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM [[COPY]](p4), 0, 0 :: (dereferenceable invariant load (s128) from %ir.filter.coerce.kernarg.offset1, addrspace 4)
  ; GFX90A-NEXT:   [[COPY2:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub1
  ; GFX90A-NEXT:   [[COPY3:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub0
  ; GFX90A-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sreg_64 = REG_SEQUENCE killed [[COPY3]], %subreg.sub0, killed [[COPY2]], %subreg.sub1
  ; GFX90A-NEXT:   [[COPY4:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[REG_SEQUENCE]]
  ; GFX90A-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 1023
  ; GFX90A-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 [[COPY1]](s32), killed [[S_MOV_B32_]], implicit $exec
  ; GFX90A-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY [[V_AND_B32_e64_]]
  ; GFX90A-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 6
  ; GFX90A-NEXT:   [[V_CMP_LT_U32_e64_:%[0-9]+]]:sreg_64 = V_CMP_LT_U32_e64 [[V_AND_B32_e64_]], killed [[S_MOV_B32_1]], implicit $exec
  ; GFX90A-NEXT:   SI_BRCOND %bb.3, killed [[V_CMP_LT_U32_e64_]]
  ; GFX90A-NEXT:   S_BRANCH %bb.1
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.1.do.body.preheader:
  ; GFX90A-NEXT:   successors: %bb.2(0x80000000)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64 = S_MOV_B64 0
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.2.do.body:
  ; GFX90A-NEXT:   successors: %bb.2(0x7c000000), %bb.4(0x04000000)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[PHI:%[0-9]+]]:sreg_64 = PHI [[COPY4]], %bb.1, %6, %bb.2
  ; GFX90A-NEXT:   [[PHI1:%[0-9]+]]:sreg_64 = PHI [[S_MOV_B64_]], %bb.1, %5, %bb.2
  ; GFX90A-NEXT:   [[S_LOAD_DWORD_IMM:%[0-9]+]]:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM [[PHI]], 0, 0 :: ("amdgpu-noclobber" load (s32) from %ir.lsr.iv, addrspace 1)
  ; GFX90A-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 7
  ; GFX90A-NEXT:   S_CMP_LT_I32 [[S_LOAD_DWORD_IMM]], killed [[S_MOV_B32_2]], implicit-def $scc
  ; GFX90A-NEXT:   [[S_CSELECT_B64_:%[0-9]+]]:sreg_64_xexec = S_CSELECT_B64 -1, 0, implicit $scc
  ; GFX90A-NEXT:   [[COPY6:%[0-9]+]]:sreg_64 = COPY [[S_CSELECT_B64_]]
  ; GFX90A-NEXT:   [[S_XOR_B64_:%[0-9]+]]:sreg_64_xexec = S_XOR_B64 [[PHI1]], killed [[COPY6]], implicit-def dead $scc
  ; GFX90A-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 11
  ; GFX90A-NEXT:   S_CMP_GT_I32 [[S_LOAD_DWORD_IMM]], killed [[S_MOV_B32_3]], implicit-def $scc
  ; GFX90A-NEXT:   [[S_CSELECT_B64_1:%[0-9]+]]:sreg_64_xexec = S_CSELECT_B64 -1, 0, implicit $scc
  ; GFX90A-NEXT:   [[COPY7:%[0-9]+]]:sreg_64 = COPY [[S_CSELECT_B64_1]]
  ; GFX90A-NEXT:   [[S_MOV_B64_1:%[0-9]+]]:sreg_64 = S_MOV_B64 64
  ; GFX90A-NEXT:   [[COPY8:%[0-9]+]]:sreg_32 = COPY [[PHI]].sub0
  ; GFX90A-NEXT:   [[COPY9:%[0-9]+]]:sreg_32 = COPY [[PHI]].sub1
  ; GFX90A-NEXT:   [[COPY10:%[0-9]+]]:sreg_32 = COPY [[S_MOV_B64_1]].sub0
  ; GFX90A-NEXT:   [[COPY11:%[0-9]+]]:sreg_32 = COPY [[S_MOV_B64_1]].sub1
  ; GFX90A-NEXT:   [[S_ADD_U32_:%[0-9]+]]:sreg_32 = S_ADD_U32 [[COPY8]], [[COPY10]], implicit-def $scc
  ; GFX90A-NEXT:   [[S_ADDC_U32_:%[0-9]+]]:sreg_32 = S_ADDC_U32 [[COPY9]], [[COPY11]], implicit-def $scc, implicit $scc
  ; GFX90A-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:sreg_64 = REG_SEQUENCE [[S_ADD_U32_]], %subreg.sub0, [[S_ADDC_U32_]], %subreg.sub1
  ; GFX90A-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[S_XOR_B64_]], implicit $exec
  ; GFX90A-NEXT:   SI_BRCOND_UNIFORM %bb.2, killed [[COPY7]]
  ; GFX90A-NEXT:   S_BRANCH %bb.4
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.3.if.else:
  ; GFX90A-NEXT:   successors: %bb.4(0x80000000)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX90A-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[S_MOV_B32_4]], [[COPY5]], implicit $exec
  ; GFX90A-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR [[COPY4]], killed [[V_LSHLREV_B32_e64_]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GFX90A-NEXT:   [[V_CMP_GT_I32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_GT_I32_e64 killed [[GLOBAL_LOAD_DWORD_SADDR]], [[S_MOV_B32_4]], implicit $exec
  ; GFX90A-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, [[V_CMP_GT_I32_e64_]], implicit $exec
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.4.if.end:
  ; GFX90A-NEXT:   successors: %bb.5(0x40000000), %bb.6(0x40000000)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[V_CNDMASK_B32_e64_1]], %bb.3, [[V_CNDMASK_B32_e64_]], %bb.2
  ; GFX90A-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 [[PHI2]], 0, implicit $exec
  ; GFX90A-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, 1, [[V_CMP_NE_U32_e64_]], implicit $exec
  ; GFX90A-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32 = S_MOV_B32 1
  ; GFX90A-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_64 = V_CMP_NE_U32_e64 killed [[V_CNDMASK_B32_e64_2]], killed [[S_MOV_B32_5]], implicit $exec
  ; GFX90A-NEXT:   SI_BRCOND %bb.6, killed [[V_CMP_NE_U32_e64_1]]
  ; GFX90A-NEXT:   S_BRANCH %bb.5
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.5.if.then11:
  ; GFX90A-NEXT:   successors: %bb.6(0x80000000)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT:   [[COPY12:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub3
  ; GFX90A-NEXT:   [[COPY13:%[0-9]+]]:sreg_32 = COPY [[S_LOAD_DWORDX4_IMM]].sub2
  ; GFX90A-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:sreg_64_xexec_xnull = REG_SEQUENCE killed [[COPY13]], %subreg.sub0, killed [[COPY12]], %subreg.sub1
  ; GFX90A-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GFX90A-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 killed [[S_MOV_B32_6]], [[COPY5]], implicit $exec
  ; GFX90A-NEXT:   GLOBAL_STORE_DWORD_SADDR killed [[V_LSHLREV_B32_e64_1]], [[COPY5]], killed [[REG_SEQUENCE2]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GFX90A-NEXT: {{  $}}
  ; GFX90A-NEXT: bb.6.if.end14:
  ; GFX90A-NEXT:   S_ENDPGM 0
  ;
  ; GFX1200-LABEL: name: loop_mix_i1
  ; GFX1200: bb.0.entry:
  ; GFX1200-NEXT:   successors: %bb.1(0x40000000), %bb.4(0x40000000)
  ; GFX1200-NEXT:   liveins: $vgpr0, $sgpr4_sgpr5
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[COPY:%[0-9]+]]:sgpr_64(p4) = COPY killed $sgpr4_sgpr5
  ; GFX1200-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32(s32) = COPY killed $vgpr0
  ; GFX1200-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM killed [[COPY]](p4), 0, 0 :: (dereferenceable invariant load (s128) from %ir.filter.coerce.kernarg.offset1, addrspace 4)
  ; GFX1200-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sreg_64_xexec_xnull = REG_SEQUENCE [[S_LOAD_DWORDX4_IMM]].sub0, %subreg.sub0, [[S_LOAD_DWORDX4_IMM]].sub1, %subreg.sub1
  ; GFX1200-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[REG_SEQUENCE]]
  ; GFX1200-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1023, killed [[COPY1]](s32), implicit $exec
  ; GFX1200-NEXT:   [[V_CMP_GT_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_U32_e64 6, [[V_AND_B32_e64_]], implicit $exec
  ; GFX1200-NEXT:   SI_BRCOND %bb.4, killed [[V_CMP_GT_U32_e64_]]
  ; GFX1200-NEXT:   S_BRANCH %bb.1
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.1.do.body.preheader:
  ; GFX1200-NEXT:   successors: %bb.2(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GFX1200-NEXT:   [[COPY3:%[0-9]+]]:sreg_64 = COPY killed [[COPY2]]
  ; GFX1200-NEXT:   [[COPY4:%[0-9]+]]:sreg_32 = COPY killed [[S_MOV_B32_]]
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.2.do.body:
  ; GFX1200-NEXT:   successors: %bb.2(0x7c000000), %bb.3(0x04000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[COPY5:%[0-9]+]]:sreg_64 = COPY killed [[COPY3]]
  ; GFX1200-NEXT:   [[S_LOAD_DWORD_IMM:%[0-9]+]]:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM [[COPY5]], 0, 0 :: ("amdgpu-noclobber" load (s32) from %ir.lsr.iv, addrspace 1)
  ; GFX1200-NEXT:   S_CMP_LT_I32 [[S_LOAD_DWORD_IMM]], 7, implicit-def $scc
  ; GFX1200-NEXT:   [[S_CSELECT_B32_:%[0-9]+]]:sreg_32_xm0_xexec = S_CSELECT_B32 -1, 0, implicit killed $scc
  ; GFX1200-NEXT:   [[COPY6:%[0-9]+]]:sreg_32 = COPY killed [[COPY4]]
  ; GFX1200-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32_xm0_xexec = S_XOR_B32 killed [[COPY6]], killed [[S_CSELECT_B32_]], implicit-def dead $scc
  ; GFX1200-NEXT:   S_CMP_GT_I32 killed [[S_LOAD_DWORD_IMM]], 11, implicit-def $scc
  ; GFX1200-NEXT:   [[S_CSELECT_B32_1:%[0-9]+]]:sreg_32_xm0_xexec = S_CSELECT_B32 -1, 0, implicit killed $scc
  ; GFX1200-NEXT:   [[S_ADD_U64_:%[0-9]+]]:sreg_64 = S_ADD_U64 killed [[COPY5]], 64
  ; GFX1200-NEXT:   [[COPY3:%[0-9]+]]:sreg_64 = COPY killed [[S_ADD_U64_]]
  ; GFX1200-NEXT:   [[COPY4:%[0-9]+]]:sreg_32 = COPY [[S_XOR_B32_]]
  ; GFX1200-NEXT:   SI_BRCOND_UNIFORM %bb.2, killed [[S_CSELECT_B32_1]]
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.3:
  ; GFX1200-NEXT:   successors: %bb.5(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, killed [[S_XOR_B32_]], implicit $exec
  ; GFX1200-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY killed [[V_CNDMASK_B32_e64_]]
  ; GFX1200-NEXT:   S_BRANCH %bb.5
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.4.if.else:
  ; GFX1200-NEXT:   successors: %bb.5(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 2, [[V_AND_B32_e64_]], implicit $exec
  ; GFX1200-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR killed [[REG_SEQUENCE]], killed [[V_LSHLREV_B32_e64_]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GFX1200-NEXT:   [[V_CMP_LT_I32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LT_I32_e64 2, killed [[GLOBAL_LOAD_DWORD_SADDR]], implicit $exec
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, -1, killed [[V_CMP_LT_I32_e64_]], implicit $exec
  ; GFX1200-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY killed [[V_CNDMASK_B32_e64_1]]
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.5.if.end:
  ; GFX1200-NEXT:   successors: %bb.6(0x40000000), %bb.7(0x40000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY killed [[COPY7]]
  ; GFX1200-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_NE_U32_e64 0, killed [[COPY8]], implicit $exec
  ; GFX1200-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, 1, killed [[V_CMP_NE_U32_e64_]], implicit $exec
  ; GFX1200-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 1, killed [[V_CNDMASK_B32_e64_2]], implicit $exec
  ; GFX1200-NEXT:   SI_BRCOND %bb.7, killed [[V_CMP_NE_U32_e64_1]]
  ; GFX1200-NEXT:   S_BRANCH %bb.6
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.6.if.then11:
  ; GFX1200-NEXT:   successors: %bb.7(0x80000000)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:sreg_64_xexec_xnull = REG_SEQUENCE killed [[S_LOAD_DWORDX4_IMM]].sub2, %subreg.sub0, [[S_LOAD_DWORDX4_IMM]].sub3, %subreg.sub1
  ; GFX1200-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 2, [[V_AND_B32_e64_]], implicit $exec
  ; GFX1200-NEXT:   GLOBAL_STORE_DWORD_SADDR killed [[V_LSHLREV_B32_e64_1]], killed [[V_AND_B32_e64_]], killed [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GFX1200-NEXT: {{  $}}
  ; GFX1200-NEXT: bb.7.if.end14:
  ; GFX1200-NEXT:   S_ENDPGM 0
  ;
  ; GISEL-LABEL: name: loop_mix_i1
  ; GISEL: bb.1.entry:
  ; GISEL-NEXT:   successors: %bb.6(0x40000000), %bb.2(0x40000000)
  ; GISEL-NEXT:   liveins: $sgpr4_sgpr5, $vgpr0
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[COPY:%[0-9]+]]:sreg_64 = COPY $sgpr4_sgpr5
  ; GISEL-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; GISEL-NEXT:   [[DEF:%[0-9]+]]:sreg_32 = IMPLICIT_DEF
  ; GISEL-NEXT:   [[S_LOAD_DWORDX4_IMM:%[0-9]+]]:sgpr_128 = S_LOAD_DWORDX4_IMM [[COPY]], 0, 0 :: (dereferenceable invariant load (<2 x s64>) from %ir.filter.coerce.kernarg.offset1, addrspace 4)
  ; GISEL-NEXT:   [[COPY2:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub0_sub1
  ; GISEL-NEXT:   [[COPY3:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub2_sub3
  ; GISEL-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 1023
  ; GISEL-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_]]
  ; GISEL-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 [[COPY1]], [[COPY4]], implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 5
  ; GISEL-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_1]]
  ; GISEL-NEXT:   [[V_CMP_LE_U32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LE_U32_e64 [[V_AND_B32_e64_]], [[COPY5]], implicit $exec
  ; GISEL-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 1, [[DEF]], implicit-def dead $scc
  ; GISEL-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 0, [[S_AND_B32_]], implicit $exec
  ; GISEL-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec = SI_IF [[V_CMP_LE_U32_e64_]], %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.6
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.2.Flow4:
  ; GISEL-NEXT:   successors: %bb.3(0x40000000), %bb.4(0x40000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI:%[0-9]+]]:sreg_32 = PHI [[V_CMP_NE_U32_e64_]], %bb.1, %75, %bb.6
  ; GISEL-NEXT:   [[COPY6:%[0-9]+]]:sreg_32_xm0_xexec = COPY [[PHI]]
  ; GISEL-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32_xm0_xexec = SI_ELSE [[SI_IF]], %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.3
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.3.do.body.preheader:
  ; GISEL-NEXT:   successors: %bb.5(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; GISEL-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_2]]
  ; GISEL-NEXT:   S_BRANCH %bb.5
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.4.Flow5:
  ; GISEL-NEXT:   successors: %bb.8(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI1:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[PHI]], %bb.2, %82, %bb.7
  ; GISEL-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.8
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.5.do.body:
  ; GISEL-NEXT:   successors: %bb.7(0x04000000), %bb.5(0x7c000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI2:%[0-9]+]]:vreg_64 = PHI %37, %bb.5, [[COPY2]], %bb.3
  ; GISEL-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI %103, %bb.5, [[COPY7]], %bb.3
  ; GISEL-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[PHI2]], 0, 0, implicit $exec :: ("amdgpu-noclobber" load (s32) from %ir.lsr.iv, addrspace 1)
  ; GISEL-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 7
  ; GISEL-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_3]]
  ; GISEL-NEXT:   [[V_CMP_LT_I32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_LT_I32_e64 [[GLOBAL_LOAD_DWORD]], [[COPY8]], implicit $exec
  ; GISEL-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 1, implicit $exec
  ; GISEL-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; GISEL-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MOV_B32_e32_1]], 0, [[V_MOV_B32_e32_]], [[V_CMP_LT_I32_e64_]], implicit $exec
  ; GISEL-NEXT:   [[V_XOR_B32_e64_:%[0-9]+]]:vgpr_32 = V_XOR_B32_e64 [[PHI3]], [[V_CNDMASK_B32_e64_]], implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32 = S_MOV_B32 11
  ; GISEL-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_4]]
  ; GISEL-NEXT:   [[V_CMP_LE_I32_e64_:%[0-9]+]]:sreg_32 = V_CMP_LE_I32_e64 [[GLOBAL_LOAD_DWORD]], [[COPY9]], implicit $exec
  ; GISEL-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64 = S_MOV_B64 64
  ; GISEL-NEXT:   [[COPY10:%[0-9]+]]:vreg_64 = COPY [[S_MOV_B64_]]
  ; GISEL-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY [[PHI2]].sub0
  ; GISEL-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY [[COPY10]].sub0
  ; GISEL-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY [[PHI2]].sub1
  ; GISEL-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY [[COPY10]].sub1
  ; GISEL-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[COPY11]], [[COPY12]], 0, implicit $exec
  ; GISEL-NEXT:   [[V_ADDC_U32_e64_:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADDC_U32_e64 [[COPY13]], [[COPY14]], killed [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; GISEL-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, [[V_ADDC_U32_e64_]], %subreg.sub1
  ; GISEL-NEXT:   $vcc_lo = COPY [[V_CMP_LE_I32_e64_]]
  ; GISEL-NEXT:   S_CBRANCH_VCCNZ %bb.7, implicit $vcc_lo
  ; GISEL-NEXT:   S_BRANCH %bb.5
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.6.if.else:
  ; GISEL-NEXT:   successors: %bb.2(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_5]]
  ; GISEL-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[COPY15]], [[V_AND_B32_e64_]], implicit $exec
  ; GISEL-NEXT:   [[COPY16:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[COPY2]]
  ; GISEL-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR [[COPY16]], [[V_LSHLREV_B32_e64_]], 0, 0, implicit $exec :: (load (s32) from %ir.arrayidx7, addrspace 1)
  ; GISEL-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_6]]
  ; GISEL-NEXT:   [[V_CMP_GT_I32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_I32_e64 [[GLOBAL_LOAD_DWORD_SADDR]], [[COPY17]], implicit $exec
  ; GISEL-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[V_CMP_NE_U32_e64_]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32 = S_AND_B32 $exec_lo, [[V_CMP_GT_I32_e64_]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_]], [[S_AND_B32_1]], implicit-def $scc
  ; GISEL-NEXT:   S_BRANCH %bb.2
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.7.Flow:
  ; GISEL-NEXT:   successors: %bb.4(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI [[V_XOR_B32_e64_]], %bb.5
  ; GISEL-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[PHI4]], implicit $exec
  ; GISEL-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
  ; GISEL-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32_xm0_xexec = S_ANDN2_B32 [[COPY6]], $exec_lo, implicit-def $scc
  ; GISEL-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32_xm0_xexec = S_AND_B32 $exec_lo, [[V_CMP_NE_U32_e64_1]], implicit-def $scc
  ; GISEL-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32_xm0_xexec = S_OR_B32 [[S_ANDN2_B32_1]], [[S_AND_B32_2]], implicit-def $scc
  ; GISEL-NEXT:   S_BRANCH %bb.4
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.8.if.end:
  ; GISEL-NEXT:   successors: %bb.9(0x40000000), %bb.10(0x40000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32_xm0_xexec = SI_IF [[PHI1]], %bb.10, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_BRANCH %bb.9
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.9.if.then11:
  ; GISEL-NEXT:   successors: %bb.10(0x80000000)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT:   [[COPY18:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub0_sub1
  ; GISEL-NEXT:   [[COPY19:%[0-9]+]]:sreg_64 = COPY [[S_LOAD_DWORDX4_IMM]].sub2_sub3
  ; GISEL-NEXT:   [[COPY20:%[0-9]+]]:sreg_64_xexec_xnull = COPY [[COPY19]]
  ; GISEL-NEXT:   [[S_MOV_B32_7:%[0-9]+]]:sreg_32 = S_MOV_B32 2
  ; GISEL-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_7]]
  ; GISEL-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e64 [[COPY21]], [[V_AND_B32_e64_]], implicit $exec
  ; GISEL-NEXT:   GLOBAL_STORE_DWORD_SADDR [[V_LSHLREV_B32_e64_1]], [[V_AND_B32_e64_]], [[COPY20]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx13, addrspace 1)
  ; GISEL-NEXT: {{  $}}
  ; GISEL-NEXT: bb.10.if.end14:
  ; GISEL-NEXT:   SI_END_CF [[SI_IF1]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; GISEL-NEXT:   S_ENDPGM 0
entry:
  %id = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
  %cmp = icmp samesign ugt i32 %id, 5
  br i1 %cmp, label %do.body, label %if.else

do.body:                                          ; preds = %entry, %do.body
  %i.0 = phi i32 [ %add, %do.body ], [ 0, %entry ]
  %sel.0.off0 = phi i1 [ %xor22, %do.body ], [ false, %entry ]
  %idxprom = zext nneg i32 %i.0 to i64
  %arrayidx = getelementptr inbounds nuw i32, ptr addrspace(1) %filter.coerce, i64 %idxprom
  %load = load i32, ptr addrspace(1) %arrayidx, align 4
  %cmp3 = icmp slt i32 %load, 7
  %xor22 = xor i1 %sel.0.off0, %cmp3
  %add = add nuw nsw i32 %i.0, 16
  %cmp5 = icmp sgt i32 %load, 11
  br i1 %cmp5, label %do.body, label %if.end

if.else:                                          ; preds = %entry
  %idxprom6 = zext nneg i32 %id to i64
  %arrayidx7 = getelementptr inbounds nuw i32, ptr addrspace(1) %filter.coerce, i64 %idxprom6
  %load2 = load i32, ptr addrspace(1) %arrayidx7, align 4
  %cmp8 = icmp sgt i32 %load2, 2
  br label %if.end

if.end:                                           ; preds = %do.body, %if.else
  %sel.1.in = phi i1 [ %cmp8, %if.else ], [ %xor22, %do.body ]
  br i1 %sel.1.in, label %if.then11, label %if.end14

if.then11:                                        ; preds = %if.end
  %idxprom12 = zext nneg i32 %id to i64
  %arrayidx13 = getelementptr inbounds nuw i32, ptr addrspace(1) %out.coerce, i64 %idxprom12
  store i32 %id, ptr addrspace(1) %arrayidx13, align 4
  br label %if.end14

if.end14:                                         ; preds = %if.then11, %if.end
  ret void
}

declare noundef i32 @llvm.amdgcn.workitem.id.x()

