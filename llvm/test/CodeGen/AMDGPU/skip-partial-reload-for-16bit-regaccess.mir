# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
# RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1100 -stress-regalloc=3 -run-pass=greedy -verify-machineinstrs %s -o - | FileCheck %s

# Do not insert partial reloads for lo16 and hi16 accesses. They must remain full reloads of their
# original register (be it a 32-bit or any larger tuple) before each use. We need to specifiy the
# 32-bit stack offset in the spill/reload instructions and the register allocator currently cannot
# reliably assign the expected 16-bit slices (lo16 or hi16) of a physical register if we introduce
# 16-bit reloads.

# This test ensures the spill reloads inserted during RA should always reload the full register
# before their 16-bit register uses.

---
name: reload_v32_for_lo16_use
tracksRegLiveness: true
machineFunctionInfo:
  scratchRSrcReg:  $sgpr0_sgpr1_sgpr2_sgpr3
  frameOffsetReg:  $sgpr33
  stackPtrOffsetReg:  $sgpr32
body:             |
  bb.0:
    liveins: $vgpr0, $sgpr0

    ; CHECK-LABEL: name: reload_v32_for_lo16_use
    ; CHECK: liveins: $vgpr0, $sgpr0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[V_MOV_B:%[0-9]+]]:vreg_64 = V_MOV_B64_PSEUDO 0, implicit $exec
    ; CHECK-NEXT: undef [[GLOBAL_LOAD_UBYTE_D16_t16_:%[0-9]+]].lo16:vgpr_32 = GLOBAL_LOAD_UBYTE_D16_t16 [[V_MOV_B]], 0, 0, implicit $exec
    ; CHECK-NEXT: SI_SPILL_V32_SAVE [[GLOBAL_LOAD_UBYTE_D16_t16_]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.0, addrspace 5)
    ; CHECK-NEXT: [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1023, $vgpr0, implicit $exec
    ; CHECK-NEXT: [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
    ; CHECK-NEXT: DS_WRITE_B32_gfx9 [[V_MOV_B32_e32_]], [[V_MOV_B32_e32_]], 0, 0, implicit $exec
    ; CHECK-NEXT: [[V_MOV_B16_t16_e64_:%[0-9]+]]:vgpr_16 = V_MOV_B16_t16_e64 0, 0, 0, implicit $exec
    ; CHECK-NEXT: [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e64 1, [[V_AND_B32_e64_]], implicit $exec
    ; CHECK-NEXT: DS_WRITE_B16_t16 [[V_LSHLREV_B32_e64_]], [[V_MOV_B16_t16_e64_]], 0, 0, implicit $exec
    ; CHECK-NEXT: dead [[DS_READ_U16_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 [[V_MOV_B32_e32_]], 0, 0, implicit $exec
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.0, addrspace 5)
    ; CHECK-NEXT: DS_WRITE_B8_t16 [[V_MOV_B32_e32_]], [[SI_SPILL_V32_RESTORE]].lo16, 0, 0, implicit $exec
    ; CHECK-NEXT: S_ENDPGM 0
    %1:vreg_64 = V_MOV_B64_PSEUDO 0, implicit $exec
    undef %3.lo16:vgpr_32 = GLOBAL_LOAD_UBYTE_D16_t16 %1:vreg_64, 0, 0, implicit $exec
    %2:vgpr_32 = V_AND_B32_e64 1023, $vgpr0, implicit $exec
    %4:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
    DS_WRITE_B32_gfx9 %4:vgpr_32, %4:vgpr_32, 0, 0, implicit $exec
    %5:vgpr_16 = V_MOV_B16_t16_e64 0, 0, 0, implicit $exec
    %6:vgpr_32 = V_LSHLREV_B32_e64 1, %2:vgpr_32, implicit $exec
    DS_WRITE_B16_t16 %6:vgpr_32, %5:vgpr_16, 0, 0, implicit $exec
    %7:vgpr_32 = DS_READ_U16_gfx9 %4:vgpr_32, 0, 0, implicit $exec
    DS_WRITE_B8_t16 %4:vgpr_32, %3.lo16:vgpr_32, 0, 0, implicit $exec
    S_ENDPGM 0
...

---
name: reload_v64_for_sub1_hi16_use
tracksRegLiveness: true
machineFunctionInfo:
  scratchRSrcReg:  $sgpr0_sgpr1_sgpr2_sgpr3
  frameOffsetReg:  $sgpr33
  stackPtrOffsetReg:  $sgpr32
body:             |
  bb.0:
    liveins: $sgpr0

    ; CHECK-LABEL: name: reload_v64_for_sub1_hi16_use
    ; CHECK: liveins: $sgpr0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[V_MOV_B:%[0-9]+]]:vreg_64 = V_MOV_B64_PSEUDO 0, implicit $exec
    ; CHECK-NEXT: [[DEF:%[0-9]+]]:vreg_128 = IMPLICIT_DEF
    ; CHECK-NEXT: undef [[GLOBAL_LOAD_UBYTE_D16_t16_:%[0-9]+]].sub1_hi16:vreg_64 = GLOBAL_LOAD_UBYTE_D16_t16 [[V_MOV_B]], 0, 0, implicit $exec
    ; CHECK-NEXT: SI_SPILL_V64_SAVE [[GLOBAL_LOAD_UBYTE_D16_t16_]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
    ; CHECK-NEXT: [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
    ; CHECK-NEXT: DS_WRITE_B32_gfx9 [[V_MOV_B32_e32_]], [[V_MOV_B32_e32_]], 0, 0, implicit $exec
    ; CHECK-NEXT: [[V_MOV_B16_t16_e64_:%[0-9]+]]:vgpr_16 = V_MOV_B16_t16_e64 0, 0, 0, implicit $exec
    ; CHECK-NEXT: [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e64 1, [[DEF]].sub1, implicit $exec
    ; CHECK-NEXT: DS_WRITE_B16_t16 [[V_LSHLREV_B32_e64_]], [[V_MOV_B16_t16_e64_]], 0, 0, implicit $exec
    ; CHECK-NEXT: dead [[DS_READ_U16_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 [[V_MOV_B32_e32_]], 0, 0, implicit $exec
    ; CHECK-NEXT: [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
    ; CHECK-NEXT: DS_WRITE_B8_t16 [[V_MOV_B32_e32_]], [[SI_SPILL_V64_RESTORE]].sub1_hi16, 0, 0, implicit $exec
    ; CHECK-NEXT: S_ENDPGM 0
    %1:vreg_64 = V_MOV_B64_PSEUDO 0, implicit $exec
    %2:vreg_128 = IMPLICIT_DEF
    undef %3.sub1_hi16:vreg_64 = GLOBAL_LOAD_UBYTE_D16_t16 %1:vreg_64, 0, 0, implicit $exec
    %4:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
    DS_WRITE_B32_gfx9 %4:vgpr_32, %4:vgpr_32, 0, 0, implicit $exec
    %5:vgpr_16 = V_MOV_B16_t16_e64 0, 0, 0, implicit $exec
    %6:vgpr_32 = V_LSHLREV_B32_e64 1, %2.sub1:vreg_128, implicit $exec
    DS_WRITE_B16_t16 %6:vgpr_32, %5:vgpr_16, 0, 0, implicit $exec
    %7:vgpr_32 = DS_READ_U16_gfx9 %4:vgpr_32, 0, 0, implicit $exec
    DS_WRITE_B8_t16 %4:vgpr_32, %3.sub1_hi16:vreg_64, 0, 0, implicit $exec
    S_ENDPGM 0
...
