; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt --si-annotate-control-flow -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx942 < %s | FileCheck %s

declare i32 @llvm.amdgcn.workitem.id.x()
declare i32 @llvm.amdgcn.workitem.id.y()

declare { i1, i64 } @llvm.amdgcn.if.i64(i1, i1)

; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_even(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2:[0-9]+]] !reqd_work_group_size [[META0:![0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_even_ann_cf(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ann_cf(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[CF_VAL:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[PSEUDO_COND:%.*]] = extractvalue { i1, i64 } [[CF_VAL]], 0
; CHECK-NEXT:    [[MASK:%.*]] = extractvalue { i1, i64 } [[CF_VAL]], 1
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[PSEUDO_COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[MASK]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  %cf_val = call { i1, i64 } @llvm.amdgcn.if.i64(i1 %cond, i1 true)
  %pseudo.cond = extractvalue { i1, i64 } %cf_val, 0
  %mask = extractvalue { i1, i64 } %cf_val, 1
  br i1 %pseudo.cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  call void @llvm.amdgcn.end.cf.i64(i64 %mask)
  ret void
}


; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_complex1(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_complex1(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:    [[TID_X:%.*]] = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[K:%.*]] = lshr i32 [[TID]], 4
; CHECK-NEXT:    [[COND:%.*]] = icmp ult i32 [[K]], 15
; CHECK-NEXT:    [[TMP1:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP1]], 0
; CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { i1, i64 } [[TMP1]], 1
; CHECK-NEXT:    br i1 [[TMP2]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP3]])
; CHECK-NEXT:    ret void
;
  %tid.x = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %k = lshr i32 %tid, 4
  %cond = icmp ult i32 %k, 15
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_complex2(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_complex2(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:    [[TID_X:%.*]] = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[J:%.*]] = and i32 [[TID]], 15
; CHECK-NEXT:    [[COND:%.*]] = icmp ult i32 [[J]], 15
; CHECK-NEXT:    [[TMP1:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP1]], 0
; CHECK-NEXT:    [[TMP3:%.*]] = extractvalue { i1, i64 } [[TMP1]], 1
; CHECK-NEXT:    br i1 [[TMP2]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP3]])
; CHECK-NEXT:    ret void
;
  %tid.x = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %j = and i32 %tid, 15
  %cond = icmp ult i32 %j, 15
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_even_only_reqd_wgsz(ptr addrspace(1) inreg %dest) !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_only_reqd_wgsz(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR3:[0-9]+]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent:
define amdgpu_kernel void @cond_store_even_only_flat_wgsz(ptr addrspace(1) inreg %dest) #0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_only_flat_wgsz(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent, since the y dimension varies in each
; wavefront with the required work group size:
define amdgpu_kernel void @cond_store_even_ydim_small_wgs(ptr addrspace(1) inreg %dest) !reqd_work_group_size !1 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ydim_small_wgs(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR3]] !reqd_work_group_size [[META1:![0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 3
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID_Y]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 3
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid.y, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent, even though there are no attributes with
; work group size information:
define amdgpu_kernel void @cond_store_even_no_attributes(ptr addrspace(1) inreg %dest) {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_no_attributes(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR3]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent, even though the condition only depends on a
; workitem id dimension that does not vary per wavefront (namely y):
define amdgpu_kernel void @cond_store_even_ydim(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ydim(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID_Y]], 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid.y, 1
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is not likely dynamically divergent, because its condition is directly
; loaded from memory:
define amdgpu_kernel void @cond_store_loaded(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loaded(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LOOKUP_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[LOOKUP]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_VALUE:%.*]] = load i32, ptr addrspace(1) [[LOOKUP_ADDR]], align 4
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LOOKUP_VALUE]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lookup.addr = getelementptr i32, ptr addrspace(1) %lookup, i64 %tid.ext
  %lookup.value = load i32, ptr addrspace(1) %lookup.addr
  %cond = icmp eq i32 %lookup.value, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The branch here is likely dynamically divergent, even though it only uses the workitem id through a PHI:
define amdgpu_kernel void @cond_store_loop_phi(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup, i32 %n) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loop_phi(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]], i32 [[N:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[VAL:%.*]] = phi i32 [ [[VAL_INC:%.*]], %[[LOOP]] ], [ [[TID]], %[[ENTRY]] ]
; CHECK-NEXT:    [[IDX:%.*]] = phi i32 [ [[IDX_DEC:%.*]], %[[LOOP]] ], [ [[N]], %[[ENTRY]] ]
; CHECK-NEXT:    [[VAL_INC]] = add i32 [[VAL]], 1
; CHECK-NEXT:    [[IDX_DEC]] = sub i32 [[IDX]], 1
; CHECK-NEXT:    [[LOOP_COND:%.*]] = icmp eq i32 [[IDX_DEC]], 0
; CHECK-NEXT:    br i1 [[LOOP_COND]], label %[[LOOP_END:.*]], label %[[LOOP]]
; CHECK:       [[LOOP_END]]:
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[VAL]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  br label %loop
loop:
  %val = phi i32 [%val.inc, %loop], [%tid, %entry]
  %idx = phi i32 [%idx.dec, %loop], [%n, %entry]
  %val.inc = add i32 %val, 1
  %idx.dec = sub i32 %idx, 1
  %loop.cond = icmp eq i32 %idx.dec, 0
  br i1 %loop.cond, label %loop.end, label %loop
loop.end:
  %cond = icmp eq i32 %val, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}

; The branch here is not likely dynamically divergent, since it doesn't use the workitem id:
define amdgpu_kernel void @cond_store_loop_unrelated_phi(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup, i32 %n) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loop_unrelated_phi(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]], i32 [[N:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LOOKUP_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[LOOKUP]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_VALUE:%.*]] = load i32, ptr addrspace(1) [[LOOKUP_ADDR]], align 4
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[VAL:%.*]] = phi i32 [ [[VAL_INC:%.*]], %[[LOOP]] ], [ [[LOOKUP_VALUE]], %[[ENTRY]] ]
; CHECK-NEXT:    [[IDX:%.*]] = phi i32 [ [[IDX_DEC:%.*]], %[[LOOP]] ], [ [[N]], %[[ENTRY]] ]
; CHECK-NEXT:    [[VAL_INC]] = add i32 [[VAL]], 1
; CHECK-NEXT:    [[IDX_DEC]] = sub i32 [[IDX]], 1
; CHECK-NEXT:    [[LOOP_COND:%.*]] = icmp eq i32 [[IDX_DEC]], 0
; CHECK-NEXT:    br i1 [[LOOP_COND]], label %[[LOOP_END:.*]], label %[[LOOP]]
; CHECK:       [[LOOP_END]]:
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[VAL]], 20
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lookup.addr = getelementptr i32, ptr addrspace(1) %lookup, i64 %tid.ext
  %lookup.value = load i32, ptr addrspace(1) %lookup.addr
  br label %loop
loop:
  %val = phi i32 [%val.inc, %loop], [%lookup.value, %entry]
  %idx = phi i32 [%idx.dec, %loop], [%n, %entry]
  %val.inc = add i32 %val, 1
  %idx.dec = sub i32 %idx, 1
  %loop.cond = icmp eq i32 %idx.dec, 0
  br i1 %loop.cond, label %loop.end, label %loop
loop.end:
  %cond = icmp eq i32 %val, 20
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}

; The branch here is likely dynamically divergent, even though it only uses the workitem id through a select:
define amdgpu_kernel void @cond_store_select_cond(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup, i32 %n) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_select_cond(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]], i32 [[N:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[COND_SELECT:%.*]] = icmp eq i32 [[TID]], 13
; CHECK-NEXT:    [[VAL:%.*]] = select i1 [[COND_SELECT]], i32 0, i32 1
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[VAL]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %cond.select = icmp eq i32 %tid, 13
  %val = select i1 %cond.select, i32 0, i32 1
  %cond = icmp eq i32 %val, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}

; The branch here is likely dynamically divergent, even though it only uses the workitem id through a select:
define amdgpu_kernel void @cond_store_select_val(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup, i32 %n) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_select_val(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]], i32 [[N:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[COND_SELECT:%.*]] = icmp eq i32 [[N]], 13
; CHECK-NEXT:    [[VAL:%.*]] = select i1 [[COND_SELECT]], i32 0, i32 [[TID]]
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[VAL]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE]]:
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP2]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %cond.select = icmp eq i32 %n, 13
  %val = select i1 %cond.select, i32 0, i32 %tid
  %cond = icmp eq i32 %val, 0
  br i1 %cond, label %do.store, label %exit
do.store:
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  store i32 0, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}

; The then and else branches are likely dynamically divergent here:
define amdgpu_kernel void @cond_store_even_ifelse(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_even_ifelse(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LSBIT:%.*]] = and i32 [[TID]], 1
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LSBIT]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE_IF:.*]], label %[[CF_MID:.*]]
; CHECK:       [[DO_STORE_IF]]:
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[CF_MID]]
; CHECK:       [[CF_MID]]:
; CHECK-NEXT:    [[TMP3:%.*]] = call { i1, i64 } @llvm.amdgcn.else.i64.i64(i64 [[TMP2]], i1 true)
; CHECK-NEXT:    [[TMP4:%.*]] = extractvalue { i1, i64 } [[TMP3]], 0
; CHECK-NEXT:    [[TMP5:%.*]] = extractvalue { i1, i64 } [[TMP3]], 1
; CHECK-NEXT:    br i1 [[TMP4]], label %[[DO_STORE_ELSE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE_ELSE]]:
; CHECK-NEXT:    store i32 1, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP5]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store.if, label %cf.mid
do.store.if:
  store i32 0, ptr addrspace(1) %local.addr
  br label %cf.mid
cf.mid:
  %notcond = phi i1 [0, %do.store.if], [1, %entry]
  br i1 %notcond, label %do.store.else, label %exit
do.store.else:
  store i32 1, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


; The then and else branches are not likely dynamically divergent here:
define amdgpu_kernel void @cond_store_loaded_ifelse(ptr addrspace(1) inreg %dest, ptr addrspace(1) inreg %lookup) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: define amdgpu_kernel void @cond_store_loaded_ifelse(
; CHECK-SAME: ptr addrspace(1) inreg [[DEST:%.*]], ptr addrspace(1) inreg [[LOOKUP:%.*]]) #[[ATTR2]] !reqd_work_group_size [[META0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TID_X:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TID_Y:%.*]] = tail call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[TID_Y_SHIFT:%.*]] = shl nuw nsw i32 [[TID_Y]], 6
; CHECK-NEXT:    [[TID:%.*]] = or disjoint i32 [[TID_X]], [[TID_Y_SHIFT]]
; CHECK-NEXT:    [[TID_EXT:%.*]] = zext nneg i32 [[TID]] to i64
; CHECK-NEXT:    [[LOCAL_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[DEST]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_ADDR:%.*]] = getelementptr i32, ptr addrspace(1) [[LOOKUP]], i64 [[TID_EXT]]
; CHECK-NEXT:    [[LOOKUP_VALUE:%.*]] = load i32, ptr addrspace(1) [[LOOKUP_ADDR]], align 4
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i32 [[LOOKUP_VALUE]], 0
; CHECK-NEXT:    [[TMP0:%.*]] = call { i1, i64 } @llvm.amdgcn.if.i64(i1 [[COND]], i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = extractvalue { i1, i64 } [[TMP0]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = extractvalue { i1, i64 } [[TMP0]], 1
; CHECK-NEXT:    br i1 [[TMP1]], label %[[DO_STORE_IF:.*]], label %[[CF_MID:.*]]
; CHECK:       [[DO_STORE_IF]]:
; CHECK-NEXT:    store i32 0, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[CF_MID]]
; CHECK:       [[CF_MID]]:
; CHECK-NEXT:    [[TMP3:%.*]] = call { i1, i64 } @llvm.amdgcn.else.i64.i64(i64 [[TMP2]], i1 false)
; CHECK-NEXT:    [[TMP4:%.*]] = extractvalue { i1, i64 } [[TMP3]], 0
; CHECK-NEXT:    [[TMP5:%.*]] = extractvalue { i1, i64 } [[TMP3]], 1
; CHECK-NEXT:    br i1 [[TMP4]], label %[[DO_STORE_ELSE:.*]], label %[[EXIT:.*]]
; CHECK:       [[DO_STORE_ELSE]]:
; CHECK-NEXT:    store i32 1, ptr addrspace(1) [[LOCAL_ADDR]], align 4
; CHECK-NEXT:    br label %[[EXIT]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    call void @llvm.amdgcn.end.cf.i64(i64 [[TMP5]])
; CHECK-NEXT:    ret void
;
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  %lookup.addr = getelementptr i32, ptr addrspace(1) %lookup, i64 %tid.ext
  %lookup.value = load i32, ptr addrspace(1) %lookup.addr
  %cond = icmp eq i32 %lookup.value, 0
  br i1 %cond, label %do.store.if, label %cf.mid
do.store.if:
  store i32 0, ptr addrspace(1) %local.addr
  br label %cf.mid
cf.mid:
  %notcond = phi i1 [0, %do.store.if], [1, %entry]
  br i1 %notcond, label %do.store.else, label %exit
do.store.else:
  store i32 1, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}



attributes #0 = {"amdgpu-flat-work-group-size"="256,256"}
!0 = !{i32 64, i32 4, i32 1}
!1 = !{i32 8, i32 32, i32 1}
;.
; CHECK: [[META0]] = !{i32 64, i32 4, i32 1}
; CHECK: [[META1]] = !{i32 8, i32 32, i32 1}
;.
