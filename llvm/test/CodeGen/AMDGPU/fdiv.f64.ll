; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn -mcpu=hawaii -verify-machineinstrs < %s | FileCheck -check-prefix=CI -check-prefix=GCN %s
; RUN: llc -mtriple=amdgcn -mcpu=tahiti -verify-machineinstrs < %s | FileCheck -check-prefix=SI -check-prefix=GCN %s
; RUN: llc -mtriple=amdgcn -mcpu=tonga -mattr=-flat-for-global -verify-machineinstrs < %s | FileCheck -check-prefix=CI -check-prefix=GCN %s


; GCN-LABEL: {{^}}fdiv_f64:
; GCN-DAG: buffer_load_dwordx2 [[NUM:v\[[0-9]+:[0-9]+\]]], off, {{s\[[0-9]+:[0-9]+\]}}, 0
; GCN-DAG: buffer_load_dwordx2 [[DEN:v\[[0-9]+:[0-9]+\]]], off, {{s\[[0-9]+:[0-9]+\]}}, 0 offset:8
; CI-DAG: v_div_scale_f64 [[SCALE0:v\[[0-9]+:[0-9]+\]]], {{s\[[0-9]+:[0-9]+\]}}, [[DEN]], [[DEN]], [[NUM]]
; CI-DAG: v_div_scale_f64 [[SCALE1:v\[[0-9]+:[0-9]+\]]], vcc, [[NUM]], [[DEN]], [[NUM]]

; Check for div_scale bug workaround on SI
; SI-DAG: v_div_scale_f64 [[SCALE0:v\[[0-9]+:[0-9]+\]]], {{s\[[0-9]+:[0-9]+\]}}, [[DEN]], [[DEN]], [[NUM]]
; SI-DAG: v_div_scale_f64 [[SCALE1:v\[[0-9]+:[0-9]+\]]], {{s\[[0-9]+:[0-9]+\]}}, [[NUM]], [[DEN]], [[NUM]]

; GCN-DAG: v_rcp_f64_e32 [[RCP_SCALE0:v\[[0-9]+:[0-9]+\]]], [[SCALE0]]

; SI-DAG: v_cmp_eq_u32_e32 vcc, {{v[0-9]+}}, {{v[0-9]+}}
; SI-DAG: v_cmp_eq_u32_e64 [[CMP0:s\[[0-9]+:[0-9]+\]]], {{v[0-9]+}}, {{v[0-9]+}}
; SI-DAG: s_xor_b64 vcc, [[CMP0]], vcc

; GCN-DAG: v_fma_f64 [[FMA0:v\[[0-9]+:[0-9]+\]]], -[[SCALE0]], [[RCP_SCALE0]], 1.0
; GCN-DAG: v_fma_f64 [[FMA1:v\[[0-9]+:[0-9]+\]]], [[RCP_SCALE0]], [[FMA0]], [[RCP_SCALE0]]
; GCN-DAG: v_fma_f64 [[FMA2:v\[[0-9]+:[0-9]+\]]], -[[SCALE0]], [[FMA1]], 1.0
; GCN-DAG: v_fma_f64 [[FMA3:v\[[0-9]+:[0-9]+\]]], [[FMA1]], [[FMA2]], [[FMA1]]
; GCN-DAG: v_mul_f64 [[MUL:v\[[0-9]+:[0-9]+\]]], [[SCALE1]], [[FMA3]]
; GCN-DAG: v_fma_f64 [[FMA4:v\[[0-9]+:[0-9]+\]]], -[[SCALE0]], [[MUL]], [[SCALE1]]
; GCN: v_div_fmas_f64 [[FMAS:v\[[0-9]+:[0-9]+\]]], [[FMA4]], [[FMA3]], [[MUL]]
; GCN: v_div_fixup_f64 [[RESULT:v\[[0-9]+:[0-9]+\]]], [[FMAS]], [[DEN]], [[NUM]]
; GCN: buffer_store_dwordx2 [[RESULT]]
; GCN: s_endpgm
define amdgpu_kernel void @fdiv_f64(ptr addrspace(1) %out, ptr addrspace(1) %in) #0 {
; SI-LABEL: fdiv_f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_mov_b32 s10, s6
; SI-NEXT:    s_mov_b32 s11, s7
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s8, s2
; SI-NEXT:    s_mov_b32 s9, s3
; SI-NEXT:    buffer_load_dwordx2 v[0:1], off, s[8:11], 0 glc
; SI-NEXT:    s_waitcnt vmcnt(0)
; SI-NEXT:    buffer_load_dwordx2 v[2:3], off, s[8:11], 0 offset:8 glc
; SI-NEXT:    s_waitcnt vmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_div_scale_f64 v[4:5], s[2:3], v[2:3], v[2:3], v[0:1]
; SI-NEXT:    v_div_scale_f64 v[10:11], s[2:3], v[0:1], v[2:3], v[0:1]
; SI-NEXT:    v_rcp_f64_e32 v[6:7], v[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, v3, v5
; SI-NEXT:    v_cndmask_b32_e64 v12, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, v1, v11
; SI-NEXT:    v_fma_f64 v[8:9], -v[4:5], v[6:7], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v13, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[8:9], v[6:7]
; SI-NEXT:    v_xor_b32_e32 v12, v13, v12
; SI-NEXT:    v_fma_f64 v[8:9], -v[4:5], v[6:7], 1.0
; SI-NEXT:    v_and_b32_e32 v12, 1, v12
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[8:9], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v12
; SI-NEXT:    v_mul_f64 v[8:9], v[10:11], v[6:7]
; SI-NEXT:    v_fma_f64 v[4:5], -v[4:5], v[8:9], v[10:11]
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[4:5], v[4:5], v[6:7], v[8:9]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[4:5], v[2:3], v[0:1]
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[4:7], 0
; SI-NEXT:    s_endpgm
  %gep.1 = getelementptr double, ptr addrspace(1) %in, i32 1
  %num = load volatile double, ptr addrspace(1) %in
  %den = load volatile double, ptr addrspace(1) %gep.1
  %result = fdiv double %num, %den
  store double %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}v_fdiv_f64_afn:
; GCN: v_rcp_f64_e32 v[4:5], v[2:3]
; GCN: v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; GCN: v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; GCN: v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; GCN: v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; GCN: v_mul_f64 v[6:7], v[0:1], v[4:5]
; GCN: v_fma_f64 v[0:1], -v[2:3], v[6:7], v[0:1]
; GCN: v_fma_f64 v[0:1], v[0:1], v[4:5], v[6:7]
; GCN: s_setpc_b64
define double @v_fdiv_f64_afn(double %x, double %y) #0 {
; CI-LABEL: v_fdiv_f64_afn:
; CI:       ; %bb.0:
; CI-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CI-NEXT:    v_rcp_f64_e32 v[4:5], v[2:3]
; CI-NEXT:    v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; CI-NEXT:    v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; CI-NEXT:    v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; CI-NEXT:    v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; CI-NEXT:    v_mul_f64 v[6:7], v[0:1], v[4:5]
; CI-NEXT:    v_fma_f64 v[0:1], -v[2:3], v[6:7], v[0:1]
; CI-NEXT:    v_fma_f64 v[0:1], v[0:1], v[4:5], v[6:7]
; CI-NEXT:    s_setpc_b64 s[30:31]
;
; SI-LABEL: v_fdiv_f64_afn:
; SI:       ; %bb.0:
; SI-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; SI-NEXT:    v_rcp_f64_e32 v[4:5], v[2:3]
; SI-NEXT:    v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; SI-NEXT:    v_fma_f64 v[6:7], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_fma_f64 v[4:5], v[6:7], v[4:5], v[4:5]
; SI-NEXT:    v_mul_f64 v[6:7], v[0:1], v[4:5]
; SI-NEXT:    v_fma_f64 v[0:1], -v[2:3], v[6:7], v[0:1]
; SI-NEXT:    v_fma_f64 v[0:1], v[0:1], v[4:5], v[6:7]
; SI-NEXT:    s_setpc_b64 s[30:31]
  %result = fdiv afn double %x, %y
  ret double %result
}

; GCN-LABEL: {{^}}v_rcp_f64_afn:
; GCN: v_rcp_f64_e32 v[2:3], v[0:1]
; GCN: v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; GCN: v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; GCN: v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; GCN: v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; GCN: v_fma_f64 v[0:1], -v[0:1], v[2:3], 1.0
; GCN: v_fma_f64 v[0:1], v[0:1], v[2:3], v[2:3]
; GCN: s_setpc_b64
define double @v_rcp_f64_afn(double %x) #0 {
; CI-LABEL: v_rcp_f64_afn:
; CI:       ; %bb.0:
; CI-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; CI-NEXT:    v_rcp_f64_e32 v[2:3], v[0:1]
; CI-NEXT:    v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; CI-NEXT:    v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; CI-NEXT:    v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; CI-NEXT:    v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; CI-NEXT:    v_fma_f64 v[0:1], -v[0:1], v[2:3], 1.0
; CI-NEXT:    v_fma_f64 v[0:1], v[0:1], v[2:3], v[2:3]
; CI-NEXT:    s_setpc_b64 s[30:31]
;
; SI-LABEL: v_rcp_f64_afn:
; SI:       ; %bb.0:
; SI-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; SI-NEXT:    v_rcp_f64_e32 v[2:3], v[0:1]
; SI-NEXT:    v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; SI-NEXT:    v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; SI-NEXT:    v_fma_f64 v[4:5], -v[0:1], v[2:3], 1.0
; SI-NEXT:    v_fma_f64 v[2:3], v[4:5], v[2:3], v[2:3]
; SI-NEXT:    v_fma_f64 v[0:1], -v[0:1], v[2:3], 1.0
; SI-NEXT:    v_fma_f64 v[0:1], v[0:1], v[2:3], v[2:3]
; SI-NEXT:    s_setpc_b64 s[30:31]
  %result = fdiv afn double 1.0, %x
  ret double %result
}

; GCN-LABEL: {{^}}fdiv_f64_s_v:
define amdgpu_kernel void @fdiv_f64_s_v(ptr addrspace(1) %out, ptr addrspace(1) %in, double %num) #0 {
; SI-LABEL: fdiv_f64_s_v:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[4:5], s[4:5], 0xd
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_load_dwordx2 s[6:7], s[2:3], 0x0
; SI-NEXT:    v_mov_b32_e32 v0, s4
; SI-NEXT:    v_mov_b32_e32 v1, s5
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_div_scale_f64 v[2:3], s[2:3], s[6:7], s[6:7], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v6, s6
; SI-NEXT:    v_mov_b32_e32 v7, s7
; SI-NEXT:    v_rcp_f64_e32 v[4:5], v[2:3]
; SI-NEXT:    v_div_scale_f64 v[6:7], s[2:3], s[4:5], v[6:7], s[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s7, v3
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s5, v7
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_mul_f64 v[8:9], v[6:7], v[4:5]
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_fma_f64 v[2:3], -v[2:3], v[8:9], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[4:5], v[8:9]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[2:3], s[6:7], v[0:1]
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %den = load double, ptr addrspace(1) %in
  %result = fdiv double %num, %den
  store double %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}fdiv_f64_v_s:
define amdgpu_kernel void @fdiv_f64_v_s(ptr addrspace(1) %out, ptr addrspace(1) %in, double %den) #0 {
; SI-LABEL: fdiv_f64_v_s:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[4:5], s[4:5], 0xd
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_load_dwordx2 s[2:3], s[2:3], 0x0
; SI-NEXT:    v_mov_b32_e32 v7, s5
; SI-NEXT:    v_mov_b32_e32 v6, s4
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s2
; SI-NEXT:    v_mov_b32_e32 v1, s3
; SI-NEXT:    v_div_scale_f64 v[2:3], s[6:7], s[4:5], s[4:5], v[0:1]
; SI-NEXT:    v_div_scale_f64 v[6:7], s[6:7], s[2:3], v[6:7], s[2:3]
; SI-NEXT:    v_rcp_f64_e32 v[4:5], v[2:3]
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s3, v7
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s5, v3
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_xor_b32_e32 v10, v10, v11
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_mul_f64 v[8:9], v[6:7], v[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_fma_f64 v[2:3], -v[2:3], v[8:9], v[6:7]
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[4:5], v[8:9]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[2:3], s[4:5], v[0:1]
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %num = load double, ptr addrspace(1) %in
  %result = fdiv double %num, %den
  store double %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}fdiv_f64_s_s:
define amdgpu_kernel void @fdiv_f64_s_s(ptr addrspace(1) %out, double %num, double %den) #0 {
; SI-LABEL: fdiv_f64_s_s:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[4:5], s[4:5], 0xd
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s2
; SI-NEXT:    v_mov_b32_e32 v1, s3
; SI-NEXT:    v_div_scale_f64 v[2:3], s[6:7], s[4:5], s[4:5], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v7, s5
; SI-NEXT:    v_mov_b32_e32 v6, s4
; SI-NEXT:    v_rcp_f64_e32 v[4:5], v[2:3]
; SI-NEXT:    v_div_scale_f64 v[6:7], s[6:7], s[2:3], v[6:7], s[2:3]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s5, v3
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s3, v7
; SI-NEXT:    v_fma_f64 v[8:9], -v[2:3], v[4:5], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[8:9], v[4:5]
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_mul_f64 v[8:9], v[6:7], v[4:5]
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_fma_f64 v[2:3], -v[2:3], v[8:9], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[4:5], v[8:9]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[2:3], s[4:5], v[0:1]
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %result = fdiv double %num, %den
  store double %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}v_fdiv_v2f64:
define amdgpu_kernel void @v_fdiv_v2f64(ptr addrspace(1) %out, ptr addrspace(1) %in) #0 {
; SI-LABEL: v_fdiv_v2f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[8:11], s[4:5], 0x9
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_load_dwordx8 s[0:7], s[10:11], 0x0
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s2
; SI-NEXT:    v_mov_b32_e32 v1, s3
; SI-NEXT:    v_div_scale_f64 v[2:3], s[10:11], s[6:7], s[6:7], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v7, s1
; SI-NEXT:    v_mov_b32_e32 v6, s0
; SI-NEXT:    v_rcp_f64_e32 v[8:9], v[2:3]
; SI-NEXT:    v_div_scale_f64 v[12:13], s[12:13], s[4:5], s[4:5], v[6:7]
; SI-NEXT:    v_mov_b32_e32 v4, s6
; SI-NEXT:    v_fma_f64 v[14:15], -v[2:3], v[8:9], 1.0
; SI-NEXT:    v_mov_b32_e32 v5, s7
; SI-NEXT:    v_fma_f64 v[8:9], v[8:9], v[14:15], v[8:9]
; SI-NEXT:    v_rcp_f64_e32 v[14:15], v[12:13]
; SI-NEXT:    v_div_scale_f64 v[4:5], s[12:13], s[2:3], v[4:5], s[2:3]
; SI-NEXT:    v_fma_f64 v[16:17], -v[2:3], v[8:9], 1.0
; SI-NEXT:    v_mov_b32_e32 v11, s5
; SI-NEXT:    v_fma_f64 v[8:9], v[8:9], v[16:17], v[8:9]
; SI-NEXT:    v_fma_f64 v[16:17], -v[12:13], v[14:15], 1.0
; SI-NEXT:    v_mov_b32_e32 v10, s4
; SI-NEXT:    v_fma_f64 v[14:15], v[14:15], v[16:17], v[14:15]
; SI-NEXT:    v_mul_f64 v[16:17], v[4:5], v[8:9]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s7, v3
; SI-NEXT:    v_cndmask_b32_e64 v18, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s3, v5
; SI-NEXT:    v_fma_f64 v[2:3], -v[2:3], v[16:17], v[4:5]
; SI-NEXT:    v_fma_f64 v[4:5], -v[12:13], v[14:15], 1.0
; SI-NEXT:    v_div_scale_f64 v[10:11], s[2:3], s[0:1], v[10:11], s[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v19, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v18, v19, v18
; SI-NEXT:    v_fma_f64 v[4:5], v[14:15], v[4:5], v[14:15]
; SI-NEXT:    v_and_b32_e32 v18, 1, v18
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v18
; SI-NEXT:    v_mul_f64 v[14:15], v[10:11], v[4:5]
; SI-NEXT:    s_mov_b32 s11, 0xf000
; SI-NEXT:    s_mov_b32 s10, -1
; SI-NEXT:    s_nop 0
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[8:9], v[16:17]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s5, v13
; SI-NEXT:    v_fma_f64 v[8:9], -v[12:13], v[14:15], v[10:11]
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s1, v11
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_div_fixup_f64 v[2:3], v[2:3], s[6:7], v[0:1]
; SI-NEXT:    s_nop 2
; SI-NEXT:    v_div_fmas_f64 v[4:5], v[8:9], v[4:5], v[14:15]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[4:5], s[4:5], v[6:7]
; SI-NEXT:    buffer_store_dwordx4 v[0:3], off, s[8:11], 0
; SI-NEXT:    s_endpgm
  %gep.1 = getelementptr <2 x double>, ptr addrspace(1) %in, i32 1
  %num = load <2 x double>, ptr addrspace(1) %in
  %den = load <2 x double>, ptr addrspace(1) %gep.1
  %result = fdiv <2 x double> %num, %den
  store <2 x double> %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}s_fdiv_v2f64:
define amdgpu_kernel void @s_fdiv_v2f64(ptr addrspace(1) %out, <2 x double> %num, <2 x double> %den) {
; SI-LABEL: s_fdiv_v2f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx8 s[8:15], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s10
; SI-NEXT:    v_mov_b32_e32 v1, s11
; SI-NEXT:    v_div_scale_f64 v[4:5], s[0:1], s[14:15], s[14:15], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v6, s8
; SI-NEXT:    v_mov_b32_e32 v7, s9
; SI-NEXT:    v_rcp_f64_e32 v[8:9], v[4:5]
; SI-NEXT:    v_div_scale_f64 v[10:11], s[0:1], s[12:13], s[12:13], v[6:7]
; SI-NEXT:    v_mov_b32_e32 v2, s14
; SI-NEXT:    v_fma_f64 v[12:13], -v[4:5], v[8:9], 1.0
; SI-NEXT:    v_mov_b32_e32 v3, s15
; SI-NEXT:    v_div_scale_f64 v[2:3], s[0:1], s[10:11], v[2:3], s[10:11]
; SI-NEXT:    v_fma_f64 v[8:9], v[8:9], v[12:13], v[8:9]
; SI-NEXT:    v_rcp_f64_e32 v[12:13], v[10:11]
; SI-NEXT:    v_fma_f64 v[14:15], -v[4:5], v[8:9], 1.0
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v5
; SI-NEXT:    v_cndmask_b32_e64 v16, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s11, v3
; SI-NEXT:    v_fma_f64 v[8:9], v[8:9], v[14:15], v[8:9]
; SI-NEXT:    v_fma_f64 v[14:15], -v[10:11], v[12:13], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v17, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v18, v17, v16
; SI-NEXT:    v_mul_f64 v[16:17], v[2:3], v[8:9]
; SI-NEXT:    v_fma_f64 v[12:13], v[12:13], v[14:15], v[12:13]
; SI-NEXT:    v_mov_b32_e32 v15, s13
; SI-NEXT:    v_mov_b32_e32 v14, s12
; SI-NEXT:    v_fma_f64 v[2:3], -v[4:5], v[16:17], v[2:3]
; SI-NEXT:    v_fma_f64 v[4:5], -v[10:11], v[12:13], 1.0
; SI-NEXT:    v_div_scale_f64 v[14:15], s[0:1], s[8:9], v[14:15], s[8:9]
; SI-NEXT:    v_fma_f64 v[4:5], v[12:13], v[4:5], v[12:13]
; SI-NEXT:    v_and_b32_e32 v18, 1, v18
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v18
; SI-NEXT:    v_mul_f64 v[12:13], v[14:15], v[4:5]
; SI-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[8:9], v[16:17]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s13, v11
; SI-NEXT:    v_fma_f64 v[8:9], -v[10:11], v[12:13], v[14:15]
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s9, v15
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_div_fixup_f64 v[2:3], v[2:3], s[14:15], v[0:1]
; SI-NEXT:    s_nop 2
; SI-NEXT:    v_div_fmas_f64 v[4:5], v[8:9], v[4:5], v[12:13]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[4:5], s[12:13], v[6:7]
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    buffer_store_dwordx4 v[0:3], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %result = fdiv <2 x double> %num, %den
  store <2 x double> %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}v_fdiv_v4f64:
define amdgpu_kernel void @v_fdiv_v4f64(ptr addrspace(1) %out, ptr addrspace(1) %in) #0 {
; SI-LABEL: v_fdiv_v4f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx4 s[16:19], s[4:5], 0x9
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_load_dwordx16 s[0:15], s[18:19], 0x0
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s2
; SI-NEXT:    v_mov_b32_e32 v1, s3
; SI-NEXT:    v_mov_b32_e32 v2, s10
; SI-NEXT:    v_div_scale_f64 v[6:7], s[18:19], s[10:11], s[10:11], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v3, s11
; SI-NEXT:    v_div_scale_f64 v[2:3], s[18:19], s[2:3], v[2:3], s[2:3]
; SI-NEXT:    v_rcp_f64_e32 v[12:13], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s11, v7
; SI-NEXT:    v_mov_b32_e32 v9, s1
; SI-NEXT:    v_cndmask_b32_e64 v16, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s3, v3
; SI-NEXT:    v_mov_b32_e32 v8, s0
; SI-NEXT:    v_cndmask_b32_e64 v17, 0, 1, vcc
; SI-NEXT:    v_div_scale_f64 v[10:11], s[18:19], s[8:9], s[8:9], v[8:9]
; SI-NEXT:    v_xor_b32_e32 v18, v17, v16
; SI-NEXT:    v_fma_f64 v[16:17], -v[6:7], v[12:13], 1.0
; SI-NEXT:    v_rcp_f64_e32 v[14:15], v[10:11]
; SI-NEXT:    v_fma_f64 v[12:13], v[12:13], v[16:17], v[12:13]
; SI-NEXT:    v_and_b32_e32 v18, 1, v18
; SI-NEXT:    v_fma_f64 v[16:17], -v[6:7], v[12:13], 1.0
; SI-NEXT:    v_mov_b32_e32 v4, s6
; SI-NEXT:    v_fma_f64 v[12:13], v[12:13], v[16:17], v[12:13]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v18
; SI-NEXT:    v_fma_f64 v[18:19], -v[10:11], v[14:15], 1.0
; SI-NEXT:    v_mul_f64 v[16:17], v[2:3], v[12:13]
; SI-NEXT:    v_mov_b32_e32 v5, s7
; SI-NEXT:    v_fma_f64 v[14:15], v[14:15], v[18:19], v[14:15]
; SI-NEXT:    v_fma_f64 v[2:3], -v[6:7], v[16:17], v[2:3]
; SI-NEXT:    v_div_scale_f64 v[6:7], s[2:3], s[14:15], s[14:15], v[4:5]
; SI-NEXT:    v_fma_f64 v[18:19], -v[10:11], v[14:15], 1.0
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[12:13], v[16:17]
; SI-NEXT:    v_fma_f64 v[12:13], v[14:15], v[18:19], v[14:15]
; SI-NEXT:    v_rcp_f64_e32 v[14:15], v[6:7]
; SI-NEXT:    v_mov_b32_e32 v17, s9
; SI-NEXT:    v_mov_b32_e32 v16, s8
; SI-NEXT:    v_div_scale_f64 v[16:17], s[2:3], s[0:1], v[16:17], s[0:1]
; SI-NEXT:    v_fma_f64 v[18:19], -v[6:7], v[14:15], 1.0
; SI-NEXT:    s_mov_b32 s19, 0xf000
; SI-NEXT:    v_fma_f64 v[14:15], v[14:15], v[18:19], v[14:15]
; SI-NEXT:    v_mul_f64 v[18:19], v[16:17], v[12:13]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s1, v17
; SI-NEXT:    v_cmp_eq_u32_e64 s[0:1], s9, v11
; SI-NEXT:    v_fma_f64 v[16:17], -v[10:11], v[18:19], v[16:17]
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, s[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_fma_f64 v[10:11], -v[6:7], v[14:15], 1.0
; SI-NEXT:    s_mov_b32 s18, -1
; SI-NEXT:    v_fma_f64 v[10:11], v[14:15], v[10:11], v[14:15]
; SI-NEXT:    v_mov_b32_e32 v14, s14
; SI-NEXT:    v_mov_b32_e32 v15, s15
; SI-NEXT:    v_div_scale_f64 v[14:15], s[0:1], s[6:7], v[14:15], s[6:7]
; SI-NEXT:    v_div_fmas_f64 v[12:13], v[16:17], v[12:13], v[18:19]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v7
; SI-NEXT:    v_mul_f64 v[16:17], v[14:15], v[10:11]
; SI-NEXT:    v_div_fixup_f64 v[2:3], v[2:3], s[10:11], v[0:1]
; SI-NEXT:    v_fma_f64 v[18:19], -v[6:7], v[16:17], v[14:15]
; SI-NEXT:    v_cndmask_b32_e64 v6, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s7, v15
; SI-NEXT:    v_cndmask_b32_e64 v7, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v6, v7, v6
; SI-NEXT:    v_mov_b32_e32 v15, s5
; SI-NEXT:    v_and_b32_e32 v6, 1, v6
; SI-NEXT:    v_mov_b32_e32 v14, s4
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v6
; SI-NEXT:    v_div_scale_f64 v[6:7], s[0:1], s[12:13], s[12:13], v[14:15]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[12:13], s[8:9], v[8:9]
; SI-NEXT:    s_nop 1
; SI-NEXT:    v_div_fmas_f64 v[10:11], v[18:19], v[10:11], v[16:17]
; SI-NEXT:    v_rcp_f64_e32 v[16:17], v[6:7]
; SI-NEXT:    v_fma_f64 v[18:19], -v[6:7], v[16:17], 1.0
; SI-NEXT:    v_fma_f64 v[16:17], v[16:17], v[18:19], v[16:17]
; SI-NEXT:    v_fma_f64 v[18:19], -v[6:7], v[16:17], 1.0
; SI-NEXT:    v_fma_f64 v[16:17], v[16:17], v[18:19], v[16:17]
; SI-NEXT:    v_mov_b32_e32 v19, s13
; SI-NEXT:    v_mov_b32_e32 v18, s12
; SI-NEXT:    v_div_scale_f64 v[18:19], s[0:1], s[4:5], v[18:19], s[4:5]
; SI-NEXT:    v_cmp_eq_u32_e64 s[0:1], s13, v7
; SI-NEXT:    v_mul_f64 v[20:21], v[18:19], v[16:17]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s5, v19
; SI-NEXT:    v_fma_f64 v[18:19], -v[6:7], v[20:21], v[18:19]
; SI-NEXT:    v_cndmask_b32_e64 v6, 0, 1, s[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v7, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v6, v7, v6
; SI-NEXT:    v_and_b32_e32 v6, 1, v6
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v6
; SI-NEXT:    v_div_fixup_f64 v[6:7], v[10:11], s[14:15], v[4:5]
; SI-NEXT:    s_nop 2
; SI-NEXT:    v_div_fmas_f64 v[16:17], v[18:19], v[16:17], v[20:21]
; SI-NEXT:    v_div_fixup_f64 v[4:5], v[16:17], s[12:13], v[14:15]
; SI-NEXT:    buffer_store_dwordx4 v[4:7], off, s[16:19], 0 offset:16
; SI-NEXT:    buffer_store_dwordx4 v[0:3], off, s[16:19], 0
; SI-NEXT:    s_endpgm
  %gep.1 = getelementptr <4 x double>, ptr addrspace(1) %in, i32 1
  %num = load <4 x double>, ptr addrspace(1) %in
  %den = load <4 x double>, ptr addrspace(1) %gep.1
  %result = fdiv <4 x double> %num, %den
  store <4 x double> %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}s_fdiv_v4f64:
define amdgpu_kernel void @s_fdiv_v4f64(ptr addrspace(1) %out, <4 x double> %num, <4 x double> %den) #0 {
; SI-LABEL: s_fdiv_v4f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx16 s[8:23], s[4:5], 0x11
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mov_b32_e32 v0, s10
; SI-NEXT:    v_mov_b32_e32 v1, s11
; SI-NEXT:    v_div_scale_f64 v[4:5], s[0:1], s[18:19], s[18:19], v[0:1]
; SI-NEXT:    v_mov_b32_e32 v8, s8
; SI-NEXT:    v_mov_b32_e32 v2, s18
; SI-NEXT:    v_rcp_f64_e32 v[6:7], v[4:5]
; SI-NEXT:    v_mov_b32_e32 v9, s9
; SI-NEXT:    v_mov_b32_e32 v3, s19
; SI-NEXT:    v_div_scale_f64 v[10:11], s[0:1], s[16:17], s[16:17], v[8:9]
; SI-NEXT:    v_fma_f64 v[12:13], -v[4:5], v[6:7], 1.0
; SI-NEXT:    v_div_scale_f64 v[2:3], s[0:1], s[10:11], v[2:3], s[10:11]
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[12:13], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s19, v5
; SI-NEXT:    v_rcp_f64_e32 v[12:13], v[10:11]
; SI-NEXT:    v_fma_f64 v[14:15], -v[4:5], v[6:7], 1.0
; SI-NEXT:    v_cndmask_b32_e64 v16, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s11, v3
; SI-NEXT:    v_cndmask_b32_e64 v17, 0, 1, vcc
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[14:15], v[6:7]
; SI-NEXT:    v_xor_b32_e32 v16, v17, v16
; SI-NEXT:    v_and_b32_e32 v18, 1, v16
; SI-NEXT:    v_mul_f64 v[14:15], v[2:3], v[6:7]
; SI-NEXT:    v_fma_f64 v[16:17], -v[10:11], v[12:13], 1.0
; SI-NEXT:    v_fma_f64 v[2:3], -v[4:5], v[14:15], v[2:3]
; SI-NEXT:    v_fma_f64 v[4:5], v[12:13], v[16:17], v[12:13]
; SI-NEXT:    v_mov_b32_e32 v17, s15
; SI-NEXT:    v_mov_b32_e32 v16, s14
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v18
; SI-NEXT:    v_div_scale_f64 v[18:19], s[0:1], s[22:23], s[22:23], v[16:17]
; SI-NEXT:    v_fma_f64 v[12:13], -v[10:11], v[4:5], 1.0
; SI-NEXT:    v_fma_f64 v[4:5], v[4:5], v[12:13], v[4:5]
; SI-NEXT:    s_nop 0
; SI-NEXT:    v_div_fmas_f64 v[2:3], v[2:3], v[6:7], v[14:15]
; SI-NEXT:    v_rcp_f64_e32 v[6:7], v[18:19]
; SI-NEXT:    v_mov_b32_e32 v12, s16
; SI-NEXT:    v_mov_b32_e32 v13, s17
; SI-NEXT:    v_div_scale_f64 v[12:13], s[0:1], s[8:9], v[12:13], s[8:9]
; SI-NEXT:    v_fma_f64 v[14:15], -v[18:19], v[6:7], 1.0
; SI-NEXT:    v_cmp_eq_u32_e64 s[0:1], s17, v11
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[14:15], v[6:7]
; SI-NEXT:    v_mul_f64 v[14:15], v[12:13], v[4:5]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s9, v13
; SI-NEXT:    v_fma_f64 v[12:13], -v[10:11], v[14:15], v[12:13]
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, s[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_fma_f64 v[10:11], -v[18:19], v[6:7], 1.0
; SI-NEXT:    v_div_fixup_f64 v[2:3], v[2:3], s[18:19], v[0:1]
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[10:11], v[6:7]
; SI-NEXT:    v_mov_b32_e32 v10, s22
; SI-NEXT:    v_mov_b32_e32 v11, s23
; SI-NEXT:    v_div_scale_f64 v[10:11], s[0:1], s[14:15], v[10:11], s[14:15]
; SI-NEXT:    v_div_fmas_f64 v[12:13], v[12:13], v[4:5], v[14:15]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s23, v19
; SI-NEXT:    v_mul_f64 v[4:5], v[10:11], v[6:7]
; SI-NEXT:    v_fma_f64 v[14:15], -v[18:19], v[4:5], v[10:11]
; SI-NEXT:    v_cndmask_b32_e64 v10, 0, 1, vcc
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s15, v11
; SI-NEXT:    v_cndmask_b32_e64 v11, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v10, v11, v10
; SI-NEXT:    v_and_b32_e32 v10, 1, v10
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v10
; SI-NEXT:    v_mov_b32_e32 v10, s12
; SI-NEXT:    v_mov_b32_e32 v11, s13
; SI-NEXT:    v_div_scale_f64 v[18:19], s[0:1], s[20:21], s[20:21], v[10:11]
; SI-NEXT:    s_nop 0
; SI-NEXT:    v_div_fmas_f64 v[4:5], v[14:15], v[6:7], v[4:5]
; SI-NEXT:    v_div_fixup_f64 v[0:1], v[12:13], s[16:17], v[8:9]
; SI-NEXT:    v_rcp_f64_e32 v[6:7], v[18:19]
; SI-NEXT:    v_fma_f64 v[14:15], -v[18:19], v[6:7], 1.0
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[14:15], v[6:7]
; SI-NEXT:    v_fma_f64 v[14:15], -v[18:19], v[6:7], 1.0
; SI-NEXT:    v_fma_f64 v[6:7], v[6:7], v[14:15], v[6:7]
; SI-NEXT:    v_mov_b32_e32 v14, s20
; SI-NEXT:    v_mov_b32_e32 v15, s21
; SI-NEXT:    v_div_scale_f64 v[14:15], s[0:1], s[12:13], v[14:15], s[12:13]
; SI-NEXT:    v_cmp_eq_u32_e64 s[0:1], s21, v19
; SI-NEXT:    v_mul_f64 v[20:21], v[14:15], v[6:7]
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, s13, v15
; SI-NEXT:    v_fma_f64 v[14:15], -v[18:19], v[20:21], v[14:15]
; SI-NEXT:    v_cndmask_b32_e64 v18, 0, 1, s[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v19, 0, 1, vcc
; SI-NEXT:    v_xor_b32_e32 v18, v19, v18
; SI-NEXT:    v_and_b32_e32 v18, 1, v18
; SI-NEXT:    v_cmp_eq_u32_e32 vcc, 1, v18
; SI-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; SI-NEXT:    s_nop 2
; SI-NEXT:    v_div_fmas_f64 v[14:15], v[14:15], v[6:7], v[20:21]
; SI-NEXT:    v_div_fixup_f64 v[6:7], v[4:5], s[22:23], v[16:17]
; SI-NEXT:    v_div_fixup_f64 v[4:5], v[14:15], s[20:21], v[10:11]
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    buffer_store_dwordx4 v[4:7], off, s[0:3], 0 offset:16
; SI-NEXT:    buffer_store_dwordx4 v[0:3], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %result = fdiv <4 x double> %num, %den
  store <4 x double> %result, ptr addrspace(1) %out
  ret void
}

; GCN-LABEL: {{^}}div_fast_2_x_pat_f64:
; GCN: v_mul_f64 [[MUL:v\[[0-9]+:[0-9]+\]]], s{{\[[0-9]+:[0-9]+\]}}, 0.5
; GCN: buffer_store_dwordx2 [[MUL]]
define amdgpu_kernel void @div_fast_2_x_pat_f64(ptr addrspace(1) %out) #1 {
; SI-LABEL: div_fast_2_x_pat_f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx2 s[6:7], s[0:1], 0x0
; SI-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mul_f64 v[0:1], s[6:7], 0.5
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %x = load double, ptr addrspace(1) poison
  %rcp = fdiv fast double %x, 2.0
  store double %rcp, ptr addrspace(1) %out, align 4
  ret void
}

; GCN-LABEL: {{^}}div_fast_k_x_pat_f64:
; GCN-DAG: v_mov_b32_e32 v[[K_LO:[0-9]+]], 0x9999999a
; GCN-DAG: v_mov_b32_e32 v[[K_HI:[0-9]+]], 0x3fb99999
; GCN: v_mul_f64 [[MUL:v\[[0-9]+:[0-9]+\]]], s{{\[[0-9]+:[0-9]+\]}}, v[[[K_LO]]:[[K_HI]]]
; GCN: buffer_store_dwordx2 [[MUL]]
define amdgpu_kernel void @div_fast_k_x_pat_f64(ptr addrspace(1) %out) #1 {
; SI-LABEL: div_fast_k_x_pat_f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx2 s[6:7], s[0:1], 0x0
; SI-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; SI-NEXT:    v_mov_b32_e32 v0, 0x9999999a
; SI-NEXT:    v_mov_b32_e32 v1, 0x3fb99999
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mul_f64 v[0:1], s[6:7], v[0:1]
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %x = load double, ptr addrspace(1) poison
  %rcp = fdiv fast double %x, 10.0
  store double %rcp, ptr addrspace(1) %out, align 4
  ret void
}

; GCN-LABEL: {{^}}div_fast_neg_k_x_pat_f64:
; GCN-DAG: v_mov_b32_e32 v[[K_LO:[0-9]+]], 0x9999999a
; GCN-DAG: v_mov_b32_e32 v[[K_HI:[0-9]+]], 0xbfb99999
; GCN: v_mul_f64 [[MUL:v\[[0-9]+:[0-9]+\]]], s{{\[[0-9]+:[0-9]+\]}}, v[[[K_LO]]:[[K_HI]]]
; GCN: buffer_store_dwordx2 [[MUL]]
define amdgpu_kernel void @div_fast_neg_k_x_pat_f64(ptr addrspace(1) %out) #1 {
; SI-LABEL: div_fast_neg_k_x_pat_f64:
; SI:       ; %bb.0:
; SI-NEXT:    s_load_dwordx2 s[6:7], s[0:1], 0x0
; SI-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; SI-NEXT:    v_mov_b32_e32 v0, 0x9999999a
; SI-NEXT:    v_mov_b32_e32 v1, 0xbfb99999
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    v_mul_f64 v[0:1], s[6:7], v[0:1]
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; SI-NEXT:    s_endpgm
  %x = load double, ptr addrspace(1) poison
  %rcp = fdiv fast double %x, -10.0
  store double %rcp, ptr addrspace(1) %out, align 4
  ret void
}

attributes #0 = { nounwind }
attributes #1 = { nounwind "unsafe-fp-math"="true" }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; GCN: {{.*}}
