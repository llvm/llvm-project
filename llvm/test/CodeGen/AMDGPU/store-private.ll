; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=amdgcn -mcpu=verde -verify-machineinstrs < %s | FileCheck -check-prefix=SI -check-prefix=FUNC %s
; RUN: llc -mtriple=amdgcn -mcpu=tonga -verify-machineinstrs < %s | FileCheck -check-prefix=SI -check-prefix=FUNC %s
; RUN: llc -mtriple=r600 -mcpu=redwood -verify-machineinstrs < %s | FileCheck -check-prefix=EG -check-prefix=FUNC %s
; RUN: llc -mtriple=r600 -mcpu=cayman -verify-machineinstrs < %s | FileCheck -check-prefix=CM -check-prefix=FUNC %s

; FUNC-LABEL: {{^}}store_i1:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_byte
define amdgpu_kernel void @store_i1(ptr addrspace(5) %out) {
; EG-LABEL: store_i1:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 17, @0, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:     AND_INT T2.W, T0.X, PV.W,
; EG-NEXT:     LSHL * T1.W, 1, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i1:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 17, @0, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:     AND_INT T0.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T1.W, 1, T1.W,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  store i1 true, ptr addrspace(5) %out
  ret void
}

; i8 store
; FUNC-LABEL: {{^}}store_i8:
; EG: LSHR * [[ADDRESS:T[0-9]\.[XYZW]]], KC0[2].Y, literal.x
; EG-NEXT: 2
; EG: MOVA_INT * AR.x (MASKED)
; EG: MOV [[OLD:T[0-9]\.[XYZW]]], {{.*}}AR.x

; IG 0: Get the byte index and truncate the value
; EG: AND_INT * T{{[0-9]}}.[[BI_CHAN:[XYZW]]], KC0[2].Y, literal.x
; EG: LSHL * T{{[0-9]}}.[[SHIFT_CHAN:[XYZW]]], PV.[[BI_CHAN]], literal.x
; EG-NEXT: 3(4.203895e-45)


; EG: LSHL * T{{[0-9]}}.[[TRUNC_CHAN:[XYZW]]], literal.x, PV.W
; EG-NEXT: 255(3.573311e-43)

; EG: NOT_INT
; EG: AND_INT {{[\* ]*}}[[CLR_CHAN:T[0-9]\.[XYZW]]], {{.*}}[[OLD]]
; EG: OR_INT * [[RES:T[0-9]\.[XYZW]]]
; TODO: Is the reload necessary?
; EG: MOVA_INT * AR.x (MASKED), [[ADDRESS]]
; EG: MOV * T(0 + AR.x).X+, [[RES]]

; SI: buffer_store_byte

define amdgpu_kernel void @store_i8(ptr addrspace(5) %out, i8 %in) {
; EG-LABEL: store_i8:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 6, @1, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     MOV * T1.X, 0.0,
; EG-NEXT:    TEX 0 @0
; EG-NEXT:     VTX_READ_8 T1.X, T1.X, 40, #3
; EG-NEXT:    ALU 11, @2, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:     AND_INT T2.W, T0.X, PV.W,
; EG-NEXT:     LSHL * T1.W, T1.X, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i8:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 6, @1, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T0.X, T(0 + AR.x).X+,
; CM-NEXT:     MOV * T1.X, 0.0,
; CM-NEXT:    TEX 0 @0
; CM-NEXT:     VTX_READ_8 T1.X, T1.X, 40, #3
; CM-NEXT:    ALU 11, @2, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:     AND_INT T0.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T1.W, T1.X, T1.W, BS:VEC_120/SCL_212
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  store i8 %in, ptr addrspace(5) %out
  ret void
}

; i16 store
; FUNC-LABEL: {{^}}store_i16:
; EG: LSHR * [[ADDRESS:T[0-9]\.[XYZW]]], KC0[2].Y, literal.x
; EG-NEXT: 2
; EG: MOVA_INT * AR.x (MASKED)
; EG: MOV [[OLD:T[0-9]\.[XYZW]]], {{.*}}AR.x

; EG: VTX_READ_16

; IG 0: Get the byte index and truncate the value
; EG: AND_INT * T{{[0-9]}}.[[BI_CHAN:[XYZW]]], KC0[2].Y, literal.x
; EG: LSHL * T{{[0-9]}}.[[SHIFT_CHAN:[XYZW]]], PV.[[BI_CHAN]], literal.x
; EG-NEXT: 3(4.203895e-45)

; EG: NOT_INT
; EG: AND_INT {{[\* ]*}}[[CLR_CHAN:T[0-9]\.[XYZW]]], {{.*}}[[OLD]]
; EG: OR_INT * [[RES:T[0-9]\.[XYZW]]]
; TODO: Is the reload necessary?
; EG: MOVA_INT * AR.x (MASKED), [[ADDRESS]]
; EG: MOV * T(0 + AR.x).X+, [[RES]]

; SI: buffer_store_short
define amdgpu_kernel void @store_i16(ptr addrspace(5) %out, i16 %in) {
; EG-LABEL: store_i16:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 6, @3, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     MOV * T1.X, 0.0,
; EG-NEXT:    TEX 0 @0
; EG-NEXT:     VTX_READ_16 T1.X, T1.X, 40, #3
; EG-NEXT:    ALU 12, @4, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T2.W, literal.x, PV.W,
; EG-NEXT:     AND_INT * T3.W, T1.X, literal.x,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:     AND_INT T2.W, T0.X, PV.W,
; EG-NEXT:     LSHL * T1.W, T3.W, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i16:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 6, @3, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T0.X, T(0 + AR.x).X+,
; CM-NEXT:     MOV * T1.X, 0.0,
; CM-NEXT:    TEX 0 @0
; CM-NEXT:     VTX_READ_16 T1.X, T1.X, 40, #3
; CM-NEXT:    ALU 13, @4, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, T1.X, literal.x,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T1.W, PV.Z, T1.W,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  store i16 %in, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_i24:
; SI: s_lshr_b32 s{{[0-9]+}}, s{{[0-9]+}}, 16
; SI-DAG: buffer_store_byte
; SI-DAG: buffer_store_short

; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store can be eliminated
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store can be eliminated
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
define amdgpu_kernel void @store_i24(ptr addrspace(5) %out, i24 %in) {
; EG-LABEL: store_i24:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 41, @5, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T0.Z, KC0[2].Y, literal.x,
; EG-NEXT:     AND_INT T3.W, KC0[2].Z, literal.y,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    2(2.802597e-45), 65535(9.183409e-41)
; EG-NEXT:     AND_INT T1.Z, T0.X, PS,
; EG-NEXT:     LSHL T1.W, PV.W, T1.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T0.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T1.W, literal.x, PV.W,
; EG-NEXT:     MOV * T3.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; EG-NEXT:     BFE_UINT T3.W, KC0[2].Z, literal.x, PS,
; EG-NEXT:     NOT_INT * T1.W, PV.W,
; EG-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, T0.X, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T0.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i24:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 41, @5, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T0.Y, KC0[2].Y, literal.x,
; CM-NEXT:     AND_INT T0.Z, KC0[2].Z, literal.y,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    2(2.802597e-45), 65535(9.183409e-41)
; CM-NEXT:     AND_INT T1.Y, T0.X, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T1.W,
; CM-NEXT:     AND_INT * T1.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T0.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T0.Z, literal.x, PV.W,
; CM-NEXT:     MOV * T1.W, literal.y,
; CM-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; CM-NEXT:     BFE_UINT T2.Z, KC0[2].Z, literal.x, PV.W,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  store i24 %in, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_i25:
; SI: s_and_b32 [[AND:s[0-9]+]], s{{[0-9]+}}, 0x1ffffff{{$}}
; SI: v_mov_b32_e32 [[VAND:v[0-9]+]], [[AND]]
; SI: buffer_store_dword [[VAND]]

; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG-NOT: MOVA_INT

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM-NOT: MOVA_INT
define amdgpu_kernel void @store_i25(ptr addrspace(5) %out, i25 %in) {
; EG-LABEL: store_i25:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 4, @6, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT T0.W, KC0[2].Z, literal.x,
; EG-NEXT:     LSHR * T1.W, KC0[2].Y, literal.y,
; EG-NEXT:    33554431(9.403954e-38), 2(2.802597e-45)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PS,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i25:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 4, @6, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT T0.Z, KC0[2].Z, literal.x,
; CM-NEXT:     LSHR * T0.W, KC0[2].Y, literal.y,
; CM-NEXT:    33554431(9.403954e-38), 2(2.802597e-45)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; CM-NEXT:    RETURN
entry:
  store i25 %in, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v2i8:
; v2i8 is naturally 2B aligned, treat as i16
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG-NOT: MOVA_INT

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM-NOT: MOVA_INT

; SI: buffer_store_short
define amdgpu_kernel void @store_v2i8(ptr addrspace(5) %out, <2 x i32> %in) {
; EG-LABEL: store_v2i8:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 23, @7, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T0.Z, KC0[3].X, literal.x,
; EG-NEXT:     AND_INT T2.W, KC0[2].W, literal.y,
; EG-NEXT:     LSHL * T1.W, PV.W, literal.z,
; EG-NEXT:    8(1.121039e-44), 255(3.573311e-43)
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T3.W, literal.x, PS,
; EG-NEXT:     OR_INT * T2.W, PV.Z, PV.W,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     AND_INT T2.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T3.W, PV.W,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     AND_INT T3.W, T0.Y, PS,
; EG-NEXT:     LSHL * T1.W, PV.W, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v2i8:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 23, @7, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T1.Y, KC0[3].X, literal.x,
; CM-NEXT:     AND_INT T0.Z, KC0[2].W, literal.y,
; CM-NEXT:     LSHL * T1.W, PV.W, literal.z,
; CM-NEXT:    8(1.121039e-44), 255(3.573311e-43)
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     OR_INT * T2.W, PV.Y, PV.Z,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T2.W, PV.Z,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T1.W, PV.Z, T1.W,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <2 x i32> %in to <2 x i8>
  store <2 x i8> %0, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v2i8_unaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_byte
define amdgpu_kernel void @store_v2i8_unaligned(ptr addrspace(5) %out, <2 x i32> %in) {
; EG-LABEL: store_v2i8_unaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 50, @8, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     MOV T0.Z, T2.X, BS:VEC_120/SCL_212
; EG-NEXT:     AND_INT * T1.W, KC0[3].X, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, PV.Z, literal.x,
; EG-NEXT:     LSHL T1.W, PV.W, literal.y,
; EG-NEXT:     AND_INT * T2.W, KC0[2].Y, literal.z,
; EG-NEXT:    -65536(nan), 8(1.121039e-44)
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T1.Z, PS, literal.x,
; EG-NEXT:     OR_INT T1.W, PV.Z, PV.W,
; EG-NEXT:     AND_INT * T2.W, KC0[2].W, literal.y,
; EG-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; EG-NEXT:     OR_INT T1.W, PV.W, PS,
; EG-NEXT:     LSHL * T2.W, literal.x, PV.Z,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T0.Z, KC0[2].Y, 1,
; EG-NEXT:     NOT_INT T2.W, PS,
; EG-NEXT:     AND_INT * T3.W, PV.W, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     LSHL T1.Z, PS, T1.Z,
; EG-NEXT:     AND_INT T2.W, T0.Y, PV.W,
; EG-NEXT:     AND_INT * T3.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T3.W, PS, literal.x,
; EG-NEXT:     OR_INT * T2.W, PV.W, PV.Z,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T2.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T0.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T2.W, literal.x, PV.W,
; EG-NEXT:     AND_INT * T1.W, T1.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 65280(9.147676e-41)
; EG-NEXT:     LSHR T1.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T2.W, T0.Y, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T0.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v2i8_unaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 50, @8, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     MOV T0.Z, T2.X, BS:VEC_120/SCL_212
; CM-NEXT:     AND_INT * T1.W, KC0[3].X, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Y, PV.Z, literal.x,
; CM-NEXT:     LSHL T0.Z, PV.W, literal.y,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.z,
; CM-NEXT:    -65536(nan), 8(1.121039e-44)
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T2.Y, PV.W, literal.x,
; CM-NEXT:     OR_INT T0.Z, PV.Y, PV.Z,
; CM-NEXT:     AND_INT * T1.W, KC0[2].W, literal.y,
; CM-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; CM-NEXT:     OR_INT T0.Z, PV.Z, PV.W,
; CM-NEXT:     LSHL * T1.W, literal.x, PV.Y,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, 1,
; CM-NEXT:     NOT_INT T1.Z, PV.W,
; CM-NEXT:     AND_INT * T1.W, PV.Z, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     LSHL T2.Y, PV.W, T2.Y,
; CM-NEXT:     AND_INT T1.Z, T0.Y, PV.Z,
; CM-NEXT:     AND_INT * T1.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.Y,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     AND_INT * T1.W, T0.Z, literal.y,
; CM-NEXT:    255(3.573311e-43), 65280(9.147676e-41)
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <2 x i32> %in to <2 x i8>
  store <2 x i8> %0, ptr addrspace(5) %out, align 1
  ret void
}


; FUNC-LABEL: {{^}}store_v2i16:
; v2i8 is naturally 2B aligned, treat as i16
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG-NOT: MOVA_INT

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM-NOT: MOVA_INT

; SI: buffer_store_dword
define amdgpu_kernel void @store_v2i16(ptr addrspace(5) %out, <2 x i32> %in) {
; EG-LABEL: store_v2i16:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 7, @9, KC0[CB0:0-32], KC1[]
; EG-NEXT:     LSHL T0.W, KC0[3].X, literal.x,
; EG-NEXT:     AND_INT * T1.W, KC0[2].W, literal.y,
; EG-NEXT:    16(2.242078e-44), 65535(9.183409e-41)
; EG-NEXT:     OR_INT T0.W, PV.W, PS,
; EG-NEXT:     LSHR * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PS,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v2i16:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 7, @9, KC0[CB0:0-32], KC1[]
; CM-NEXT:     LSHL T0.Z, KC0[3].X, literal.x,
; CM-NEXT:     AND_INT * T0.W, KC0[2].W, literal.y,
; CM-NEXT:    16(2.242078e-44), 65535(9.183409e-41)
; CM-NEXT:     OR_INT T0.Z, PV.Z, PV.W,
; CM-NEXT:     LSHR * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <2 x i32> %in to <2 x i16>
  store <2 x i16> %0, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v2i16_unaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_short
; SI: buffer_store_short
define amdgpu_kernel void @store_v2i16_unaligned(ptr addrspace(5) %out, <2 x i32> %in) {
; EG-LABEL: store_v2i16_unaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 41, @10, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T0.Z, KC0[2].Y, literal.x,
; EG-NEXT:     AND_INT T3.W, KC0[2].W, literal.y,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    2(2.802597e-45), 65535(9.183409e-41)
; EG-NEXT:     AND_INT T1.Z, T0.Y, PS,
; EG-NEXT:     LSHL T1.W, PV.W, T1.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T0.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T1.W, literal.x, PV.W,
; EG-NEXT:     LSHL * T3.W, KC0[3].X, literal.y,
; EG-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; EG-NEXT:     LSHR T3.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T1.W, PV.W,
; EG-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, T0.Y, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T0.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v2i16_unaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 41, @10, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     AND_INT T0.Z, KC0[2].W, literal.y,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    2(2.802597e-45), 65535(9.183409e-41)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T1.W,
; CM-NEXT:     AND_INT * T1.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T0.Z, literal.x, PV.W,
; CM-NEXT:     LSHL * T1.W, KC0[3].X, literal.y,
; CM-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <2 x i32> %in to <2 x i16>
  store <2 x i16> %0, ptr addrspace(5) %out, align 2
  ret void
}

; FUNC-LABEL: {{^}}store_v4i8:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG-NOT: MOVA_INT

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM-NOT: MOVA_INT

; SI: buffer_store_dword
define amdgpu_kernel void @store_v4i8(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i8:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 17, @11, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[3].W, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, KC0[3].Z, literal.x,
; EG-NEXT:     LSHL T0.W, PV.W, literal.y,
; EG-NEXT:     LSHL * T1.W, KC0[4].X, literal.z,
; EG-NEXT:    255(3.573311e-43), 16(2.242078e-44)
; EG-NEXT:    24(3.363116e-44), 0(0.000000e+00)
; EG-NEXT:     OR_INT T0.W, PS, PV.W,
; EG-NEXT:     LSHL * T1.W, PV.Z, literal.x,
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     OR_INT T0.W, PV.W, PS,
; EG-NEXT:     AND_INT * T1.W, KC0[3].Y, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     OR_INT T0.W, PV.W, PS,
; EG-NEXT:     LSHR * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PS,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i8:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 17, @11, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[3].W, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Y, KC0[3].Z, literal.x,
; CM-NEXT:     LSHL T0.Z, PV.W, literal.y,
; CM-NEXT:     LSHL * T0.W, KC0[4].X, literal.z,
; CM-NEXT:    255(3.573311e-43), 16(2.242078e-44)
; CM-NEXT:    24(3.363116e-44), 0(0.000000e+00)
; CM-NEXT:     OR_INT T0.Z, PV.W, PV.Z,
; CM-NEXT:     LSHL * T0.W, PV.Y, literal.x,
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     OR_INT T0.Z, PV.Z, PV.W,
; CM-NEXT:     AND_INT * T0.W, KC0[3].Y, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     OR_INT T0.Z, PV.Z, PV.W,
; CM-NEXT:     LSHR * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <4 x i32> %in to <4 x i8>
  store <4 x i8> %0, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v4i8_unaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI-NOT: buffer_store_dword
define amdgpu_kernel void @store_v4i8_unaligned(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i8_unaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 92, @12, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     AND_INT * T1.W, PV.W, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T1.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T0.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T2.W, literal.x, PV.W,
; EG-NEXT:     LSHL * T3.W, KC0[4].X, literal.y,
; EG-NEXT:    255(3.573311e-43), 24(3.363116e-44)
; EG-NEXT:     ADD_INT T0.Z, KC0[2].Y, literal.x,
; EG-NEXT:     LSHR T4.W, PS, literal.y,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    2(2.802597e-45), 24(3.363116e-44)
; EG-NEXT:     AND_INT T1.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT T0.W, T0.Z, literal.x,
; EG-NEXT:     AND_INT * T1.W, KC0[3].W, literal.y,
; EG-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T4.W, literal.x, PV.W,
; EG-NEXT:     LSHL * T1.W, T1.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 16(2.242078e-44)
; EG-NEXT:     ADD_INT T0.Z, KC0[2].Y, 1,
; EG-NEXT:     LSHR T5.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T4.W, PV.W,
; EG-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T4.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T4.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T4.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT T0.W, T0.Z, literal.x,
; EG-NEXT:     AND_INT * T2.W, KC0[3].Z, literal.y,
; EG-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T5.W, literal.x, PV.W,
; EG-NEXT:     LSHL * T2.W, T2.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; EG-NEXT:     LSHR T6.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T5.W, PV.W,
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T5.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T5.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T4.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T5.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT T0.W, KC0[2].Y, literal.x,
; EG-NEXT:     OR_INT * T2.W, T3.W, T2.W,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     OR_INT T0.Z, PS, T1.W,
; EG-NEXT:     AND_INT T1.W, KC0[3].Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 3(4.203895e-45)
; EG-NEXT:     LSHL T2.W, literal.x, PS,
; EG-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T2.W, T0.Y, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T0.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T5.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i8_unaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 92, @12, KC0[CB0:0-32], KC1[]
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT * T1.W, PV.W, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T1.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T0.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T0.Z, literal.x, PV.W,
; CM-NEXT:     LSHL * T2.W, KC0[4].X, literal.y,
; CM-NEXT:    255(3.573311e-43), 24(3.363116e-44)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     LSHR T1.Z, PV.W, literal.y,
; CM-NEXT:     NOT_INT * T3.W, PV.Z,
; CM-NEXT:    2(2.802597e-45), 24(3.363116e-44)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, KC0[3].W, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    255(3.573311e-43), 3(4.203895e-45)
; CM-NEXT:     LSHL T2.Z, literal.x, PV.W,
; CM-NEXT:     LSHL * T1.W, PV.Z, literal.y,
; CM-NEXT:    255(3.573311e-43), 16(2.242078e-44)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, 1,
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T3.W, PV.Z,
; CM-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, KC0[3].Z, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    255(3.573311e-43), 3(4.203895e-45)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     LSHL * T3.W, PV.Z, literal.y,
; CM-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T4.W, PV.Z,
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT T0.Z, KC0[2].Y, literal.x,
; CM-NEXT:     OR_INT * T0.W, T2.W, T3.W,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     OR_INT T1.Y, PV.W, T1.W,
; CM-NEXT:     AND_INT T2.Z, KC0[3].Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.Z, literal.y,
; CM-NEXT:    255(3.573311e-43), 3(4.203895e-45)
; CM-NEXT:     LSHL T0.Z, literal.x, PV.W,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T2.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <4 x i32> %in to <4 x i8>
  store <4 x i8> %0, ptr addrspace(5) %out, align 1
  ret void
}

; FUNC-LABEL: {{^}}store_v8i8_unaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI: buffer_store_byte
; SI-NOT: buffer_store_dword
define amdgpu_kernel void @store_v8i8_unaligned(ptr addrspace(5) %out, <8 x i32> %in) {
; EG-LABEL: store_v8i8_unaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 103, @13, KC0[CB0:0-32], KC1[]
; EG-NEXT:     MOV T0.Y, T3.X,
; EG-NEXT:     AND_INT * T0.W, KC0[5].X, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    16777215(2.350989e-38), 24(3.363116e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T3.X, PV.W,
; EG-NEXT:     MOV T0.Y, T2.X,
; EG-NEXT:     AND_INT * T0.W, KC0[6].X, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    16777215(2.350989e-38), 24(3.363116e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T2.X, PV.W,
; EG-NEXT:     MOV T0.Y, T3.X,
; EG-NEXT:     AND_INT * T0.W, KC0[4].Z, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    -65281(nan), 8(1.121039e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T3.X, PV.W,
; EG-NEXT:     MOV T0.Y, T2.X,
; EG-NEXT:     AND_INT * T0.W, KC0[5].Z, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    -65281(nan), 8(1.121039e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T2.X, PV.W,
; EG-NEXT:     MOV T0.Y, T3.X,
; EG-NEXT:     AND_INT * T0.W, KC0[4].W, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    -16711681(-1.714704e+38), 16(2.242078e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T3.X, PV.W,
; EG-NEXT:     MOV T0.Y, T2.X,
; EG-NEXT:     AND_INT * T0.W, KC0[5].W, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    -16711681(-1.714704e+38), 16(2.242078e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T2.X, PV.W,
; EG-NEXT:     MOV T0.Y, T3.X,
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; EG-NEXT:     AND_INT T1.W, KC0[4].Y, literal.y,
; EG-NEXT:     AND_INT * T2.W, PV.W, literal.z,
; EG-NEXT:    -256(nan), 255(3.573311e-43)
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOV * T3.X, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.Z, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T0.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T3.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     LSHR T4.W, T0.Y, literal.y,
; EG-NEXT:     NOT_INT * T3.W, PV.W,
; EG-NEXT:    2(2.802597e-45), 24(3.363116e-44)
; EG-NEXT:     AND_INT T0.Z, T0.Z, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T3.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T3.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOV * T0.Z, T2.X,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV T0.W, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T2.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, PS, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T4.W, literal.x, PV.W,
; EG-NEXT:     MOV * T5.W, literal.y,
; EG-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, 1,
; EG-NEXT:     BFE_UINT T6.W, T0.Y, literal.x, PS,
; EG-NEXT:     NOT_INT * T4.W, PV.W,
; EG-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T2.Z, T0.W, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T2.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    ALU 110, @14, KC0[CB0:0-32], KC1[]
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.W, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T3.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T3.W, PS, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T4.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     BFE_UINT T6.W, T0.Y, literal.x, T5.W,
; EG-NEXT:     NOT_INT * T4.W, PV.W,
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.Z, T0.W, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T3.W,
; EG-NEXT:     AND_INT * T3.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T3.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     AND_INT T1.W, T1.W, literal.y,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    7(9.809089e-45), 255(3.573311e-43)
; EG-NEXT:     AND_INT T2.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T1.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T1.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     LSHR T3.W, T0.Z, literal.y,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    6(8.407791e-45), 24(3.363116e-44)
; EG-NEXT:     AND_INT T2.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     BFE_UINT T3.W, T0.Z, literal.y, T5.W,
; EG-NEXT:     NOT_INT * T1.W, PV.W,
; EG-NEXT:    5(7.006492e-45), 16(2.242078e-44)
; EG-NEXT:     AND_INT T2.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T1.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T1.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T0.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T2.W, literal.x, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     BFE_UINT T3.W, T0.Z, literal.y, T5.W,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    4(5.605194e-45), 8(1.121039e-44)
; EG-NEXT:     AND_INT T2.Z, T0.Y, PS,
; EG-NEXT:     LSHL T0.W, PV.W, T0.W,
; EG-NEXT:     AND_INT * T2.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T2.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T0.W, T1.Z, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, T0.Z, literal.x,
; EG-NEXT:     AND_INT * T1.W, KC0[5].Y, literal.y,
; EG-NEXT:    -256(nan), 255(3.573311e-43)
; EG-NEXT:    ALU 12, @15, KC0[], KC1[]
; EG-NEXT:     LSHL * T0.W, T0.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T3.W, literal.x, PV.W,
; EG-NEXT:     OR_INT * T1.W, T0.Z, T1.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T3.W, PV.W,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     AND_INT T3.W, T0.Y, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T0.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T2.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v8i8_unaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 103, @13, KC0[CB0:0-32], KC1[]
; CM-NEXT:     MOV T0.Y, T3.X,
; CM-NEXT:     AND_INT * T0.W, KC0[5].X, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    16777215(2.350989e-38), 24(3.363116e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T3.X, PV.W,
; CM-NEXT:     MOV T0.Y, T2.X,
; CM-NEXT:     AND_INT * T0.W, KC0[6].X, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    16777215(2.350989e-38), 24(3.363116e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T2.X, PV.W,
; CM-NEXT:     MOV T0.Y, T3.X,
; CM-NEXT:     AND_INT * T0.W, KC0[4].Z, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    -65281(nan), 8(1.121039e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T3.X, PV.W,
; CM-NEXT:     MOV T0.Y, T2.X,
; CM-NEXT:     AND_INT * T0.W, KC0[5].Z, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    -65281(nan), 8(1.121039e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T2.X, PV.W,
; CM-NEXT:     MOV T0.Y, T3.X,
; CM-NEXT:     AND_INT * T0.W, KC0[4].W, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    -16711681(-1.714704e+38), 16(2.242078e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T3.X, PV.W,
; CM-NEXT:     MOV T0.Y, T2.X,
; CM-NEXT:     AND_INT * T0.W, KC0[5].W, literal.x,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    -16711681(-1.714704e+38), 16(2.242078e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T2.X, PV.W,
; CM-NEXT:     MOV T0.Y, T3.X,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Y, PV.Y, literal.x,
; CM-NEXT:     AND_INT T0.Z, KC0[4].Y, literal.y,
; CM-NEXT:     AND_INT * T1.W, PV.W, literal.z,
; CM-NEXT:    -256(nan), 255(3.573311e-43)
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOV * T3.X, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV T0.Z, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T0.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     LSHR T2.Z, T0.Y, literal.y,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    2(2.802597e-45), 24(3.363116e-44)
; CM-NEXT:     AND_INT T2.Y, T0.Z, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOV * T0.Z, T2.X,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T0.W, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T2.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     MOV * T3.W, literal.y,
; CM-NEXT:    255(3.573311e-43), 8(1.121039e-44)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, 1,
; CM-NEXT:     BFE_UINT T3.Z, T0.Y, literal.x, PV.W,
; CM-NEXT:     NOT_INT * T4.W, PV.Z,
; CM-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T2.Y, T0.W, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T2.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T3.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    ALU 110, @14, KC0[CB0:0-32], KC1[]
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV * T0.W, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T2.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T4.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     BFE_UINT T1.Z, T0.Y, literal.x, T3.W,
; CM-NEXT:     NOT_INT * T4.W, PV.W,
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Y, T0.W, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T2.W,
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     AND_INT T1.Z, T1.W, literal.y,
; CM-NEXT:     NOT_INT * T1.W, PV.W,
; CM-NEXT:    7(9.809089e-45), 255(3.573311e-43)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T3.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     LSHR T1.Z, T0.Z, literal.y,
; CM-NEXT:     NOT_INT * T1.W, PV.W,
; CM-NEXT:    6(8.407791e-45), 24(3.363116e-44)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     BFE_UINT T1.Z, T0.Z, literal.y, T3.W,
; CM-NEXT:     NOT_INT * T1.W, PV.W,
; CM-NEXT:    5(7.006492e-45), 16(2.242078e-44)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T3.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T0.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     BFE_UINT T1.Z, T0.Z, literal.y, T3.W,
; CM-NEXT:     NOT_INT * T1.W, PV.W,
; CM-NEXT:    4(5.605194e-45), 8(1.121039e-44)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T0.W,
; CM-NEXT:     AND_INT * T0.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T3.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T0.W, T1.Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Y, T0.Z, literal.x,
; CM-NEXT:     AND_INT * T0.Z, KC0[5].Y, literal.y,
; CM-NEXT:    -256(nan), 255(3.573311e-43)
; CM-NEXT:    ALU 12, @15, KC0[], KC1[]
; CM-NEXT:     LSHL * T0.W, T0.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     OR_INT * T1.W, T1.Y, T0.Z,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <8 x i32> %in to <8 x i8>
  store <8 x i8> %0, ptr addrspace(5) %out, align 1
  ret void
}

; FUNC-LABEL: {{^}}store_v4i8_halfaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; TODO: This load and store cannot be eliminated,
;       they might be different locations
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_short
; SI: buffer_store_short
; SI-NOT: buffer_store_dword
define amdgpu_kernel void @store_v4i8_halfaligned(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i8_halfaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 50, @16, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT T1.W, KC0[2].Y, literal.x,
; EG-NEXT:     AND_INT * T2.W, KC0[3].Z, literal.y,
; EG-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T0.Z, literal.x, PV.W,
; EG-NEXT:     LSHL T2.W, T2.W, literal.y,
; EG-NEXT:     AND_INT * T3.W, KC0[3].Y, literal.z,
; EG-NEXT:    65535(9.183409e-41), 8(1.121039e-44)
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     OR_INT T3.W, PV.W, PS,
; EG-NEXT:     NOT_INT * T4.W, PV.Z,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     AND_INT T0.Z, T0.Y, PS,
; EG-NEXT:     LSHL T1.W, PV.W, T1.W,
; EG-NEXT:     AND_INT * T3.W, PV.Z, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR T3.W, PS, literal.x,
; EG-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT T0.Z, T1.Z, literal.x,
; EG-NEXT:     AND_INT T0.W, KC0[3].W, literal.y,
; EG-NEXT:     LSHL * T1.W, KC0[4].X, literal.z,
; EG-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; EG-NEXT:    24(3.363116e-44), 0(0.000000e+00)
; EG-NEXT:     OR_INT T1.Z, PS, T2.W,
; EG-NEXT:     LSHL T0.W, PV.W, literal.x,
; EG-NEXT:     LSHL * T1.W, PV.Z, literal.y,
; EG-NEXT:    16(2.242078e-44), 3(4.203895e-45)
; EG-NEXT:     LSHL T2.W, literal.x, PS,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.W, PS, literal.x,
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; EG-NEXT:     AND_INT T2.W, T0.Y, PS,
; EG-NEXT:     LSHL * T0.W, PV.W, T1.W,
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T3.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i8_halfaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 50, @16, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, KC0[3].Z, literal.x,
; CM-NEXT:     LSHL * T1.W, PV.W, literal.y,
; CM-NEXT:    255(3.573311e-43), 3(4.203895e-45)
; CM-NEXT:     LSHL T1.Y, literal.x, PV.W,
; CM-NEXT:     LSHL T0.Z, PV.Z, literal.y,
; CM-NEXT:     AND_INT * T2.W, KC0[3].Y, literal.z,
; CM-NEXT:    65535(9.183409e-41), 8(1.121039e-44)
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     ADD_INT T2.Y, KC0[2].Y, literal.x,
; CM-NEXT:     OR_INT T1.Z, PV.Z, PV.W,
; CM-NEXT:     NOT_INT * T2.W, PV.Y,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Y, T0.Y, PV.W,
; CM-NEXT:     LSHL T1.Z, PV.Z, T1.W,
; CM-NEXT:     AND_INT * T1.W, PV.Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR T2.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T0.Y, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT T1.Y, T2.Y, literal.x,
; CM-NEXT:     AND_INT T1.Z, KC0[3].W, literal.y,
; CM-NEXT:     LSHL * T0.W, KC0[4].X, literal.z,
; CM-NEXT:    3(4.203895e-45), 255(3.573311e-43)
; CM-NEXT:    24(3.363116e-44), 0(0.000000e+00)
; CM-NEXT:     OR_INT T2.Y, PV.W, T0.Z,
; CM-NEXT:     LSHL T0.Z, PV.Z, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.Y, literal.y,
; CM-NEXT:    16(2.242078e-44), 3(4.203895e-45)
; CM-NEXT:     LSHL T1.Z, literal.x, PV.W,
; CM-NEXT:     OR_INT * T1.W, PV.Y, PV.Z,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     NOT_INT * T1.W, PV.Z,
; CM-NEXT:    16(2.242078e-44), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.Y, PV.W,
; CM-NEXT:     LSHL * T0.W, PV.Z, T0.W,
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <4 x i32> %in to <4 x i8>
  store <4 x i8> %0, ptr addrspace(5) %out, align 2
  ret void
}

; floating-point store
; FUNC-LABEL: {{^}}store_f32:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_dword

define amdgpu_kernel void @store_f32(ptr addrspace(5) %out, float %in) {
; EG-LABEL: store_f32:
; EG:       ; %bb.0:
; EG-NEXT:    ALU 4, @17, KC0[CB0:0-32], KC1[]
; EG-NEXT:     LSHR T0.W, KC0[2].Y, literal.x,
; EG-NEXT:     MOV * T1.W, KC0[2].Z,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_f32:
; CM:       ; %bb.0:
; CM-NEXT:    ALU 4, @17, KC0[CB0:0-32], KC1[]
; CM-NEXT:     LSHR T0.Z, KC0[2].Y, literal.x,
; CM-NEXT:     MOV * T0.W, KC0[2].Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
  store float %in, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v4i16:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x2?
; XSI: buffer_store_dwordx2
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @store_v4i16(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i16:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 37, @18, KC0[CB0:0-32], KC1[]
; EG-NEXT:     MOV T0.Y, T3.X,
; EG-NEXT:     AND_INT * T0.W, KC0[4].X, literal.x,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV * T3.X, PV.W,
; EG-NEXT:     MOV * T0.Y, PV.X,
; EG-NEXT:     AND_INT T0.W, PV.Y, literal.x,
; EG-NEXT:     AND_INT * T1.W, KC0[3].W, literal.y,
; EG-NEXT:    -65536(nan), 65535(9.183409e-41)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV T3.X, PV.W,
; EG-NEXT:     MOV T0.Y, T2.X,
; EG-NEXT:     AND_INT * T0.W, KC0[3].Z, literal.x,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     AND_INT T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHL * T0.W, PV.W, literal.y,
; EG-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; EG-NEXT:     OR_INT * T0.W, PV.W, PS,
; EG-NEXT:     MOV * T2.X, PV.W,
; EG-NEXT:     MOV * T0.Y, PV.X,
; EG-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; EG-NEXT:     AND_INT T0.W, KC0[3].Y, literal.y,
; EG-NEXT:     ADD_INT * T1.W, KC0[2].Y, literal.z,
; EG-NEXT:    -65536(nan), 65535(9.183409e-41)
; EG-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; EG-NEXT:     LSHR T1.Z, KC0[2].Y, literal.x,
; EG-NEXT:     LSHR T1.W, PS, literal.x,
; EG-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOV T2.X, PS,
; EG-NEXT:     MOV * T0.Y, T3.X,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i16:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 37, @18, KC0[CB0:0-32], KC1[]
; CM-NEXT:     MOV T0.Y, T3.X,
; CM-NEXT:     AND_INT * T0.W, KC0[4].X, literal.x,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV * T3.X, PV.W,
; CM-NEXT:     MOV * T0.Y, PV.X,
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     AND_INT * T0.W, KC0[3].W, literal.y,
; CM-NEXT:    -65536(nan), 65535(9.183409e-41)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV T3.X, PV.W,
; CM-NEXT:     MOV T0.Y, T2.X,
; CM-NEXT:     AND_INT * T0.W, KC0[3].Z, literal.x,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, PV.Y, literal.x,
; CM-NEXT:     LSHL * T0.W, PV.W, literal.y,
; CM-NEXT:    65535(9.183409e-41), 16(2.242078e-44)
; CM-NEXT:     OR_INT * T0.W, PV.Z, PV.W,
; CM-NEXT:     MOV * T2.X, PV.W,
; CM-NEXT:     MOV * T0.Y, PV.X,
; CM-NEXT:     AND_INT T0.Y, PV.Y, literal.x,
; CM-NEXT:     AND_INT T0.Z, KC0[3].Y, literal.y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.z,
; CM-NEXT:    -65536(nan), 65535(9.183409e-41)
; CM-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.Y, KC0[2].Y, literal.x,
; CM-NEXT:     LSHR T1.Z, PV.W, literal.x,
; CM-NEXT:     OR_INT * T0.W, PV.Y, PV.Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOV T2.X, PV.W,
; CM-NEXT:     MOV * T0.Y, T3.X,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Y,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc <4 x i32> %in to <4 x i16>
  store <4 x i16> %0, ptr addrspace(5) %out
  ret void
}

; vec2 floating-point stores
; FUNC-LABEL: {{^}}store_v2f32:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x2?
; XSI: buffer_store_dwordx2
; SI: buffer_store_dword
; SI: buffer_store_dword

define amdgpu_kernel void @store_v2f32(ptr addrspace(5) %out, float %a, float %b) {
; EG-LABEL: store_v2f32:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 10, @19, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.Y, KC0[2].Y, literal.x,
; EG-NEXT:     MOV T0.Z, KC0[2].Z,
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     MOV * T1.W, KC0[2].W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Y,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v2f32:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 10, @19, KC0[CB0:0-32], KC1[]
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; CM-NEXT:     LSHR T0.X, KC0[2].Y, literal.x,
; CM-NEXT:     MOV T0.Y, KC0[2].Z,
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     MOV * T0.W, KC0[2].W,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; CM-NEXT:    RETURN
entry:
  %0 = insertelement <2 x float> <float 0.0, float 0.0>, float %a, i32 0
  %1 = insertelement <2 x float> %0, float %b, i32 1
  store <2 x float> %1, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v3i32:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x2?
; XSI-DAG: buffer_store_dwordx2
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword

define amdgpu_kernel void @store_v3i32(ptr addrspace(5) %out, <3 x i32> %a) nounwind {
; EG-LABEL: store_v3i32:
; EG:       ; %bb.0:
; EG-NEXT:    ALU 16, @20, KC0[CB0:0-32], KC1[]
; EG-NEXT:     LSHR T0.Z, KC0[2].Y, literal.x,
; EG-NEXT:     ADD_INT T0.W, KC0[2].Y, literal.y,
; EG-NEXT:     ADD_INT * T1.W, KC0[2].Y, literal.z,
; EG-NEXT:    2(2.802597e-45), 8(1.121039e-44)
; EG-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; EG-NEXT:     MOV T0.X, KC0[3].Y,
; EG-NEXT:     LSHR T0.Y, PS, literal.x,
; EG-NEXT:     MOV T1.Z, KC0[3].Z,
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     MOV * T1.W, KC0[3].W,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Y,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.Z,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.X,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v3i32:
; CM:       ; %bb.0:
; CM-NEXT:    ALU 16, @20, KC0[CB0:0-32], KC1[]
; CM-NEXT:     LSHR T0.X, KC0[2].Y, literal.x,
; CM-NEXT:     MOV T0.Y, KC0[3].Y,
; CM-NEXT:     ADD_INT T0.Z, KC0[2].Y, literal.y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.z,
; CM-NEXT:    2(2.802597e-45), 8(1.121039e-44)
; CM-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.X, PV.W, literal.x,
; CM-NEXT:     MOV T1.Y, KC0[3].Z,
; CM-NEXT:     LSHR T0.Z, PV.Z, literal.x,
; CM-NEXT:     MOV * T0.W, KC0[3].W,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; CM-NEXT:    RETURN
  store <3 x i32> %a, ptr addrspace(5) %out, align 16
  ret void
}

; FUNC-LABEL: {{^}}store_v4i32:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x4?
; XSI: buffer_store_dwordx4
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @store_v4i32(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i32:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 22, @21, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    12(1.681558e-44), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.X, KC0[2].Y, literal.x,
; EG-NEXT:     MOV T0.Y, KC0[3].Y,
; EG-NEXT:     LSHR T0.Z, PV.W, literal.x,
; EG-NEXT:     ADD_INT T0.W, KC0[2].Y, literal.y,
; EG-NEXT:     ADD_INT * T1.W, KC0[2].Y, literal.z,
; EG-NEXT:    2(2.802597e-45), 4(5.605194e-45)
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     MOV T1.X, KC0[4].X,
; EG-NEXT:     LSHR T1.Y, PS, literal.x,
; EG-NEXT:     MOV T1.Z, KC0[3].W,
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     MOV * T1.W, KC0[3].Z,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.Y,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.Z,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.X,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i32:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 22, @21, KC0[CB0:0-32], KC1[]
; CM-NEXT:     LSHR T0.Y, KC0[2].Y, literal.x,
; CM-NEXT:     MOV T0.Z, KC0[3].Y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.y,
; CM-NEXT:    2(2.802597e-45), 12(1.681558e-44)
; CM-NEXT:     LSHR T0.X, PV.W, literal.x,
; CM-NEXT:     MOV T1.Y, KC0[4].X,
; CM-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.z,
; CM-NEXT:    2(2.802597e-45), 4(5.605194e-45)
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.X, PV.W, literal.x,
; CM-NEXT:     MOV T2.Y, KC0[3].W,
; CM-NEXT:     LSHR T1.Z, PV.Z, literal.x,
; CM-NEXT:     MOV * T0.W, KC0[3].Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T2.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.Y,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; CM-NEXT:    RETURN
entry:
  store <4 x i32> %in, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_v4i32_unaligned:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x4?
; XSI: buffer_store_dwordx4
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @store_v4i32_unaligned(ptr addrspace(5) %out, <4 x i32> %in) {
; EG-LABEL: store_v4i32_unaligned:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 22, @22, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    12(1.681558e-44), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.X, KC0[2].Y, literal.x,
; EG-NEXT:     MOV T0.Y, KC0[3].Y,
; EG-NEXT:     LSHR T0.Z, PV.W, literal.x,
; EG-NEXT:     ADD_INT T0.W, KC0[2].Y, literal.y,
; EG-NEXT:     ADD_INT * T1.W, KC0[2].Y, literal.z,
; EG-NEXT:    2(2.802597e-45), 4(5.605194e-45)
; EG-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; EG-NEXT:     MOV T1.X, KC0[4].X,
; EG-NEXT:     LSHR T1.Y, PS, literal.x,
; EG-NEXT:     MOV T1.Z, KC0[3].W,
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     MOV * T1.W, KC0[3].Z,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.Y,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.Z,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.X,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4i32_unaligned:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 22, @22, KC0[CB0:0-32], KC1[]
; CM-NEXT:     LSHR T0.Y, KC0[2].Y, literal.x,
; CM-NEXT:     MOV T0.Z, KC0[3].Y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.y,
; CM-NEXT:    2(2.802597e-45), 12(1.681558e-44)
; CM-NEXT:     LSHR T0.X, PV.W, literal.x,
; CM-NEXT:     MOV T1.Y, KC0[4].X,
; CM-NEXT:     ADD_INT T1.Z, KC0[2].Y, literal.y,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.z,
; CM-NEXT:    2(2.802597e-45), 4(5.605194e-45)
; CM-NEXT:    8(1.121039e-44), 0(0.000000e+00)
; CM-NEXT:     LSHR T1.X, PV.W, literal.x,
; CM-NEXT:     MOV T2.Y, KC0[3].W,
; CM-NEXT:     LSHR T1.Z, PV.Z, literal.x,
; CM-NEXT:     MOV * T0.W, KC0[3].Z,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T2.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.Y,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Z,
; CM-NEXT:    RETURN
entry:
  store <4 x i32> %in, ptr addrspace(5) %out, align 4
  ret void
}

; v4f32 store
; FUNC-LABEL: {{^}}store_v4f32:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x4?
; XSI: buffer_store_dwordx4
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @store_v4f32(ptr addrspace(5) %out, ptr addrspace(5) %in) {
; EG-LABEL: store_v4f32:
; EG:       ; %bb.0:
; EG-NEXT:    ALU 34, @23, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    12(1.681558e-44), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.Z, KC0[2].Y, literal.x,
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     ADD_INT * T1.W, KC0[2].Y, literal.y,
; EG-NEXT:    2(2.802597e-45), 8(1.121039e-44)
; EG-NEXT:     LSHR T0.X, PS, literal.x,
; EG-NEXT:     ADD_INT T0.Y, KC0[2].Z, literal.y,
; EG-NEXT:     ADD_INT T1.Z, KC0[2].Z, literal.z,
; EG-NEXT:     ADD_INT T1.W, KC0[2].Z, literal.w,
; EG-NEXT:     ADD_INT * T2.W, KC0[2].Y, literal.w,
; EG-NEXT:    2(2.802597e-45), 12(1.681558e-44)
; EG-NEXT:    8(1.121039e-44), 4(5.605194e-45)
; EG-NEXT:     LSHR T1.X, PS, literal.x,
; EG-NEXT:     LSHR T1.Y, PV.W, literal.x,
; EG-NEXT:     LSHR T1.Z, PV.Z, literal.x,
; EG-NEXT:     LSHR T1.W, PV.Y, literal.x,
; EG-NEXT:     LSHR * T2.W, KC0[2].Z, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PS,
; EG-NEXT:     MOV * T0.Y, T(0 + AR.x).X+,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.W,
; EG-NEXT:     MOV * T1.W, T(0 + AR.x).X+,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; EG-NEXT:     MOV * T1.Z, T(0 + AR.x).X+,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.Y,
; EG-NEXT:     MOV * T1.Y, T(0 + AR.x).X+,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T1.X,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.Y,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.Z,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_v4f32:
; CM:       ; %bb.0:
; CM-NEXT:    ALU 34, @23, KC0[CB0:0-32], KC1[]
; CM-NEXT:     ADD_INT T0.Z, KC0[2].Y, literal.x,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.y,
; CM-NEXT:    8(1.121039e-44), 12(1.681558e-44)
; CM-NEXT:     LSHR T0.X, KC0[2].Y, literal.x,
; CM-NEXT:     LSHR T0.Y, PV.W, literal.x,
; CM-NEXT:     LSHR T0.Z, PV.Z, literal.x,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.y,
; CM-NEXT:    2(2.802597e-45), 4(5.605194e-45)
; CM-NEXT:     LSHR T1.X, PV.W, literal.x,
; CM-NEXT:     ADD_INT T1.Y, KC0[2].Z, literal.y,
; CM-NEXT:     ADD_INT T1.Z, KC0[2].Z, literal.z,
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Z, literal.w,
; CM-NEXT:    2(2.802597e-45), 12(1.681558e-44)
; CM-NEXT:    8(1.121039e-44), 4(5.605194e-45)
; CM-NEXT:     LSHR T2.X, PV.W, literal.x,
; CM-NEXT:     LSHR T2.Y, PV.Z, literal.x,
; CM-NEXT:     LSHR T1.Z, PV.Y, literal.x,
; CM-NEXT:     LSHR * T0.W, KC0[2].Z, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T0.W, T(0 + AR.x).X+,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.Z,
; CM-NEXT:     MOV * T1.Y, T(0 + AR.x).X+,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.Y,
; CM-NEXT:     MOV * T1.Z, T(0 + AR.x).X+,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T2.X,
; CM-NEXT:     MOV * T1.W, T(0 + AR.x).X+,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T1.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.Z,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.Y,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.Y,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.X,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.W,
; CM-NEXT:    RETURN
  %1 = load <4 x float>, ptr addrspace(5) %in
  store <4 x float> %1, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_i64_i8:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_byte
define amdgpu_kernel void @store_i64_i8(ptr addrspace(5) %out, i64 %in) {
; EG-LABEL: store_i64_i8:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 18, @24, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T2.W, literal.x, PV.W,
; EG-NEXT:     AND_INT * T3.W, KC0[2].W, literal.x,
; EG-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:     AND_INT T2.W, T0.X, PV.W,
; EG-NEXT:     LSHL * T1.W, T3.W, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i64_i8:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 19, @24, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, KC0[2].W, literal.x,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    255(3.573311e-43), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T1.W, PV.Z, T1.W,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc i64 %in to i8
  store i8 %0, ptr addrspace(5) %out
  ret void
}

; FUNC-LABEL: {{^}}store_i64_i16:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}{{T[0-9]+\.[XYZW]}}, T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

; SI: buffer_store_short
define amdgpu_kernel void @store_i64_i16(ptr addrspace(5) %out, i64 %in) {
; EG-LABEL: store_i64_i16:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 18, @25, KC0[CB0:0-32], KC1[]
; EG-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    -4(nan), 0(0.000000e+00)
; EG-NEXT:     LSHR * T0.W, PV.W, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; EG-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; EG-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL * T1.W, PV.W, literal.x,
; EG-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; EG-NEXT:     LSHL T2.W, literal.x, PV.W,
; EG-NEXT:     AND_INT * T3.W, KC0[2].W, literal.x,
; EG-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; EG-NEXT:     NOT_INT * T2.W, PV.W,
; EG-NEXT:     AND_INT T2.W, T0.X, PV.W,
; EG-NEXT:     LSHL * T1.W, T3.W, T1.W,
; EG-NEXT:     OR_INT * T1.W, PV.W, PS,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; EG-NEXT:    RETURN
;
; CM-LABEL: store_i64_i16:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 19, @25, KC0[CB0:0-32], KC1[]
; CM-NEXT:     AND_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    -4(nan), 0(0.000000e+00)
; CM-NEXT:     LSHR * T0.W, PV.W, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV T0.X, T(0 + AR.x).X+,
; CM-NEXT:     AND_INT * T1.W, KC0[2].Y, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T1.W, PV.W, literal.x,
; CM-NEXT:    3(4.203895e-45), 0(0.000000e+00)
; CM-NEXT:     LSHL * T2.W, literal.x, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T0.Z, KC0[2].W, literal.x,
; CM-NEXT:     NOT_INT * T2.W, PV.W,
; CM-NEXT:    65535(9.183409e-41), 0(0.000000e+00)
; CM-NEXT:     AND_INT T1.Z, T0.X, PV.W,
; CM-NEXT:     LSHL * T1.W, PV.Z, T1.W,
; CM-NEXT:     OR_INT * T1.W, PV.Z, PV.W,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T1.W,
; CM-NEXT:    RETURN
entry:
  %0 = trunc i64 %in to i16
  store i16 %0, ptr addrspace(5) %out
  ret void
}

; The stores in this function are combined by the optimizer to create a
; 64-bit store with 32-bit alignment.  This is legal and the legalizer
; should not try to split the 64-bit store back into 2 32-bit stores.

; FUNC-LABEL: {{^}}vecload2:
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x2?
; XSI: buffer_store_dwordx2
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @vecload2(ptr addrspace(5) nocapture %out, ptr addrspace(4) nocapture %mem) #0 {
; EG-LABEL: vecload2:
; EG:       ; %bb.0: ; %entry
; EG-NEXT:    ALU 0, @26, KC0[CB0:0-32], KC1[]
; EG-NEXT:     MOV * T0.X, KC0[2].Z,
; EG-NEXT:    TEX 0 @0
; EG-NEXT:     VTX_READ_64 T0.XY, T0.X, 0, #1
; EG-NEXT:    ALU 8, @27, KC0[CB0:0-32], KC1[]
; EG-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; EG-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; EG-NEXT:     LSHR T0.W, PV.W, literal.x,
; EG-NEXT:     LSHR * T1.W, KC0[2].Y, literal.x,
; EG-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; EG-NEXT:     MOVA_INT * AR.x (MASKED), PS,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.X,
; EG-NEXT:     MOVA_INT * AR.x (MASKED), T0.W,
; EG-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; EG-NEXT:    RETURN
;
; CM-LABEL: vecload2:
; CM:       ; %bb.0: ; %entry
; CM-NEXT:    ALU 0, @26, KC0[CB0:0-32], KC1[]
; CM-NEXT:     MOV * T0.X, KC0[2].Z,
; CM-NEXT:    TEX 0 @0
; CM-NEXT:     VTX_READ_64 T0.XY, T0.X, 0, #1
; CM-NEXT:    ALU 8, @27, KC0[CB0:0-32], KC1[]
; CM-NEXT:     ADD_INT * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    4(5.605194e-45), 0(0.000000e+00)
; CM-NEXT:     LSHR T0.Z, PV.W, literal.x,
; CM-NEXT:     LSHR * T0.W, KC0[2].Y, literal.x,
; CM-NEXT:    2(2.802597e-45), 0(0.000000e+00)
; CM-NEXT:     MOVA_INT * AR.x (MASKED), PV.W,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.X,
; CM-NEXT:     MOVA_INT * AR.x (MASKED), T0.Z,
; CM-NEXT:     MOV * T(0 + AR.x).X+, T0.Y,
; CM-NEXT:    RETURN
entry:
  %0 = load i32, ptr addrspace(4) %mem, align 4
  %arrayidx1.i = getelementptr inbounds i32, ptr addrspace(4) %mem, i64 1
  %1 = load i32, ptr addrspace(4) %arrayidx1.i, align 4
  store i32 %0, ptr addrspace(5) %out, align 4
  %arrayidx1 = getelementptr inbounds i32, ptr addrspace(5) %out, i64 1
  store i32 %1, ptr addrspace(5) %arrayidx1, align 4
  ret void
}

; When i128 was a legal type this program generated cannot select errors:

; FUNC-LABEL: {{^}}"i128-const-store":
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,
; EG: MOVA_INT
; EG: MOV {{[\* ]*}}T(0 + AR.x).X+,

; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,
; CM: MOVA_INT
; CM: MOV {{[\* ]*}}T(0 + AR.x).X+,

;TODO: why not x4?
; XSI: buffer_store_dwordx4
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
; SI: buffer_store_dword
define amdgpu_kernel void @i128-const-store(ptr addrspace(5) %out) {
entry:
  store i32 1, ptr addrspace(5) %out, align 4
  %arrayidx2 = getelementptr inbounds i32, ptr addrspace(5) %out, i64 1
  store i32 1, ptr addrspace(5) %arrayidx2, align 4
  %arrayidx4 = getelementptr inbounds i32, ptr addrspace(5) %out, i64 2
  store i32 2, ptr addrspace(5) %arrayidx4, align 4
  %arrayidx6 = getelementptr inbounds i32, ptr addrspace(5) %out, i64 3
  store i32 2, ptr addrspace(5) %arrayidx6, align 4
  ret void
}


attributes #0 = { nounwind }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; FUNC: {{.*}}
; SI: {{.*}}
