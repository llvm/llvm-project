; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13

@acc_in_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu4_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu4 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu4_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_iu4_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_iu4_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_iu4_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_iu4_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu4_3x3_4x2
  ret void
}

@acc_in_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu4_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_iu4_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu4_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu4_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu4_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_3x3_4x2
  ret void
}

@acc_in_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_iu4_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_iu4_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu4_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu4_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu4_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_3x3_4x4
  ret void
}

@acc_in_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_iu4_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_iu4_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu4_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu4_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu4_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_3x3_8x4
  ret void
}

@acc_in_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu4_3x3_4x2() {
; GFX13-LABEL: test_convolve.i32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu4 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu4_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_i32_iu4_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_i32_iu4_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_i32_iu4_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_i32_iu4_3x3_4x2

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.3x3.v4i32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu4_3x3_4x2
  ret void
}

@acc_in_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32i32_iu4_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu4_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32i32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu4 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu4_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32i32_iu4_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32i32_iu4_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32i32_iu4_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32i32_iu4_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.3x3.v4f32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu4_3x3_4x2
  ret void
}

@acc_in_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_iu8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_iu8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_iu8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_iu8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu8_3x3_4x2
  ret void
}

@acc_in_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_iu8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu8_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_3x3_4x2
  ret void
}

@acc_in_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_iu8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_iu8_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu8_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu8_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu8_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_3x3_4x4
  ret void
}

@acc_in_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_iu8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_iu8_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_iu8_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_iu8_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_iu8_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_3x3_8x4
  ret void
}

@acc_in_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu8_3x3_4x2() {
; GFX13-LABEL: test_convolve.i32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_i32_iu8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_i32_iu8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_i32_iu8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_i32_iu8_3x3_4x2

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.3x3.v4i32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu8_3x3_4x2
  ret void
}

@acc_in_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32i32_iu8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32i32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32i32_iu8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32i32_iu8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32i32_iu8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32i32_iu8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.3x3.v4f32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu8_3x3_4x2
  ret void
}

@acc_in_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_fp8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_fp8_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_fp8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_fp8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_fp8_fp8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_fp8_fp8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_fp8_fp8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_fp8_fp8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.fp8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_fp8_3x3_4x2
  ret void
}

@acc_in_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_fp8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_fp8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_fp8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_fp8_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_3x3_4x2
  ret void
}

@acc_in_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_fp8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_fp8_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_fp8_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_fp8_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_3x3_4x4
  ret void
}

@acc_in_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_fp8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_fp8_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_fp8_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_fp8_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_3x3_8x4
  ret void
}

@acc_in_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_bf8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_fp8_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_bf8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_bf8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_fp8_bf8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_fp8_bf8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_fp8_bf8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_fp8_bf8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.bf8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_bf8_3x3_4x2
  ret void
}

@acc_in_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_fp8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_bf8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_bf8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_bf8_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_3x3_4x2
  ret void
}

@acc_in_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_fp8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_bf8_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_bf8_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_bf8_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_3x3_4x4
  ret void
}

@acc_in_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_fp8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_fp8_bf8_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_fp8_bf8_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_fp8_bf8_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_3x3_8x4
  ret void
}

@acc_in_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_fp8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_bf8_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_fp8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_fp8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_bf8_fp8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_bf8_fp8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_bf8_fp8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_bf8_fp8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.fp8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_fp8_3x3_4x2
  ret void
}

@acc_in_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_bf8_fp8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_fp8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_fp8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_fp8_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_3x3_4x2
  ret void
}

@acc_in_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_bf8_fp8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_fp8_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_fp8_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_fp8_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_3x3_4x4
  ret void
}

@acc_in_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_bf8_fp8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_fp8_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_fp8_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_fp8_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_3x3_8x4
  ret void
}

@acc_in_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f32_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_bf8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_bf8_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_bf8 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_bf8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f32_bf8_bf8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f32_bf8_bf8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f32_bf8_bf8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f32_bf8_bf8_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.bf8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_bf8_3x3_4x2
  ret void
}

@acc_in_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <18 x i32>
@tensor_col_center_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_bf8_bf8_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_3x3_4x2
  %weights = load <18 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_3x3_4x2
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_bf8_3x3_4x2
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_bf8_3x3_4x2
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_bf8_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_3x3_4x2
  ret void
}

@acc_in_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <9 x i32>
@tensor_col_center_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_left_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_col_right_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <3 x i32>
@out_var_f16_bf8_bf8_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_3x3_4x4
  %weights = load <9 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_3x3_4x4
  %tensor_col_center = load <3 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_bf8_3x3_4x4
  %tensor_col_left = load <3 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_bf8_3x3_4x4
  %tensor_col_right = load <3 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_bf8_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_3x3_4x4
  ret void
}

@acc_in_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <5 x i32>
@tensor_col_center_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_left_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_col_right_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@out_var_f16_bf8_bf8_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_3x3_8x4
  %weights = load <5 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_3x3_8x4
  %tensor_col_center = load <4 x i32>, ptr addrspace(10) @tensor_col_center_var_f16_bf8_bf8_3x3_8x4
  %tensor_col_left = load <4 x i32>, ptr addrspace(10) @tensor_col_left_var_f16_bf8_bf8_3x3_8x4
  %tensor_col_right = load <4 x i32>, ptr addrspace(10) @tensor_col_right_var_f16_bf8_bf8_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_3x3_8x4
  ret void
}

@acc_in_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <36 x half>
@tensor_col_center_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_left_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_right_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@out_var_f32_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_f16_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_f16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_f16 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_f16_3x3_4x2
  %weights = load <36 x half>, ptr addrspace(10) @weights_var_f32_f16_3x3_4x2
  %tensor_col_center = load <6 x half>, ptr addrspace(10) @tensor_col_center_var_f32_f16_3x3_4x2
  %tensor_col_left = load <6 x half>, ptr addrspace(10) @tensor_col_left_var_f32_f16_3x3_4x2
  %tensor_col_right = load <6 x half>, ptr addrspace(10) @tensor_col_right_var_f32_f16_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.3x3.v4f32.v4f32.v36f16.v6f16(<4 x float> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_f16_3x3_4x2
  ret void
}

@acc_in_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <36 x half>
@tensor_col_center_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_left_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_right_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x half>
@out_var_f16_f16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_f16_3x3_4x2() {
; GFX13-LABEL: test_convolve.f16_f16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_f16_3x3_4x2
  %weights = load <36 x half>, ptr addrspace(10) @weights_var_f16_f16_3x3_4x2
  %tensor_col_center = load <6 x half>, ptr addrspace(10) @tensor_col_center_var_f16_f16_3x3_4x2
  %tensor_col_left = load <6 x half>, ptr addrspace(10) @tensor_col_left_var_f16_f16_3x3_4x2
  %tensor_col_right = load <6 x half>, ptr addrspace(10) @tensor_col_right_var_f16_f16_3x3_4x2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v4f16.v4f16.v36f16.v6f16(<4 x half> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_f16_3x3_4x2
  ret void
}

@acc_in_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <18 x half>
@tensor_col_center_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_left_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_col_right_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x half>
@out_var_f16_f16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_3x3_4x4() {
; GFX13-LABEL: test_convolve.f16_f16_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_3x3_4x4
  %weights = load <18 x half>, ptr addrspace(10) @weights_var_f16_f16_3x3_4x4
  %tensor_col_center = load <6 x half>, ptr addrspace(10) @tensor_col_center_var_f16_f16_3x3_4x4
  %tensor_col_left = load <6 x half>, ptr addrspace(10) @tensor_col_left_var_f16_f16_3x3_4x4
  %tensor_col_right = load <6 x half>, ptr addrspace(10) @tensor_col_right_var_f16_f16_3x3_4x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v8f16.v8f16.v18f16.v6f16(<8 x half> %acc_in, <18 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_3x3_4x4
  ret void
}

@acc_in_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <10 x half>
@tensor_col_center_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@tensor_col_left_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@tensor_col_right_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>
@out_var_f16_f16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_3x3_8x4() {
; GFX13-LABEL: test_convolve.f16_f16_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_3x3_8x4
  %weights = load <10 x half>, ptr addrspace(10) @weights_var_f16_f16_3x3_8x4
  %tensor_col_center = load <8 x half>, ptr addrspace(10) @tensor_col_center_var_f16_f16_3x3_8x4
  %tensor_col_left = load <8 x half>, ptr addrspace(10) @tensor_col_left_var_f16_f16_3x3_8x4
  %tensor_col_right = load <8 x half>, ptr addrspace(10) @tensor_col_right_var_f16_f16_3x3_8x4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v8f16.v8f16.v10f16.v8f16(<8 x half> %acc_in, <10 x half> %weights, <8 x half> %tensor_col_center, <8 x half> %tensor_col_left, <8 x half> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_3x3_8x4
  ret void
}

@acc_in_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <36 x bfloat>
@tensor_col_center_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_left_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_right_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@out_var_f32_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf16_3x3_4x2() {
; GFX13-LABEL: test_convolve.f32_bf16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf16 g1[0:3], g1[48:51], g1[16:33], g1[12:14], g1[8:10], g1[4:6] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf16_3x3_4x2
  %weights = load <36 x bfloat>, ptr addrspace(10) @weights_var_f32_bf16_3x3_4x2
  %tensor_col_center = load <6 x bfloat>, ptr addrspace(10) @tensor_col_center_var_f32_bf16_3x3_4x2
  %tensor_col_left = load <6 x bfloat>, ptr addrspace(10) @tensor_col_left_var_f32_bf16_3x3_4x2
  %tensor_col_right = load <6 x bfloat>, ptr addrspace(10) @tensor_col_right_var_f32_bf16_3x3_4x2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.3x3.v4f32.v4f32.v36bf16.v6bf16(<4 x float> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf16_3x3_4x2
  ret void
}

@acc_in_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@weights_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <36 x bfloat>
@tensor_col_center_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_left_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_right_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@out_var_bf16_bf16_3x3_4x2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_4x2() {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:1], g1[46:47], g1[14:31], g1[10:12], g1[6:8], g1[2:4] aux_data:11 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <4 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_3x3_4x2
  %weights = load <36 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_3x3_4x2
  %tensor_col_center = load <6 x bfloat>, ptr addrspace(10) @tensor_col_center_var_bf16_bf16_3x3_4x2
  %tensor_col_left = load <6 x bfloat>, ptr addrspace(10) @tensor_col_left_var_bf16_bf16_3x3_4x2
  %tensor_col_right = load <6 x bfloat>, ptr addrspace(10) @tensor_col_right_var_bf16_bf16_3x3_4x2

  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v4bf16.v4bf16.v36bf16.v6bf16(<4 x bfloat> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_3x3 ) == (3 << 0 | 1 << 3)
              i32 11,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_3x3_4x2
  ret void
}

@acc_in_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <18 x bfloat>
@tensor_col_center_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_left_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_col_right_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@out_var_bf16_bf16_3x3_4x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_4x4() {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[32:35], g1[16:24], g1[12:14], g1[8:10], g1[4:6] aux_data:10 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_3x3_4x4
  %weights = load <18 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_3x3_4x4
  %tensor_col_center = load <6 x bfloat>, ptr addrspace(10) @tensor_col_center_var_bf16_bf16_3x3_4x4
  %tensor_col_left = load <6 x bfloat>, ptr addrspace(10) @tensor_col_left_var_bf16_bf16_3x3_4x4
  %tensor_col_right = load <6 x bfloat>, ptr addrspace(10) @tensor_col_right_var_bf16_bf16_3x3_4x4

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v8bf16.v8bf16.v18bf16.v6bf16(<8 x bfloat> %acc_in, <18 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_3x3 ) == (2 << 0 | 1 << 3)
              i32 10,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_3x3_4x4
  ret void
}

@acc_in_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <10 x bfloat>
@tensor_col_center_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@tensor_col_left_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@tensor_col_right_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@out_var_bf16_bf16_3x3_8x4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_8x4() {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[24:27], g1[16:20], g1[12:15], g1[8:11], g1[4:7] aux_data:8 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:

  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_3x3_8x4
  %weights = load <10 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_3x3_8x4
  %tensor_col_center = load <8 x bfloat>, ptr addrspace(10) @tensor_col_center_var_bf16_bf16_3x3_8x4
  %tensor_col_left = load <8 x bfloat>, ptr addrspace(10) @tensor_col_left_var_bf16_bf16_3x3_8x4
  %tensor_col_right = load <8 x bfloat>, ptr addrspace(10) @tensor_col_right_var_bf16_bf16_3x3_8x4

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v8bf16.v8bf16.v10bf16.v8bf16(<8 x bfloat> %acc_in, <10 x bfloat> %weights, <8 x bfloat> %tensor_col_center, <8 x bfloat> %tensor_col_left, <8 x bfloat> %tensor_col_right,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_3x3 ) == (0 << 0 | 1 << 3)
              i32 8,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_3x3_8x4
  ret void
}

@acc_in_var_f32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_iu4 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu4_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_iu4_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu4_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu4_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_iu4_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu4_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_iu4_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu4_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu4_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu4_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_8x4_iter_1
  ret void
}

@acc_in_var_i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_i32_iu4 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu4_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_i32_iu4_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu4_1x1_4x2_iter_1

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu4_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f32i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu4_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32i32_iu4 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu4_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32i32_iu4_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu4_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu4_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu4 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu4_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_iu4_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu4_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu4_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu4_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_iu4_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu4_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_iu4_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu4_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu4_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu4_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu4_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_8x4_iter_2
  ret void
}

@acc_in_var_i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu4 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu4_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_i32_iu4_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu4_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu4_1x1_4x2_iter_2

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu4_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f32i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu4_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu4 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu4_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32i32_iu4_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu4_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu4_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu4_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_iu4 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu4_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_iu4_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu4_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu4_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_iu4_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu4_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu4_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu4_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_8x4_iter_3
  ret void
}

@acc_in_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_i32_iu4 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu4_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_i32_iu4_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu4_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu4_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_i32_iu4_1x1_4x2_iter_3

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu4_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu4_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32i32_iu4 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu4_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32i32_iu4_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu4_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu4_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32i32_iu4_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu4_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu4 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu4_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_iu4_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu4_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu4_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_iu4_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_iu4_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu4_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_iu4_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_iu4_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_iu4_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_iu4_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu4 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu4_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu4_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu4_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu4_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_iu4_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_iu4_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu4_1x1_8x4_iter_4
  ret void
}

@acc_in_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu4 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu4_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_i32_iu4_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu4_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu4_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_i32_iu4_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_i32_iu4_1x1_4x2_iter_4

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu4_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32i32_iu4_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu4 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu4_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32i32_iu4_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu4_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu4_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32i32_iu4_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32i32_iu4_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu4_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_iu8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_iu8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_iu8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_iu8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu8_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_8x4_iter_1
  ret void
}

@acc_in_var_i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_i32_iu8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_i32_iu8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu8_1x1_4x2_iter_1

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f32i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32i32_iu8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32i32_iu8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_iu8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_iu8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_iu8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_iu8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_iu8_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_8x4_iter_2
  ret void
}

@acc_in_var_i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_i32_iu8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu8_1x1_4x2_iter_2

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f32i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32i32_iu8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_iu8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_iu8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_iu8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_iu8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_iu8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_8x4_iter_3
  ret void
}

@acc_in_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_i32_iu8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_i32_iu8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_i32_iu8_1x1_4x2_iter_3

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32i32_iu8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32i32_iu8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32i32_iu8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32i32_iu8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_iu8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_iu8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_iu8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_iu8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_iu8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_iu8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_iu8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_iu8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_iu8_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_iu8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_iu8_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_iu8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_iu8 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_iu8_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_iu8_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_iu8_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_iu8_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_iu8_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_iu8_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_iu8_1x1_8x4_iter_4
  ret void
}

@acc_in_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_i32_iu8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_i32_iu8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_i32_iu8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_i32_iu8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_i32_iu8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_i32_iu8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_i32_iu8_1x1_4x2_iter_4

  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x i32> %dst, ptr addrspace(10) @out_var_i32_iu8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@weights_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32i32_iu8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32i32_iu8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x i32>, ptr addrspace(10) @acc_in_var_f32i32_iu8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32i32_iu8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32i32_iu8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32i32_iu8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32i32_iu8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32i32_iu8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32i32_iu8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f32_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_fp8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_fp8_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_fp8_fp8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_fp8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_fp8_fp8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.fp8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_fp8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_fp8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_fp8_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_fp8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_fp8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_fp8_fp8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.fp8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_fp8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_fp8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_fp8_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_fp8_fp8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_fp8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_fp8_fp8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_fp8_fp8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.fp8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_fp8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_fp8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_fp8_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_fp8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_fp8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_fp8_fp8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_fp8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_fp8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_fp8_fp8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_fp8_fp8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.fp8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_fp8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_fp8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_fp8_fp8_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_fp8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_fp8_fp8_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_fp8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_fp8_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_fp8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_fp8 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_fp8_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_fp8_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_fp8_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_fp8_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_fp8_fp8_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_fp8_fp8_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_fp8_1x1_8x4_iter_4
  ret void
}

@acc_in_var_f32_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_bf8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_fp8_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_fp8_bf8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_bf8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_fp8_bf8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.bf8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_bf8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_bf8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_fp8_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_bf8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_bf8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_fp8_bf8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.bf8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_bf8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_bf8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_fp8_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_fp8_bf8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_bf8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_fp8_bf8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_fp8_bf8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.bf8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_bf8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_fp8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_fp8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_fp8_bf8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_fp8_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_fp8_bf8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_fp8_bf8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_fp8_bf8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_fp8_bf8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_fp8_bf8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_fp8_bf8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_fp8_bf8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.bf8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_fp8_bf8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_fp8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_fp8_bf8_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_fp8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_fp8_bf8_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_fp8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_fp8_bf8_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_fp8_bf8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_fp8_bf8 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_fp8_bf8_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_fp8_bf8_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_fp8_bf8_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_fp8_bf8_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_fp8_bf8_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_fp8_bf8_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.bf8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_fp8_bf8_1x1_8x4_iter_4
  ret void
}

@acc_in_var_f32_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_fp8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_bf8_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_bf8_fp8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_fp8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_bf8_fp8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.fp8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_fp8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_fp8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_fp8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_bf8_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_fp8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_fp8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_bf8_fp8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.fp8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_fp8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_fp8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_fp8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_bf8_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_bf8_fp8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_fp8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_bf8_fp8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_bf8_fp8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.fp8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_fp8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_fp8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_fp8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_fp8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_bf8_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_fp8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_fp8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_bf8_fp8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_fp8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_fp8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_bf8_fp8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_bf8_fp8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.fp8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_fp8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_bf8_fp8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_bf8_fp8_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_bf8_fp8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_bf8_fp8_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_bf8_fp8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_fp8_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_fp8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_fp8 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_fp8_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_fp8_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_fp8_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_fp8_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_bf8_fp8_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_bf8_fp8_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_fp8_1x1_8x4_iter_4
  ret void
}

@acc_in_var_f32_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_bf8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_bf8_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_bf8_bf8 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_bf8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f32_bf8_bf8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.bf8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_bf8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x2_iter_1
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x2_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x4_iter_1
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 undef, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_bf8_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_8x4_iter_1
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_8x4_iter_1
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> undef, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_bf8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_bf8_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_bf8 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_bf8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f32_bf8_bf8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.bf8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_bf8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x2_iter_2
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x2_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x4_iter_2
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x4_iter_2
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_2
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 undef, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global i32
@tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_bf8_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_8x4_iter_2
  %weights = load i32, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_8x4_iter_2
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_2
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> undef, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f32_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_bf8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_bf8_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[18]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_convolve_f32_bf8_bf8 g1[0:3], v[6:9], v[0:5], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_bf8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f32_bf8_bf8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_bf8_bf8_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.bf8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_bf8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <6 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:1], v[6:7], v[0:5], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x2_iter_3
  %weights = load <6 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x2_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <3 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global i32

@out_var_f16_bf8_bf8_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x4_iter_3
  %weights = load <3 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x4_iter_3
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_3
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_3
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x i32>

@out_var_f16_bf8_bf8_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_8x4_iter_3
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_8x4_iter_3
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_3
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_3
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f32_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf8_bf8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_bf8_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf8_bf8 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf8_bf8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f32_bf8_bf8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f32_bf8_bf8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f32_bf8_bf8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f32_bf8_bf8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f32_bf8_bf8_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.bf8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf8_bf8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <8 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_bf8_bf8_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x2_iter_4
  %weights = load <8 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x2_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x2_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x2_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_4x2_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_bf8_bf8_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_2_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@tensor_3_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global i32
@out_var_f16_bf8_bf8_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_4x4_iter_4
  %weights = load <4 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_4x4_iter_4
  %tensor_0 = load i32, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_4x4_iter_4
  %tensor_1 = load i32, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_4x4_iter_4
  %tensor_2 = load i32, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_4x4_iter_4
  %tensor_3 = load i32, ptr addrspace(10) @tensor_3_var_f16_bf8_bf8_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_2_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@tensor_3_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x i32>
@out_var_f16_bf8_bf8_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_bf8_bf8_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_bf8_bf8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_bf8_bf8 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_bf8_bf8_1x1_8x4_iter_4
  %weights = load <2 x i32>, ptr addrspace(10) @weights_var_f16_bf8_bf8_1x1_8x4_iter_4
  %tensor_0 = load <2 x i32>, ptr addrspace(10) @tensor_0_var_f16_bf8_bf8_1x1_8x4_iter_4
  %tensor_1 = load <2 x i32>, ptr addrspace(10) @tensor_1_var_f16_bf8_bf8_1x1_8x4_iter_4
  %tensor_2 = load <2 x i32>, ptr addrspace(10) @tensor_2_var_f16_bf8_bf8_1x1_8x4_iter_4
  %tensor_3 = load <2 x i32>, ptr addrspace(10) @tensor_3_var_f16_bf8_bf8_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.bf8.bf8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_bf8_bf8_1x1_8x4_iter_4
  ret void
}

@acc_in_var_f32_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_0_var_f32_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f32_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_f16 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_f16_1x1_4x2_iter_1
  %weights = load <4 x half>, ptr addrspace(10) @weights_var_f32_f16_1x1_4x2_iter_1
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f32_f16_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v4f16.v2f16(<4 x float> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> undef, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_f16_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_0_var_f16_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x2_iter_1
  %weights = load <4 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x2_iter_1
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x2_iter_1

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v4f16.v2f16(<4 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> undef, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x2_iter_1
  ret void
}

@acc_in_var_f16_f16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_0_var_f16_f16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x4_iter_1
  %weights = load <2 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x4_iter_1
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v2f16(<8 x half> %acc_in, <2 x half> %weights, <2 x half> %tensor_0, <2 x half> undef, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x4_iter_1
  ret void
}

@acc_in_var_f16_f16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_0_var_f16_f16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <4 x half>

@out_var_f16_f16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_8x4_iter_1
  %weights = load <2 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_8x4_iter_1
  %tensor_0 = load <4 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_8x4_iter_1

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> undef, <4 x half> undef, <4 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@tensor_0_var_f32_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f32_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f32_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_f16 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_f16_1x1_4x2_iter_2
  %weights = load <8 x half>, ptr addrspace(10) @weights_var_f32_f16_1x1_4x2_iter_2
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f32_f16_1x1_4x2_iter_2
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f32_f16_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v8f16.v2f16(<4 x float> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_f16_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@tensor_0_var_f16_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x2_iter_2
  %weights = load <8 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x2_iter_2
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x2_iter_2
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x2_iter_2

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v8f16.v2f16(<4 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x2_iter_2
  ret void
}

@acc_in_var_f16_f16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_0_var_f16_f16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x4_iter_2
  %weights = load <4 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x4_iter_2
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x4_iter_2
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v2f16(<8 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> undef, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x4_iter_2
  ret void
}

@acc_in_var_f16_f16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_0_var_f16_f16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_1_var_f16_f16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x half>

@out_var_f16_f16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_8x4_iter_2
  %weights = load <2 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_8x4_iter_2
  %tensor_0 = load <4 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_8x4_iter_2
  %tensor_1 = load <4 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_8x4_iter_2

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> undef, <4 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <16 x half>
@tensor_0_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f32_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v10, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v11, g1[18]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_convolve_f32_f16 g1[0:3], v[8:11], v[0:7], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_f16_1x1_4x2_iter_3
  %weights = load <16 x half>, ptr addrspace(10) @weights_var_f32_f16_1x1_4x2_iter_3
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f32_f16_1x1_4x2_iter_3
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f32_f16_1x1_4x2_iter_3
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f32_f16_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v16f16.v2f16(<4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_f16_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <16 x half>
@tensor_0_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[12]
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:1], v[8:9], v[0:7], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x2_iter_3
  %weights = load <16 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x2_iter_3
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x2_iter_3
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x2_iter_3
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_4x2_iter_3

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v16f16.v2f16(<4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x2_iter_3
  ret void
}

@acc_in_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <6 x half>
@tensor_0_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x half>

@out_var_f16_f16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x4_iter_3
  %weights = load <6 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x4_iter_3
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x4_iter_3
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x4_iter_3
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_4x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v6f16.v2f16(<8 x half> %acc_in, <6 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x4_iter_3
  ret void
}

@acc_in_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_0_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_1_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_2_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x half>

@out_var_f16_f16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_8x4_iter_3
  %weights = load <4 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_8x4_iter_3
  %tensor_0 = load <4 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_8x4_iter_3
  %tensor_1 = load <4 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_8x4_iter_3
  %tensor_2 = load <4 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_8x4_iter_3

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v4f16(<8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <16 x half>
@tensor_0_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_3_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@out_var_f32_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_f16 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_f16_1x1_4x2_iter_4
  %weights = load <16 x half>, ptr addrspace(10) @weights_var_f32_f16_1x1_4x2_iter_4
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f32_f16_1x1_4x2_iter_4
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f32_f16_1x1_4x2_iter_4
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f32_f16_1x1_4x2_iter_4
  %tensor_3 = load <2 x half>, ptr addrspace(10) @tensor_3_var_f32_f16_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v16f16.v2f16(<4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_f16_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@weights_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <16 x half>
@tensor_0_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_3_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@out_var_f16_f16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x2_iter_4
  %weights = load <16 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x2_iter_4
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x2_iter_4
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x2_iter_4
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_4x2_iter_4
  %tensor_3 = load <2 x half>, ptr addrspace(10) @tensor_3_var_f16_f16_1x1_4x2_iter_4

  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v16f16.v2f16(<4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x2_iter_4
  ret void
}

@acc_in_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@tensor_0_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_1_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_2_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@tensor_3_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x half>
@out_var_f16_f16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_4x4_iter_4
  %weights = load <8 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_4x4_iter_4
  %tensor_0 = load <2 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_4x4_iter_4
  %tensor_1 = load <2 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_4x4_iter_4
  %tensor_2 = load <2 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_4x4_iter_4
  %tensor_3 = load <2 x half>, ptr addrspace(10) @tensor_3_var_f16_f16_1x1_4x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v8f16.v2f16(<8 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_4x4_iter_4
  ret void
}

@acc_in_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>
@weights_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_0_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_1_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_2_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@tensor_3_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x half>
@out_var_f16_f16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x half>

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f16_f16 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x half>, ptr addrspace(10) @acc_in_var_f16_f16_1x1_8x4_iter_4
  %weights = load <4 x half>, ptr addrspace(10) @weights_var_f16_f16_1x1_8x4_iter_4
  %tensor_0 = load <4 x half>, ptr addrspace(10) @tensor_0_var_f16_f16_1x1_8x4_iter_4
  %tensor_1 = load <4 x half>, ptr addrspace(10) @tensor_1_var_f16_f16_1x1_8x4_iter_4
  %tensor_2 = load <4 x half>, ptr addrspace(10) @tensor_2_var_f16_f16_1x1_8x4_iter_4
  %tensor_3 = load <4 x half>, ptr addrspace(10) @tensor_3_var_f16_f16_1x1_8x4_iter_4

  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v4f16(<8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x half> %dst, ptr addrspace(10) @out_var_f16_f16_1x1_8x4_iter_4
  ret void
}

@acc_in_var_f32_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_0_var_f32_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_f32_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[6]
; GFX13-NEXT:    v_convolve_f32_bf16 g1[0:3], v[0:3], v[4:5], g1[4] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf16_1x1_4x2_iter_1
  %weights = load <4 x bfloat>, ptr addrspace(10) @weights_var_f32_bf16_1x1_4x2_iter_1
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_f32_bf16_1x1_4x2_iter_1

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v4bf16.v2bf16(<4 x float> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> undef, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf16_1x1_4x2_iter_1
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@weights_var_bf16_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x2_iter_1 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_1() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[3]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[4]
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:1], v[0:1], v[2:3], g1[2] aux_data:3 clamp idxs:0x1001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x2_iter_1
  %weights = load <4 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x2_iter_1
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x2_iter_1

  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v4bf16.v2bf16(<4 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> undef, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_1) == (3 << 0 | 0 << 3 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)

  store <4 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x2_iter_1
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_1() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[6:9], g1[5], g1[4] aux_data:2 clamp idxs:0x1111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x4_iter_1
  %weights = load <2 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x4_iter_1
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x4_iter_1

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v2bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> undef, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (2 << 0 | 0 << 3 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x4_iter_1
  ret void
}

@acc_in_var_bf16_bf16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_0_var_bf16_bf16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

@out_var_bf16_bf16_1x1_8x4_iter_1 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_1() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], v[0:3], g1[6], g1[4:5] clamp idxs:0x1101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_8x4_iter_1
  %weights = load <2 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_8x4_iter_1
  %tensor_0 = load <4 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_8x4_iter_1

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v4bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> undef, <4 x bfloat> undef, <4 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_1) == (0 << 0 | 0 << 3 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_8x4_iter_1
  ret void
}

@acc_in_var_f32_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@tensor_0_var_f32_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_f32_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_f32_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf16 g1[0:3], g1[10:13], g1[6:9], g1[5], g1[4] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf16_1x1_4x2_iter_2
  %weights = load <8 x bfloat>, ptr addrspace(10) @weights_var_f32_bf16_1x1_4x2_iter_2
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_f32_bf16_1x1_4x2_iter_2
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_f32_bf16_1x1_4x2_iter_2

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v8bf16.v2bf16(<4 x float> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf16_1x1_4x2_iter_2
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@weights_var_bf16_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x2_iter_2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_2() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:1], g1[8:9], g1[4:7], g1[3], g1[2] aux_data:4099 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x2_iter_2
  %weights = load <8 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x2_iter_2
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x2_iter_2
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x2_iter_2

  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v8bf16.v2bf16(<4 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_2) == (3 << 0 | 0 << 3 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)

  store <4 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x2_iter_2
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_2() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[8:11], g1[6:7], g1[5], g1[4] aux_data:4098 clamp idxs:0x11111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x4_iter_2
  %weights = load <4 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x4_iter_2
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x4_iter_2
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x4_iter_2

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v2bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> undef, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (2 << 0 | 0 << 3 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x4_iter_2
  ret void
}

@acc_in_var_bf16_bf16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_0_var_bf16_bf16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_1_var_bf16_bf16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

@out_var_bf16_bf16_1x1_8x4_iter_2 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_2() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[12]
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], v[0:3], g1[8], g1[6:7], g1[4:5] aux_data:4096 clamp idxs:0x11101
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_8x4_iter_2
  %weights = load <2 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_8x4_iter_2
  %tensor_0 = load <4 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_8x4_iter_2
  %tensor_1 = load <4 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_8x4_iter_2

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v4bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> undef, <4 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_2) == (0 << 0 | 0 << 3 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_8x4_iter_2
  ret void
}

@acc_in_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <16 x bfloat>
@tensor_0_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_f32_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[15]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[16]
; GFX13-NEXT:    v_mov_b32_e32 v10, g1[17]
; GFX13-NEXT:    v_mov_b32_e32 v11, g1[18]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[14]
; GFX13-NEXT:    v_convolve_f32_bf16 g1[0:3], v[8:11], v[0:7], g1[6], g1[5], g1[4] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf16_1x1_4x2_iter_3
  %weights = load <16 x bfloat>, ptr addrspace(10) @weights_var_f32_bf16_1x1_4x2_iter_3
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_f32_bf16_1x1_4x2_iter_3
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_f32_bf16_1x1_4x2_iter_3
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_f32_bf16_1x1_4x2_iter_3

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v16bf16.v2bf16(<4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf16_1x1_4x2_iter_3
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@weights_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <16 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x2_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_3() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v8, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v9, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[5]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[6]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[9]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[10]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v7, g1[12]
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:1], v[8:9], v[0:7], g1[4], g1[3], g1[2] aux_data:8195 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x2_iter_3
  %weights = load <16 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x2_iter_3
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x2_iter_3
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x2_iter_3
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_4x2_iter_3

  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v16bf16.v2bf16(<4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_3) == (3 << 0 | 0 << 3 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)

  store <4 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x2_iter_3
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <6 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <2 x bfloat>

@out_var_bf16_bf16_1x1_4x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_3() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; GFX13-NEXT:    v_mov_b32_e32 v0, g1[11]
; GFX13-NEXT:    v_mov_b32_e32 v1, g1[12]
; GFX13-NEXT:    v_mov_b32_e32 v2, g1[13]
; GFX13-NEXT:    v_mov_b32_e32 v3, g1[14]
; GFX13-NEXT:    v_mov_b32_e32 v4, g1[7]
; GFX13-NEXT:    v_mov_b32_e32 v5, g1[8]
; GFX13-NEXT:    v_mov_b32_e32 v6, g1[9]
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], v[0:3], v[4:6], g1[6], g1[5], g1[4] aux_data:8194 clamp idxs:0x111001
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x4_iter_3
  %weights = load <6 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x4_iter_3
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x4_iter_3
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x4_iter_3
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_4x4_iter_3

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v6bf16.v2bf16(<8 x bfloat> %acc_in, <6 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (2 << 0 | 0 << 3 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x4_iter_3
  ret void
}

@acc_in_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_0_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_1_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_2_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

@out_var_bf16_bf16_1x1_8x4_iter_3 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_3() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[12:15], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:8192 clamp idxs:0x111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_8x4_iter_3
  %weights = load <4 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_8x4_iter_3
  %tensor_0 = load <4 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_8x4_iter_3
  %tensor_1 = load <4 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_8x4_iter_3
  %tensor_2 = load <4 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_8x4_iter_3

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v4bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> undef,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_3) == (0 << 0 | 0 << 3 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_8x4_iter_3
  ret void
}

@acc_in_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>
@weights_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <16 x bfloat>
@tensor_0_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_3_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@out_var_f32_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x float>

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_f32_bf16 g1[0:3], g1[16:19], g1[8:15], g1[7], g1[6], g1[5], g1[4] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x float>, ptr addrspace(10) @acc_in_var_f32_bf16_1x1_4x2_iter_4
  %weights = load <16 x bfloat>, ptr addrspace(10) @weights_var_f32_bf16_1x1_4x2_iter_4
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_f32_bf16_1x1_4x2_iter_4
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_f32_bf16_1x1_4x2_iter_4
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_f32_bf16_1x1_4x2_iter_4
  %tensor_3 = load <2 x bfloat>, ptr addrspace(10) @tensor_3_var_f32_bf16_1x1_4x2_iter_4

  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v16bf16.v2bf16(<4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x float> %dst, ptr addrspace(10) @out_var_f32_bf16_1x1_4x2_iter_4
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@weights_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <16 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_3_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@out_var_bf16_bf16_1x1_4x2_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_4() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:1], g1[14:15], g1[6:13], g1[5], g1[4], g1[3], g1[2] aux_data:12291 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <4 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x2_iter_4
  %weights = load <16 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x2_iter_4
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x2_iter_4
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x2_iter_4
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_4x2_iter_4
  %tensor_3 = load <2 x bfloat>, ptr addrspace(10) @tensor_3_var_bf16_bf16_1x1_4x2_iter_4

  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v16bf16.v2bf16(<4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_FILTER_1x1 | CONV_ITER_4) == (3 << 0 | 0 << 3 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)

  store <4 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x2_iter_4
  ret void
}

@acc_in_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@tensor_0_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_1_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_2_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@tensor_3_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <2 x bfloat>
@out_var_bf16_bf16_1x1_4x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_4() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[12:15], g1[8:11], g1[7], g1[6], g1[5], g1[4] aux_data:12290 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_4x4_iter_4
  %weights = load <8 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_4x4_iter_4
  %tensor_0 = load <2 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_4x4_iter_4
  %tensor_1 = load <2 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_4x4_iter_4
  %tensor_2 = load <2 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_4x4_iter_4
  %tensor_3 = load <2 x bfloat>, ptr addrspace(10) @tensor_3_var_bf16_bf16_1x1_4x4_iter_4

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v8bf16.v2bf16(<8 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (2 << 0 | 0 << 3 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_4x4_iter_4
  ret void
}

@acc_in_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>
@weights_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_0_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_1_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_2_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@tensor_3_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <4 x bfloat>
@out_var_bf16_bf16_1x1_8x4_iter_4 = external local_unnamed_addr addrspace(10) global <8 x bfloat>

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_4() {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    s_set_gpr_idx_u32 idx1, 0
; GFX13-NEXT:    v_convolve_bf16_bf16 g1[0:3], g1[14:17], g1[12:13], g1[10:11], g1[8:9], g1[6:7], g1[4:5] aux_data:12288 clamp idxs:0x1111111
; GFX13-NEXT:    s_endpgm
bb:
  %acc_in = load <8 x bfloat>, ptr addrspace(10) @acc_in_var_bf16_bf16_1x1_8x4_iter_4
  %weights = load <4 x bfloat>, ptr addrspace(10) @weights_var_bf16_bf16_1x1_8x4_iter_4
  %tensor_0 = load <4 x bfloat>, ptr addrspace(10) @tensor_0_var_bf16_bf16_1x1_8x4_iter_4
  %tensor_1 = load <4 x bfloat>, ptr addrspace(10) @tensor_1_var_bf16_bf16_1x1_8x4_iter_4
  %tensor_2 = load <4 x bfloat>, ptr addrspace(10) @tensor_2_var_bf16_bf16_1x1_8x4_iter_4
  %tensor_3 = load <4 x bfloat>, ptr addrspace(10) @tensor_3_var_bf16_bf16_1x1_8x4_iter_4

  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v4bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_FILTER_1x1 | CONV_ITER_4) == (0 << 0 | 0 << 3 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)

  store <8 x bfloat> %dst, ptr addrspace(10) @out_var_bf16_bf16_1x1_8x4_iter_4
  ret void
}
