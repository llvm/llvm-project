; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=10 < %s 2>&1 | FileCheck %s

;
;       bb.0.entry
;        /    |
;   bb.3.bb2  |
;        \    |
;       bb.1.Flow
;        /    |
;   bb.2.bb1  |
;        \    |
;      bb.4.exit
define amdgpu_ps i64 @test(ptr addrspace(3) %p1, ptr addrspace(3) %p2, i1 %cond1, i64 %val) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.0, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_NE_U32_e64_]], %bb.1, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.Flow:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.4(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:vreg_64 = PHI undef %12:vreg_64, %bb.0, %4, %bb.3
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[COPY4]], %bb.0, undef %67:vgpr_32, %bb.3
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF]], %bb.4, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb1:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_4:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 4, 0, implicit $exec :: (load (s8) from %ir.p1 + 4, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_5:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 5, 0, implicit $exec :: (load (s8) from %ir.p1 + 5, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_6:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 6, 0, implicit $exec :: (load (s8) from %ir.p1 + 6, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_7:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[PHI1]], 7, 0, implicit $exec :: (load (s8) from %ir.p1 + 7, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_5]], 8, [[DS_READ_U8_gfx9_4]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_7]], 8, [[DS_READ_U8_gfx9_6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_1]], 16, [[V_LSHL_OR_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_1]], 8, [[DS_READ_U8_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_3]], 8, [[DS_READ_U8_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_4]], 16, [[V_LSHL_OR_B32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_5]], %subreg.sub0, [[V_LSHL_OR_B32_e64_2]], %subreg.sub1
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vreg_64 = COPY [[REG_SEQUENCE]]
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.bb2:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_8:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 0, 0, implicit $exec :: (load (s8) from %ir.p2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_9:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 1, 0, implicit $exec :: (load (s8) from %ir.p2 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_10:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 2, 0, implicit $exec :: (load (s8) from %ir.p2 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_11:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 3, 0, implicit $exec :: (load (s8) from %ir.p2 + 3, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_12:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 4, 0, implicit $exec :: (load (s8) from %ir.p2 + 4, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_13:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 5, 0, implicit $exec :: (load (s8) from %ir.p2 + 5, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_14:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 6, 0, implicit $exec :: (load (s8) from %ir.p2 + 6, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_15:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY3]], 7, 0, implicit $exec :: (load (s8) from %ir.p2 + 7, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_13]], 8, [[DS_READ_U8_gfx9_12]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_15]], 8, [[DS_READ_U8_gfx9_14]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_8:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_7]], 16, [[V_LSHL_OR_B32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_9:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_9]], 8, [[DS_READ_U8_gfx9_8]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_10:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_11]], 8, [[DS_READ_U8_gfx9_10]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_11:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_10]], 16, [[V_LSHL_OR_B32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_11]], %subreg.sub0, [[V_LSHL_OR_B32_e64_8]], %subreg.sub1
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vreg_64 = COPY [[REG_SEQUENCE1]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.exit:
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vreg_64 = PHI [[PHI]], %bb.1, [[COPY5]], %bb.2
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[PHI2]].sub0, [[COPY1]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.0, addrspace 5)
  ; CHECK-NEXT:   %59:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[PHI2]].sub1, [[SI_SPILL_V32_RESTORE]], [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_READFIRSTLANE_B32_:%[0-9]+]]:sreg_32_xm0 = V_READFIRSTLANE_B32 [[V_ADD_CO_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_READFIRSTLANE_B32_1:%[0-9]+]]:sreg_32_xm0 = V_READFIRSTLANE_B32 %59, implicit $exec
  ; CHECK-NEXT:   $sgpr0 = COPY [[V_READFIRSTLANE_B32_]]
  ; CHECK-NEXT:   $sgpr1 = COPY [[V_READFIRSTLANE_B32_1]]
  ; CHECK-NEXT:   SI_RETURN_TO_EPILOG killed $sgpr0, killed $sgpr1
entry:
;    entry
;    /   \
;  bb1   bb2
;    \   /
;     exit
   br i1 %cond1, label %bb1, label %bb2

bb1:
  %ld1 = load i64, ptr addrspace(3) %p1, align 1
  br label %exit

bb2:
  %ld2 = load i64, ptr addrspace(3) %p2, align 1
  br label %exit

exit:
  %phi = phi i64 [ %ld1, %bb1 ], [ %ld2, %bb2 ]
  %add = add i64 %phi, %val
  ret i64 %add
}
