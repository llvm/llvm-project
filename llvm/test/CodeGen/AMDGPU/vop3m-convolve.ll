; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_convolve.f32_iu4_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_iu4 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_3x3_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_3x3_4x4(ptr addrspace(1) %out, <8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_3x3_8x4(ptr addrspace(1) %out, <8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu4_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu4_3x3_4x2(ptr addrspace(1) %out, <4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.i32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_i32_iu4 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.3x3.v4i32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu4_3x3_4x2(ptr addrspace(1) %out, <4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32i32_iu4_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32i32_iu4 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.3x3.v4f32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu8_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_iu8 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_3x3_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_3x3_4x4(ptr addrspace(1) %out, <8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_3x3_8x4(ptr addrspace(1) %out, <8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_iu8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu8_3x3_4x2(ptr addrspace(1) %out, <4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.i32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_i32_iu8 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.3x3.v4i32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu8_3x3_4x2(ptr addrspace(1) %out, <4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32i32_iu8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32i32_iu8 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.3x3.v4f32.v4i32.v18i32.v3i32(<4 x i32> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_fp8_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_fp8 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_3x3_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_fp8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.3x3.v4f16.v4f16.v18i32.v3i32(<4 x half> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_3x3_4x4(ptr addrspace(1) %out, <8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_fp8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_3x3_8x4(ptr addrspace(1) %out, <8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_fp8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.3x3.v8f16.v8f16.v5i32.v4i32(<8 x half> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf8_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_bf8 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.3x3.v4f32.v4f32.v18i32.v3i32(<4 x float> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_3x3_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf8_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.3x3.v4bf16.v4bf16.v18i32.v3i32(<4 x bfloat> %acc_in, <18 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_3x3_4x4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf8_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.3x3.v8bf16.v8bf16.v9i32.v3i32(<8 x bfloat> %acc_in, <9 x i32> %weights, <3 x i32> %tensor_col_center, <3 x i32> %tensor_col_left, <3 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_3x3_8x4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf8_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.3x3.v8bf16.v8bf16.v5i32.v4i32(<8 x bfloat> %acc_in, <5 x i32> %weights, <4 x i32> %tensor_col_center, <4 x i32> %tensor_col_left, <4 x i32> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_f16_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_f16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_f16 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.3x3.v4f32.v4f32.v36f16.v6f16(<4 x float> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_3x3_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_f16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_f16_f16 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v4f16.v4f16.v36f16.v6f16(<4 x half> %acc_in, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_3x3_4x4(ptr addrspace(1) %out, <8 x half> %acc_in, <18 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_f16_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v8f16.v8f16.v18f16.v6f16(<8 x half> %acc_in, <18 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_3x3_8x4(ptr addrspace(1) %out, <8 x half> %acc_in, <10 x half> %weights, <8 x half> %tensor_col_center, <8 x half> %tensor_col_left, <8 x half> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f16_f16_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.3x3.v8f16.v8f16.v10f16.v8f16(<8 x half> %acc_in, <10 x half> %weights, <8 x half> %tensor_col_center, <8 x half> %tensor_col_left, <8 x half> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf16_3x3_4x2(ptr addrspace(1) %out, <4 x float> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right) {
; GFX13-LABEL: test_convolve.f32_bf16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_convolve_f32_bf16 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.3x3.v4f32.v4f32.v36bf16.v6bf16(<4 x float> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v34, v27 :: v_dual_mov_b32 v33, v26
; GFX13-NEXT:    v_mov_b32_e32 v32, v25
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:3], v[2:3], v[4:21], v[22:24], v[32:34], v[28:30] aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v4bf16.v4bf16.v36bf16.v6bf16(<4 x bfloat> %acc_in, <36 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x2 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_4x4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <18 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v8bf16.v8bf16.v18bf16.v6bf16(<8 x bfloat> %acc_in, <18 x bfloat> %weights, <6 x bfloat> %tensor_col_center, <6 x bfloat> %tensor_col_left, <6 x bfloat> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_4x4 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_3x3_8x4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <10 x bfloat> %weights, <8 x bfloat> %tensor_col_center, <8 x bfloat> %tensor_col_left, <8 x bfloat> %tensor_col_right) {
; GFX13-LABEL: test_convolve.bf16_bf16_3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.3x3.v8bf16.v8bf16.v10bf16.v8bf16(<8 x bfloat> %acc_in, <10 x bfloat> %weights, <8 x bfloat> %tensor_col_center, <8 x bfloat> %tensor_col_left, <8 x bfloat> %tensor_col_right,
              ;   AUX_DATA: PIXEL_SHAPE_8x4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu4 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu4 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu4 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu4 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu4 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu4 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu4 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:3], v[2:3], v[4:9], v10, v11, v12 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu4 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu4 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu4_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu4 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu4.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu4_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu4_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu4 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu4.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu4_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu4 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu4.1x1.v4i32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu4_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu4_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu4 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu4.1x1.v4f32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu8 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu8 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu8 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v2i32.i32(<4 x i32> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu8 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu8 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu8 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v4i32.i32(<4 x i32> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu8 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:3], v[2:3], v[4:9], v10, v11, v12 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu8 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu8 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v6i32.i32(<4 x i32> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_iu8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_iu8 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.iu8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_iu8_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_iu8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_iu8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.iu8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.i32_iu8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.i32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_i32_iu8 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x i32> @llvm.amdgcn.convolve.i32.iu8.1x1.v4i32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x i32> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32i32_iu8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32i32_iu8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32i32_iu8 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32i32.iu8.1x1.v4f32.v4i32.v8i32.i32(<4 x i32> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_fp8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_fp8 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v4f16.v4f16.v2i32.i32(<4 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.i32.i32(<8 x half> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_fp8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_fp8 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v4f16.v4f16.v4i32.i32(<4 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.v2i32.i32(<8 x half> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.i32.v2i32(<8 x half> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_fp8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_fp8 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:3], v[2:3], v[4:9], v10, v11, v12 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v4f16.v4f16.v6i32.i32(<4 x half> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.v3i32.i32(<8 x half> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_fp8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_fp8 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.fp8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v4f16.v4f16.v8i32.i32(<4 x half> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.v4i32.i32(<8 x half> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_fp8_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_fp8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_fp8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.fp8.1x1.v8f16.v8f16.v2i32.v2i32(<8 x half> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf8 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.1x1.v4f32.v4f32.v2i32.i32(<4 x float> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v4bf16.v4bf16.v2i32.i32(<4 x bfloat> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x bfloat> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.i32.i32(<8 x bfloat> %acc_in, i32 %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x bfloat> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.i32.v2i32(<8 x bfloat> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf8 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.1x1.v4f32.v4f32.v4i32.i32(<4 x float> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v4bf16.v4bf16.v4i32.i32(<4 x bfloat> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.v2i32.i32(<8 x bfloat> %acc_in, <2 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x bfloat> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.i32.v2i32(<8 x bfloat> %acc_in, i32 %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf8 v[2:5], v[2:5], v[6:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.1x1.v4f32.v4f32.v6i32.i32(<4 x float> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:3], v[2:3], v[4:9], v10, v11, v12 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v4bf16.v4bf16.v6i32.i32(<4 x bfloat> %acc_in, <6 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.v3i32.i32(<8 x bfloat> %acc_in, <3 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.v2i32.v2i32(<8 x bfloat> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf8 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf8.1x1.v4f32.v4f32.v8i32.i32(<4 x float> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v4bf16.v4bf16.v8i32.i32(<4 x bfloat> %acc_in, <8 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.v4i32.i32(<8 x bfloat> %acc_in, <4 x i32> %weights, i32 %tensor_0, i32 %tensor_1, i32 %tensor_2, i32 %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf8_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf8_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf8 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf8.1x1.v8bf16.v8bf16.v2i32.v2i32(<8 x bfloat> %acc_in, <2 x i32> %weights, <2 x i32> %tensor_0, <2 x i32> %tensor_1, <2 x i32> %tensor_2, <2 x i32> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_f16 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v4f16.v2f16(<4 x float> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v4f16.v2f16(<4 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v2f16(<8 x half> %acc_in, <2 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_f16 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v8f16.v2f16(<4 x float> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v8f16.v2f16(<4 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v2f16(<8 x half> %acc_in, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half> %acc_in, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_f16 v[2:5], v[2:5], v[6:13], v14, v15, v16 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v16f16.v2f16(<4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:3], v[2:3], v[4:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v16f16.v2f16(<4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <6 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v6f16.v2f16(<8 x half> %acc_in, <6 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v4f16(<8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_f16_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_f16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_f16 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.f16.1x1.v4f32.v4f32.v16f16.v2f16(<4 x float> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v4f16.v4f16.v16f16.v2f16(<4 x half> %acc_in, <16 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v8f16.v2f16(<8 x half> %acc_in, <8 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f16_f16_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3) {
; GFX13-LABEL: test_convolve.f16_f16_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f16_f16 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.convolve.f16.f16.1x1.v8f16.v8f16.v4f16.v4f16(<8 x half> %acc_in, <4 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x float> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf16 v[2:5], v[2:5], v[6:7], v8 aux_data:3 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v4bf16.v2bf16(<4 x float> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_1(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:3], v[2:3], v[4:5], v6 aux_data:3 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v4bf16.v2bf16(<4 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_1) == (3 << 0 | 0 << 12)
              i32 3,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_1(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v6, v7 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v2bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_1) == (2 << 0 | 0 << 12)
              i32 2,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_1(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_1:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v6, v[8:9] clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v4bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_1) == (0 << 0 | 0 << 12)
              i32 0,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x float> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf16 v[2:5], v[2:5], v[6:9], v10, v11 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v8bf16.v2bf16(<4 x float> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:3], v[2:3], v[4:7], v8, v9 aux_data:4099 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v8bf16.v2bf16(<4 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_2) == (3 << 0 | 1 << 12)
              i32 4099,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_2(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:7], v8, v9 aux_data:4098 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v2bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_2) == (2 << 0 | 1 << 12)
              i32 4098,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_2(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v6, v[8:9], v[10:11] aux_data:4096 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v2bf16.v4bf16(<8 x bfloat> %acc_in, <2 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_2) == (0 << 0 | 1 << 12)
              i32 4096,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf16 v[2:5], v[2:5], v[6:13], v14, v15, v16 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v16bf16.v2bf16(<4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_3(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:3], v[2:3], v[4:11], v12, v13, v14 aux_data:8195 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v16bf16.v2bf16(<4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_3) == (3 << 0 | 2 << 12)
              i32 8195,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_3(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <6 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:8], v9, v10, v11 aux_data:8194 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v6bf16.v2bf16(<8 x bfloat> %acc_in, <6 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_3) == (2 << 0 | 2 << 12)
              i32 8194,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_3(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_3:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13] aux_data:8192 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v4bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_3) == (0 << 0 | 2 << 12)
              i32 8192,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.f32_bf16_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.f32_bf16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_f32_bf16 v[2:5], v[2:5], v[6:13], v14, v15, v16, v17 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x float> @llvm.amdgcn.convolve.f32.bf16.1x1.v4f32.v4f32.v16bf16.v2bf16(<4 x float> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x2_iter_4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x2_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:3], v[2:3], v[4:11], v12, v13, v14, v15 aux_data:12291 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v4bf16.v4bf16.v16bf16.v2bf16(<4 x bfloat> %acc_in, <16 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x2 | CONV_ITER_4) == (3 << 0 | 3 << 12)
              i32 12291,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_4x4_iter_4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_4x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:9], v10, v11, v12, v13 aux_data:12290 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v8bf16.v2bf16(<8 x bfloat> %acc_in, <8 x bfloat> %weights, <2 x bfloat> %tensor_0, <2 x bfloat> %tensor_1, <2 x bfloat> %tensor_2, <2 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_4x4 | CONV_ITER_4) == (2 << 0 | 3 << 12)
              i32 12290,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_convolve.bf16_bf16_1x1_8x4_iter_4(ptr addrspace(1) %out, <8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3) {
; GFX13-LABEL: test_convolve.bf16_bf16_1x1_8x4_iter_4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_convolve_bf16_bf16 v[2:5], v[2:5], v[6:7], v[8:9], v[10:11], v[12:13], v[14:15] aux_data:12288 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x bfloat> @llvm.amdgcn.convolve.bf16.bf16.1x1.v8bf16.v8bf16.v4bf16.v4bf16(<8 x bfloat> %acc_in, <4 x bfloat> %weights, <4 x bfloat> %tensor_0, <4 x bfloat> %tensor_1, <4 x bfloat> %tensor_2, <4 x bfloat> %tensor_3,
              ;   AUX_DATA: (PIXEL_SHAPE_8x4 | CONV_ITER_4) == (0 << 0 | 3 << 12)
              i32 12288,
              ;   CLAMP
              i1 1)
  store <8 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

