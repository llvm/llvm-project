; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx942 < %s | FileCheck %s

; Check that simple conditional memory accesses that are guarded by likely
; divergent conditions are not lowered with an s_cbranch_execz to bypass them.
; Instructions like s_waitcnt vmcnt(0) block the elimination of s_cbranch_execz.

declare i32 @llvm.amdgcn.workitem.id.x()
declare i32 @llvm.amdgcn.workitem.id.y()

define amdgpu_kernel void @cond_ops(ptr addrspace(1) inreg %x, ptr addrspace(1) inreg %y) !reqd_work_group_size !0 {
; CHECK-LABEL: cond_ops:
; CHECK:       ; %bb.5:
; CHECK-NEXT:    s_load_dwordx4 s[8:11], s[4:5], 0x0
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_branch .LBB0_0
; CHECK-NEXT:    .p2align 8
; CHECK-NEXT:  ; %bb.6:
; CHECK-NEXT:  .LBB0_0: ; %entry
; CHECK-NEXT:    v_and_b32_e32 v1, 0x3ff, v0
; CHECK-NEXT:    v_bfe_u32 v0, v0, 10, 10
; CHECK-NEXT:    v_lshl_or_b32 v5, v0, 6, v1
; CHECK-NEXT:    v_lshrrev_b32_e32 v0, 4, v5
; CHECK-NEXT:    v_cmp_gt_u32_e32 vcc, 15, v0
; CHECK-NEXT:    v_mov_b32_e32 v0, 0
; CHECK-NEXT:    v_lshlrev_b32_e32 v4, 4, v5
; CHECK-NEXT:    v_mov_b32_e32 v1, 0
; CHECK-NEXT:    v_mov_b32_e32 v2, 0
; CHECK-NEXT:    v_mov_b32_e32 v3, 0
; CHECK-NEXT:    s_and_saveexec_b64 s[0:1], vcc
; CHECK-NEXT:  ; %bb.1: ; %do.load
; CHECK-NEXT:    global_load_dwordx4 v[0:3], v4, s[8:9]
; CHECK-NEXT:  ; %bb.2: ; %post.load
; CHECK-NEXT:    s_or_b64 exec, exec, s[0:1]
; CHECK-NEXT:    v_and_b32_e32 v5, 15, v5
; CHECK-NEXT:    v_cmp_gt_u32_e32 vcc, 15, v5
; CHECK-NEXT:    s_and_saveexec_b64 s[0:1], vcc
; CHECK-NEXT:    s_cbranch_execz .LBB0_4
; CHECK-NEXT:  ; %bb.3: ; %do.store
; CHECK-NEXT:    s_waitcnt vmcnt(0)
; CHECK-NEXT:    global_store_dwordx4 v4, v[0:3], s[10:11]
; CHECK-NEXT:  .LBB0_4: ; %exit
; CHECK-NEXT:    s_endpgm
entry:
  %tid.x = tail call range(i32 0, 64) i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call range(i32 0, 4) i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %k = lshr i32 %tid, 4
  %j = and i32 %tid, 15
  %load.cond = icmp ult i32 %k, 15
  %tid.ext = zext nneg i32 %tid to i64
  %my.x = getelementptr <4 x float>, ptr addrspace(1) %x, i64 %tid.ext
  br i1 %load.cond, label %do.load, label %post.load
do.load:
  %loaded = load <4 x float>, ptr addrspace(1) %my.x
  br label %post.load
post.load:
  %maybe.loaded = phi <4 x float> [ %loaded, %do.load ], [ zeroinitializer, %entry ]
  %my.y = getelementptr <4 x float>, ptr addrspace(1) %y, i64 %tid.ext
  %store.cond = icmp ult i32 %j, 15
  br i1 %store.cond, label %do.store, label %exit
do.store:
  store <4 x float> %maybe.loaded, ptr addrspace(1) %my.y
  br label %exit
exit:
  ret void
}

define amdgpu_kernel void @cond_store_even_ifelse(ptr addrspace(1) inreg %dest) #0 !reqd_work_group_size !0 {
; CHECK-LABEL: cond_store_even_ifelse:
; CHECK:       ; %bb.5:
; CHECK-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x0
; CHECK-NEXT:    s_waitcnt lgkmcnt(0)
; CHECK-NEXT:    s_branch .LBB1_0
; CHECK-NEXT:    .p2align 8
; CHECK-NEXT:  ; %bb.6:
; CHECK-NEXT:  .LBB1_0: ; %entry
; CHECK-NEXT:    v_lshrrev_b32_e32 v1, 4, v0
; CHECK-NEXT:    v_and_b32_e32 v1, 0xffc0, v1
; CHECK-NEXT:    s_movk_i32 s0, 0x3ff
; CHECK-NEXT:    v_and_or_b32 v0, v0, s0, v1
; CHECK-NEXT:    v_and_b32_e32 v4, 1, v0
; CHECK-NEXT:    v_lshlrev_b32_e32 v2, 2, v0
; CHECK-NEXT:    v_mov_b32_e32 v3, 0
; CHECK-NEXT:    v_lshl_add_u64 v[0:1], s[8:9], 0, v[2:3]
; CHECK-NEXT:    v_cmp_eq_u32_e32 vcc, 0, v4
; CHECK-NEXT:    s_and_saveexec_b64 s[0:1], vcc
; CHECK-NEXT:    s_xor_b64 s[0:1], exec, s[0:1]
; CHECK-NEXT:  ; %bb.1: ; %do.store.if
; CHECK-NEXT:    global_store_dword v[0:1], v3, off
; CHECK-NEXT:    ; implicit-def: $vgpr0_vgpr1
; CHECK-NEXT:  ; %bb.2: ; %cf.mid
; CHECK-NEXT:    s_andn2_saveexec_b64 s[0:1], s[0:1]
; CHECK-NEXT:  ; %bb.3: ; %do.store.else
; CHECK-NEXT:    v_mov_b32_e32 v2, 1
; CHECK-NEXT:    global_store_dword v[0:1], v2, off
; CHECK-NEXT:  ; %bb.4: ; %exit
; CHECK-NEXT:    s_endpgm
entry:
  %tid.x = tail call i32 @llvm.amdgcn.workitem.id.x()
  %tid.y = tail call i32 @llvm.amdgcn.workitem.id.y()
  %tid.y.shift = shl nuw nsw i32 %tid.y, 6
  %tid = or disjoint i32 %tid.x, %tid.y.shift
  %tid.ext = zext nneg i32 %tid to i64
  %lsbit = and i32 %tid, 1
  %local.addr = getelementptr i32, ptr addrspace(1) %dest, i64 %tid.ext
  %cond = icmp eq i32 %lsbit, 0
  br i1 %cond, label %do.store.if, label %cf.mid
do.store.if:
  store i32 0, ptr addrspace(1) %local.addr
  br label %cf.mid
cf.mid:
  %notcond = phi i1 [0, %do.store.if], [1, %entry]
  br i1 %notcond, label %do.store.else, label %exit
do.store.else:
  store i32 1, ptr addrspace(1) %local.addr
  br label %exit
exit:
  ret void
}


!0 = !{i32 64, i32 4, i32 1}
