# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc -mtriple=amdgcn -mcpu=gfx90a -run-pass=amdgpu-pre-ra-optimizations -verify-machineinstrs %s -o - | FileCheck %s

# Test for GCNPreRAOptimizationsImpl::revertConditionalFMAPattern
---
name: cond_sub_in_loop
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: cond_sub_in_loop
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY $vgpr0_vgpr1
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = COPY $vgpr2_vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec = COPY $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub0, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub1, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = V_ADD_F64_e64 0, [[COPY]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY [[V_ADD_F64_e64_]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc

    %0:vreg_64_align2 = COPY $vgpr0_vgpr1
    %1:vreg_64_align2 = COPY $vgpr2_vgpr3
    %2:sreg_64_xexec = COPY $vcc

  bb.1:
    ; Pattern: FMA with conditional multiplier (cond ? -1.0 : 0.0) * value
    ; Expecting revert. It's almost impossible to get this pattern in a loop
    ; unless rematerializer goes wild, but this also checks the reverter in a
    ; non-loop context.
    %3:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec  ; -1.0 high bits (0xbff00000)
    undef %4.sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, %3, %2, implicit $exec  ; mul.hi = cond ? -1.0_hi : 0
    %4.sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec                          ; mul.lo = 0
    %5:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 %4, %1, %0, implicit $mode, implicit $exec  ; accum + mul * value
    %0:vreg_64_align2 = COPY %5
    S_BRANCH %bb.1

  bb.2:
    S_ENDPGM 0
...

---
name: cond_sub_const_before_loop
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: cond_sub_const_before_loop
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY $vgpr0_vgpr1
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = COPY $vgpr2_vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec = COPY $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub0, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub1, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = V_ADD_F64_e64 0, [[COPY]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY [[V_ADD_F64_e64_]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc

    %0:vreg_64_align2 = COPY $vgpr0_vgpr1
    %1:vreg_64_align2 = COPY $vgpr2_vgpr3
    %2:sreg_64_xexec = COPY $vcc
    ; -1 constant initialization before loop
    %3:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec

  bb.1:
    ; Pattern: Only V_MOV(0) and V_CNDMASK are in loop
    ; Same as previous, expecting revert.
    undef %4.sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, %3, %2, implicit $exec
    %4.sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec
    %5:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 %4, %1, %0, implicit $mode, implicit $exec
    %0:vreg_64_align2 = COPY %5
    S_BRANCH %bb.1

  bb.2:
    S_ENDPGM 0
...

---
name: cond_sub_zero_const_before_loop
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: cond_sub_zero_const_before_loop
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY $vgpr0_vgpr1
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = COPY $vgpr2_vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec = COPY $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub0, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]].sub1, [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = V_ADD_F64_e64 0, [[COPY]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY [[V_ADD_F64_e64_]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc

    %0:vreg_64_align2 = COPY $vgpr0_vgpr1
    %1:vreg_64_align2 = COPY $vgpr2_vgpr3
    %2:sreg_64_xexec = COPY $vcc
    ; 0 constant initialization before loop
    undef %4.sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec

  bb.1:
    ; Pattern: V_MOV(-1.0) and V_CNDMASK are in loop
    ; Same as previous, expecting revert.
    %3:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
    %4.sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, %3, %2, implicit $exec
    %5:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 %4, %1, %0, implicit $mode, implicit $exec
    %0:vreg_64_align2 = COPY %5
    S_BRANCH %bb.1

  bb.2:
    S_ENDPGM 0
...

---
name: cond_sub_both_consts_before_loop
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: cond_sub_both_consts_before_loop
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY $vgpr0_vgpr1
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = COPY $vgpr2_vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec = COPY $vcc
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
  ; CHECK-NEXT:   undef [[V_MOV_B32_e32_1:%[0-9]+]].sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[COPY2]], implicit $exec
  ; CHECK-NEXT:   [[V_FMAC_F64_e32_:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[V_MOV_B32_e32_1]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY [[V_FMAC_F64_e32_]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc

    %0:vreg_64_align2 = COPY $vgpr0_vgpr1
    %1:vreg_64_align2 = COPY $vgpr2_vgpr3
    %2:sreg_64_xexec = COPY $vcc
    ; BOTH constant initializations before loop
    %3:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
    undef %4.sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec

  bb.1:
    ; Pattern: Only V_FMAC and V_CNDMASK are in loop
    ; This is the most expected pattern, no revert should be done.
    %4.sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, %3, %2, implicit $exec
    %5:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 %4, %1, %0, implicit $mode, implicit $exec
    %0:vreg_64_align2 = COPY %5
    S_BRANCH %bb.1

  bb.2:
    S_ENDPGM 0
...
---
name: cond_only_fma_in_loop
tracksRegLiveness: true
body: |
  ; CHECK-LABEL: name: cond_only_fma_in_loop
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY $vgpr0_vgpr1
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = COPY $vgpr2_vgpr3
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64_xexec = COPY $vcc
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
  ; CHECK-NEXT:   undef [[V_MOV_B32_e32_1:%[0-9]+]].sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[COPY2]], implicit $exec
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_FMAC_F64_e32_:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[V_MOV_B32_e32_1]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = COPY [[V_FMAC_F64_e32_]]
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vcc
    %0:vreg_64_align2 = COPY $vgpr0_vgpr1
    %1:vreg_64_align2 = COPY $vgpr2_vgpr3
    %2:sreg_64_xexec = COPY $vcc
    ; BOTH constant initializations and V_CNDMASK before loop
    %3:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
    undef %4.sub0:vreg_64_align2 = V_MOV_B32_e32 0, implicit $exec
    %4.sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, %3, %2, implicit $exec

  bb.1:
    ; Pattern: Only V_FMAC is in loop
    ; Selectors don't generate this pattern, but it probably may come from user
    ; code. Ideally we should revert this to save registers and use cheaper f_add
    ; but skip this for now as it's harder to revert.
    %5:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 %4, %1, %0, implicit $mode, implicit $exec
    %0:vreg_64_align2 = COPY %5
    S_BRANCH %bb.1

  bb.2:
    S_ENDPGM 0
...
