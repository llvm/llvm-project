# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
# RUN: llc -mcpu=gfx1100 -mtriple=amdgcn-amd-amdhsa -stress-regalloc=4 --verify-machineinstrs -run-pass=greedy -o - %s | FileCheck %s

# This test demonstrates register pressure mitigation through subreg-aware
# reloads during register allocation. This test originally failed with
# insufficient registers during RA due to a high register pressure. The
# subreg-aware reload, when implemented, mitigated register pressure by
# reloading only the used portions of tuple registers from their spilled
# locations instead of reloading entire tuples, thereby enhancing register
# allocation.

---
name:            subreg-reload
tracksRegLiveness: true
machineFunctionInfo:
  frameOffsetReg:  '$sgpr33'
  stackPtrOffsetReg: '$sgpr32'
body:             |
  bb.0:
    liveins: $vgpr1, $sgpr0_sgpr1, $sgpr2_sgpr3

    ; CHECK-LABEL: name: subreg-reload
    ; CHECK: liveins: $vgpr1, $sgpr0_sgpr1, $sgpr2_sgpr3
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr1
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vreg_64 = COPY renamable $sgpr0_sgpr1
    ; CHECK-NEXT: SI_SPILL_V64_SAVE [[COPY1]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
    ; CHECK-NEXT: [[FLAT_LOAD_DWORDX4_:%[0-9]+]]:vreg_128 = FLAT_LOAD_DWORDX4 [[COPY1]], 0, 0, implicit $exec, implicit $flat_scr
    ; CHECK-NEXT: SI_SPILL_V128_SAVE [[FLAT_LOAD_DWORDX4_]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s128) into %stack.1, align 4, addrspace 5)
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
    ; CHECK-NEXT: undef early-clobber %31.sub0_sub1:vreg_128, $sgpr_null = V_MAD_U64_U32_gfx11_e64 [[SI_SPILL_V32_RESTORE]], 42, 0, 0, implicit $exec
    ; CHECK-NEXT: undef [[COPY2:%[0-9]+]].sub0:vreg_64 = COPY %31.sub1
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 4, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
    ; CHECK-NEXT: early-clobber %31.sub1_sub2:vreg_128, $sgpr_null = V_MAD_U64_U32_gfx11_e64 [[SI_SPILL_V32_RESTORE1]], 42, [[COPY2]], 0, implicit $exec
    ; CHECK-NEXT: SI_SPILL_V128_SAVE %31, %stack.3, $sgpr32, 0, implicit $exec :: (store (s128) into %stack.3, align 4, addrspace 5)
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 8, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
    ; CHECK-NEXT: early-clobber %24:vreg_64, $sgpr_null = V_MAD_U64_U32_gfx11_e64 [[SI_SPILL_V32_RESTORE2]], 42, 0, 0, implicit $exec
    ; CHECK-NEXT: SI_SPILL_V64_SAVE %24, %stack.2, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.2, align 4, addrspace 5)
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE3:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 4, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
    ; CHECK-NEXT: undef [[COPY3:%[0-9]+]].sub0:vreg_64 = COPY [[SI_SPILL_V32_RESTORE3]]
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE4:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 12, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
    ; CHECK-NEXT: early-clobber %35:vreg_64, $sgpr_null = V_MAD_U64_U32_gfx11_e64 [[SI_SPILL_V32_RESTORE4]], 42, [[COPY3]], 0, implicit $exec
    ; CHECK-NEXT: SI_SPILL_V64_SAVE %35, %stack.4, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.4, align 4, addrspace 5)
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE5:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
    ; CHECK-NEXT: [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.3, align 4, addrspace 5)
    ; CHECK-NEXT: undef [[COPY4:%[0-9]+]].sub0_sub1:vreg_128 = COPY [[SI_SPILL_V64_RESTORE]]
    ; CHECK-NEXT: [[COPY4:%[0-9]+]].sub2:vreg_128 = COPY [[SI_SPILL_V32_RESTORE5]]
    ; CHECK-NEXT: [[SI_SPILL_V32_RESTORE6:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.4, addrspace 5)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]].sub3:vreg_128 = COPY [[SI_SPILL_V32_RESTORE6]]
    ; CHECK-NEXT: [[COPY5:%[0-9]+]]:vreg_128 = COPY [[COPY4]]
    ; CHECK-NEXT: $vgpr31 = COPY [[COPY]]
    ; CHECK-NEXT: INLINEASM &"; use v1", 1 /* sideeffect attdialect */, 327690 /* regdef:VS_16_Lo128 */, $vgpr1
    ; CHECK-NEXT: [[SI_SPILL_V64_RESTORE1:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
    ; CHECK-NEXT: FLAT_STORE_DWORDX4 [[SI_SPILL_V64_RESTORE1]], [[COPY5]], 0, 0, implicit $exec, implicit $flat_scr
    ; CHECK-NEXT: SI_RETURN
    %1:vgpr_32 = COPY $vgpr1
    %2:vreg_64 = COPY killed renamable $sgpr0_sgpr1
    %3:vreg_128 = FLAT_LOAD_DWORDX4 %2, 0, 0, implicit $exec, implicit $flat_scr
    undef early-clobber %4.sub0_sub1:vreg_128, $sgpr_null = V_MAD_U64_U32_gfx11_e64 %3.sub0, 42, 0, 0, implicit $exec
    undef %5.sub0:vreg_64 = COPY %4.sub1
    early-clobber %4.sub1_sub2:vreg_128, $sgpr_null = V_MAD_U64_U32_gfx11_e64 %3.sub1, 42, %5, 0, implicit $exec
    early-clobber %6:vreg_64, $sgpr_null = V_MAD_U64_U32_gfx11_e64 %3.sub2, 42, 0, 0, implicit $exec
    undef %7.sub0:vreg_64 = COPY %6.sub1
    early-clobber %8:vreg_64, $sgpr_null = V_MAD_U64_U32_gfx11_e64 %3.sub3, 42, %7, 0, implicit $exec
    %4.sub2:vreg_128 = COPY %6.sub0
    %4.sub3:vreg_128 = COPY %8.sub0
    $vgpr31 = COPY %1
    INLINEASM &"; use v1", 1, 327690, $vgpr1
    FLAT_STORE_DWORDX4 %2, %4, 0, 0, implicit $exec, implicit $flat_scr
    SI_RETURN
...
