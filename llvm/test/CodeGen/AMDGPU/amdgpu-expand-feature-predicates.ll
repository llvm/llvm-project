; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; REQUIRES: amdgpu-registered-target

; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx906 -passes='amdgpu-expand-feature-predicates' %s -o - | FileCheck --check-prefix=GFX906 %s
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1010 -passes='amdgpu-expand-feature-predicates' %s -o - | FileCheck --check-prefix=GFX1010 %s
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1101 -passes='amdgpu-expand-feature-predicates' %s -o - | FileCheck --check-prefix=GFX1101 %s
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1201 -passes='amdgpu-expand-feature-predicates' %s -o - | FileCheck --check-prefix=GFX1201 %s
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1201 -mattr=+wavefrontsize64 -passes='amdgpu-expand-feature-predicates' %s -o - | FileCheck --check-prefix=GFX1201-W64 %s

;; The IR was derived from the following source:
;; extern "C" __global__ void kernel(int* p, int x)
;; {
;;     if (__builtin_amdgcn_processor_is("gfx1201") ||
;;         __builtin_amdgcn_is_invocable(__builtin_amdgcn_s_sleep_var))
;;         __builtin_amdgcn_s_sleep_var(x);
;;     if (!__builtin_amdgcn_processor_is("gfx906"))
;;         __builtin_amdgcn_s_wait_event_export_ready();
;;     else if (__builtin_amdgcn_processor_is("gfx1010") ||
;;         __builtin_amdgcn_processor_is("gfx1101"))
;;         __builtin_amdgcn_s_ttracedata_imm(1);
;;     while (__builtin_amdgcn_processor_is("gfx1101")) *p += x;
;;     do {
;;         *p -= x;
;;     } while (__builtin_amdgcn_processor_is("gfx1010"));
;;     for (; __builtin_amdgcn_processor_is("gfx1201"); ++*p) break;
;;
;;     if (__builtin_amdgcn_is_invocable(__builtin_amdgcn_s_wait_event_export_ready))
;;         __builtin_amdgcn_s_wait_event_export_ready();
;;     else if (__builtin_amdgcn_is_invocable(__builtin_amdgcn_s_ttracedata_imm))
;;         __builtin_amdgcn_s_ttracedata_imm(1);
;;
;;     do {
;;         *p -= x;
;;     } while (__builtin_amdgcn_is_invocable(__builtin_amdgcn_global_load_tr_b64_i32));
;;     for (; __builtin_amdgcn_is_invocable(__builtin_amdgcn_permlane64); ++*p) break;
;; }

@llvm.amdgcn.is.gfx1201 = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.has.gfx12-insts = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.is.gfx906 = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.is.gfx1010 = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.is.gfx1101 = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.has.gfx11-insts = external addrspace(1) externally_initialized constant i1
@llvm.amdgcn.has.gfx10-insts = external addrspace(1) externally_initialized constant i1
@"llvm.amdgcn.has.gfx12-insts,wavefrontsize64" = external addrspace(1) externally_initialized constant i1

declare void @llvm.amdgcn.s.sleep.var(i32)
declare void @llvm.amdgcn.s.wait.event.export.ready()
declare void @llvm.amdgcn.s.ttracedata.imm(i16 immarg)

define amdgpu_kernel void @kernel(ptr addrspace(1) %p.coerce, i32 %x) {
; GFX906-LABEL: define amdgpu_kernel void @kernel(
; GFX906-SAME: ptr addrspace(1) [[P_COERCE:%.*]], i32 [[X:%.*]]) #[[ATTR2:[0-9]+]] {
; GFX906-NEXT:  [[ENTRY:.*:]]
; GFX906-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[P_COERCE]] to i64
; GFX906-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[TMP0]] to ptr
; GFX906-NEXT:    br label %[[IF_GFX1201_OR_GFX12_INSTS:.*]]
; GFX906:       [[IF_GFX1201_OR_GFX12_INSTS]]:
; GFX906-NEXT:    br label %[[IF_NOT_GFX907:.*]]
; GFX906:       [[IF_NOT_GFX907]]:
; GFX906-NEXT:    br label %[[IF_GFX1010_OR_GFX1101:.*]]
; GFX906:       [[IF_GFX1010_OR_GFX1101]]:
; GFX906-NEXT:    br label %[[LOR_NOT_GFX1010:.*]]
; GFX906:       [[LOR_NOT_GFX1010]]:
; GFX906-NEXT:    br label %[[FOR_COND:.*]]
; GFX906:       [[FOR_COND]]:
; GFX906-NEXT:    [[DOTPROMOTED:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX906-NEXT:    [[SUB_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED]], [[X]]
; GFX906-NEXT:    store i32 [[SUB_PEEL]], ptr [[TMP1]], align 4
; GFX906-NEXT:    br label %[[IF_GFX11_INSTS:.*]]
; GFX906:       [[IF_GFX11_INSTS]]:
; GFX906-NEXT:    br label %[[IF_GFX10_INSTS:.*]]
; GFX906:       [[IF_GFX10_INSTS]]:
; GFX906-NEXT:    call void @llvm.assume(i1 true)
; GFX906-NEXT:    [[DOTPROMOTED9:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX906-NEXT:    [[SUB13_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED9]], [[X]]
; GFX906-NEXT:    store i32 [[SUB13_PEEL]], ptr [[TMP1]], align 4
; GFX906-NEXT:    ret void
;
; GFX1010-LABEL: define amdgpu_kernel void @kernel(
; GFX1010-SAME: ptr addrspace(1) [[P_COERCE:%.*]], i32 [[X:%.*]]) #[[ATTR2:[0-9]+]] {
; GFX1010-NEXT:  [[ENTRY:.*:]]
; GFX1010-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[P_COERCE]] to i64
; GFX1010-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[TMP0]] to ptr
; GFX1010-NEXT:    br label %[[IF_GFX1201_OR_GFX12_INSTS:.*]]
; GFX1010:       [[IF_GFX1201_OR_GFX12_INSTS]]:
; GFX1010-NEXT:    br label %[[IF_NOT_GFX906:.*]]
; GFX1010:       [[IF_NOT_GFX906]]:
; GFX1010-NEXT:    br label %[[LOR_NOT_GFX1010:.*]]
; GFX1010:       [[LOR_NOT_GFX1010]]:
; GFX1010-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1010-NEXT:    br label %[[IF_END6:.*]]
; GFX1010:       [[IF_END6]]:
; GFX1010-NEXT:    call void @llvm.assume(i1 true)
; GFX1010-NEXT:    call void @llvm.assume(i1 true)
; GFX1010-NEXT:    br label %[[FOR_COND:.*]]
; GFX1010:       [[FOR_COND]]:
; GFX1010-NEXT:    [[DOTPROMOTED:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1010-NEXT:    [[SUB_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED]], [[X]]
; GFX1010-NEXT:    store i32 [[SUB_PEEL]], ptr [[TMP1]], align 4
; GFX1010-NEXT:    br label %[[IF_GFX11_INSTS:.*]]
; GFX1010:       [[IF_GFX11_INSTS]]:
; GFX1010-NEXT:    br label %[[IF_GFX10_INSTS:.*]]
; GFX1010:       [[IF_GFX10_INSTS]]:
; GFX1010-NEXT:    call void @llvm.amdgcn.s.ttracedata.imm(i16 1)
; GFX1010-NEXT:    br label %[[IF_END11:.*]]
; GFX1010:       [[IF_END11]]:
; GFX1010-NEXT:    call void @llvm.assume(i1 true)
; GFX1010-NEXT:    [[DOTPROMOTED9:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1010-NEXT:    [[SUB13_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED9]], [[X]]
; GFX1010-NEXT:    store i32 [[SUB13_PEEL]], ptr [[TMP1]], align 4
; GFX1010-NEXT:    ret void
;
; GFX1101-LABEL: define amdgpu_kernel void @kernel(
; GFX1101-SAME: ptr addrspace(1) [[P_COERCE:%.*]], i32 [[X:%.*]]) #[[ATTR2:[0-9]+]] {
; GFX1101-NEXT:  [[ENTRY:.*:]]
; GFX1101-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[P_COERCE]] to i64
; GFX1101-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[TMP0]] to ptr
; GFX1101-NEXT:    br label %[[IF_GFX1201_OR_GFX12_INSTS:.*]]
; GFX1101:       [[IF_GFX1201_OR_GFX12_INSTS]]:
; GFX1101-NEXT:    br label %[[IF_END:.*]]
; GFX1101:       [[IF_END]]:
; GFX1101-NEXT:    br label %[[IF_NOT_GFX907:.*]]
; GFX1101:       [[IF_NOT_GFX907]]:
; GFX1101-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1101-NEXT:    br label %[[IF_NOT_GFX906:.*]]
; GFX1101:       [[IF_NOT_GFX906]]:
; GFX1101-NEXT:    call void @llvm.assume(i1 true)
; GFX1101-NEXT:    call void @llvm.assume(i1 true)
; GFX1101-NEXT:    br label %[[FOR_COND:.*]]
; GFX1101:       [[FOR_COND]]:
; GFX1101-NEXT:    [[DOTPROMOTED:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1101-NEXT:    [[SUB_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED]], [[X]]
; GFX1101-NEXT:    store i32 [[SUB_PEEL]], ptr [[TMP1]], align 4
; GFX1101-NEXT:    br label %[[IF_GFX11_INSTS:.*]]
; GFX1101:       [[IF_GFX11_INSTS]]:
; GFX1101-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1101-NEXT:    br label %[[IF_ELSE8:.*]]
; GFX1101:       [[IF_ELSE8]]:
; GFX1101-NEXT:    call void @llvm.assume(i1 true)
; GFX1101-NEXT:    [[DOTPROMOTED9:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1101-NEXT:    [[SUB13_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED9]], [[X]]
; GFX1101-NEXT:    store i32 [[SUB13_PEEL]], ptr [[TMP1]], align 4
; GFX1101-NEXT:    ret void
;
; GFX1201-LABEL: define amdgpu_kernel void @kernel(
; GFX1201-SAME: ptr addrspace(1) [[P_COERCE:%.*]], i32 [[X:%.*]]) #[[ATTR2:[0-9]+]] {
; GFX1201-NEXT:  [[ENTRY:.*:]]
; GFX1201-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[P_COERCE]] to i64
; GFX1201-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[TMP0]] to ptr
; GFX1201-NEXT:    br label %[[LOR_NOT_GFX1201:.*]]
; GFX1201:       [[LOR_NOT_GFX1201]]:
; GFX1201-NEXT:    call void @llvm.amdgcn.s.sleep.var(i32 [[X]])
; GFX1201-NEXT:    br label %[[IF_NOT_GFX906:.*]]
; GFX1201:       [[IF_NOT_GFX906]]:
; GFX1201-NEXT:    br label %[[IF_GFX1010_OR_GFX1101:.*]]
; GFX1201:       [[IF_GFX1010_OR_GFX1101]]:
; GFX1201-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1201-NEXT:    br label %[[IF_END6:.*]]
; GFX1201:       [[IF_END6]]:
; GFX1201-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-NEXT:    br label %[[FOR_COND:.*]]
; GFX1201:       [[FOR_COND]]:
; GFX1201-NEXT:    [[DOTPROMOTED:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1201-NEXT:    [[SUB_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED]], [[X]]
; GFX1201-NEXT:    store i32 [[SUB_PEEL]], ptr [[TMP1]], align 4
; GFX1201-NEXT:    br label %[[IF_GFX11_INSTS:.*]]
; GFX1201:       [[IF_GFX11_INSTS]]:
; GFX1201-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1201-NEXT:    br label %[[IF_ELSE8:.*]]
; GFX1201:       [[IF_ELSE8]]:
; GFX1201-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-NEXT:    [[DOTPROMOTED9:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1201-NEXT:    [[SUB13_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED9]], [[X]]
; GFX1201-NEXT:    store i32 [[SUB13_PEEL]], ptr [[TMP1]], align 4
; GFX1201-NEXT:    ret void
;
; GFX1201-W64-LABEL: define amdgpu_kernel void @kernel(
; GFX1201-W64-SAME: ptr addrspace(1) [[P_COERCE:%.*]], i32 [[X:%.*]]) #[[ATTR2:[0-9]+]] {
; GFX1201-W64-NEXT:  [[ENTRY:.*:]]
; GFX1201-W64-NEXT:    [[TMP0:%.*]] = ptrtoint ptr addrspace(1) [[P_COERCE]] to i64
; GFX1201-W64-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[TMP0]] to ptr
; GFX1201-W64-NEXT:    br label %[[LOR_NOT_GFX1201:.*]]
; GFX1201-W64:       [[LOR_NOT_GFX1201]]:
; GFX1201-W64-NEXT:    call void @llvm.amdgcn.s.sleep.var(i32 [[X]])
; GFX1201-W64-NEXT:    br label %[[IF_NOT_GFX906:.*]]
; GFX1201-W64:       [[IF_NOT_GFX906]]:
; GFX1201-W64-NEXT:    br label %[[IF_GFX1010_OR_GFX1101:.*]]
; GFX1201-W64:       [[IF_GFX1010_OR_GFX1101]]:
; GFX1201-W64-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1201-W64-NEXT:    br label %[[IF_END6:.*]]
; GFX1201-W64:       [[IF_END6]]:
; GFX1201-W64-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-W64-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-W64-NEXT:    br label %[[FOR_COND:.*]]
; GFX1201-W64:       [[FOR_COND]]:
; GFX1201-W64-NEXT:    [[DOTPROMOTED:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1201-W64-NEXT:    [[SUB_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED]], [[X]]
; GFX1201-W64-NEXT:    store i32 [[SUB_PEEL]], ptr [[TMP1]], align 4
; GFX1201-W64-NEXT:    br label %[[IF_GFX11_INSTS:.*]]
; GFX1201-W64:       [[IF_GFX11_INSTS]]:
; GFX1201-W64-NEXT:    call void @llvm.amdgcn.s.wait.event.export.ready()
; GFX1201-W64-NEXT:    br label %[[IF_ELSE8:.*]]
; GFX1201-W64:       [[IF_ELSE8]]:
; GFX1201-W64-NEXT:    call void @llvm.assume(i1 true)
; GFX1201-W64-NEXT:    [[DOTPROMOTED9:%.*]] = load i32, ptr [[TMP1]], align 4
; GFX1201-W64-NEXT:    [[SUB13_PEEL:%.*]] = sub nsw i32 [[DOTPROMOTED9]], [[X]]
; GFX1201-W64-NEXT:    store i32 [[SUB13_PEEL]], ptr [[TMP1]], align 4
; GFX1201-W64-NEXT:    ret void
;
entry:
  %0 = ptrtoint ptr addrspace(1) %p.coerce to i64
  %1 = inttoptr i64 %0 to ptr
  %2 = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx1201, align 1
  br i1 %2, label %if.gfx1201.or.gfx12-insts, label %lor.not.gfx1201

lor.not.gfx1201:
  %3 = load i1, ptr addrspace(1) @llvm.amdgcn.has.gfx12-insts, align 1
  br i1 %3, label %if.gfx1201.or.gfx12-insts, label %if.end

if.gfx1201.or.gfx12-insts:
  call void @llvm.amdgcn.s.sleep.var(i32 %x)
  br label %if.end

if.end:
  %4 = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx906, align 1
  br i1 %4, label %if.gfx906, label %if.not.gfx906

if.not.gfx906:
  call void @llvm.amdgcn.s.wait.event.export.ready()
  br label %if.end6

if.gfx906:
  %5 = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx1010, align 1
  br i1 %5, label %if.gfx1010.or.gfx1101, label %lor.not.gfx1010

lor.not.gfx1010:
  %6 = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx1101, align 1
  br i1 %6, label %if.gfx1010.or.gfx1101, label %for.cond

if.gfx1010.or.gfx1101:
  call void @llvm.amdgcn.s.ttracedata.imm(i16 1)
  br label %if.end6

if.end6:
  %.pr.pr = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx1101, align 1
  %7 = icmp ne i1 %.pr.pr, true
  call void @llvm.assume(i1 %7)
  %.pr6.pr = load i1, ptr addrspace(1) @llvm.amdgcn.is.gfx1010, align 1
  %8 = icmp ne i1 %.pr6.pr, true
  call void @llvm.assume(i1 %8)
  br label %for.cond

for.cond:
  %.promoted = load i32, ptr %1, align 4
  %sub.peel = sub nsw i32 %.promoted, %x
  store i32 %sub.peel, ptr %1, align 4
  %9 = load i1, ptr addrspace(1) @llvm.amdgcn.has.gfx11-insts, align 1
  br i1 %9, label %if.gfx11-insts, label %if.else8

if.gfx11-insts:
  call void @llvm.amdgcn.s.wait.event.export.ready()
  br label %if.end11

if.else8:
  %10 = load i1, ptr addrspace(1) @llvm.amdgcn.has.gfx10-insts, align 1
  br i1 %10, label %if.gfx10-insts, label %if.end11

if.gfx10-insts:
  call void @llvm.amdgcn.s.ttracedata.imm(i16 1)
  br label %if.end11

if.end11:
  %.pr7 = load i1, ptr addrspace(1) @"llvm.amdgcn.has.gfx12-insts,wavefrontsize64", align 1
  %11 = icmp ne i1 %.pr7, true
  call void @llvm.assume(i1 %11)
  %.promoted9 = load i32, ptr %1, align 4
  %sub13.peel = sub nsw i32 %.promoted9, %x
  store i32 %sub13.peel, ptr %1, align 4
  ret void
}

declare void @llvm.assume(i1 noundef)
