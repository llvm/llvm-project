; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1300 -verify-machineinstrs -o - %s | FileCheck --check-prefixes=CHECK,KERNEL %s

target datalayout = "A5"

@out = external local_unnamed_addr addrspace(10) global <8 x i16>, align 16

define void @dummy_store() #5 {
; CHECK-LABEL: dummy_store:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_mov_b32 s0, 0
; CHECK-NEXT:    v_mov_b32_e32 v0, 1
; CHECK-NEXT:    s_wait_loadcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    scratch_store_b32 off, v0, s0 scope:SCOPE_SYS
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
entry:
  %out_i32_ptr = bitcast <8 x i16> addrspace(10)* @out to i32 addrspace(10)*
  store volatile i32 1, i32 addrspace(10)* %out_i32_ptr, align 4
  ret void
}

define void @dummy_rank1a() #5 {
; CHECK-LABEL: dummy_rank1a:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_mov_b32 s0, 0
; CHECK-NEXT:    v_mov_b32_e32 v0, 1
; CHECK-NEXT:    s_wait_loadcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    scratch_store_b32 off, v0, s0 scope:SCOPE_SYS
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
entry:
  %out_i32_ptr = bitcast <8 x i16> addrspace(10)* @out to i32 addrspace(10)*
  store volatile i32 1, i32 addrspace(10)* %out_i32_ptr, align 4
  ret void
}

define void @dummy_rank1b() #5 {
; CHECK-LABEL: dummy_rank1b:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_mov_b32 s0, 0
; CHECK-NEXT:    v_mov_b32_e32 v0, 1
; CHECK-NEXT:    s_wait_loadcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    scratch_store_b32 off, v0, s0 scope:SCOPE_SYS
; CHECK-NEXT:    s_wait_storecnt 0x0
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
entry:
  %out_i32_ptr = bitcast <8 x i16> addrspace(10)* @out to i32 addrspace(10)*
  store volatile i32 1, i32 addrspace(10)* %out_i32_ptr, align 4
  ret void
}

declare i32 @llvm.amdgcn.wave.id.in.wavegroup() #2

define internal void @test_kernel_1.rank_0_2_3_4_5_6_7() #3 {
; CHECK-LABEL: test_kernel_1.rank_0_2_3_4_5_6_7:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    v_writelane_b32 v127, s30, 0
; CHECK-NEXT:    v_writelane_b32 v127, s31, 1
; CHECK-NEXT:    s_get_pc_i64 s[0:1]
; CHECK-NEXT:    s_add_nc_u64 s[0:1], s[0:1], dummy_store@gotpcrel+4
; CHECK-NEXT:    s_load_b64 s[2:3], s[0:1], 0x0
; CHECK-NEXT:    s_mov_b32 s1, 0
; CHECK-NEXT:    s_branch .LBB3_2
; CHECK-NEXT:  .LBB3_1: ; %Flow
; CHECK-NEXT:    ; in Loop: Header=BB3_2 Depth=1
; CHECK-NEXT:    s_and_not1_b32 vcc_lo, exec_lo, s0
; CHECK-NEXT:    s_cbranch_vccz .LBB3_4
; CHECK-NEXT:  .LBB3_2: ; %for.cond
; CHECK-NEXT:    ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_swap_pc_i64 s[30:31], s[2:3]
; CHECK-NEXT:    s_cmp_gt_u32 s1, 1
; CHECK-NEXT:    s_mov_b32 s0, -1
; CHECK-NEXT:    s_cbranch_scc1 .LBB3_1
; CHECK-NEXT:  ; %bb.3: ; %if.end
; CHECK-NEXT:    ; in Loop: Header=BB3_2 Depth=1
; CHECK-NEXT:    s_add_co_i32 s1, s1, 1
; CHECK-NEXT:    s_mov_b32 s0, 0
; CHECK-NEXT:    s_branch .LBB3_1
; CHECK-NEXT:  .LBB3_4: ; %if.end7
; CHECK-NEXT:    v_readlane_b32 s30, v127, 0
; CHECK-NEXT:    v_readlane_b32 s31, v127, 1
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
entry:
  br label %for.cond

for.cond:                                         ; preds = %if.end, %entry
  %i.0 = phi i32 [ 0, %entry ], [ %inc, %if.end ]
  call void @dummy_store()
  %cmp = icmp samesign ult i32 %i.0, 2
  br i1 %cmp, label %if.end, label %if.end7

if.end:                                           ; preds = %for.cond
  %inc = add nuw nsw i32 %i.0, 1
  br label %for.cond

if.end7:                                          ; preds = %for.cond
  ret void
}

define internal void @test_kernel_1.rank_1() #3 {
; CHECK-LABEL: test_kernel_1.rank_1:
; CHECK:       ; %bb.0: ; %entry
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    v_writelane_b32 v127, s30, 0
; CHECK-NEXT:    v_writelane_b32 v127, s31, 1
; CHECK-NEXT:    s_get_pc_i64 s[0:1]
; CHECK-NEXT:    s_add_nc_u64 s[0:1], s[0:1], dummy_store@gotpcrel+4
; CHECK-NEXT:    s_get_pc_i64 s[16:17]
; CHECK-NEXT:    s_add_nc_u64 s[16:17], s[16:17], dummy_rank1a@gotpcrel+4
; CHECK-NEXT:    s_load_b64 s[2:3], s[0:1], 0x0
; CHECK-NEXT:    s_load_b64 s[16:17], s[16:17], 0x0
; CHECK-NEXT:    s_mov_b32 s1, 0
; CHECK-NEXT:    s_branch .LBB4_2
; CHECK-NEXT:  .LBB4_1: ; %Flow
; CHECK-NEXT:    ; in Loop: Header=BB4_2 Depth=1
; CHECK-NEXT:    s_and_not1_b32 vcc_lo, exec_lo, s0
; CHECK-NEXT:    s_cbranch_vccz .LBB4_4
; CHECK-NEXT:  .LBB4_2: ; %for.cond
; CHECK-NEXT:    ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_swap_pc_i64 s[30:31], s[2:3]
; CHECK-NEXT:    s_cmp_gt_u32 s1, 1
; CHECK-NEXT:    s_mov_b32 s0, -1
; CHECK-NEXT:    s_cbranch_scc1 .LBB4_1
; CHECK-NEXT:  ; %bb.3: ; %if.then
; CHECK-NEXT:    ; in Loop: Header=BB4_2 Depth=1
; CHECK-NEXT:    s_swap_pc_i64 s[30:31], s[16:17]
; CHECK-NEXT:    s_add_co_i32 s1, s1, 1
; CHECK-NEXT:    s_mov_b32 s0, 0
; CHECK-NEXT:    s_branch .LBB4_1
; CHECK-NEXT:  .LBB4_4: ; %if.then5
; CHECK-NEXT:    s_get_pc_i64 s[0:1]
; CHECK-NEXT:    s_add_nc_u64 s[0:1], s[0:1], dummy_rank1b@gotpcrel+4
; CHECK-NEXT:    s_load_b64 s[0:1], s[0:1], 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_swap_pc_i64 s[30:31], s[0:1]
; CHECK-NEXT:    v_readlane_b32 s30, v127, 0
; CHECK-NEXT:    v_readlane_b32 s31, v127, 1
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
entry:
  br label %for.cond

for.cond:                                         ; preds = %if.then, %entry
  %i.0 = phi i32 [ 0, %entry ], [ %inc, %if.then ]
  call void @dummy_store()
  %cmp = icmp samesign ult i32 %i.0, 2
  br i1 %cmp, label %if.then, label %if.then5

if.then:                                          ; preds = %for.cond
  call void @dummy_rank1a()
  %inc = add nuw nsw i32 %i.0, 1
  br label %for.cond

if.then5:                                         ; preds = %for.cond
  call void @dummy_rank1b()
  ret void
}

define dso_local amdgpu_kernel void @test_kernel_1() local_unnamed_addr #1 !reqd_work_group_size !{i32 32, i32 12, i32 1} {
; CHECK-LABEL: test_kernel_1:
; CHECK:       test_kernel_1$local:
; CHECK-NEXT:    .type test_kernel_1$local,@function
; CHECK-NEXT:  ; %bb.0: ; %entry
; CHECK-NEXT:    s_getreg_b32 s9, hwreg(HW_REG_WAVE_GROUP_INFO, 16, 4)
; CHECK-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; CHECK-NEXT:    s_mul_i32 s33, s9, s8
; CHECK-NEXT:    s_mul_i32 s10, s9, 0
; CHECK-NEXT:    s_add_co_u32 s33, s33, 16
; CHECK-NEXT:    s_set_gpr_idx_u32 idx0, s10
; CHECK-NEXT:    s_add_co_u32 s32, s33, 0
; CHECK-NEXT:    ; sched_barrier mask(0x00000000)
; CHECK-NEXT:    s_getreg_b32 s9, hwreg(HW_REG_WAVE_GROUP_INFO, 16, 4)
; CHECK-NEXT:    s_mov_b32 s10, -1
; CHECK-NEXT:    s_cmp_lg_u32 s9, 1
; CHECK-NEXT:    s_cbranch_scc1 .LBB5_3
; CHECK-NEXT:  ; %bb.1: ; %Flow
; CHECK-NEXT:    s_and_not1_b32 vcc_lo, exec_lo, s10
; CHECK-NEXT:    s_cbranch_vccz .LBB5_18
; CHECK-NEXT:  .LBB5_2: ; %common.ret
; CHECK-NEXT:    s_endpgm
; CHECK-NEXT:  .LBB5_3: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_set_gpr_idx_u32 idx0, 0
; CHECK-NEXT:    s_cmp_eq_u32 s9, 0
; CHECK-NEXT:    s_get_pc_i64 s[10:11]
; CHECK-NEXT:    s_add_nc_u64 s[10:11], s[10:11], test_kernel_1.rank_0_2_3_4_5_6_7@rel64+4
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_5
; CHECK-NEXT:  ; %bb.4:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_5: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 2
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_7
; CHECK-NEXT:  ; %bb.6:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_7: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 3
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_9
; CHECK-NEXT:  ; %bb.8:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_9: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 4
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_11
; CHECK-NEXT:  ; %bb.10:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_11: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 5
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_13
; CHECK-NEXT:  ; %bb.12:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_13: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 6
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_15
; CHECK-NEXT:  ; %bb.14:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_15: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cmp_eq_u32 s9, 7
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_17
; CHECK-NEXT:  ; %bb.16:
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_17: ; %bb.rank_0_2_3_4_5_6_7
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr)
; CHECK-NEXT:    s_cbranch_execnz .LBB5_2
; CHECK-NEXT:  .LBB5_18: ; %bb.rank_1
; CHECK-NEXT:    s_set_gpr_idx_u32 idx0, 0
; CHECK-NEXT:    s_cmp_eq_u32 s9, 1
; CHECK-NEXT:    s_cbranch_scc0 .LBB5_20
; CHECK-NEXT:  ; %bb.19:
; CHECK-NEXT:    s_get_pc_i64 s[10:11]
; CHECK-NEXT:    s_add_nc_u64 s[10:11], s[10:11], test_kernel_1.rank_1@rel64+4
; CHECK-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; CHECK-NEXT:    s_set_pc_i64 s[10:11]
; CHECK-NEXT:  .LBB5_20: ; %bb.rank_1
; CHECK-NEXT:    s_add_gpr_idx_u32 idx0, max(128, dummy_store.num_vgpr, dummy_rank1a.num_vgpr, dummy_rank1b.num_vgpr)
; CHECK-NEXT:    s_endpgm
entry:
  %0 = call i32 @llvm.amdgcn.wave.id.in.wavegroup()
  %cond = icmp eq i32 %0, 1
  br i1 %cond, label %bb.rank_1, label %bb.rank_0_2_3_4_5_6_7

common.ret:                                       ; preds = %bb.rank_1, %bb.rank_0_2_3_4_5_6_7
  ret void

bb.rank_0_2_3_4_5_6_7:                            ; preds = %entry
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 0, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 2, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 3, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 4, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 5, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 6, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 7, ptr @test_kernel_1.rank_0_2_3_4_5_6_7)
  br label %common.ret

bb.rank_1:                                        ; preds = %entry
  call void @llvm.amdgcn.wavegroup.rank.p0(i32 1, ptr @test_kernel_1.rank_1)
  br label %common.ret
}

; KERNEL:	.set test_kernel_1.private_seg_size, max(0, 0+max(.Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_0_2_3_4_5_6_7.private_seg_size, .Ltest_kernel_1.rank_1.private_seg_size))
; KERNEL:	.set test_kernel_1.num_vgpr_rank_sum, 0+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_0_2_3_4_5_6_7.num_vgpr+.Ltest_kernel_1.rank_1.num_vgpr

declare !callback !0 void @llvm.amdgcn.wavegroup.rank.p0(i32 immarg, ptr)
!1 = !{i64 1, i1 false}
!0 = !{!1}

attributes #1 = { "amdgpu-wavegroup-enable" }
attributes #3 = { "amdgpu-wavegroup-enable" "amdgpu-wavegroup-rank-function" }
attributes #5 = { noinline optnone nounwind }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; KERNEL: {{.*}}
