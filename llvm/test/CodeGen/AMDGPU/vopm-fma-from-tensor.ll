; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_fma_from_tensor_f32_i4_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_i4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.i4.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i4_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i4_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i4_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i4_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i4_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i4 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i4_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i4_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i4_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i4_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i4_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i4 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_u4_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_u4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.u4.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u4_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u4_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u4_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u4_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u4_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u4 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u4.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u4_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u4_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u4_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u4_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u4_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u4_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u4 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u4.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_i8_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_i8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.i8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_i8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.i8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_i8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_u8_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_u8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.u8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_u8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.u8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_u8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_fp8_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_fp8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.fp8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_fp8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_fp8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.fp8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_fp8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_fp8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.fp8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_fp8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_fp8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_fp8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.fp8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_fp8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_fp8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.fp8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_fp8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_fp8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.fp8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_fp8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_fp8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_fp8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.fp8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_bf8_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_bf8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.bf8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_bf8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_bf8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.bf8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_bf8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_bf8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.bf8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_bf8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, i32 %resid_0, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_bf8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f16_bf8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.bf8.v4f16(<4 x half> %acc_in, i32 %resid_0, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf8_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf8_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf8_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf8_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf8_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf8_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf8 v[2:3], v[2:3], v4, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf8.v4bf16(<4 x bfloat> %acc_in, i32 %resid_0, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_f16_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, <2 x half> %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_f16_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_f16 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.f16.v2f32(<2 x float> %acc_in, <2 x half> %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_f16_dequant_disable_4x2(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_f16_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_f16 v[2:3], v[2:3], v4, v5, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.f16.v4f16(<4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_f16_dequant_disable_4x4(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_f16_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_f16 v[2:3], v[2:3], v4, v5, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.f16.v4f16(<4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_f16_dequant_disable_8x4(ptr addrspace(1) %out, <4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_f16_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_f16 v[2:3], v[2:3], v4, v5, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x half> @llvm.amdgcn.fma.from.tensor.f16.f16.v4f16(<4 x half> %acc_in, <2 x half> %resid_0, <2 x half> %resid_1, <4 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_bf16_dequant_disable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, <2 x bfloat> %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_bf16_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_bf16 v[2:3], v[2:3], v4, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.bf16.v2f32(<2 x float> %acc_in, <2 x bfloat> %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf16_dequant_disable_4x2(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf16_dequant_disable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf16 v[2:3], v[2:3], v4, v5, v[6:7] aux_data:2 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf16.v4bf16(<4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X2 == (2 << 0)
              i32 2,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf16_dequant_disable_4x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf16_dequant_disable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf16 v[2:3], v[2:3], v4, v5, v[6:7] aux_data:1 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf16.v4bf16(<4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_4X4 == (1 << 0)
              i32 1,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf16_dequant_disable_8x4(ptr addrspace(1) %out, <4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf16_dequant_disable_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf16 v[2:3], v[2:3], v4, v5, v[6:7] clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <4 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf16.v4bf16(<4 x bfloat> %acc_in, <2 x bfloat> %resid_0, <2 x bfloat> %resid_1, <4 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_CONV_8X4 == (0 << 0)
              i32 0,
              ;   CLAMP
              i1 1)
  store <4 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_i4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_i4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_i4 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.i4.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_i4 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.i4.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i4_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i4_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_i4 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.i4.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_i4 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i4.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i4_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i4_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_i4 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i4.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_u4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_u4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_u4 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.u4.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_u4 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.u4.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u4_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u4_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_u4 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.u4.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u4_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u4_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_u4 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u4.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u4_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u4_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_u4 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u4.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_i8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_i8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_i8 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.i8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_i8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.i8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_i8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_i8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_i8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.i8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_i8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_i8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_i8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_i8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.i8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_u8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_u8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_u8 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.u8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_u8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.u8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_u8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_u8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_u8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.u8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_u8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_u8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_u8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_u8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.u8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_fp8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_fp8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_fp8 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.fp8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_fp8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_fp8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_fp8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.fp8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_fp8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_fp8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_fp8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.fp8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_fp8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_fp8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_fp8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.fp8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_fp8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_fp8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_fp8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.fp8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_bf8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, i32 %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_bf8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_bf8 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.bf8.v2f32(<2 x float> %acc_in, i32 %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_bf8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_bf8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_bf8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.bf8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f16_bf8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x half> %acc_in, i32 %resid_0, <2 x half> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f16_bf8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_f16_bf8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x half> @llvm.amdgcn.fma.from.tensor.f16.bf8.v2f16(<2 x half> %acc_in, i32 %resid_0, <2 x half> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf8_dequant_enable_4x2(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf8_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf8 v2, v2, v3, v4 aux_data:4 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_bf16_bf8_dequant_enable_4x4(ptr addrspace(1) %out, <2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale) {
; GFX13-LABEL: test_fma_from_tensor_bf16_bf8_dequant_enable_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_fma_from_tensor_bf16_bf8 v2, v2, v3, v4 aux_data:3 clamp
; GFX13-NEXT:    global_store_b32 v[0:1], v2, off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x bfloat> @llvm.amdgcn.fma.from.tensor.bf16.bf8.v2bf16(<2 x bfloat> %acc_in, i32 %resid_0, <2 x bfloat> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X4 == (3 << 0)
              i32 3,
              ;   CLAMP
              i1 1)
  store <2 x bfloat> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_f16_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, <2 x half> %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_f16_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_f16 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.f16.v2f32(<2 x float> %acc_in, <2 x half> %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_fma_from_tensor_f32_bf16_dequant_enable_4x2(ptr addrspace(1) %out, <2 x float> %acc_in, <2 x bfloat> %resid_0, <2 x float> %scale) {
; GFX13-LABEL: test_fma_from_tensor_f32_bf16_dequant_enable_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v7, v6 :: v_dual_mov_b32 v6, v5
; GFX13-NEXT:    v_fma_from_tensor_f32_bf16 v[2:3], v[2:3], v4, v[6:7] aux_data:4 clamp
; GFX13-NEXT:    global_store_b64 v[0:1], v[2:3], off
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <2 x float> @llvm.amdgcn.fma.from.tensor.f32.bf16.v2f32(<2 x float> %acc_in, <2 x bfloat> %resid_0, <2 x float> %scale,
              ;   AUX_DATA: PIXEL_SHAPE_DEQUANT_4X2 == (4 << 0)
              i32 4,
              ;   CLAMP
              i1 1)
  store <2 x float> %dst, ptr addrspace(1) %out
  ret void
}

