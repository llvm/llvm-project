; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=30 < %s 2>&1 | FileCheck %s

@array2 = global [5 x i32] zeroinitializer, align 4
@array3 = global [5 x i32] zeroinitializer, align 4
@array4 = global [5 x i32] zeroinitializer, align 4
@array5 = global [5 x i32] zeroinitializer, align 4

@array6 = global [5 x i32] zeroinitializer, align 4
@array7 = global [5 x i32] zeroinitializer, align 4
@array8 = global [5 x i32] zeroinitializer, align 4
@array9 = global [5 x i32] zeroinitializer, align 4

;                bb.0.entry
;                  /    |
;            bb.1.bb1   |
;                  \    |
;                  bb.2.bb2
;                  |    \
;                  |   bb.3.bb4.preheader
;                  |           |
;                  |         bb.19<-+
;                  |           +----+
;                  |           |
;                  |     bb.20.bb14.loopexit
;                  |      /
;                bb.18.Flow17
;                 /        |
;              bb.4.bb3    |
;              /    |      |
;        bb.10.bb7  |      |
;              \    |      |
;           bb.5.Flow16    |
;              /    |      |
;        bb.6.bb6   |      |
;           /  |    |      |
;    bb.9.bb9  |    |      |
;           \  |    |      |
;     bb.7.Flow14   |      |
;           /  |    |      |
;     bb.8.bb8 |    |      |
;           \  |    |      |
;     bb.11.Flow15  |      |
;              \    |      |
;            bb.13.bb10    |
;              /    |      |
;     bb.16.bb12    |      |
;              \    |      |
;            bb.14.Flow    |
;              /    |      |
;      bb.15.bb11   |      |
;              \    |      |
;            bb.17.bb13    |
;                    \     |
;                  bb.12.Flow18
;                       |
;                  bb.21.bb14
;
define amdgpu_ps void @test15(ptr addrspace(1) %p1, ptr addrspace(3) %p2, i1 %cond1, i1 %cond2, ptr addrspace(1) %p3, ptr addrspace(1) %p4, ptr addrspace(1) %p5, ptr addrspace(1) %p6, ptr addrspace(1) %p7, ptr addrspace(1) %p8, ptr addrspace(1) %p9, i32 %TC1) {
  ; CHECK-LABEL: name: test15
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13, $vgpr14, $vgpr15, $vgpr16, $vgpr17, $vgpr18, $vgpr19
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr19
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr18
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr17
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr16
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr15
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr14
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr13
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr12
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr11
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY $vgpr10
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY $vgpr9
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY $vgpr8
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY $vgpr7
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY $vgpr6
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY $vgpr5
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY14]], %subreg.sub0, [[COPY13]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY19]], %subreg.sub0, [[COPY18]], %subreg.sub1
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY16]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_EQ_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_AND_B32_e64_1:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY15]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_EQ_U32_e64_1:%[0-9]+]]:sreg_32 = V_CMP_EQ_U32_e64 1, [[V_AND_B32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32 = S_XOR_B32 [[V_CMP_EQ_U32_e64_1]], -1, implicit-def dead $scc
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE1]], 8, [[GLOBAL_LOAD_UBYTE]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE3:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE3]], 8, [[GLOBAL_LOAD_UBYTE2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_1]], 16, [[V_LSHL_OR_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE4:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 12, 0, implicit $exec :: (load (s8) from %ir.gep1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE5:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 13, 0, implicit $exec :: (load (s8) from %ir.gep1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE6:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 14, 0, implicit $exec :: (load (s8) from %ir.gep1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE7:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 15, 0, implicit $exec :: (load (s8) from %ir.gep1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX4_:%[0-9]+]]:vreg_128 = GLOBAL_LOAD_DWORDX4 [[REG_SEQUENCE]], 16, 0, implicit $exec :: (load (s128) from %ir.p3 + 16, align 4, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY4]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX4_1:%[0-9]+]]:vreg_128 = GLOBAL_LOAD_DWORDX4 [[REG_SEQUENCE]], 0, 0, implicit $exec :: (load (s128) from %ir.p3, align 4, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY3]], %stack.3, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[GLOBAL_LOAD_DWORDX4_1]].sub0, [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE]], [[V_ADD_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p3, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_]], [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_EQ_U32_e64_]], %bb.2, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.bb1:
  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 100, implicit $exec
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb2:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.18(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_1]], %bb.0, [[V_MOV_B32_e32_]], %bb.1
  ; CHECK-NEXT:   SI_END_CF [[SI_IF]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32 = SI_IF [[S_XOR_B32_]], %bb.18, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.bb4.preheader:
  ; CHECK-NEXT:   successors: %bb.19(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array6, target-flags(amdgpu-gotprel32-hi) @array6, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY20:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY20]], 28, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array6, i64 28)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET1:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array8, target-flags(amdgpu-gotprel32-hi) @array8, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM1:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET1]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY21:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM1]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY21]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array8, i64 20)`)
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 -1
  ; CHECK-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   S_BRANCH %bb.19
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.bb3:
  ; CHECK-NEXT:   successors: %bb.10(0x40000000), %bb.5(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 %523, 8, %521, implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 %527, 8, %525, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE %513, %subreg.sub0, %515, %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE %505, %subreg.sub0, %507, %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_4]], 16, [[V_LSHL_OR_B32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 %503, 0, 0, implicit $exec :: (load (s8) from %ir.p2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 %503, 1, 0, implicit $exec :: (load (s8) from %ir.p2 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 %503, 2, 0, implicit $exec :: (load (s8) from %ir.p2 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 %503, 3, 0, implicit $exec :: (load (s8) from %ir.p2 + 3, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_B32_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_B32_gfx9 %503, 12, 0, implicit $exec :: (load (s32) from %ir.gep2, align 8, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_1]], 8, [[DS_READ_U8_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_3]], 8, [[DS_READ_U8_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_8:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_7]], 16, [[V_LSHL_OR_B32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.13, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.13, addrspace 5)
  ; CHECK-NEXT:   %231:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[SI_SPILL_V32_RESTORE]], [[PHI]], 1900, 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[DS_READ_B32_gfx9_]], %231.sub0, 0, implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE8:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 20, 0, implicit $exec :: (load (s8) from %ir.p4 + 20, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE9:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 21, 0, implicit $exec :: (load (s8) from %ir.p4 + 21, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_9:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE9]], 8, [[GLOBAL_LOAD_UBYTE8]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE10:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 22, 0, implicit $exec :: (load (s8) from %ir.p4 + 22, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE11:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 23, 0, implicit $exec :: (load (s8) from %ir.p4 + 23, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_10:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE11]], 8, [[GLOBAL_LOAD_UBYTE10]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE12:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 12, 0, implicit $exec :: (load (s8) from %ir.p4 + 12, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE13:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 13, 0, implicit $exec :: (load (s8) from %ir.p4 + 13, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_11:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE13]], 8, [[GLOBAL_LOAD_UBYTE12]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE14:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 14, 0, implicit $exec :: (load (s8) from %ir.p4 + 14, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE15:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 15, 0, implicit $exec :: (load (s8) from %ir.p4 + 15, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_12:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE15]], 8, [[GLOBAL_LOAD_UBYTE14]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE16:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 8, 0, implicit $exec :: (load (s8) from %ir.p4 + 8, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE17:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 9, 0, implicit $exec :: (load (s8) from %ir.p4 + 9, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_13:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE17]], 8, [[GLOBAL_LOAD_UBYTE16]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE18:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 10, 0, implicit $exec :: (load (s8) from %ir.p4 + 10, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE19:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 11, 0, implicit $exec :: (load (s8) from %ir.p4 + 11, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_14:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE19]], 8, [[GLOBAL_LOAD_UBYTE18]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE20:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 4, 0, implicit $exec :: (load (s8) from %ir.p4 + 4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE21:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 5, 0, implicit $exec :: (load (s8) from %ir.p4 + 5, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_15:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE21]], 8, [[GLOBAL_LOAD_UBYTE20]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE22:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 6, 0, implicit $exec :: (load (s8) from %ir.p4 + 6, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE23:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 7, 0, implicit $exec :: (load (s8) from %ir.p4 + 7, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_16:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE23]], 8, [[GLOBAL_LOAD_UBYTE22]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE3]], 0, 0, implicit $exec :: (load (s16) from %ir.p4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE3]], 2, 0, implicit $exec :: (load (s16) from %ir.p4 + 2, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_17:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_USHORT1]], 16, [[GLOBAL_LOAD_USHORT]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.12, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.12, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_LSHL_OR_B32_e64_17]], [[SI_SPILL_V32_RESTORE1]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE3]], [[V_ADD_U32_e64_3]], 0, 0, implicit $exec :: (store (s32) into %ir.p4, addrspace 1)
  ; CHECK-NEXT:   ADJCALLSTACKUP 0, 0, implicit-def dead $scc, implicit-def $sgpr32, implicit $sgpr32
  ; CHECK-NEXT:   [[COPY22:%[0-9]+]]:sreg_32_xexec_hi = COPY $sgpr32
  ; CHECK-NEXT:   [[S_ADD_I32_:%[0-9]+]]:sreg_32 = S_ADD_I32 [[COPY22]], 1024, implicit-def dead $scc
  ; CHECK-NEXT:   $sgpr32 = COPY [[S_ADD_I32_]]
  ; CHECK-NEXT:   ADJCALLSTACKDOWN 0, 0, implicit-def dead $scc, implicit-def $sgpr32, implicit $sgpr32
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX2_:%[0-9]+]]:vreg_64 = GLOBAL_LOAD_DWORDX2 [[REG_SEQUENCE2]], 0, 0, implicit $exec :: (load (s64) from %ir.p6, align 4, addrspace 1)
  ; CHECK-NEXT:   [[COPY23:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX2_]].sub0
  ; CHECK-NEXT:   [[COPY24:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX2_]].sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e64_:%[0-9]+]]:vgpr_32 = nsw V_LSHLREV_B32_e64 2, [[GLOBAL_LOAD_DWORDX2_]].sub0, implicit $exec
  ; CHECK-NEXT:   SCRATCH_STORE_DWORD_SVS [[PHI]], [[V_LSHLREV_B32_e64_]], [[COPY22]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s32) into %ir.arrayidx11, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e64_1:%[0-9]+]]:vgpr_32 = nsw V_LSHLREV_B32_e64 2, [[GLOBAL_LOAD_DWORDX2_]].sub1, implicit $exec
  ; CHECK-NEXT:   SCRATCH_STORE_SHORT_SVS [[V_LSHL_OR_B32_e64_7]], [[V_LSHLREV_B32_e64_1]], [[COPY22]], 2, 0, implicit $exec, implicit $flat_scr :: (store (s16) into %ir.arrayidx33 + 2, addrspace 5)
  ; CHECK-NEXT:   SCRATCH_STORE_SHORT_SVS [[V_LSHL_OR_B32_e64_6]], [[V_LSHLREV_B32_e64_1]], [[COPY22]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s16) into %ir.arrayidx33, addrspace 5)
  ; CHECK-NEXT:   [[S_XOR_B32_1:%[0-9]+]]:sreg_32 = S_XOR_B32 [[V_CMP_EQ_U32_e64_]], [[V_CMP_EQ_U32_e64_1]], implicit-def dead $scc
  ; CHECK-NEXT:   [[S_XOR_B32_2:%[0-9]+]]:sreg_32 = S_XOR_B32 [[S_XOR_B32_1]], -1, implicit-def dead $scc
  ; CHECK-NEXT:   [[SI_IF2:%[0-9]+]]:sreg_32 = SI_IF [[S_XOR_B32_2]], %bb.5, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.10
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.Flow18:
  ; CHECK-NEXT:   successors: %bb.6(0x40000000), %bb.13(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI undef %222:vgpr_32, %bb.4, %27, %bb.10
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_5]], %bb.4, undef %536:vgpr_32, %bb.10
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_2]], %bb.4, undef %538:vgpr_32, %bb.10
  ; CHECK-NEXT:   [[SI_SPILL_V128_RESTORE:%[0-9]+]]:vreg_128 = SI_SPILL_V128_RESTORE %stack.10, $sgpr32, 0, implicit $exec :: (load (s128) from %stack.10, align 4, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V128_RESTORE1:%[0-9]+]]:vreg_128 = SI_SPILL_V128_RESTORE %stack.11, $sgpr32, 0, implicit $exec :: (load (s128) from %stack.11, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[DS_READ_B32_gfx9_]], %stack.16, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.16, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE3]], %stack.17, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.17, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE %231, %stack.18, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.18, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE2]], %stack.20, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.20, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_17]], %stack.21, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.21, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY23]], %stack.22, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.22, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY24]], %stack.23, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.23, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_256 = REG_SEQUENCE undef [[SI_SPILL_V128_RESTORE1]].sub0, %subreg.sub0, undef [[SI_SPILL_V128_RESTORE1]].sub1, %subreg.sub1, undef [[SI_SPILL_V128_RESTORE1]].sub2, %subreg.sub2, [[SI_SPILL_V128_RESTORE1]].sub3, %subreg.sub3, undef [[SI_SPILL_V128_RESTORE]].sub0, %subreg.sub4, [[SI_SPILL_V128_RESTORE]].sub1, %subreg.sub5, undef [[SI_SPILL_V128_RESTORE]].sub2, %subreg.sub6, undef [[SI_SPILL_V128_RESTORE]].sub3, %subreg.sub7
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_18:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_10]], 16, [[V_LSHL_OR_B32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_19:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_12]], 16, [[V_LSHL_OR_B32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_20:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_14]], 16, [[V_LSHL_OR_B32_e64_13]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_21:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_16]], 16, [[V_LSHL_OR_B32_e64_15]], implicit $exec
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF2]], %bb.13, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.6
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.bb6:
  ; CHECK-NEXT:   successors: %bb.9(0x40000000), %bb.7(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 [[V_CMP_EQ_U32_e64_]], [[V_CMP_EQ_U32_e64_1]], implicit-def dead $scc
  ; CHECK-NEXT:   [[S_XOR_B32_3:%[0-9]+]]:sreg_32 = S_XOR_B32 [[S_AND_B32_]], -1, implicit-def dead $scc
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET2:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM2:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET2]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY25:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM2]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD2:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY25]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 20)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET3:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM3:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET3]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY26:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM3]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY26]], [[FLAT_LOAD_DWORD2]], 4, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 4)`)
  ; CHECK-NEXT:   [[SI_IF3:%[0-9]+]]:sreg_32 = SI_IF [[S_XOR_B32_3]], %bb.7, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.9
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.7.Flow16:
  ; CHECK-NEXT:   successors: %bb.8(0x40000000), %bb.11(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI undef %329:vgpr_32, %bb.6, %26, %bb.9
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI [[PHI2]], %bb.6, undef %540:vgpr_32, %bb.9
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI [[PHI3]], %bb.6, undef %542:vgpr_32, %bb.9
  ; CHECK-NEXT:   [[SI_ELSE1:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF3]], %bb.11, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.8
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.8.bb8:
  ; CHECK-NEXT:   successors: %bb.11(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI5]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET4:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM4:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET4]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY27:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM4]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD3:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY27]], 28, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 28)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET5:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM5:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET5]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY28:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM5]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY28]], [[FLAT_LOAD_DWORD3]], 68, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 68)`)
  ; CHECK-NEXT:   S_BRANCH %bb.11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.9.bb9:
  ; CHECK-NEXT:   successors: %bb.7(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI2]], [[PHI3]], implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET6:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array5, target-flags(amdgpu-gotprel32-hi) @array5, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM6:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET6]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY29:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM6]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD4:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY29]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array5, i64 20)`)
  ; CHECK-NEXT:   [[COPY30:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM3]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY30]], [[FLAT_LOAD_DWORD4]], 60, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 60)`)
  ; CHECK-NEXT:   S_BRANCH %bb.7
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.10.bb7:
  ; CHECK-NEXT:   successors: %bb.5(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_SUB_U32_e64_:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[DS_READ_B32_gfx9_]], [[V_ADD_U32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_]], [[V_LSHL_OR_B32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_CVT_F32_U32_e64_:%[0-9]+]]:vgpr_32 = V_CVT_F32_U32_e64 [[SI_SPILL_V32_RESTORE]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_RCP_IFLAG_F32_e64_:%[0-9]+]]:vgpr_32 = nofpexcept V_RCP_IFLAG_F32_e64 0, [[V_CVT_F32_U32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_F32_e64_:%[0-9]+]]:vgpr_32 = nnan ninf nsz arcp contract afn reassoc nofpexcept V_MUL_F32_e64 0, 1333788670, 0, [[V_RCP_IFLAG_F32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_U32_F32_e64_:%[0-9]+]]:vgpr_32 = nofpexcept V_CVT_U32_F32_e64 0, [[V_MUL_F32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.13, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.13, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_1:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 0, [[SI_SPILL_V32_RESTORE2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_1]], [[V_CVT_U32_F32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_CVT_U32_F32_e64_]], [[V_MUL_LO_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CVT_U32_F32_e64_]], [[V_MUL_HI_U32_e64_]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_MUL_LO_U32_e64_1]], [[V_ADD_U32_e64_5]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_HI_U32_e64_1]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_2:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_1]], [[V_MUL_LO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_SUB_U32_e64_2]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_6:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_MUL_HI_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MUL_HI_U32_e64_1]], 0, [[V_ADD_U32_e64_6]], [[V_CMP_GE_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_3:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_2]], [[SI_SPILL_V32_RESTORE2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_SUB_U32_e64_2]], 0, [[V_SUB_U32_e64_3]], [[V_CMP_GE_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_CNDMASK_B32_e64_1]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_7:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_CNDMASK_B32_e64_]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_CNDMASK_B32_e64_]], 0, [[V_ADD_U32_e64_7]], [[V_CMP_GE_U32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_8:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CNDMASK_B32_e64_2]], [[V_LSHL_OR_B32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET7:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM7:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET7]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY31:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM7]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD5:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY31]], 84, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 84)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET8:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM8:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET8]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY32:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM8]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY32]], [[FLAT_LOAD_DWORD5]], 60, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 60)`)
  ; CHECK-NEXT:   S_BRANCH %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.11.Flow17:
  ; CHECK-NEXT:   successors: %bb.13(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vgpr_32 = PHI [[PHI4]], %bb.7, [[V_ADD_U32_e64_4]], %bb.8
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE1]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.13
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.12.Flow20:
  ; CHECK-NEXT:   successors: %bb.21(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI %69, %bb.18, %65, %bb.17
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:vgpr_32 = PHI %70, %bb.18, %65, %bb.17
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:vgpr_32 = PHI %71, %bb.18, %68, %bb.17
  ; CHECK-NEXT:   [[PHI11:%[0-9]+]]:vgpr_32 = PHI %72, %bb.18, %67, %bb.17
  ; CHECK-NEXT:   [[PHI12:%[0-9]+]]:vgpr_32 = PHI %73, %bb.18, %64, %bb.17
  ; CHECK-NEXT:   [[PHI13:%[0-9]+]]:vgpr_32 = PHI %74, %bb.18, %66, %bb.17
  ; CHECK-NEXT:   [[PHI14:%[0-9]+]]:vgpr_32 = PHI %75, %bb.18, %65, %bb.17
  ; CHECK-NEXT:   [[PHI15:%[0-9]+]]:vgpr_32 = PHI %76, %bb.18, %63, %bb.17
  ; CHECK-NEXT:   SI_END_CF %77, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE3:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.14, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.14, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE4:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.15, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.15, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE3]], %subreg.sub0, [[SI_SPILL_V32_RESTORE4]], %subreg.sub1
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE5:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE6:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE6:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE5]], %subreg.sub0, [[SI_SPILL_V32_RESTORE6]], %subreg.sub1
  ; CHECK-NEXT:   S_BRANCH %bb.21
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.13.bb10:
  ; CHECK-NEXT:   successors: %bb.16(0x40000000), %bb.14(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI16:%[0-9]+]]:vgpr_32 = PHI [[PHI1]], %bb.5, [[PHI7]], %bb.11
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_9:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 100, [[V_LSHL_OR_B32_e64_8]], 0, implicit $exec
  ; CHECK-NEXT:   [[COPY33:%[0-9]+]]:vgpr_32 = COPY [[SI_SPILL_V128_RESTORE1]].sub1
  ; CHECK-NEXT:   [[COPY34:%[0-9]+]]:vgpr_32 = COPY [[SI_SPILL_V128_RESTORE1]].sub2
  ; CHECK-NEXT:   [[COPY35:%[0-9]+]]:vgpr_32 = COPY [[SI_SPILL_V128_RESTORE]].sub2
  ; CHECK-NEXT:   [[COPY36:%[0-9]+]]:vgpr_32 = COPY [[SI_SPILL_V128_RESTORE]].sub3
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[SI_SPILL_V128_RESTORE1]].sub1, [[SI_SPILL_V128_RESTORE]].sub3, [[SI_SPILL_V128_RESTORE]].sub2, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE7:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V128_RESTORE1]].sub2, %subreg.sub0, [[V_MOV_B32_e32_1]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_pseudo_e64_:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 2, [[REG_SEQUENCE7]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.9, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.9, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[SI_SPILL_V64_RESTORE]].sub0, [[V_LSHLREV_B64_pseudo_e64_]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   %466:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[SI_SPILL_V64_RESTORE]].sub1, [[V_LSHLREV_B64_pseudo_e64_]].sub1, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE8:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, %466, %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE8]], [[V_ADD3_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx1, addrspace 1)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE7:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.7, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.7, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE8:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.8, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.8, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE1:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.18, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.18, align 4, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF4:%[0-9]+]]:sreg_32 = SI_IF [[S_XOR_B32_2]], %bb.14, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.16
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.14.Flow:
  ; CHECK-NEXT:   successors: %bb.15(0x40000000), %bb.17(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI17:%[0-9]+]]:vgpr_32 = PHI undef %356:vgpr_32, %bb.13, %51, %bb.16
  ; CHECK-NEXT:   [[PHI18:%[0-9]+]]:vgpr_32 = PHI undef %356:vgpr_32, %bb.13, %50, %bb.16
  ; CHECK-NEXT:   [[PHI19:%[0-9]+]]:vgpr_32 = PHI undef %356:vgpr_32, %bb.13, %52, %bb.16
  ; CHECK-NEXT:   [[PHI20:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_19]], %bb.13, undef %544:vgpr_32, %bb.16
  ; CHECK-NEXT:   [[PHI21:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_18]], %bb.13, undef %546:vgpr_32, %bb.16
  ; CHECK-NEXT:   [[PHI22:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE8]], %bb.13, undef %548:vgpr_32, %bb.16
  ; CHECK-NEXT:   [[PHI23:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE7]], %bb.13, undef %550:vgpr_32, %bb.16
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE9:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.5, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.5, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE10:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.6, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.6, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE9:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE9]], %subreg.sub0, [[SI_SPILL_V32_RESTORE10]], %subreg.sub1
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE11:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.21, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.21, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE12:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.22, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.22, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE13:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.23, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.23, addrspace 5)
  ; CHECK-NEXT:   [[SI_ELSE2:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF4]], %bb.17, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.15
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.15.bb11:
  ; CHECK-NEXT:   successors: %bb.17(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_U32_e64_10:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI20]], [[PHI16]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_2:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE10:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI21]], %subreg.sub0, [[V_MOV_B32_e32_2]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_pseudo_e64_1:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 2, [[REG_SEQUENCE10]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[PHI22]], [[V_LSHLREV_B64_pseudo_e64_1]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   %474:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[PHI23]], [[V_LSHLREV_B64_pseudo_e64_1]].sub1, [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE11:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]], %subreg.sub0, %474, %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE11]], [[V_ADD_U32_e64_10]], 0, 0, implicit $exec :: (store (s32) into %ir.arrayidx2, align 8, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.17
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.16.bb12:
  ; CHECK-NEXT:   successors: %bb.14(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD_U32_e64_9]], [[PHI16]], [[REG_SEQUENCE4]].sub3, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE12:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[REG_SEQUENCE4]].sub5, %subreg.sub0, [[V_MOV_B32_e32_1]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_pseudo_e64_2:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 2, [[REG_SEQUENCE12]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_4:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[SI_SPILL_V32_RESTORE8]], [[V_LSHLREV_B64_pseudo_e64_2]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   %482:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[SI_SPILL_V32_RESTORE7]], [[V_LSHLREV_B64_pseudo_e64_2]].sub1, [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE13:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]], %subreg.sub0, %482, %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[REG_SEQUENCE13]], [[V_ADD3_U32_e64_1]], 2, 0, implicit $exec :: (store (s16) into %ir.arrayidx3 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[REG_SEQUENCE13]], [[V_ADD3_U32_e64_1]], 0, 0, implicit $exec :: (store (s16) into %ir.arrayidx3, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_11:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_LSHL_OR_B32_e64_21]], [[V_ADD3_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE14:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_20]], %subreg.sub0, [[V_MOV_B32_e32_1]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_pseudo_e64_3:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 2, [[REG_SEQUENCE14]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_6:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_7:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 %513, [[V_LSHLREV_B64_pseudo_e64_3]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   %490:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 %515, [[V_LSHLREV_B64_pseudo_e64_3]].sub1, [[V_ADD_CO_U32_e64_7]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE15:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_6]], %subreg.sub0, %490, %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[REG_SEQUENCE15]], [[V_ADD_U32_e64_11]], 2, 0, implicit $exec :: (store (s16) into %ir.arrayidx5 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[REG_SEQUENCE15]], [[V_ADD_U32_e64_11]], 0, 0, implicit $exec :: (store (s16) into %ir.arrayidx5, addrspace 1)
  ; CHECK-NEXT:   ADJCALLSTACKUP 0, 0, implicit-def dead $scc, implicit-def $sgpr32, implicit $sgpr32
  ; CHECK-NEXT:   [[COPY37:%[0-9]+]]:sreg_32_xexec_hi = COPY $sgpr32
  ; CHECK-NEXT:   [[S_ADD_I32_1:%[0-9]+]]:sreg_32 = S_ADD_I32 [[COPY37]], 1024, implicit-def dead $scc
  ; CHECK-NEXT:   $sgpr32 = COPY [[S_ADD_I32_1]]
  ; CHECK-NEXT:   ADJCALLSTACKDOWN 0, 0, implicit-def dead $scc, implicit-def $sgpr32, implicit $sgpr32
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE2:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.20, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.20, align 4, addrspace 5)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[SI_SPILL_V64_RESTORE2]], 0, 0, implicit $exec :: (load (s32) from %ir.p6, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e64 2, [[GLOBAL_LOAD_DWORD]], implicit $exec
  ; CHECK-NEXT:   SCRATCH_STORE_DWORD_SVS [[V_ADD3_U32_e64_1]], [[V_LSHLREV_B32_e64_2]], [[COPY37]], 40, 0, implicit $exec, implicit $flat_scr :: (store (s32) into %ir.arrayidx1111, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE3:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.17, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.17, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE3]], [[SI_SPILL_V64_RESTORE1]].sub0, 4, 0, implicit $exec :: (store (s32) into %ir.arrayidx444, addrspace 1)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET9:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM9:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET9]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY38:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM9]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD6:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY38]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array4)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET10:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM10:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET10]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY39:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM10]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD7:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY39]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array2)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET11:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM11:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET11]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY40:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM11]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD8:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY40]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array3)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET12:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array5, target-flags(amdgpu-gotprel32-hi) @array5, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM12:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET12]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY41:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM12]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD9:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY41]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_4:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD8]], [[FLAT_LOAD_DWORD9]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE16:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_MUL_LO_U32_e64_4]], %subreg.sub0, undef %455:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %406:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[FLAT_LOAD_DWORD6]], [[FLAT_LOAD_DWORD7]], [[REG_SEQUENCE16]], 0, implicit $exec
  ; CHECK-NEXT:   [[COPY42:%[0-9]+]]:vgpr_32 = COPY %406.sub0
  ; CHECK-NEXT:   S_BRANCH %bb.14
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.17.bb13:
  ; CHECK-NEXT:   successors: %bb.12(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI24:%[0-9]+]]:vgpr_32 = PHI [[PHI19]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI25:%[0-9]+]]:vgpr_32 = PHI [[PHI18]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI26:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE13]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI27:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE12]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI28:%[0-9]+]]:vgpr_32 = PHI [[PHI17]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI29:%[0-9]+]]:vgpr_32 = PHI [[COPY33]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI30:%[0-9]+]]:vgpr_32 = PHI [[COPY34]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI31:%[0-9]+]]:vgpr_32 = PHI [[COPY35]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI32:%[0-9]+]]:vgpr_32 = PHI [[COPY36]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   [[PHI33:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE11]], %bb.14, [[V_ADD_U32_e64_10]], %bb.15
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE2]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_12:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI24]], [[PHI16]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE14:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE15:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.19, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.19, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_4:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[SI_SPILL_V32_RESTORE15]], [[SI_SPILL_V32_RESTORE14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_13:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_12]], [[V_SUB_U32_e64_4]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_5:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_13]], [[V_ADD_U32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_14:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_5]], [[SI_SPILL_V64_RESTORE1]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE16:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.16, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.16, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_5:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_14]], [[SI_SPILL_V32_RESTORE16]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_15:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_SUB_U32_e64_5]], [[PHI25]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_6:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_15]], [[PHI26]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_6:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_6]], [[PHI27]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE17:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI29]], %subreg.sub0, undef %453:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %422:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[V_SUB_U32_e64_6]], [[PHI28]], [[REG_SEQUENCE17]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_7:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %422.sub0, [[PHI30]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_8:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_7]], [[PHI31]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_16:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_SUB_U32_e64_8]], [[PHI32]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_7:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_16]], [[PHI33]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[REG_SEQUENCE9]], [[V_MUL_LO_U32_e64_7]], 2, 0, implicit $exec :: (store (s16) into %ir.p7 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[REG_SEQUENCE9]], [[V_MUL_LO_U32_e64_7]], 0, 0, implicit $exec :: (store (s16) into %ir.p7, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.18.Flow19:
  ; CHECK-NEXT:   successors: %bb.4(0x40000000), %bb.12(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI34:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %91, %bb.20
  ; CHECK-NEXT:   [[PHI35:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %90, %bb.20
  ; CHECK-NEXT:   [[PHI36:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %89, %bb.20
  ; CHECK-NEXT:   [[PHI37:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %88, %bb.20
  ; CHECK-NEXT:   [[PHI38:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %87, %bb.20
  ; CHECK-NEXT:   [[PHI39:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %86, %bb.20
  ; CHECK-NEXT:   [[PHI40:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %85, %bb.20
  ; CHECK-NEXT:   [[PHI41:%[0-9]+]]:vgpr_32 = PHI undef %172:vgpr_32, %bb.2, %84, %bb.20
  ; CHECK-NEXT:   [[PHI42:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_2]], %bb.2, undef %498:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI43:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_]], %bb.2, undef %500:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI44:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_1]], %bb.2, undef %502:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI45:%[0-9]+]]:vgpr_32 = PHI [[COPY17]], %bb.2, undef %504:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI46:%[0-9]+]]:vgpr_32 = PHI [[COPY12]], %bb.2, undef %506:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI47:%[0-9]+]]:vgpr_32 = PHI [[COPY11]], %bb.2, undef %508:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI48:%[0-9]+]]:vgpr_32 = PHI [[COPY10]], %bb.2, undef %510:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI49:%[0-9]+]]:vgpr_32 = PHI [[COPY9]], %bb.2, undef %512:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI50:%[0-9]+]]:vgpr_32 = PHI [[COPY8]], %bb.2, undef %514:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI51:%[0-9]+]]:vgpr_32 = PHI [[COPY7]], %bb.2, undef %516:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI52:%[0-9]+]]:vgpr_32 = PHI [[COPY6]], %bb.2, undef %518:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI53:%[0-9]+]]:vgpr_32 = PHI [[COPY5]], %bb.2, undef %520:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI54:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_UBYTE4]], %bb.2, undef %522:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI55:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_UBYTE5]], %bb.2, undef %524:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI56:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_UBYTE6]], %bb.2, undef %526:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI57:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_UBYTE7]], %bb.2, undef %528:vgpr_32, %bb.20
  ; CHECK-NEXT:   [[PHI58:%[0-9]+]]:vreg_128 = PHI [[GLOBAL_LOAD_DWORDX4_]], %bb.2, undef %530:vreg_128, %bb.20
  ; CHECK-NEXT:   [[PHI59:%[0-9]+]]:vreg_128 = PHI [[GLOBAL_LOAD_DWORDX4_1]], %bb.2, undef %532:vreg_128, %bb.20
  ; CHECK-NEXT:   [[PHI60:%[0-9]+]]:vreg_64 = PHI [[REG_SEQUENCE1]], %bb.2, undef %534:vreg_64, %bb.20
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI42]], %stack.13, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.13, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI43]], %stack.12, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.12, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V128_SAVE [[PHI59]], %stack.11, $sgpr32, 0, implicit $exec :: (store (s128) into %stack.11, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V128_SAVE [[PHI58]], %stack.10, $sgpr32, 0, implicit $exec :: (store (s128) into %stack.10, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[PHI60]], %stack.9, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.9, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI48]], %stack.8, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.8, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI49]], %stack.7, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.7, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI53]], %stack.6, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.6, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI52]], %stack.5, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.5, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI44]], %stack.4, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY2]], %stack.14, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.14, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY1]], %stack.15, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.15, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[PHI]], %stack.19, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.19, addrspace 5)
  ; CHECK-NEXT:   [[SI_ELSE3:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF1]], %bb.12, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.19.bb4:
  ; CHECK-NEXT:   successors: %bb.20(0x04000000), %bb.19(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI61:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_1]], %bb.3, %81, %bb.19
  ; CHECK-NEXT:   [[PHI62:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.3, %80, %bb.19
  ; CHECK-NEXT:   [[S_ADD_I32_2:%[0-9]+]]:sreg_32 = S_ADD_I32 [[PHI62]], 1, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_ADD_I32_3:%[0-9]+]]:sreg_32 = S_ADD_I32 [[PHI62]], 2, implicit-def dead $scc
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_2:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[S_ADD_I32_3]], [[COPY]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_2]], [[PHI61]], implicit-def dead $scc
  ; CHECK-NEXT:   [[COPY43:%[0-9]+]]:vgpr_32 = COPY [[S_ADD_I32_2]], implicit $exec
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK]], %bb.19, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.20
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.20.bb14.loopexit:
  ; CHECK-NEXT:   successors: %bb.18(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_17:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_18:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD1]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET13:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array7, target-flags(amdgpu-gotprel32-hi) @array7, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM13:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET13]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY44:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM13]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY44]], [[V_ADD_U32_e64_17]], 68, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array7, i64 68)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET14:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array9, target-flags(amdgpu-gotprel32-hi) @array9, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM14:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET14]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY45:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM14]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY45]], [[V_ADD_U32_e64_18]], 60, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array9, i64 60)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET15:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array6, target-flags(amdgpu-gotprel32-hi) @array6, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM15:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET15]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY46:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM15]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD10:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY46]], 44, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array6, i64 44)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_8:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD10]], [[COPY43]], implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD11:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY44]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array7, i64 20)`)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_9:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[FLAT_LOAD_DWORD11]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET16:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array8, target-flags(amdgpu-gotprel32-hi) @array8, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM16:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET16]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY47:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM16]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD12:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY47]], 44, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array8, i64 44)`, align 8)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_19:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD12]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD13:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY45]], 24, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array9, i64 24)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_9:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD13]], [[COPY43]], implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET17:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM17:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET17]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY48:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM17]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD14:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY48]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 80)`)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_20:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD14]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET18:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM18:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET18]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY49:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM18]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD15:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY49]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 80)`)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_10:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[FLAT_LOAD_DWORD15]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET19:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM19:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET19]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY50:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM19]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD16:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY50]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 80)`, align 8)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_21:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD16]], [[COPY43]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET20:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array5, target-flags(amdgpu-gotprel32-hi) @array5, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM20:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET20]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY51:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM20]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD17:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY51]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array5, i64 80)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_10:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD17]], [[COPY43]], implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.18
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.21.bb14:
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE17:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.19, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.19, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[PHI15]], [[SI_SPILL_V32_RESTORE17]], [[PHI14]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD3_U32_e64_2]], [[PHI13]], [[PHI12]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD3_U32_e64_3]], [[PHI11]], 100, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_22:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD3_U32_e64_4]], [[PHI10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD_U32_e64_22]], [[PHI9]], [[PHI8]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE6]], [[V_ADD3_U32_e64_5]], 4, 0, implicit $exec :: (store (s32) into %ir.gep3, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE5]], [[V_ADD3_U32_e64_4]], 4, 0, implicit $exec :: (store (s32) into %ir.gep4, addrspace 1)
  ; CHECK-NEXT:   S_ENDPGM 0
entry:
   %ld1 = load i32, ptr addrspace(1) %p1, align 1
   %gep1 = getelementptr inbounds i32, ptr addrspace(1) %p1, i64 3
   %ld2 = load i32, ptr addrspace(1) %gep1, align 1
   %load1 = load i32, ptr addrspace(1) %p3, align 4
   %tmp1 = add i32 %load1, %ld1
   %load2 = load <8 x i32>, ptr addrspace(1) %p3, align 1
   store i32 %tmp1, ptr addrspace(1) %p3
   %add1 = add i32 %ld1, %tmp1
   br i1 %cond1, label %bb1, label %bb2

bb1:
  br label %bb2

bb2:
  %phi0 = phi i32 [ 100, %bb1 ], [ %add1, %entry ]
  %ld3 = load i32, ptr addrspace(3) %p2, align 1
  %add2 = add i32 %ld3, 100
  br i1 %cond2, label %bb3, label %bb4

bb3:
  %mul1 = mul i32 %ld1, %phi0
  %add3 = add i32 %mul1, 1000
  br label %bb5

bb5:
  %add30 = add i32 %add3, 900
  %gep2 = getelementptr inbounds i32, ptr addrspace(3) %p2, i64 3
  %ld4 = load i32, ptr addrspace(3) %gep2, align 8
  %add5 = add i32 %ld4, %add30
  %load3 = load <8 x i32>, ptr addrspace(1) %p4, align 1
  %load4 = load i32, ptr addrspace(1) %p4, align 2
  %tmp2 = add i32 %load4, %tmp1
  store i32 %tmp2, ptr addrspace(1) %p4
  %stack = alloca [5 x i32], align 4, addrspace(5)
  %load6 = load i32, ptr addrspace(1) %p6, align 4
  %arrayidx11 = getelementptr inbounds [5 x i32], ptr addrspace(5) %stack, i32 0, i32 %load6
  store i32 %phi0, ptr addrspace(5) %arrayidx11, align 4
  %arrayidx22 = getelementptr inbounds i32, ptr addrspace(1) %p6, i32 1
  %load7 = load i32, ptr addrspace(1) %arrayidx22, align 4
  %arrayidx33 = getelementptr inbounds [5 x i32], ptr addrspace(5) %stack, i32 0, i32 %load7
  store i32 %ld3, ptr addrspace(5) %arrayidx33, align 2
  %xor = xor i1 %cond1, %cond2
  br i1 %xor, label %bb6, label %bb7

bb6:
  %and = and i1 %cond1, %cond2
  %idx10 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 1, i64 0
  %val0 = load i32, i32* %idx10, align 4
  %idx20 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 0, i64 1
  store i32 %val0, i32 *%idx20
  br i1 %and, label %bb8, label %bb9

bb8:
  %add6 = add i32 %ld2, %add5
  %idx12 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 1, i64 2
  %val2 = load i32, i32* %idx12, align 4
  %idx22 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 3, i64 2
  store i32 %val2, i32 *%idx22
  br label %bb10

bb9:
  %mul2 = mul i32 %ld2, %add5
  %idx13 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 1, i64 0
  %val3 = load i32, i32* %idx13, align 4
  %idx23 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 3, i64 0
  store i32 %val3, i32 *%idx23
  br label %bb10

bb7:
  %sub1 = sub i32 %ld4, %add5
  %mul3 = mul i32 %sub1, %ld3
  %div = udiv i32 %mul3, %ld1
  %add7 = add i32 %div, %ld2
  %idx14 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 4, i64 1
  %val4 = load i32, i32* %idx14, align 4
  %idx24 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 3, i64 0
  store i32 %val4, i32 *%idx24
  br label %bb10

bb10:
  %phi2 = phi i32 [ %add6, %bb8 ], [ %mul2, %bb9], [ %add7, %bb7 ]
  %add8 = add i32 %add2, %phi2
  %extract1 = extractelement < 8 x i32> %load2, i32 1
  %extract2 = extractelement < 8 x i32> %load2, i32 2
  %extract3 = extractelement < 8 x i32> %load2, i32 6
  %extract4 = extractelement < 8 x i32> %load2, i32 7
  %add101 = add i32 %extract1, %extract4
  %add102 = add i32 %add101, %extract3
  %idx1 = zext i32 %extract2 to i64
  %arrayidx1 = getelementptr inbounds i32, ptr addrspace(1) %p1, i64 %idx1
  store i32 %add102, ptr addrspace(1) %arrayidx1, align 4
  %cond3 = icmp ne i1 %cond1, %cond2
  br i1 %cond3, label %bb11, label %bb12

bb11:
  %extract5 = extractelement < 8 x i32> %load3, i32 3
  %extract6 = extractelement < 8 x i32> %load3, i32 5
  %tmp3 = add i32 %extract5, %phi2
  %idx2 = zext i32 %extract6 to i64
  %arrayidx2 = getelementptr inbounds i32, ptr addrspace(1) %p5, i64 %idx2
  store i32 %tmp3, ptr addrspace(1) %arrayidx2, align 8
  br label %bb13

bb12:
  %extract7 = extractelement < 8 x i32> %load3, i32 1
  %extract8 = extractelement < 8 x i32> %load3, i32 2
  %extract9 = extractelement < 8 x i32> %load2, i32 3
  %extract10 = extractelement < 8 x i32> %load2, i32 5
  %tmp4 = add i32 %extract9, %add8
  %idx3 = zext i32 %extract10 to i64
  %arrayidx3 = getelementptr inbounds i32, ptr addrspace(1) %p5, i64 %idx3
  store i32 %tmp4, ptr addrspace(1) %arrayidx3, align 2
  %tmp5 = add i32 %extract7, %tmp4
  %idx4 = zext i32 %extract8 to i64
  %arrayidx5 = getelementptr inbounds i32, ptr addrspace(1) %p6, i64 %idx4
  store i32 %tmp5, ptr addrspace(1) %arrayidx5, align 2
  %array1 = alloca [5 x i32], align 4, addrspace(5)
  %load8 = load i32, ptr addrspace(1) %p6, align 4
  %arrayidx111 = getelementptr inbounds [5 x i32], ptr addrspace(5) %array1, i32 2, i32 %load8
  store i32 %tmp4, ptr addrspace(5) %arrayidx111, align 4
  %arrayidx222 = getelementptr inbounds i32, ptr addrspace(1) %p6, i32 1
  %load9 = load i32, ptr addrspace(1) %arrayidx222, align 4
  %arrayidx333 = getelementptr inbounds [5 x i32], ptr addrspace(5) %array1, i32 1, i32 %load9
  %load10 = load i32, ptr addrspace(5) %arrayidx333
  %arrayidx444 = getelementptr inbounds i32, ptr addrspace(1) %p4, i32 1
  store i32 %add30, ptr addrspace(1) %arrayidx444
  %idx15 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 0, i64 0
  %val5 = load i32, i32* %idx15, align 4
  %idx16 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 0, i64 0
  %val6 = load i32, i32* %idx16, align 4
  %idx17 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 0, i64 0
  %val7 = load i32, i32* %idx17, align 4
  %idx18 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 0, i64 0
  %val8 = load i32, i32* %idx18, align 4
  %mul10 = mul i32 %val5, %val6
  %mul11 = mul i32 %val7, %val8
  %add100 = add i32 %mul10, %mul11
  br label %bb13

bb13:
  %phi3 = phi i32 [ %tmp3, %bb11 ], [ %add100, %bb12]
  %phi4 = phi i32 [ %tmp3, %bb11 ], [ %val5, %bb12]
  %phi5 = phi i32 [ %tmp3, %bb11 ], [ %load7, %bb12]
  %phi6 = phi i32 [ %tmp3, %bb11 ], [ %load6, %bb12]
  %phi7 = phi i32 [ %tmp3, %bb11 ], [ %val8, %bb12]
  %phi8 = phi i32 [ %tmp3, %bb11 ], [ %extract1, %bb12]
  %phi9 = phi i32 [ %tmp3, %bb11 ], [ %extract2, %bb12]
  %phi10 = phi i32 [ %tmp3, %bb11 ], [ %extract3, %bb12]
  %phi11 = phi i32 [ %tmp3, %bb11 ], [ %extract4, %bb12]
  %phi12 = phi i32 [ %tmp3, %bb11 ], [ %load4, %bb12]
  %add200 = add i32 %phi3, %phi2
  %add300 = sub i32 %phi0, %add1
  %add400 = add i32 %add200, %add300
  %add500 = mul i32 %add400, %add2
  %add600 = add i32 %add500, %add30
  %add700 = sub i32 %add600, %ld4
  %add800 = add i32 %add700, %phi4
  %add900 = mul i32 %add800, %phi5
  %add1000 = sub i32 %add900, %phi6
  %add1100 = mul i32 %add1000, %phi7
  %add1200 = add i32 %add1100, %phi8
  %add1300 = sub i32 %add1200, %phi9
  %add1400 = sub i32 %add1300, %phi10
  %add1500 = add i32 %add1400, %phi11
  %add1600 = mul i32 %add1500, %phi12
  store i32 %add1600, ptr addrspace(1) %p7, align 2
  br label %bb14

bb4:
   %phi13 = phi i32 [ 0, %bb2 ], [ %ind, %bb4 ]
   %idx600 = getelementptr inbounds [5 x i32], [5 x i32]* @array6, i64 1, i64 2
   %val600 = load i32, i32* %idx600, align 4
   %idx700 = getelementptr inbounds [5 x i32], [5 x i32]* @array7, i64 3, i64 2
   %addval600 = add i32 %val600, %phi13
   store i32 %addval600, i32 *%idx700
   %idx800 = getelementptr inbounds [5 x i32], [5 x i32]* @array8, i64 1, i64 0
   %val800 = load i32, i32* %idx800, align 4
   %idx900 = getelementptr inbounds [5 x i32], [5 x i32]* @array9, i64 3, i64 0
   %addval800 = add i32 %val800, %phi13
   store i32 %addval800, i32 *%idx900
   %idx601 = getelementptr inbounds [5 x i32], [5 x i32]* @array6, i64 2, i64 1
   %val601 = load i32, i32* %idx601, align 1
   %val611 = mul i32 %val601, %phi13
   %idx701 = getelementptr inbounds [5 x i32], [5 x i32]* @array7, i64 1, i64 0
   %val701 = load i32, i32* %idx701, align 2
   %val711 = sub i32 %val701, %phi13
   %idx801 = getelementptr inbounds [5 x i32], [5 x i32]* @array8, i64 2, i64 1
   %val801 = load i32, i32* %idx801, align 8
   %val811 = add i32 %val801, %phi13
   %idx901 = getelementptr inbounds [5 x i32], [5 x i32]* @array9, i64 1, i64 1
   %val901 = load i32, i32* %idx901, align 1
   %val911 = mul i32 %val901, %phi13
   %idx602 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 4, i64 0
   %val602 = load i32, i32* %idx602, align 1
   %val612 = add i32 %val602, %phi13
   %idx702 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 4, i64 0
   %val702 = load i32, i32* %idx702, align 2
   %val712 = sub i32 %val702, %phi13
   %idx802 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 4, i64 0
   %val802 = load i32, i32* %idx802, align 8
   %val812 = add i32 %val802, %phi13
   %idx902 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 4, i64 0
   %val902 = load i32, i32* %idx902, align 1
   %val912 = mul i32 %val902, %phi13
   %ind = add i32 %phi13, 1
   %loop.cond = icmp ult i32 %ind, %TC1
   br i1 %loop.cond, label %bb4, label %bb14

bb14:
  %phi14 = phi i32 [ %add200, %bb13 ], [ %val611, %bb4 ]
  %phi15 = phi i32 [ %add500, %bb13 ], [ %val711, %bb4 ]
  %phi16 = phi i32 [ %add600, %bb13 ], [ %val811, %bb4 ]
  %phi17 = phi i32 [ %add300, %bb13 ], [ %val911, %bb4 ]
  %phi18 = phi i32 [ %add1000, %bb13 ], [ %val612, %bb4 ]
  %phi19 = phi i32 [ %add1600, %bb13 ], [ %val712, %bb4 ]
  %phi20 = phi i32 [ %add500, %bb13 ], [ %val812, %bb4 ]
  %phi21 = phi i32 [ %add500, %bb13 ], [ %val912, %bb4 ]
  %addall1 = add i32 %phi14, %phi0
  %addall2 = add i32 %addall1, %phi15
  %addall3 = add i32 %addall2, 100
  %addall4 = add i32 %addall3, %phi16
  %addall5 = add i32 %addall4, %phi17
  %addall6 = add i32 %addall5, %phi18
  %addall7 = add i32 %addall6, %phi19
  %addall8 = add i32 %addall7, %phi20
  %addall9 = add i32 %addall8, %phi21
  %gep3 = getelementptr inbounds i32, ptr addrspace(1) %p8, i64 1
  store i32 %addall9, ptr addrspace(1) %gep3
  %gep4 = getelementptr inbounds i32, ptr addrspace(1) %p9, i64 1
  store i32 %addall6, ptr addrspace(1) %gep4
  ret void
}
