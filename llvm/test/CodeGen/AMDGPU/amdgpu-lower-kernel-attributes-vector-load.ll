; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -mtriple=amdgcn-amd-amdhsa -S -passes=amdgpu-lower-kernel-attributes %s | FileCheck %s

; Test that we correctly handle vector load types when folding with reqd_work_group_size.
; This tests the fix for a crash where <1 x i16> vector types would cause an assertion
; failure in ConstantExpr::getCast because we tried to cast a scalar constant to a vector type.

; Single-element vector <1 x i16> should be folded.
; CHECK-LABEL: @load_group_size_x_v1i16(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_X:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 12
; CHECK-NEXT:    [[GROUP_SIZE_X:%.*]] = load <1 x i16>, ptr addrspace(4) [[GEP_GROUP_SIZE_X]], align 2
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <1 x i16>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <1 x i16> splat (i16 8), ptr addrspace(1) [[OUT_PTR]], align 2
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_x_v1i16(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.x = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 12
  %group.size.x = load <1 x i16>, ptr addrspace(4) %gep.group.size.x, align 2
  %out.ptr = getelementptr inbounds <1 x i16>, ptr addrspace(1) %out, i64 0
  store <1 x i16> %group.size.x, ptr addrspace(1) %out.ptr, align 2
  ret void
}

; Multi-element vector <2 x i16> should NOT be folded (semantically incorrect - can't cast one int to two).
; CHECK-LABEL: @load_group_size_x_v2i16(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_X:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 12
; CHECK-NEXT:    [[GROUP_SIZE_X:%.*]] = load <2 x i16>, ptr addrspace(4) [[GEP_GROUP_SIZE_X]], align 2
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <2 x i16>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <2 x i16> [[GROUP_SIZE_X]], ptr addrspace(1) [[OUT_PTR]], align 4
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_x_v2i16(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.x = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 12
  %group.size.x = load <2 x i16>, ptr addrspace(4) %gep.group.size.x, align 2
  %out.ptr = getelementptr inbounds <2 x i16>, ptr addrspace(1) %out, i64 0
  store <2 x i16> %group.size.x, ptr addrspace(1) %out.ptr, align 4
  ret void
}

; Single-element vector <1 x i32> load from i16 position - should NOT be folded (wrong size).
; CHECK-LABEL: @load_group_size_y_v1i32(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_Y:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 14
; CHECK-NEXT:    [[GROUP_SIZE_Y:%.*]] = load <1 x i32>, ptr addrspace(4) [[GEP_GROUP_SIZE_Y]], align 4
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <1 x i32>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <1 x i32> [[GROUP_SIZE_Y]], ptr addrspace(1) [[OUT_PTR]], align 4
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_y_v1i32(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.y = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 14
  %group.size.y = load <1 x i32>, ptr addrspace(4) %gep.group.size.y, align 4
  %out.ptr = getelementptr inbounds <1 x i32>, ptr addrspace(1) %out, i64 0
  store <1 x i32> %group.size.y, ptr addrspace(1) %out.ptr, align 4
  ret void
}

; Multi-element vector <2 x i32> should NOT be folded.
; CHECK-LABEL: @load_group_size_y_v2i32(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_Y:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 14
; CHECK-NEXT:    [[GROUP_SIZE_Y:%.*]] = load <2 x i32>, ptr addrspace(4) [[GEP_GROUP_SIZE_Y]], align 4
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <2 x i32>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <2 x i32> [[GROUP_SIZE_Y]], ptr addrspace(1) [[OUT_PTR]], align 8
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_y_v2i32(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.y = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 14
  %group.size.y = load <2 x i32>, ptr addrspace(4) %gep.group.size.y, align 4
  %out.ptr = getelementptr inbounds <2 x i32>, ptr addrspace(1) %out, i64 0
  store <2 x i32> %group.size.y, ptr addrspace(1) %out.ptr, align 8
  ret void
}

; Single-element vector <1 x i64> load from i16 position - should NOT be folded (wrong size).
; CHECK-LABEL: @load_group_size_z_v1i64(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_Z:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 16
; CHECK-NEXT:    [[GROUP_SIZE_Z:%.*]] = load <1 x i64>, ptr addrspace(4) [[GEP_GROUP_SIZE_Z]], align 8
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <1 x i64>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <1 x i64> [[GROUP_SIZE_Z]], ptr addrspace(1) [[OUT_PTR]], align 8
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_z_v1i64(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.z = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 16
  %group.size.z = load <1 x i64>, ptr addrspace(4) %gep.group.size.z, align 8
  %out.ptr = getelementptr inbounds <1 x i64>, ptr addrspace(1) %out, i64 0
  store <1 x i64> %group.size.z, ptr addrspace(1) %out.ptr, align 8
  ret void
}

; Multi-element vector <2 x i64> should NOT be folded.
; CHECK-LABEL: @load_group_size_z_v2i64(
; CHECK-NEXT:    [[IMPLICITARG_PTR:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; CHECK-NEXT:    [[GEP_GROUP_SIZE_Z:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[IMPLICITARG_PTR]], i64 16
; CHECK-NEXT:    [[GROUP_SIZE_Z:%.*]] = load <2 x i64>, ptr addrspace(4) [[GEP_GROUP_SIZE_Z]], align 8
; CHECK-NEXT:    [[OUT_PTR:%.*]] = getelementptr inbounds <2 x i64>, ptr addrspace(1) [[OUT:%.*]], i64 0
; CHECK-NEXT:    store <2 x i64> [[GROUP_SIZE_Z]], ptr addrspace(1) [[OUT_PTR]], align 16
; CHECK-NEXT:    ret void
;
define amdgpu_kernel void @load_group_size_z_v2i64(ptr addrspace(1) %out) #0 !reqd_work_group_size !0 {
  %implicitarg.ptr = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %gep.group.size.z = getelementptr inbounds i8, ptr addrspace(4) %implicitarg.ptr, i64 16
  %group.size.z = load <2 x i64>, ptr addrspace(4) %gep.group.size.z, align 8
  %out.ptr = getelementptr inbounds <2 x i64>, ptr addrspace(1) %out, i64 0
  store <2 x i64> %group.size.z, ptr addrspace(1) %out.ptr, align 16
  ret void
}

declare ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr() #1

attributes #0 = { nounwind "uniform-work-group-size"="true" }
attributes #1 = { nounwind readnone speculatable }

!llvm.module.flags = !{!1}

!0 = !{i32 8, i32 16, i32 2}
!1 = !{i32 1, !"amdgpu_code_object_version", i32 500}
