; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13


; 3x3

define amdgpu_ps void @test_wconv.3x3_4x2(ptr addrspace(1) %out, <8 x half> %acc, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right) {
; GFX13-LABEL: test_wconv.3x3_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v36, v29 :: v_dual_mov_b32 v35, v28
; GFX13-NEXT:    v_mov_b32_e32 v34, v27
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v[6:23], v[24:26], v[34:36], v[30:32] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v36f16(<8 x half> %acc, <36 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_wconv.3x3_4x4(ptr addrspace(1) %out, <8 x half> %acc, <18 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right) {
; GFX13-LABEL: test_wconv.3x3_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v24, v23 :: v_dual_mov_b32 v23, v22
; GFX13-NEXT:    v_dual_mov_b32 v22, v21 :: v_dual_mov_b32 v28, v17
; GFX13-NEXT:    v_dual_mov_b32 v27, v16 :: v_dual_mov_b32 v26, v15
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v[6:14], v[26:28], v[18:20], v[22:24] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v8f16(<8 x half> %acc, <18 x half> %weights, <6 x half> %tensor_col_center, <6 x half> %tensor_col_left, <6 x half> %tensor_col_right, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_wconv.3x3_8x4(ptr addrspace(1) %out, <8 x half> %acc, <10 x half> %weights, <8 x half> %tensor_col_center, <8 x half> %tensor_col_left, <8 x half> %tensor_col_right) {
; GFX13-LABEL: test_wconv.3x3_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_dual_mov_b32 v21, v20 :: v_dual_mov_b32 v20, v19
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v[6:10], v[12:15], v[16:19], v[20:23] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v10f16(<8 x half> %acc, <10 x half> %weights, <8 x half> %tensor_col_center, <8 x half> %tensor_col_left, <8 x half> %tensor_col_right, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

; 1x1

define amdgpu_ps void @test_wconv.1x1_4x2(ptr addrspace(1) %out, <8 x half> %acc, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_wconv.1x1_4x2:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v[6:7], v8, v9, v10, v11 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v4f16.v2f16(<8 x half> %acc, <4 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_wconv.1x1_4x4(ptr addrspace(1) %out, <8 x half> %acc, <2 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3) {
; GFX13-LABEL: test_wconv.1x1_4x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v6, v7, v8, v9, v10 aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v2f16.v2f16(<8 x half> %acc, <2 x half> %weights, <2 x half> %tensor_0, <2 x half> %tensor_1, <2 x half> %tensor_2, <2 x half> %tensor_3, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_wconv.1x1_8x4(ptr addrspace(1) %out, <8 x half> %acc, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3) {
; GFX13-LABEL: test_wconv.1x1_8x4:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v15, v14 :: v_dual_mov_b32 v14, v13
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_dual_mov_b32 v11, v10 :: v_dual_mov_b32 v10, v9
; GFX13-NEXT:    v_dual_mov_b32 v9, v8 :: v_dual_mov_b32 v8, v7
; GFX13-NEXT:    v_wconv_fp16_fp16 v[2:5], v[2:5], v6, v[8:9], v[10:11], v[12:13], v[14:15] aux_data:2 clamp
; GFX13-NEXT:    global_store_b128 v[0:1], v[2:5], off
; GFX13-NEXT:    s_sendmsg sendmsg(MSG_DEALLOC_VGPRS)
; GFX13-NEXT:    s_endpgm
bb:
  %dst = call <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half> %acc, <2 x half> %weights, <4 x half> %tensor_0, <4 x half> %tensor_1, <4 x half> %tensor_2, <4 x half> %tensor_3, i32 2, i1 1)
  store <8 x half> %dst, ptr addrspace(1) %out
  ret void
}


declare <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v36f16(<8 x half>, <36 x half>, <6 x half>, <6 x half>, <6 x half>, i32, i1)
declare <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v18f16(<8 x half>, <18 x half>, <6 x half>, <6 x half>, <6 x half>, i32, i1)
declare <8 x half> @llvm.amdgcn.wconv.3x3.v8f16.v8f16.v10f16(<8 x half>, <10 x half>, <8 x half>, <8 x half>, <8 x half>, i32, i1)


declare <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v4f16.v2f16(<8 x half>, <4 x half>, <2 x half>, <2 x half>, <2 x half>, <2 x half>, i32, i1)
declare <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v2f16.v2f16(<8 x half>, <2 x half>, <2 x half>, <2 x half>, <2 x half>, <2 x half>, i32, i1)
declare <8 x half> @llvm.amdgcn.wconv.1x1.v8f16.v8f16.v2f16.v4f16(<8 x half>, <2 x half>, <4 x half>, <4 x half>, <4 x half>, <4 x half>, i32, i1)

