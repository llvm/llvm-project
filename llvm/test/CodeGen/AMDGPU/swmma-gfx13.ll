; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -march=amdgcn -mcpu=gfx1300 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX13

define amdgpu_ps void @test_swmma_f32_16x16x32_fp8_fp8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_fp8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_swmma_f32_16x16_fp8_fp8 v[6:13], v[0:1], v[2:5], v[6:13], v14 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[16:17], v[10:13], off offset:16
; GFX13-NEXT:    global_store_b128 v[16:17], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.fp8.fp8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x32_fp8_fp8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x32_fp8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_swmma_f16_16x16_fp8_fp8 v[6:9], v[0:1], v[2:5], v[6:9], v10 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[12:13], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x32.fp8.fp8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x32_fp8_bf8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_fp8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_swmma_f32_16x16_fp8_bf8 v[6:13], v[0:1], v[2:5], v[6:13], v14 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[16:17], v[10:13], off offset:16
; GFX13-NEXT:    global_store_b128 v[16:17], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.fp8.bf8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x32_fp8_bf8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x32_fp8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_swmma_f16_16x16_fp8_bf8 v[6:9], v[0:1], v[2:5], v[6:9], v10 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[12:13], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x32.fp8.bf8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x32_bf8_fp8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_bf8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_swmma_f32_16x16_bf8_fp8 v[6:13], v[0:1], v[2:5], v[6:13], v14 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[16:17], v[10:13], off offset:16
; GFX13-NEXT:    global_store_b128 v[16:17], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.bf8.fp8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x32_bf8_fp8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x32_bf8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_swmma_f16_16x16_bf8_fp8 v[6:9], v[0:1], v[2:5], v[6:9], v10 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[12:13], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x32.bf8.fp8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x32_bf8_bf8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_bf8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v17, v16 :: v_dual_mov_b32 v16, v15
; GFX13-NEXT:    v_swmma_f32_16x16_bf8_bf8 v[6:13], v[0:1], v[2:5], v[6:13], v14 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[16:17], v[10:13], off offset:16
; GFX13-NEXT:    global_store_b128 v[16:17], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.bf8.bf8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x32_bf8_bf8_clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x32_bf8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v13, v12 :: v_dual_mov_b32 v12, v11
; GFX13-NEXT:    v_swmma_f16_16x16_bf8_bf8 v[6:9], v[0:1], v[2:5], v[6:9], v10 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[12:13], v[6:9], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x32.bf8.bf8.clamp(<2 x i32> %A, <4 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x32_f16_clamp(<8 x half> %A, <16 x half> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_f16_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_f16 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.f16.clamp(<8 x half> %A, <16 x half> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x32_f16_clamp(<8 x half> %A, <16 x half> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x32_f16_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_f16_16x16_f16 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x32.f16.clamp(<8 x half> %A, <16 x half> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x32_bf16_clamp(<8 x bfloat> %A, <16 x bfloat> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x32_bf16_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_bf16 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x32.bf16.clamp(<8 x bfloat> %A, <16 x bfloat> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_bf16_16x16x32_bf16_clamp(<8 x bfloat> %A, <16 x bfloat> %B, <8 x bfloat> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_bf16_16x16x32_bf16_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_bf16_16x16_bf16 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x bfloat> @llvm.amdgcn.swmma.bf16.16x16x32.bf16.clamp(<8 x bfloat> %A, <16 x bfloat> %B, <8 x bfloat> %C, i32 %Index, i1 1, i1 1)
  store <8 x bfloat> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x64_fp8_fp8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x64_fp8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_fp8_fp8 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x64.fp8.fp8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x64_fp8_fp8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x64_fp8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_f16_16x16_fp8_fp8 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x64.fp8.fp8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x64_fp8_bf8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x64_fp8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_fp8_bf8 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x64.fp8.bf8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x64_fp8_bf8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x64_fp8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_f16_16x16_fp8_bf8 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x64.fp8.bf8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x64_bf8_fp8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x64_bf8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_bf8_fp8 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x64.bf8.fp8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x64_bf8_fp8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x64_bf8_fp8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_f16_16x16_bf8_fp8 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x64.bf8.fp8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f32_16x16x64_bf8_bf8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f32_16x16x64_bf8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v23, v22 :: v_dual_mov_b32 v22, v21
; GFX13-NEXT:    v_swmma_f32_16x16_bf8_bf8 v[12:19], v[0:3], v[4:11], v[12:19], v20 sparse_index_odd clamp
; GFX13-NEXT:    s_clause 0x1
; GFX13-NEXT:    global_store_b128 v[22:23], v[16:19], off offset:16
; GFX13-NEXT:    global_store_b128 v[22:23], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x float> @llvm.amdgcn.swmma.f32.16x16x64.bf8.bf8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x float> %C, i32 %Index, i1 1, i1 1)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmma_f16_16x16x64_bf8_bf8_clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, ptr addrspace(1) %out) {
; GFX13-LABEL: test_swmma_f16_16x16x64_bf8_bf8_clamp:
; GFX13:       ; %bb.0: ; %bb
; GFX13-NEXT:    v_dual_mov_b32 v19, v18 :: v_dual_mov_b32 v18, v17
; GFX13-NEXT:    v_swmma_f16_16x16_bf8_bf8 v[12:15], v[0:3], v[4:11], v[12:15], v16 sparse_index_odd clamp
; GFX13-NEXT:    global_store_b128 v[18:19], v[12:15], off
; GFX13-NEXT:    s_endpgm
bb:
  %res = call <8 x half> @llvm.amdgcn.swmma.f16.16x16x64.bf8.bf8.clamp(<4 x i32> %A, <8 x i32> %B, <8 x half> %C, i32 %Index, i1 1, i1 1)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}
