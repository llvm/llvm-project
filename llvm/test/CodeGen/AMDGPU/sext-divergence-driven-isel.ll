; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=verde < %s | FileCheck -enable-var-scope -check-prefix=WAVE64 %s
; RUN: llc -mtriple=amdgcn -mcpu=gfx1010 -mattr=+wavefrontsize32 < %s | FileCheck -enable-var-scope -check-prefix=WAVE32 %s

define amdgpu_kernel void @sext_i16_to_i32_uniform(ptr addrspace(1) %out, i16 %a, i32 %b) {
; WAVE64-LABEL: sext_i16_to_i32_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_mov_b64 s[4:5], s[2:3]
; WAVE64-NEXT:    s_sext_i32_i16 s4, s4
; WAVE64-NEXT:    s_add_i32 s4, s5, s4
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i16_to_i32_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v0, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_sext_i32_i16 s2, s2
; WAVE32-NEXT:    s_add_i32 s2, s3, s2
; WAVE32-NEXT:    v_mov_b32_e32 v1, s2
; WAVE32-NEXT:    global_store_dword v0, v1, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %sext = sext i16 %a to i32
  %res = add i32 %b, %sext
  store i32 %res, ptr addrspace(1) %out
  ret void
}


define amdgpu_kernel void @sext_i16_to_i64_uniform(ptr addrspace(1) %out, i16 %a, i64 %b) {
; WAVE64-LABEL: sext_i16_to_i64_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_bfe_i64 s[4:5], s[6:7], 0x100000
; WAVE64-NEXT:    s_add_u32 s4, s8, s4
; WAVE64-NEXT:    s_addc_u32 s5, s9, s5
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    v_mov_b32_e32 v1, s5
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i16_to_i64_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x2
; WAVE32-NEXT:    s_load_dword s0, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[2:3], s[4:5], 0x34
; WAVE32-NEXT:    s_load_dwordx2 s[6:7], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_bfe_i64 s[0:1], s[0:1], 0x100000
; WAVE32-NEXT:    s_add_u32 s0, s2, s0
; WAVE32-NEXT:    s_addc_u32 s1, s3, s1
; WAVE32-NEXT:    v_mov_b32_e32 v0, s0
; WAVE32-NEXT:    v_mov_b32_e32 v1, s1
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[6:7]
; WAVE32-NEXT:    s_endpgm
  %sext = sext i16 %a to i64
  %res = add i64 %b, %sext
  store i64 %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @sext_i16_to_i32_divergent(ptr addrspace(1) %out, i16 %a, i32 %b) {
; WAVE64-LABEL: sext_i16_to_i32_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_add_i32_e32 v0, vcc, s6, v0
; WAVE64-NEXT:    v_bfe_i32 v0, v0, 0, 16
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i16_to_i32_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v1, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_add_nc_u16 v0, s2, v0
; WAVE32-NEXT:    v_bfe_i32 v0, v0, 0, 16
; WAVE32-NEXT:    global_store_dword v1, v0, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid.truncated = trunc i32 %tid to i16
  %divergent.a = add i16 %a, %tid.truncated
  %sext = sext i16 %divergent.a to i32
  store i32 %sext, ptr addrspace(1) %out
  ret void
}


define amdgpu_kernel void @sext_i16_to_i64_divergent(ptr addrspace(1) %out, i16 %a, i64 %b) {
; WAVE64-LABEL: sext_i16_to_i64_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_add_i32_e32 v0, vcc, s6, v0
; WAVE64-NEXT:    v_bfe_i32 v0, v0, 0, 16
; WAVE64-NEXT:    v_ashrrev_i32_e32 v1, 31, v0
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i16_to_i64_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_add_nc_u16 v0, s2, v0
; WAVE32-NEXT:    v_bfe_i32 v0, v0, 0, 16
; WAVE32-NEXT:    v_ashrrev_i32_e32 v1, 31, v0
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid.truncated = trunc i32 %tid to i16
  %divergent.a = add i16 %a, %tid.truncated
  %sext = sext i16 %divergent.a to i64
  store i64 %sext, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @sext_i32_to_i64_uniform(ptr addrspace(1) %out, i32 %a, i64 %b) {
; WAVE64-LABEL: sext_i32_to_i64_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s8, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[6:7], s[4:5], 0xd
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_ashr_i32 s5, s8, 31
; WAVE64-NEXT:    s_add_u32 s4, s6, s8
; WAVE64-NEXT:    s_addc_u32 s5, s7, s5
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    v_mov_b32_e32 v1, s5
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i32_to_i64_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x2
; WAVE32-NEXT:    s_load_dword s6, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x34
; WAVE32-NEXT:    s_load_dwordx2 s[2:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_ashr_i32 s4, s6, 31
; WAVE32-NEXT:    s_add_u32 s0, s0, s6
; WAVE32-NEXT:    s_addc_u32 s1, s1, s4
; WAVE32-NEXT:    v_mov_b32_e32 v0, s0
; WAVE32-NEXT:    v_mov_b32_e32 v1, s1
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[2:3]
; WAVE32-NEXT:    s_endpgm
  %sext = sext i32 %a to i64
  %res = add i64 %b, %sext
  store i64 %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @sext_i32_to_i64_divergent(ptr addrspace(1) %out, i32 %a, i64 %b) {
; WAVE64-LABEL: sext_i32_to_i64_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_add_i32_e32 v0, vcc, s6, v0
; WAVE64-NEXT:    v_ashrrev_i32_e32 v1, 31, v0
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i32_to_i64_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_add_nc_u32_e32 v0, s2, v0
; WAVE32-NEXT:    v_ashrrev_i32_e32 v1, 31, v0
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %divergent.a = add i32 %a, %tid
  %sext = sext i32 %divergent.a to i64
  store i64 %sext, ptr addrspace(1) %out
  ret void
}

; Test that uniform i1 -> i32 sext uses the value directly (already -1 or 0)
; instead of VALU V_CNDMASK_B32 + V_READFIRSTLANE roundtrip
define amdgpu_kernel void @sext_i1_to_i32_uniform(ptr addrspace(1) %out, i32 %a, i32 %b) {
; WAVE64-LABEL: sext_i1_to_i32_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_mov_b64 s[4:5], s[2:3]
; WAVE64-NEXT:    s_cmp_eq_u32 s4, s5
; WAVE64-NEXT:    s_cselect_b64 s[4:5], -1, 0
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i1_to_i32_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v0, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_cmp_eq_u32 s2, s3
; WAVE32-NEXT:    s_cselect_b32 s2, -1, 0
; WAVE32-NEXT:    v_mov_b32_e32 v1, s2
; WAVE32-NEXT:    global_store_dword v0, v1, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %cmp = icmp eq i32 %a, %b
  %ext = sext i1 %cmp to i32
  store i32 %ext, ptr addrspace(1) %out
  ret void
}

; Test that divergent i1 -> i32 sext still uses V_CNDMASK_B32
define amdgpu_kernel void @sext_i1_to_i32_divergent(ptr addrspace(1) %out, i32 %a) {
; WAVE64-LABEL: sext_i1_to_i32_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_cmp_eq_u32_e32 vcc, s6, v0
; WAVE64-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: sext_i1_to_i32_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v1, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_cmp_eq_u32_e32 vcc_lo, s2, v0
; WAVE32-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc_lo
; WAVE32-NEXT:    global_store_dword v1, v0, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %cmp = icmp eq i32 %tid, %a
  %ext = sext i1 %cmp to i32
  store i32 %ext, ptr addrspace(1) %out
  ret void
}

declare i32 @llvm.amdgcn.workitem.id.x() #1

attributes #0 = { nounwind }
attributes #1 = { nounwind readnone speculatable }
