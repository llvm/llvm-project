; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=verde < %s | FileCheck -enable-var-scope -check-prefix=WAVE64 %s
; RUN: llc -mtriple=amdgcn -mcpu=gfx1010 -mattr=+wavefrontsize32 < %s | FileCheck -enable-var-scope -check-prefix=WAVE32 %s

define amdgpu_kernel void @zext_i16_to_i32_uniform(ptr addrspace(1) %out, i16 %a, i32 %b) {
; WAVE64-LABEL: zext_i16_to_i32_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_mov_b64 s[4:5], s[2:3]
; WAVE64-NEXT:    s_and_b32 s4, s4, 0xffff
; WAVE64-NEXT:    s_add_i32 s4, s5, s4
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i16_to_i32_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v0, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_and_b32 s2, s2, 0xffff
; WAVE32-NEXT:    s_add_i32 s2, s3, s2
; WAVE32-NEXT:    v_mov_b32_e32 v1, s2
; WAVE32-NEXT:    global_store_dword v0, v1, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %zext = zext i16 %a to i32
  %res = add i32 %b, %zext
  store i32 %res, ptr addrspace(1) %out
  ret void
}


define amdgpu_kernel void @zext_i16_to_i64_uniform(ptr addrspace(1) %out, i16 %a, i64 %b) {
; WAVE64-LABEL: zext_i16_to_i64_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s8, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[6:7], s[4:5], 0xd
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_and_b32 s4, s8, 0xffff
; WAVE64-NEXT:    s_add_u32 s4, s6, s4
; WAVE64-NEXT:    s_addc_u32 s5, s7, 0
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    v_mov_b32_e32 v1, s5
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i16_to_i64_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x2
; WAVE32-NEXT:    s_load_dword s6, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x34
; WAVE32-NEXT:    s_load_dwordx2 s[2:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_and_b32 s4, s6, 0xffff
; WAVE32-NEXT:    s_add_u32 s0, s0, s4
; WAVE32-NEXT:    s_addc_u32 s1, s1, 0
; WAVE32-NEXT:    v_mov_b32_e32 v0, s0
; WAVE32-NEXT:    v_mov_b32_e32 v1, s1
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[2:3]
; WAVE32-NEXT:    s_endpgm
  %zext = zext i16 %a to i64
  %res = add i64 %b, %zext
  store i64 %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @zext_i16_to_i32_divergent(ptr addrspace(1) %out, i16 %a, i32 %b) {
; WAVE64-LABEL: zext_i16_to_i32_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_add_i32_e32 v0, vcc, s6, v0
; WAVE64-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i16_to_i32_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v1, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_add_nc_u16 v0, s2, v0
; WAVE32-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; WAVE32-NEXT:    global_store_dword v1, v0, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid.truncated = trunc i32 %tid to i16
  %divergent.a = add i16 %a, %tid.truncated
  %zext = zext i16 %divergent.a to i32
  store i32 %zext, ptr addrspace(1) %out
  ret void
}


define amdgpu_kernel void @zext_i16_to_i64_divergent(ptr addrspace(1) %out, i16 %a, i64 %b) {
; WAVE64-LABEL: zext_i16_to_i64_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    v_mov_b32_e32 v1, 0
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_add_i32_e32 v0, vcc, s6, v0
; WAVE64-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; WAVE64-NEXT:    buffer_store_dwordx2 v[0:1], off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i16_to_i64_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v2, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_add_nc_u16 v0, s2, v0
; WAVE32-NEXT:    s_mov_b32 s2, 0
; WAVE32-NEXT:    v_mov_b32_e32 v1, s2
; WAVE32-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; WAVE32-NEXT:    global_store_dwordx2 v2, v[0:1], s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %tid.truncated = trunc i32 %tid to i16
  %divergent.a = add i16 %a, %tid.truncated
  %zext = zext i16 %divergent.a to i64
  store i64 %zext, ptr addrspace(1) %out
  ret void
}

; Test that uniform i1 -> i32 zext uses SALU S_AND_B32 instead of
; VALU V_CNDMASK_B32 + V_READFIRSTLANE roundtrip
define amdgpu_kernel void @zext_i1_to_i32_uniform(ptr addrspace(1) %out, i32 %a, i32 %b) {
; WAVE64-LABEL: zext_i1_to_i32_uniform:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    s_mov_b64 s[4:5], s[2:3]
; WAVE64-NEXT:    s_cmp_eq_u32 s4, s5
; WAVE64-NEXT:    s_cselect_b64 s[4:5], -1, 0
; WAVE64-NEXT:    s_and_b32 s4, s4, 1
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    v_mov_b32_e32 v0, s4
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i1_to_i32_uniform:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v0, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    s_cmp_eq_u32 s2, s3
; WAVE32-NEXT:    s_cselect_b32 s2, -1, 0
; WAVE32-NEXT:    s_and_b32 s2, s2, 1
; WAVE32-NEXT:    v_mov_b32_e32 v1, s2
; WAVE32-NEXT:    global_store_dword v0, v1, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %cmp = icmp eq i32 %a, %b
  %ext = zext i1 %cmp to i32
  store i32 %ext, ptr addrspace(1) %out
  ret void
}

; Test that divergent i1 -> i32 zext still uses V_CNDMASK_B32
define amdgpu_kernel void @zext_i1_to_i32_divergent(ptr addrspace(1) %out, i32 %a) {
; WAVE64-LABEL: zext_i1_to_i32_divergent:
; WAVE64:       ; %bb.0:
; WAVE64-NEXT:    s_load_dword s6, s[4:5], 0xb
; WAVE64-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x9
; WAVE64-NEXT:    s_mov_b32 s3, 0xf000
; WAVE64-NEXT:    s_mov_b32 s2, -1
; WAVE64-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE64-NEXT:    v_cmp_eq_u32_e32 vcc, s6, v0
; WAVE64-NEXT:    v_cndmask_b32_e64 v0, 0, 1, vcc
; WAVE64-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; WAVE64-NEXT:    s_endpgm
;
; WAVE32-LABEL: zext_i1_to_i32_divergent:
; WAVE32:       ; %bb.0:
; WAVE32-NEXT:    s_clause 0x1
; WAVE32-NEXT:    s_load_dword s2, s[4:5], 0x2c
; WAVE32-NEXT:    s_load_dwordx2 s[0:1], s[4:5], 0x24
; WAVE32-NEXT:    v_mov_b32_e32 v1, 0
; WAVE32-NEXT:    s_waitcnt lgkmcnt(0)
; WAVE32-NEXT:    v_cmp_eq_u32_e32 vcc_lo, s2, v0
; WAVE32-NEXT:    v_cndmask_b32_e64 v0, 0, 1, vcc_lo
; WAVE32-NEXT:    global_store_dword v1, v0, s[0:1]
; WAVE32-NEXT:    s_endpgm
  %tid = call i32 @llvm.amdgcn.workitem.id.x()
  %cmp = icmp eq i32 %tid, %a
  %ext = zext i1 %cmp to i32
  store i32 %ext, ptr addrspace(1) %out
  ret void
}

declare i32 @llvm.amdgcn.workitem.id.x() #1

attributes #0 = { nounwind }
attributes #1 = { nounwind readnone speculatable }
