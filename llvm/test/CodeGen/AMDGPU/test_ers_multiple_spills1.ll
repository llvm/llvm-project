; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=50 < %s 2>&1 | FileCheck %s

@array2 = global [5 x i32] zeroinitializer, align 4
@array3 = global [5 x i32] zeroinitializer, align 4
@array4 = global [5 x i32] zeroinitializer, align 4
@array5 = global [5 x i32] zeroinitializer, align 4

@array6 = global [5 x i32] zeroinitializer, align 4
@array7 = global [5 x i32] zeroinitializer, align 4
@array8 = global [5 x i32] zeroinitializer, align 4
@array9 = global [5 x i32] zeroinitializer, align 4

;      bb.0.entry
;       /     |
;  bb.3.bb2   |
;       \     |
;      bb.1.Flow3
;       /     |
;  bb.2.bb1   |
;       \     |
;      bb.4.bb3
;       /     |
;  bb.7.bb5   |
;       \     |
;      bb.5.Flow2
;       /     |
;  bb.6.bb4   |
;       \     |
;      bb.8.bb6
;       /     |
;  bb.11.bb8  |
;       \     |
;      bb.9.Flow
;       /     |
;  bb.10.bb7  |
;       \     |
;      bb.12.Flow1
;       /     |
;  bb.13.bb9  |
;       \     |
;      bb.14.bb10
;
define amdgpu_ps void @test(ptr addrspace(1) %p1, ptr addrspace(3) %p2, ptr addrspace(1) %p3, ptr addrspace(1) %p4, ptr addrspace(1) %p5, ptr addrspace(1) %p6, ptr addrspace(1) %p7, ptr addrspace(1) %p8, ptr addrspace(1) %p9, ptr addrspace(1) %p10) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13, $vgpr14, $vgpr15, $vgpr16, $vgpr17, $vgpr18
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr18
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr17
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr16
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr15
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr14
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr13
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr12
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr11
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr10
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY $vgpr9
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY $vgpr8
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY $vgpr7
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY $vgpr6
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY $vgpr5
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY5]], %subreg.sub0, [[COPY4]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY18]], %subreg.sub0, [[COPY17]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY11]], %subreg.sub0, [[COPY10]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY13]], %subreg.sub0, [[COPY12]], %subreg.sub1
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE3]], 0, 0, implicit $exec :: (load (s16) from %ir.p4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE3]], 2, 0, implicit $exec :: (load (s16) from %ir.p4 + 2, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_USHORT1]], 16, [[GLOBAL_LOAD_USHORT]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE2]], 0, 0, implicit $exec :: (load (s32) from %ir.p5, align 8, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (load (s32) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 12, 0, implicit $exec :: (load (s8) from %ir.gep1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 13, 0, implicit $exec :: (load (s8) from %ir.gep1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE1]], 8, [[GLOBAL_LOAD_UBYTE]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 14, 0, implicit $exec :: (load (s8) from %ir.gep1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE3:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 15, 0, implicit $exec :: (load (s8) from %ir.gep1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE3]], 8, [[GLOBAL_LOAD_UBYTE2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_LSHL_OR_B32_e64_]], [[GLOBAL_LOAD_DWORD]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE1]], [[V_ADD_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[V_CMP_LT_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_LT_U32_e64 [[GLOBAL_LOAD_DWORD]], [[V_ADD_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_LT_U32_e64_]], %bb.1, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.Flow6:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.4(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:vgpr_32 = PHI undef %63:vgpr_32, %bb.0, %13, %bb.3
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI undef %63:vgpr_32, %bb.0, %12, %bb.3
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI undef %63:vgpr_32, %bb.0, %14, %bb.3
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY15]], %subreg.sub0, [[COPY14]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_2]], 16, [[V_LSHL_OR_B32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF]], %bb.4, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb1:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX4_:%[0-9]+]]:vreg_128 = GLOBAL_LOAD_DWORDX4 [[REG_SEQUENCE4]], 0, 0, implicit $exec :: (load (s128) from %ir.p3, align 4, addrspace 1)
  ; CHECK-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_]].sub2
  ; CHECK-NEXT:   [[COPY20:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_]].sub0
  ; CHECK-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_]].sub1
  ; CHECK-NEXT:   [[V_ADD_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[GLOBAL_LOAD_DWORDX4_]].sub0, [[GLOBAL_LOAD_DWORDX4_]].sub1, 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_1]], [[GLOBAL_LOAD_DWORDX4_]].sub2, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE4]], [[V_MUL_LO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p3, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE]], [[V_ADD_U32_e64_1]], 0, 0, implicit $exec :: (store (s32) into %ir.p8, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.bb2:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY16]], 0, 0, implicit $exec :: (load (s8) from %ir.p2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY16]], 1, 0, implicit $exec :: (load (s8) from %ir.p2 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY16]], 2, 0, implicit $exec :: (load (s8) from %ir.p2 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY16]], 3, 0, implicit $exec :: (load (s8) from %ir.p2 + 3, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_1]], 8, [[DS_READ_U8_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_3]], 8, [[DS_READ_U8_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_5]], 16, [[V_LSHL_OR_B32_e64_4]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE4:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 4, 0, implicit $exec :: (load (s8) from %ir.p8 + 4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE5:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 5, 0, implicit $exec :: (load (s8) from %ir.p8 + 5, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE5]], 8, [[GLOBAL_LOAD_UBYTE4]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE6:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 6, 0, implicit $exec :: (load (s8) from %ir.p8 + 6, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE7:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 7, 0, implicit $exec :: (load (s8) from %ir.p8 + 7, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_8:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE7]], 8, [[GLOBAL_LOAD_UBYTE6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_9:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_8]], 16, [[V_LSHL_OR_B32_e64_7]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE8:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 8, 0, implicit $exec :: (load (s8) from %ir.p8 + 8, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE9:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 9, 0, implicit $exec :: (load (s8) from %ir.p8 + 9, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_10:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE9]], 8, [[GLOBAL_LOAD_UBYTE8]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE10:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 10, 0, implicit $exec :: (load (s8) from %ir.p8 + 10, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE11:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 11, 0, implicit $exec :: (load (s8) from %ir.p8 + 11, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_11:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE11]], 8, [[GLOBAL_LOAD_UBYTE10]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_12:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_11]], 16, [[V_LSHL_OR_B32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_12]], %subreg.sub0, undef %448:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %118:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[V_LSHL_OR_B32_e64_6]], [[V_LSHL_OR_B32_e64_9]], [[REG_SEQUENCE5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %118.sub0, [[GLOBAL_LOAD_DWORD1]], 0, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.bb3:
  ; CHECK-NEXT:   successors: %bb.7(0x40000000), %bb.5(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[PHI2]], %bb.1, [[COPY20]], %bb.2
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_]], %bb.1, [[COPY20]], %bb.2
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI [[PHI1]], %bb.1, [[COPY21]], %bb.2
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI [[PHI]], %bb.1, [[COPY19]], %bb.2
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE6:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_]], %subreg.sub0, undef %446:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %127:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[PHI3]], [[GLOBAL_LOAD_DWORD1]], [[REG_SEQUENCE6]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE7:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI4]], %subreg.sub0, undef %444:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %133:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 %127.sub0, [[V_ADD_U32_e64_]], [[REG_SEQUENCE7]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_1:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %133.sub0, [[PHI5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 %133.sub0, [[PHI6]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI3]], [[PHI5]], implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY22:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY22]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 20)`)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET1:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM1:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET1]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY23:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM1]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY23]], [[V_ADD_U32_e64_2]], 4, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 4)`)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE12:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 20, 0, implicit $exec :: (load (s8) from %ir.p3 + 20, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE13:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 21, 0, implicit $exec :: (load (s8) from %ir.p3 + 21, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_13:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE13]], 8, [[GLOBAL_LOAD_UBYTE12]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE14:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 22, 0, implicit $exec :: (load (s8) from %ir.p3 + 22, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE15:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 23, 0, implicit $exec :: (load (s8) from %ir.p3 + 23, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_14:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE15]], 8, [[GLOBAL_LOAD_UBYTE14]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_15:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_14]], 16, [[V_LSHL_OR_B32_e64_13]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE16:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 0, 0, implicit $exec :: (load (s8) from %ir.p3, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE17:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 1, 0, implicit $exec :: (load (s8) from %ir.p3 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_16:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE17]], 8, [[GLOBAL_LOAD_UBYTE16]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE18:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 2, 0, implicit $exec :: (load (s8) from %ir.p3 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE19:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 3, 0, implicit $exec :: (load (s8) from %ir.p3 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_17:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE19]], 8, [[GLOBAL_LOAD_UBYTE18]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_18:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_17]], 16, [[V_LSHL_OR_B32_e64_16]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE20:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 28, 0, implicit $exec :: (load (s8) from %ir.p3 + 28, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE21:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 29, 0, implicit $exec :: (load (s8) from %ir.p3 + 29, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_19:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE21]], 8, [[GLOBAL_LOAD_UBYTE20]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE22:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 30, 0, implicit $exec :: (load (s8) from %ir.p3 + 30, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE23:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 31, 0, implicit $exec :: (load (s8) from %ir.p3 + 31, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_20:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE23]], 8, [[GLOBAL_LOAD_UBYTE22]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_21:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_20]], 16, [[V_LSHL_OR_B32_e64_19]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE24:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 24, 0, implicit $exec :: (load (s8) from %ir.p3 + 24, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE25:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 25, 0, implicit $exec :: (load (s8) from %ir.p3 + 25, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_22:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE25]], 8, [[GLOBAL_LOAD_UBYTE24]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE26:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 26, 0, implicit $exec :: (load (s8) from %ir.p3 + 26, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE27:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 27, 0, implicit $exec :: (load (s8) from %ir.p3 + 27, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_23:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE27]], 8, [[GLOBAL_LOAD_UBYTE26]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_24:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_23]], 16, [[V_LSHL_OR_B32_e64_22]], implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY22]], 28, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 28)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD1]], [[V_MUL_LO_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_2]], [[PHI5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_4:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI6]], [[FLAT_LOAD_DWORD]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_2:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_4]], [[V_MUL_LO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_5:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_3]], [[V_SUB_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_5]], [[V_LSHL_OR_B32_e64_24]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET2:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM2:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET2]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY24:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM2]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY24]], [[V_ADD_U32_e64_4]], 68, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 68)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET3:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array5, target-flags(amdgpu-gotprel32-hi) @array5, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM3:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET3]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY25:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM3]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD2:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY25]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array5, i64 20)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_6:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD2]], [[V_LSHL_OR_B32_e64_21]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_6]], [[PHI4]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_3:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[FLAT_LOAD_DWORD]], [[V_LSHL_OR_B32_e64_21]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_7:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI6]], [[PHI5]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_6:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_5]], [[V_ADD_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_7:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_6]], [[V_MUL_LO_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE1]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_4:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_LSHL_OR_B32_e64_21]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_8:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_SUB_U32_e64_3]], [[FLAT_LOAD_DWORD1]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET4:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array6, target-flags(amdgpu-gotprel32-hi) @array6, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM4:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET4]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY3]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[COPY26:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM4]]
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY2]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD3:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY26]], 44, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array6, i64 44)`)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY1]], %stack.3, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_8:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD3]], [[V_SUB_U32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET5:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array7, target-flags(amdgpu-gotprel32-hi) @array7, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM5:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET5]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY]], %stack.4, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[COPY27:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM5]]
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY8]], %stack.5, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.5, addrspace 5)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD4:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY27]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array7, i64 20)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET6:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array8, target-flags(amdgpu-gotprel32-hi) @array8, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM6:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET6]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY9]], %stack.6, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.6, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY7]], %stack.7, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.7, addrspace 5)
  ; CHECK-NEXT:   [[COPY28:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM6]]
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY6]], %stack.8, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.8, addrspace 5)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD5:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY28]], 44, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array8, i64 44)`, align 8)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET7:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array9, target-flags(amdgpu-gotprel32-hi) @array9, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM7:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET7]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[GLOBAL_LOAD_DWORD1]], %stack.9, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.9, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE2]], %stack.10, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.10, align 4, addrspace 5)
  ; CHECK-NEXT:   [[COPY29:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM7]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD6:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY29]], 24, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array9, i64 24)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_9:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD6]], [[V_ADD_U32_e64_4]], implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD7:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY22]], 84, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 84)`)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE]], %stack.11, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.11, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_9:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD7]], [[V_MUL_LO_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD8:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY24]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 80)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD9:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY23]], 80, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 80)`, align 8)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD10:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY25]], 88, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array5, i64 88)`)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_15]], %stack.12, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.12, addrspace 5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_10:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD10]], [[V_ADD_U32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD11:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY28]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array8, i64 20)`)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_10:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD11]], [[V_MUL_LO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD12:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY23]], 8, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 8)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD13:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY22]], 12, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 12)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD14:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY24]], 4, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 4)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD15:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY25]], 4, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array5, i64 4)`)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_11:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD15]], [[V_MUL_LO_U32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE28:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 16, 0, implicit $exec :: (load (s8) from %ir.p4 + 16, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[FLAT_LOAD_DWORD15]], %stack.13, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.13, addrspace 5)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE29:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 17, 0, implicit $exec :: (load (s8) from %ir.p4 + 17, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_18]], %stack.14, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.14, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_25:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE29]], 8, [[GLOBAL_LOAD_UBYTE28]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE30:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 18, 0, implicit $exec :: (load (s8) from %ir.p4 + 18, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE31:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 19, 0, implicit $exec :: (load (s8) from %ir.p4 + 19, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[COPY26]], %stack.15, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.15, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_26:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE31]], 8, [[GLOBAL_LOAD_UBYTE30]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_27:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_26]], 16, [[V_LSHL_OR_B32_e64_25]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE32:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 12, 0, implicit $exec :: (load (s8) from %ir.p4 + 12, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE33:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 13, 0, implicit $exec :: (load (s8) from %ir.p4 + 13, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_28:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE33]], 8, [[GLOBAL_LOAD_UBYTE32]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE34:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 14, 0, implicit $exec :: (load (s8) from %ir.p4 + 14, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE35:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 15, 0, implicit $exec :: (load (s8) from %ir.p4 + 15, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_29:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE35]], 8, [[GLOBAL_LOAD_UBYTE34]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_30:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_29]], 16, [[V_LSHL_OR_B32_e64_28]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_12:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_LSHL_OR_B32_e64_30]], [[V_SUB_U32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_LSHL_OR_B32_e64_27]], [[V_ADD_U32_e64_5]], [[V_MUL_LO_U32_e64_12]], implicit $exec
  ; CHECK-NEXT:   [[V_CVT_F32_U32_e64_:%[0-9]+]]:vgpr_32 = V_CVT_F32_U32_e64 [[V_MUL_LO_U32_e64_11]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_RCP_IFLAG_F32_e64_:%[0-9]+]]:vgpr_32 = nofpexcept V_RCP_IFLAG_F32_e64 0, [[V_CVT_F32_U32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_F32_e64_:%[0-9]+]]:vgpr_32 = nnan ninf nsz arcp contract afn reassoc nofpexcept V_MUL_F32_e64 0, 1333788670, 0, [[V_RCP_IFLAG_F32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_U32_F32_e64_:%[0-9]+]]:vgpr_32 = nofpexcept V_CVT_U32_F32_e64 0, [[V_MUL_F32_e64_]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_5:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 0, [[V_MUL_LO_U32_e64_11]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_13:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_5]], [[V_CVT_U32_F32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_CVT_U32_F32_e64_]], [[V_MUL_LO_U32_e64_13]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_11:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CVT_U32_F32_e64_]], [[V_MUL_HI_U32_e64_]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_ADD3_U32_e64_]], [[V_ADD_U32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_14:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_HI_U32_e64_1]], [[V_MUL_LO_U32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_6:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD3_U32_e64_]], [[V_MUL_LO_U32_e64_14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_SUB_U32_e64_6]], [[V_MUL_LO_U32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_12:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_MUL_HI_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MUL_HI_U32_e64_1]], 0, [[V_ADD_U32_e64_12]], [[V_CMP_GE_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_7:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_6]], [[V_MUL_LO_U32_e64_11]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_SUB_U32_e64_6]], 0, [[V_SUB_U32_e64_7]], [[V_CMP_GE_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_CNDMASK_B32_e64_1]], [[V_MUL_LO_U32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_13:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_CNDMASK_B32_e64_]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_2:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_CNDMASK_B32_e64_]], 0, [[V_ADD_U32_e64_13]], [[V_CMP_GE_U32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[FLAT_LOAD_DWORD14]], [[V_ADD_U32_e64_2]], [[V_CNDMASK_B32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_15:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD3_U32_e64_1]], [[FLAT_LOAD_DWORD10]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_8:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_15]], [[FLAT_LOAD_DWORD6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[FLAT_LOAD_DWORD13]], [[V_ADD_U32_e64_4]], [[V_SUB_U32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE8:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_SUB_U32_e64_1]], %subreg.sub0, undef %442:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %283:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[V_ADD3_U32_e64_2]], [[V_MUL_LO_U32_e64_1]], [[REG_SEQUENCE8]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_9:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %283.sub0, [[V_LSHL_OR_B32_e64_24]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_F32_U32_e64_1:%[0-9]+]]:vgpr_32 = V_CVT_F32_U32_e64 [[V_LSHL_OR_B32_e64_21]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_RCP_IFLAG_F32_e64_1:%[0-9]+]]:vgpr_32 = nofpexcept V_RCP_IFLAG_F32_e64 0, [[V_CVT_F32_U32_e64_1]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_F32_e64_1:%[0-9]+]]:vgpr_32 = nnan ninf nsz arcp contract afn reassoc nofpexcept V_MUL_F32_e64 0, 1333788670, 0, [[V_RCP_IFLAG_F32_e64_1]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_U32_F32_e64_1:%[0-9]+]]:vgpr_32 = nofpexcept V_CVT_U32_F32_e64 0, [[V_MUL_F32_e64_1]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_10:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 0, [[V_LSHL_OR_B32_e64_21]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_16:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_10]], [[V_CVT_U32_F32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_CVT_U32_F32_e64_1]], [[V_MUL_LO_U32_e64_16]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_14:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CVT_U32_F32_e64_1]], [[V_MUL_HI_U32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_SUB_U32_e64_9]], [[V_ADD_U32_e64_14]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_17:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_HI_U32_e64_3]], [[V_LSHL_OR_B32_e64_21]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_11:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_9]], [[V_MUL_LO_U32_e64_17]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_2:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_SUB_U32_e64_11]], [[V_LSHL_OR_B32_e64_21]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_15:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_MUL_HI_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_3:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MUL_HI_U32_e64_3]], 0, [[V_ADD_U32_e64_15]], [[V_CMP_GE_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_12:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_11]], [[V_LSHL_OR_B32_e64_21]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_4:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_SUB_U32_e64_11]], 0, [[V_SUB_U32_e64_12]], [[V_CMP_GE_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_CNDMASK_B32_e64_4]], [[V_LSHL_OR_B32_e64_21]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_16:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_CNDMASK_B32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_5:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_CNDMASK_B32_e64_3]], 0, [[V_ADD_U32_e64_16]], [[V_CMP_GE_U32_e64_3]], implicit $exec
  ; CHECK-NEXT:   DS_WRITE_B8_D16_HI [[COPY16]], [[V_CNDMASK_B32_e64_5]], 2, 0, implicit $exec :: (store (s8) into %ir.p2 + 2, addrspace 3)
  ; CHECK-NEXT:   DS_WRITE_B8_gfx9 [[COPY16]], [[V_CNDMASK_B32_e64_5]], 0, 0, implicit $exec :: (store (s8) into %ir.p2, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHRREV_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHRREV_B32_e64 24, [[V_CNDMASK_B32_e64_5]], implicit $exec
  ; CHECK-NEXT:   DS_WRITE_B8_gfx9 [[COPY16]], [[V_LSHRREV_B32_e64_]], 3, 0, implicit $exec :: (store (s8) into %ir.p2 + 3, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHRREV_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHRREV_B32_e64 8, [[V_CNDMASK_B32_e64_5]], implicit $exec
  ; CHECK-NEXT:   DS_WRITE_B8_gfx9 [[COPY16]], [[V_LSHRREV_B32_e64_1]], 1, 0, implicit $exec :: (store (s8) into %ir.p2 + 1, addrspace 3)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_13:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_CNDMASK_B32_e64_5]], [[FLAT_LOAD_DWORD11]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE9:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[FLAT_LOAD_DWORD5]], %subreg.sub0, undef %440:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %313:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[V_SUB_U32_e64_13]], [[V_LSHL_OR_B32_e64_27]], [[REG_SEQUENCE9]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE10:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[FLAT_LOAD_DWORD]], %subreg.sub0, undef %438:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %319:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 %313.sub0, [[FLAT_LOAD_DWORD1]], [[REG_SEQUENCE10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_14:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %319.sub0, [[V_MUL_LO_U32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE11:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_SUB_U32_e64_14]], %subreg.sub0, undef %436:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %325:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[FLAT_LOAD_DWORD12]], [[V_ADD_U32_e64_2]], [[REG_SEQUENCE11]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_18:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 %325.sub0, [[V_ADD_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[V_CVT_F32_U32_e64_2:%[0-9]+]]:vgpr_32 = V_CVT_F32_U32_e64 [[V_MUL_LO_U32_e64_10]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_RCP_IFLAG_F32_e64_2:%[0-9]+]]:vgpr_32 = nofpexcept V_RCP_IFLAG_F32_e64 0, [[V_CVT_F32_U32_e64_2]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_F32_e64_2:%[0-9]+]]:vgpr_32 = nnan ninf nsz arcp contract afn reassoc nofpexcept V_MUL_F32_e64 0, 1333788670, 0, [[V_RCP_IFLAG_F32_e64_2]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_U32_F32_e64_2:%[0-9]+]]:vgpr_32 = nofpexcept V_CVT_U32_F32_e64 0, [[V_MUL_F32_e64_2]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_15:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 0, [[V_MUL_LO_U32_e64_10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_19:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_15]], [[V_CVT_U32_F32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_4:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_CVT_U32_F32_e64_2]], [[V_MUL_LO_U32_e64_19]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_17:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CVT_U32_F32_e64_2]], [[V_MUL_HI_U32_e64_4]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_5:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_MUL_LO_U32_e64_18]], [[V_ADD_U32_e64_17]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_20:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_HI_U32_e64_5]], [[V_MUL_LO_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_16:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_18]], [[V_MUL_LO_U32_e64_20]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_4:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_SUB_U32_e64_16]], [[V_MUL_LO_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_18:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_MUL_HI_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_6:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MUL_HI_U32_e64_5]], 0, [[V_ADD_U32_e64_18]], [[V_CMP_GE_U32_e64_4]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_17:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_16]], [[V_MUL_LO_U32_e64_10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_7:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_SUB_U32_e64_16]], 0, [[V_SUB_U32_e64_17]], [[V_CMP_GE_U32_e64_4]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_CNDMASK_B32_e64_7]], [[V_MUL_LO_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_19:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_CNDMASK_B32_e64_6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_8:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_CNDMASK_B32_e64_6]], 0, [[V_ADD_U32_e64_19]], [[V_CMP_GE_U32_e64_5]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[FLAT_LOAD_DWORD9]], [[V_MUL_LO_U32_e64_4]], [[V_CNDMASK_B32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_18:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_2]], [[FLAT_LOAD_DWORD8]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD3_U32_e64_3]], [[V_SUB_U32_e64_18]], [[V_ADD_U32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_21:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD3_U32_e64_4]], [[V_MUL_LO_U32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[FLAT_LOAD_DWORD5]], [[V_MUL_LO_U32_e64_6]], [[V_MUL_LO_U32_e64_21]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_19:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_5]], [[FLAT_LOAD_DWORD4]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_6:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD3_U32_e64_5]], [[V_SUB_U32_e64_19]], [[V_MUL_LO_U32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_22:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD3_U32_e64_6]], [[V_ADD_U32_e64_2]], implicit $exec
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY27]], [[V_MUL_LO_U32_e64_22]], 68, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array7, i64 68)`)
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_7:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[FLAT_LOAD_DWORD]], [[V_ADD_U32_e64_4]], [[V_MUL_LO_U32_e64_22]], implicit $exec
  ; CHECK-NEXT:   [[V_CVT_F32_U32_e64_3:%[0-9]+]]:vgpr_32 = V_CVT_F32_U32_e64 [[V_ADD_U32_e64_8]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_RCP_IFLAG_F32_e64_3:%[0-9]+]]:vgpr_32 = nofpexcept V_RCP_IFLAG_F32_e64 0, [[V_CVT_F32_U32_e64_3]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_F32_e64_3:%[0-9]+]]:vgpr_32 = nnan ninf nsz arcp contract afn reassoc nofpexcept V_MUL_F32_e64 0, 1333788670, 0, [[V_RCP_IFLAG_F32_e64_3]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_CVT_U32_F32_e64_3:%[0-9]+]]:vgpr_32 = nofpexcept V_CVT_U32_F32_e64 0, [[V_MUL_F32_e64_3]], 0, 0, implicit $mode, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_20:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 0, [[V_ADD_U32_e64_8]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_23:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_20]], [[V_CVT_U32_F32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_6:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_CVT_U32_F32_e64_3]], [[V_MUL_LO_U32_e64_23]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_20:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_CVT_U32_F32_e64_3]], [[V_MUL_HI_U32_e64_6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_7:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 [[V_ADD3_U32_e64_7]], [[V_ADD_U32_e64_20]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_24:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_HI_U32_e64_7]], [[V_ADD_U32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_21:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD3_U32_e64_7]], [[V_MUL_LO_U32_e64_24]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_6:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_SUB_U32_e64_21]], [[V_ADD_U32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_21:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_MUL_HI_U32_e64_7]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_9:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_MUL_HI_U32_e64_7]], 0, [[V_ADD_U32_e64_21]], [[V_CMP_GE_U32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_22:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_21]], [[V_ADD_U32_e64_8]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_10:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_SUB_U32_e64_21]], 0, [[V_SUB_U32_e64_22]], [[V_CMP_GE_U32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_7:%[0-9]+]]:sreg_32_xm0_xexec = V_CMP_GE_U32_e64 [[V_CNDMASK_B32_e64_10]], [[V_ADD_U32_e64_8]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_22:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_CNDMASK_B32_e64_9]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CNDMASK_B32_e64_11:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, [[V_CNDMASK_B32_e64_9]], 0, [[V_ADD_U32_e64_22]], [[V_CMP_GE_U32_e64_7]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_25:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_CNDMASK_B32_e64_11]], [[V_SUB_U32_e64_4]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_8:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_MUL_LO_U32_e64_25]], [[V_ADD_U32_e64_7]], [[V_ADD_U32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_26:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD3_U32_e64_8]], [[V_MUL_LO_U32_e64_7]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_27:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_LO_U32_e64_26]], [[V_LSHL_OR_B32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD16:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY24]], 84, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 84)`)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_23:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD16]], [[V_LSHL_OR_B32_e64_27]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_23:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_23]], [[V_MUL_LO_U32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE12:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_MUL_LO_U32_e64_27]], %subreg.sub0, [[V_SUB_U32_e64_23]], %subreg.sub1
  ; CHECK-NEXT:   FLAT_STORE_DWORDX2 [[COPY23]], [[REG_SEQUENCE12]], 76, 0, implicit $exec, implicit $flat_scr :: (store (s64) into `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 76)`, align 4)
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.15, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.15, align 4, addrspace 5)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD17:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[SI_SPILL_V64_RESTORE]], 28, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array6, i64 28)`)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.14, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.14, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_24:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[FLAT_LOAD_DWORD17]], [[SI_SPILL_V32_RESTORE]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.12, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.12, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_24:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[SI_SPILL_V32_RESTORE1]], [[V_ADD_U32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32 = V_ADD_CO_U32_e64 [[V_ADD_U32_e64_24]], [[V_SUB_U32_e64_24]], 0, implicit $exec
  ; CHECK-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32 = S_XOR_B32 [[V_ADD_CO_U32_e64_1]], -1, implicit-def dead $scc
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_28:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_CO_U32_e64_]], [[V_MUL_LO_U32_e64_7]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE1:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.11, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.11, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[SI_SPILL_V64_RESTORE1]], [[V_MUL_LO_U32_e64_28]], 2, 0, implicit $exec :: (store (s16) into %ir.p8 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[SI_SPILL_V64_RESTORE1]], [[V_MUL_LO_U32_e64_28]], 0, 0, implicit $exec :: (store (s16) into %ir.p8, addrspace 1)
  ; CHECK-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32 = SI_IF [[S_XOR_B32_]], %bb.5, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.7
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.Flow5:
  ; CHECK-NEXT:   successors: %bb.6(0x40000000), %bb.8(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vreg_64 = PHI [[REG_SEQUENCE3]], %bb.4, undef %464:vreg_64, %bb.7
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI [[V_SUB_U32_e64_24]], %bb.4, undef %466:vgpr_32, %bb.7
  ; CHECK-NEXT:   [[SI_ELSE1:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF1]], %bb.8, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.6
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.bb4:
  ; CHECK-NEXT:   successors: %bb.8(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[PHI7]], [[PHI8]], 0, 0, implicit $exec :: (store (s32) into %ir.p4, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.8
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.7.bb5:
  ; CHECK-NEXT:   successors: %bb.5(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE2:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.10, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.10, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE2]], [[V_ADD_CO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p5, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.8.bb6:
  ; CHECK-NEXT:   successors: %bb.11(0x40000000), %bb.9(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE1]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.9, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.9, addrspace 5)
  ; CHECK-NEXT:   [[V_CMP_GT_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_U32_e64 [[SI_SPILL_V32_RESTORE2]], [[FLAT_LOAD_DWORD16]], implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE3:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.5, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.5, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE4:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.6, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.6, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF2:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_GT_U32_e64_]], %bb.9, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.9.Flow:
  ; CHECK-NEXT:   successors: %bb.10(0x40000000), %bb.12(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.8, %458, %bb.11
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE4]], %bb.8, undef %468:vgpr_32, %bb.11
  ; CHECK-NEXT:   [[PHI11:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE3]], %bb.8, undef %470:vgpr_32, %bb.11
  ; CHECK-NEXT:   [[PHI12:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_24]], %bb.8, undef %472:vgpr_32, %bb.11
  ; CHECK-NEXT:   [[PHI13:%[0-9]+]]:vreg_64 = PHI %319, %bb.8, undef %474:vreg_64, %bb.11
  ; CHECK-NEXT:   [[SI_ELSE2:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF2]], %bb.12, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.10
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.10.bb7:
  ; CHECK-NEXT:   successors: %bb.12(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[REG_SEQUENCE13:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI10]], %subreg.sub0, [[PHI11]], %subreg.sub1
  ; CHECK-NEXT:   [[V_ADD_U32_e64_25:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI12]], [[PHI13]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE13]], [[V_ADD_U32_e64_25]], 0, 0, implicit $exec :: (store (s32) into %ir.p6, addrspace 1)
  ; CHECK-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32 = S_OR_B32 [[PHI9]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   S_BRANCH %bb.12
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.11.bb8:
  ; CHECK-NEXT:   successors: %bb.9(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE5:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.7, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.7, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE6:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.8, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.8, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE14:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE5]], %subreg.sub0, [[SI_SPILL_V32_RESTORE6]], %subreg.sub1
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE7:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.13, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.13, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_26:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_28]], [[SI_SPILL_V32_RESTORE7]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE14]], [[V_ADD_U32_e64_26]], 0, 0, implicit $exec :: (store (s32) into %ir.p7, addrspace 1)
  ; CHECK-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 [[V_ADD_CO_U32_e64_1]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[COPY30:%[0-9]+]]:sreg_32 = COPY [[S_AND_B32_]]
  ; CHECK-NEXT:   S_BRANCH %bb.9
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.12.Flow4:
  ; CHECK-NEXT:   successors: %bb.13(0x40000000), %bb.14(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI14:%[0-9]+]]:sreg_32 = PHI [[PHI9]], %bb.9, [[S_OR_B32_]], %bb.10
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE2]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE8:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE9:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE15:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE8]], %subreg.sub0, [[SI_SPILL_V32_RESTORE9]], %subreg.sub1
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE10:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE11:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE16:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[SI_SPILL_V32_RESTORE10]], %subreg.sub0, [[SI_SPILL_V32_RESTORE11]], %subreg.sub1
  ; CHECK-NEXT:   [[SI_IF3:%[0-9]+]]:sreg_32 = SI_IF [[PHI14]], %bb.14, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.13
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.13.bb9:
  ; CHECK-NEXT:   successors: %bb.14(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_U32_e64_27:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[SI_SPILL_V32_RESTORE]], [[V_LSHL_OR_B32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE3:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE3]], [[V_ADD_U32_e64_27]], 0, 0, implicit $exec :: (store (s32) into %ir.p1, addrspace 1)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.14.bb10:
  ; CHECK-NEXT:   SI_END_CF [[SI_IF3]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE36:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE15]], 0, 0, implicit $exec :: (load (s8) from %ir.p10, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE37:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE15]], 1, 0, implicit $exec :: (load (s8) from %ir.p10 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_31:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE37]], 8, [[GLOBAL_LOAD_UBYTE36]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE38:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE15]], 2, 0, implicit $exec :: (load (s8) from %ir.p10 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE39:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE15]], 3, 0, implicit $exec :: (load (s8) from %ir.p10 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_32:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE39]], 8, [[GLOBAL_LOAD_UBYTE38]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_33:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_32]], 16, [[V_LSHL_OR_B32_e64_31]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_28:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_LSHL_OR_B32_e64_33]], [[V_LSHL_OR_B32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE16]], [[V_ADD_U32_e64_28]], 0, 0, implicit $exec :: (store (s32) into %ir.p9, addrspace 1)
  ; CHECK-NEXT:   S_ENDPGM 0
entry:
  %ld = load i32, ptr addrspace(1) %p4, align 2
  %ld0 = load i32, ptr addrspace(1) %p5, align 8
  %ld1 = load i32, ptr addrspace(1) %p1, align 4
  %gep1 = getelementptr inbounds i32, ptr addrspace(1) %p1, i64 3
  %ld2 = load i32, ptr addrspace(1) %gep1, align 1
  %tmp1 = add i32 %ld, %ld0
  store i32 %tmp1, ptr addrspace(1) %p1
  %cond1 = icmp uge i32 %ld0, %tmp1
  br i1 %cond1, label %bb1, label %bb2

bb1:
  %load1 = load i32, ptr addrspace(1) %p3, align 4
  %load2 = load <8 x i32>, ptr addrspace(1) %p3, align 1
  %extract1 = extractelement < 8 x i32> %load2, i32 1
  %extract2 = extractelement < 8 x i32> %load2, i32 2
  %tmp84 = add i32 %load1, %extract1
  %tmp85 = mul i32 %tmp84, %extract2
  store i32 %tmp85, ptr addrspace(1) %p3
  store i32 %tmp84, ptr addrspace(1) %p8
  br label %bb3

bb2:
  %ld3 = load i32, ptr addrspace(3) %p2, align 1
  %load4 = load <8 x i32>, ptr addrspace(1) %p8, align 1
  %extract11 = extractelement < 8 x i32> %load4, i32 1
  %extract12 = extractelement < 8 x i32> %load4, i32 2
  %tmp70 = mul i32 %ld3, %extract11
  %tmp71 = add i32 %tmp70, %extract12
  %tmp72 = sub i32 %tmp71, %ld1
  br label %bb3

bb3:
  %phi1 = phi i32 [ %load1, %bb1 ], [ %tmp72, %bb2 ]
  %phi2 = phi i32 [ %load1, %bb1 ], [ %tmp1, %bb2 ]
  %phi3 = phi i32 [ %extract1, %bb1 ], [ %extract11, %bb2 ]
  %phi4 = phi i32 [ %extract2, %bb1 ], [ %extract12, %bb2 ]
  %tmp73 = mul i32 %phi1, %ld1
  %tmp74 = add i32 %tmp73, %ld
  %tmp75 = mul i32 %tmp74, %tmp1
  %tmp76 = add i32 %tmp75, %phi2
  %tmp77 = sub i32 %tmp76, %phi3
  %tmp78 = mul i32 %tmp76, %phi4
  %tmp2 = mul i32 %phi1, %phi3
  %idx10 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 1, i64 0
  %val0 = load i32, i32* %idx10, align 4
  %tmp3 = add i32 %val0, %phi4
  %idx20 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 0, i64 1
  store i32 %tmp3, i32 *%idx20
  %load22 = load <8 x i32>, ptr addrspace(1) %p3, align 1
  %extract3 = extractelement < 8 x i32> %load22, i32 6
  %idx12 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 1, i64 2
  %val2 = load i32, i32* %idx12, align 4
  %tmp4 = mul i32 %val2, %tmp2
  %tmp5= add i32 %tmp3, %phi3
  %tmp6 = mul i32 %phi4, %val0
  %tmp7 = sub i32 %tmp6, %tmp4
  %tmp8 = mul i32 %tmp5, %tmp7
  %tmp9 = add i32 %tmp8, %extract3
  %idx22 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 3, i64 2
  store i32 %tmp9, i32 *%idx22
  %extract4 = extractelement < 8 x i32> %load22, i32 7
  %idx13 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 1, i64 0
  %val3 = load i32, i32* %idx13, align 4
  %tmp10 = mul i32 %val3, %extract4
  %tmp11 = add i32 %tmp10, %phi2
  %tmp12 = sub i32 %val0, %extract4
  %tmp13 = mul i32 %phi4, %phi3
  %tmp14 = add i32 %tmp11, %tmp5
  %tmp15 = add i32 %tmp10, %tmp8
  %tmp16 = sub i32 %extract4, %phi4
  %tmp17 = add i32 %tmp12, %val2
  %tmp18 = add i32 %val0, %tmp9
  %idx601 = getelementptr inbounds [5 x i32], [5 x i32]* @array6, i64 2, i64 1
  %val601 = load i32, i32* %idx601, align 1
  %tmp19 = mul i32 %val601, %tmp12
  %idx701 = getelementptr inbounds [5 x i32], [5 x i32]* @array7, i64 1, i64 0
  %val701 = load i32, i32* %idx701, align 2
  %tmp20 = sub i32 %val701, %tmp11
  %idx801 = getelementptr inbounds [5 x i32], [5 x i32]* @array8, i64 2, i64 1
  %val801 = load i32, i32* %idx801, align 8
  %tmp21 = add i32 %val801, %tmp10
  %idx901 = getelementptr inbounds [5 x i32], [5 x i32]* @array9, i64 1, i64 1
  %val901 = load i32, i32* %idx901, align 1
  %tmp22 = mul i32 %val901, %tmp9
  %idx602 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 4, i64 1
  %val602 = load i32, i32* %idx602, align 1
  %tmp23 = add i32 %val602, %tmp8
  %idx702 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 4, i64 0
  %val702 = load i32, i32* %idx702, align 2
  %tmp24 = sub i32 %val702, %tmp7
  %idx802 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 4, i64 0
  %val802 = load i32, i32* %idx802, align 8
  %tmp25 = add i32 %val802, %tmp6
  %idx902 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 4, i64 2
  %val902 = load i32, i32* %idx902, align 1
  %tmp26 = mul i32 %val902, %tmp5
  %idx800 = getelementptr inbounds [5 x i32], [5 x i32]* @array8, i64 1, i64 0
  %val800 = load i32, i32* %idx800, align 4
  %tmp27 = add i32 %val800, %tmp4
  %idx15 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 0, i64 2
  %val5 = load i32, i32* %idx15, align 4
  %tmp28 = mul i32 %val5, %tmp3
  %idx16 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 0, i64 3
  %val6 = load i32, i32* %idx16, align 4
  %tmp206 = add i32 %val6, %tmp9
  %idx17 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 0, i64 1
  %val7 = load i32, i32* %idx17, align 4
  %tmp207 = add i32 %val7, %tmp3
  %idx18 = getelementptr inbounds [5 x i32], [5 x i32]* @array5, i64 0, i64 1
  %val8 = load i32, i32* %idx18, align 4
  %tmp208 = mul i32 %val8, %tmp10
  %load3 = load <8 x i32>, ptr addrspace(1) %p4, align 1
  %extract7 = extractelement < 8 x i32> %load3, i32 4
  %tmp209 = add i32 %extract7, %tmp11
  %extract8 = extractelement < 8 x i32> %load3, i32 3
  %tmp30 = mul i32 %extract8, %tmp12
  %tmp31 = add i32 %tmp30, %tmp209
  %tmp32 = udiv i32 %tmp31, %tmp208
  %tmp33 = add i32 %tmp32, %tmp207
  %tmp34 = mul i32 %tmp33, %val902
  %tmp35 = sub i32 %tmp34, %val901
  %tmp36 = add i32 %tmp35, %tmp206
  %tmp37 = mul i32 %tmp36, %tmp78
  %tmp38 = add i32 %tmp37, %tmp77
  %tmp39 = sub i32 %tmp38, %extract3
  %tmp40 = udiv i32 %tmp39, %extract4
  store i32 %tmp40, ptr addrspace(3) %p2, align 1
  %tmp41 = sub i32 %tmp40, %val800
  %tmp42 = mul i32 %tmp41, %extract7
  %tmp43 = add i32 %tmp42, %val801
  %tmp44 = mul i32 %tmp43, %val2
  %tmp45 = add i32 %tmp44, %val0
  %tmp46 = sub i32 %tmp45, %tmp2
  %tmp47 = add i32 %tmp46, %tmp28
  %tmp48 = mul i32 %tmp47, %tmp27
  %tmp49 = udiv i32 %tmp48, %tmp26
  %tmp50 = add i32 %tmp49, %tmp25
  %tmp51 = sub i32 %tmp50, %tmp24
  %tmp52 = add i32 %tmp51, %tmp23
  %tmp53 = mul i32 %tmp52, %tmp22
  %tmp54 = add i32 %tmp53, %tmp21
  %tmp55 = sub i32 %tmp54, %tmp20
  %tmp56 = add i32 %tmp55, %tmp19
  %tmp57 = mul i32 %tmp56, %tmp3
  %idx700 = getelementptr inbounds [5 x i32], [5 x i32]* @array7, i64 3, i64 2
  store i32 %tmp57, i32 *%idx700
  %tmp58 = add i32 %tmp57, %tmp18
  %tmp59 = udiv i32 %tmp58, %tmp17
  %tmp60 = mul i32 %tmp59, %tmp16
  %tmp61 = add i32 %tmp60, %tmp15
  %tmp62 = add i32 %tmp61, %tmp14
  %tmp63 = mul i32 %tmp62, %tmp13
  %tmp64 = mul i32 %tmp63, %ld2
  %idx23 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 3, i64 4
  store i32 %tmp64, i32 *%idx23
  %extract17 = extractelement < 8 x i32> %load3, i32 4
  %idx14 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 4, i64 1
  %val4 = load i32, i32* %idx14, align 4
  %tmp65 = add i32 %val4, %extract17
  %tmp66 = sub i32 %tmp65, %tmp2
  %idx24 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 3, i64 5
  store i32 %tmp66, i32 *%idx24
  %extract9 = extractelement < 8 x i32> %load22, i32 0
  %idx600 = getelementptr inbounds [5 x i32], [5 x i32]* @array6, i64 1, i64 2
  %val600 = load i32, i32* %idx600, align 4
  %tmp67 = add i32 %val600, %extract9
  %extract10 = extractelement < 8 x i32> %load22, i32 5
  %tmp68 = sub i32 %extract10, %tmp3
  %tmp69 = add i32 %tmp67, %tmp68
  %tmp79 = mul i32 %tmp69, %tmp13
  store i32 %tmp79, ptr addrspace(1) %p8, align 2
  %cond2 = icmp ult i32 %tmp69, %tmp68
  br i1 %cond2, label %bb4, label %bb5

bb4:
  store i32 %tmp68, ptr addrspace(1) %p4
  br label %bb6

bb5:
  store i32 %tmp69, ptr addrspace(1) %p5
  br label %bb6

bb6:
  %tmp80 = mul i32 %tmp66, %ld2
  %cond3 = icmp ule i32 %ld1, %val4
  br i1 %cond3, label %bb7, label %bb8

bb7:
  %tmp81 = add i32 %tmp67, %tmp45
  store i32 %tmp81, ptr addrspace(1) %p6
  br label %bb9

bb8:
  %tmp82 = add i32 %tmp79, %val8
  store i32 %tmp82, ptr addrspace(1) %p7
  %xor = xor i1 %cond2, %cond3
  br i1 %xor, label %bb9, label %bb10

bb9:
  %phi5 = phi i32 [ %tmp81, %bb7], [%tmp82, %bb8]
  %tmp83 = add i32 %extract9, %ld2
  store i32 %tmp83, ptr addrspace(1) %p1
  br label %bb10

bb10:
  %ld10 = load i32, ptr addrspace(1) %p10, align 1
  %tmp90 = add i32 %ld10, %ld2
  store i32 %tmp90, ptr addrspace(1) %p9, align 4
  ret void
}
