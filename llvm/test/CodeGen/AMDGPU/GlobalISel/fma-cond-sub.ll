; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn -global-isel -mcpu=gfx1250 -O2 -verify-machineinstrs < %s | FileCheck -check-prefix=GFX1250 %s
; RUN: llc -mtriple=amdgcn -global-isel -mcpu=gfx90a -O2 -verify-machineinstrs < %s | FileCheck -check-prefix=GFX90A %s
; RUN: llc -mtriple=amdgcn -global-isel -mcpu=gfx90a -O2 -stop-before=amdgpu-pre-ra-optimizations -verify-machineinstrs < %s | FileCheck -check-prefix=GFX90A-MIR %s


; Test conditional subtraction optimization to FMA on gfx9 with FMA.
; The optimization doesn't make sense on gfx12 due to dual-issued v_cndmask.
;
; Patterns optimized:
;   1. Direct fsub:    result = a - (cond ? c : 0.0)
;   2. Canonicalized:  result = a + (cond ? -c : 0.0)
;   3. Negated select: result = a + (-(cond ? c : 0.0))
;
; These are converted to:
;   result = fma((cond ? -1.0, 0.0), c, a)
;
; This saves one v_cndmask per pattern which provides the most benefit in loops.
;
; As the optimization may be reverted by amdgpu-pre-ra-optimizations pass
; GFX90A-MIR checks show MIR on the entry to this pass.

; ============================================================================
; Basic patterns - single basic block
; ============================================================================

; Pattern 1: Direct fsub with select
; result = a - (cond ? c : 0.0)
define double @cond_sub_basic(double %a, double %c, i1 %cond) {
; GFX1250-LABEL: cond_sub_basic:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v4
; GFX1250-NEXT:    v_dual_cndmask_b32 v2, 0, v2 :: v_dual_cndmask_b32 v3, 0, v3
; GFX1250-NEXT:    v_add_f64_e64 v[0:1], v[0:1], -v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_basic:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v4
; GFX90A-NEXT:    v_cndmask_b32_e32 v2, 0, v2, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v3, vcc
; GFX90A-NEXT:    v_add_f64 v[0:1], v[0:1], -v[2:3]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_basic
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY2]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[COPY]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[COPY]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
; Expected conversion to FMA and revert
  %sel = select i1 %cond, double %c, double 0.0
  %result = fsub double %a, %sel
  ret double %result
}

; Pattern 2: fadd with negated select (canonicalized form)
; result = a + (cond ? -c : 0.0)
define double @cond_sub_fadd_neg_select(double %a, double %c, i1 %cond) {
; GFX1250-LABEL: cond_sub_fadd_neg_select:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX1250-NEXT:    v_xor_b32_e32 v3, 0x80000000, v3
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v4
; GFX1250-NEXT:    v_dual_cndmask_b32 v2, 0, v2 :: v_dual_cndmask_b32 v3, 0, v3
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GFX1250-NEXT:    v_add_f64_e32 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_fadd_neg_select:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v4
; GFX90A-NEXT:    v_cndmask_b32_e32 v2, 0, v2, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v3, vcc
; GFX90A-NEXT:    v_add_f64 v[0:1], v[0:1], -v[2:3]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_fadd_neg_select
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY2]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[COPY]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[COPY]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
; Expected conversion to FMA and revert
  %neg_c = fneg double %c
  %sel = select i1 %cond, double %neg_c, double 0.0
  %result = fadd double %a, %sel
  ret double %result
}

; Pattern 3: fadd with fneg of select
; result = a + (-(cond ? c : 0.0))
define double @cond_sub_fadd_fneg(double %a, double %c, i1 %cond) {
; GFX1250-LABEL: cond_sub_fadd_fneg:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v4
; GFX1250-NEXT:    v_dual_cndmask_b32 v2, 0, v2 :: v_dual_cndmask_b32 v3, 0, v3
; GFX1250-NEXT:    v_add_f64_e64 v[0:1], v[0:1], -v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_fadd_fneg:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v4
; GFX90A-NEXT:    v_cndmask_b32_e32 v2, 0, v2, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v3, vcc
; GFX90A-NEXT:    v_add_f64 v[0:1], v[0:1], -v[2:3]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_fadd_fneg
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY2]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[COPY]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[COPY]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
; Expected conversion to FMA and revert
  %sel = select i1 %cond, double %c, double 0.0
  %neg_sel = fneg double %sel
  %result = fadd double %a, %neg_sel
  ret double %result
}

; Test with constant value
define double @cond_sub_constant(double %a, i1 %cond) {
; GFX1250-LABEL: cond_sub_constant:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_dual_mov_b32 v2, 0 :: v_dual_bitop2_b32 v3, 1, v2 bitop3:0x40
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v3
; GFX1250-NEXT:    v_cndmask_b32_e64 v3, 0, 0x40140000, vcc_lo
; GFX1250-NEXT:    v_add_f64_e64 v[0:1], v[0:1], -v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_constant:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v3, 1, v2
; GFX90A-NEXT:    v_mov_b32_e32 v4, 0x40140000
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v3
; GFX90A-NEXT:    v_mov_b32_e32 v2, 0
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v4, vcc
; GFX90A-NEXT:    v_add_f64 v[0:1], v[0:1], -v[2:3]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_constant
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY1]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 1075052544, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = nofpexcept V_ADD_F64_e64 0, [[COPY]], 1, [[AV_MOV_]], 0, 0, implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[V_ADD_F64_e64_]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[V_ADD_F64_e64_]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
; Constants aren't expected.
  %sel = select i1 %cond, double 5.0, double 0.0
  %result = fsub double %a, %sel
  ret double %result
}

; ============================================================================
; Multiple patterns in single basic block
; ============================================================================

; Two independent conditional subtractions
define double @two_cond_sub(double %a, double %b, double %c1, double %c2, i1 %cond1, i1 %cond2) {
; GFX1250-LABEL: two_cond_sub:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v2, 1, v8
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v2
; GFX1250-NEXT:    v_dual_cndmask_b32 v2, 0, v4 :: v_dual_cndmask_b32 v3, 0, v5
; GFX1250-NEXT:    v_dual_add_f64 v[0:1], v[0:1], -v[2:3] :: v_dual_bitop2_b32 v2, 1, v9 bitop3:0x40
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v2
; GFX1250-NEXT:    v_dual_cndmask_b32 v2, 0, v6 :: v_dual_cndmask_b32 v3, 0, v7
; GFX1250-NEXT:    v_add_f64_e64 v[0:1], v[0:1], -v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: two_cond_sub:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v3, 1, v8
; GFX90A-NEXT:    v_mov_b32_e32 v8, 0xbff00000
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v3
; GFX90A-NEXT:    v_mov_b32_e32 v2, 0
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v8, vcc
; GFX90A-NEXT:    v_fmac_f64_e32 v[0:1], v[2:3], v[4:5]
; GFX90A-NEXT:    v_and_b32_e32 v3, 1, v9
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v3
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, 0, v8, vcc
; GFX90A-NEXT:    v_fmac_f64_e32 v[0:1], v[2:3], v[6:7]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: two_cond_sub
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr5
; GFX90A-MIR-NEXT:   undef [[COPY2:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr8
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr9
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY3]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY1]], [[COPY]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_1:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY4]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY2]], [[COPY]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[COPY]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[COPY]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
; Constants are shared between patterns, expecting FMA for both.
  %sel1 = select i1 %cond1, double %c1, double 0.0
  %tmp = fsub double %a, %sel1
  %sel2 = select i1 %cond2, double %c2, double 0.0
  %result = fsub double %tmp, %sel2
  ret double %result
}

; Two conditional subtractions with different base values
define void @two_cond_sub_different_base(ptr %out1, ptr %out2, double %a, double %b, double %c1, double %c2, i1 %cond1, i1 %cond2) {
; GFX1250-LABEL: two_cond_sub_different_base:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v12, 1, v12
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v12
; GFX1250-NEXT:    v_dual_cndmask_b32 v8, 0, v8, vcc_lo :: v_dual_bitop2_b32 v13, 1, v13 bitop3:0x40
; GFX1250-NEXT:    v_cmp_ne_u32_e64 s0, 0, v13
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_dual_cndmask_b32 v9, 0, v9, vcc_lo :: v_dual_cndmask_b32 v10, 0, v10, s0
; GFX1250-NEXT:    v_dual_add_f64 v[4:5], v[4:5], -v[8:9] :: v_dual_cndmask_b32 v11, 0, v11, s0
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GFX1250-NEXT:    v_add_f64_e64 v[6:7], v[6:7], -v[10:11]
; GFX1250-NEXT:    flat_store_b64 v[0:1], v[4:5]
; GFX1250-NEXT:    flat_store_b64 v[2:3], v[6:7]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: two_cond_sub_different_base:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v12, 1, v12
; GFX90A-NEXT:    v_mov_b32_e32 v16, 0xbff00000
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v12
; GFX90A-NEXT:    v_mov_b32_e32 v14, 0
; GFX90A-NEXT:    v_cndmask_b32_e32 v15, 0, v16, vcc
; GFX90A-NEXT:    v_fmac_f64_e32 v[4:5], v[14:15], v[8:9]
; GFX90A-NEXT:    flat_store_dwordx2 v[0:1], v[4:5]
; GFX90A-NEXT:    v_and_b32_e32 v0, 1, v13
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v0
; GFX90A-NEXT:    v_cndmask_b32_e32 v15, 0, v16, vcc
; GFX90A-NEXT:    v_fmac_f64_e32 v[6:7], v[14:15], v[10:11]
; GFX90A-NEXT:    flat_store_dwordx2 v[2:3], v[6:7]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: two_cond_sub_different_base
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   undef [[COPY2:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr5
; GFX90A-MIR-NEXT:   undef [[COPY3:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr7
; GFX90A-MIR-NEXT:   undef [[COPY4:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr8
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr9
; GFX90A-MIR-NEXT:   undef [[COPY5:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr10
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr11
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr12
; GFX90A-MIR-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr13
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY4]], [[COPY2]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY]], [[COPY2]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out1)
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_1:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY7]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY5]], [[COPY3]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY1]], [[COPY3]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out2)
; GFX90A-MIR-NEXT:   SI_RETURN
; Constants are shared between patterns, expecting FMA for both.
  %sel1 = select i1 %cond1, double %c1, double 0.0
  %result1 = fsub double %a, %sel1
  store double %result1, ptr %out1

  %sel2 = select i1 %cond2, double %c2, double 0.0
  %result2 = fsub double %b, %sel2
  store double %result2, ptr %out2
  ret void
}

; ============================================================================
; Patterns in loops
; ============================================================================
; Loop with conditional subtraction where c is loop-invariant
define void @cond_sub_loop_invariant(ptr %out, double %a, double %c, i1 %cond, i32 %n) {
; GFX1250-LABEL: cond_sub_loop_invariant:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v6, 1, v6
; GFX1250-NEXT:    s_mov_b32 s0, 0
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v6
; GFX1250-NEXT:    v_dual_cndmask_b32 v4, 0, v4 :: v_dual_cndmask_b32 v5, 0, v5
; GFX1250-NEXT:  .LBB6_1: ; %loop
; GFX1250-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_dual_add_f64 v[2:3], v[2:3], -v[4:5] :: v_dual_add_nc_u32 v7, -1, v7
; GFX1250-NEXT:    v_cmp_eq_u32_e32 vcc_lo, 0, v7
; GFX1250-NEXT:    s_or_b32 s0, vcc_lo, s0
; GFX1250-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; GFX1250-NEXT:    s_and_not1_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    s_cbranch_execnz .LBB6_1
; GFX1250-NEXT:  ; %bb.2: ; %exit
; GFX1250-NEXT:    s_or_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    flat_store_b64 v[0:1], v[2:3]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_loop_invariant:
; GFX90A:       ; %bb.0: ; %entry
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v6, 1, v6
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v6
; GFX90A-NEXT:    v_cndmask_b32_e32 v4, 0, v4, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v5, 0, v5, vcc
; GFX90A-NEXT:    s_mov_b64 s[4:5], 0
; GFX90A-NEXT:  .LBB6_1: ; %loop
; GFX90A-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX90A-NEXT:    v_add_u32_e32 v7, -1, v7
; GFX90A-NEXT:    v_cmp_eq_u32_e32 vcc, 0, v7
; GFX90A-NEXT:    s_or_b64 s[4:5], vcc, s[4:5]
; GFX90A-NEXT:    v_add_f64 v[2:3], v[2:3], -v[4:5]
; GFX90A-NEXT:    s_andn2_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    s_cbranch_execnz .LBB6_1
; GFX90A-NEXT:  ; %bb.2: ; %exit
; GFX90A-NEXT:    s_or_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    flat_store_dwordx2 v[0:1], v[2:3]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_loop_invariant
; GFX90A-MIR: bb.0.entry:
; GFX90A-MIR-NEXT:   successors: %bb.1(0x80000000)
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr5
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY4]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY2]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY3]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.1.loop:
; GFX90A-MIR-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 -1, [[COPY5]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_EQ_U32_e64 0, [[COPY5]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_OR_B64 [[V_CMP_EQ_U32_e64_]], [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = nofpexcept V_ADD_F64_e64 0, [[COPY1]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $exec = S_ANDN2_B64_term $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   S_CBRANCH_EXECNZ %bb.1, implicit $exec
; GFX90A-MIR-NEXT:   S_BRANCH %bb.2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.2.exit:
; GFX90A-MIR-NEXT:   $exec = S_OR_B64 $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY]], [[COPY1]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out)
; GFX90A-MIR-NEXT:   SI_RETURN
; 'select' is loop-invariant and hoisted before the loop. If we convert this to
; FMA it would be the single instruction from this pattern inside the loop.
; Having instead cheaper 'f_add' looks more efficient and also reduces live-in
; register pressure for the loop. Note that for legacy selector this happens
; automatically as the pattern is split between basic blocks, but requires
; special effort in Global ISel.
entry:
  br label %loop

loop:
  %i = phi i32 [ 0, %entry ], [ %i.next, %loop ]
  %acc = phi double [ %a, %entry ], [ %result, %loop ]

  %sel = select i1 %cond, double %c, double 0.0
  %result = fsub double %acc, %sel

  %i.next = add i32 %i, 1
  %exit.cond = icmp eq i32 %i.next, %n
  br i1 %exit.cond, label %exit, label %loop

exit:
  store double %result, ptr %out
  ret void
}

; Loop with conditional subtraction where c depends on loop index
define void @cond_sub_loop_variant(ptr %in, ptr %out, double %a, i1 %cond, i32 %n) {
; GFX1250-LABEL: cond_sub_loop_variant:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_mov_b32 s0, 0
; GFX1250-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; GFX1250-NEXT:    v_dual_mov_b32 v8, s0 :: v_dual_bitop2_b32 v6, 1, v6 bitop3:0x40
; GFX1250-NEXT:  .LBB7_1: ; %loop
; GFX1250-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v6
; GFX1250-NEXT:    v_ashrrev_i32_e32 v9, 31, v8
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GFX1250-NEXT:    v_lshl_add_u64 v[10:11], v[8:9], 3, v[0:1]
; GFX1250-NEXT:    v_add_nc_u32_e32 v8, 1, v8
; GFX1250-NEXT:    flat_load_b64 v[10:11], v[10:11]
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    v_dual_cndmask_b32 v10, 0, v10 :: v_dual_cndmask_b32 v11, 0, v11
; GFX1250-NEXT:    v_cmp_eq_u32_e32 vcc_lo, v7, v8
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(SKIP_1) | instid1(SALU_CYCLE_1)
; GFX1250-NEXT:    v_add_f64_e64 v[4:5], v[4:5], -v[10:11]
; GFX1250-NEXT:    s_or_b32 s0, vcc_lo, s0
; GFX1250-NEXT:    s_and_not1_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    s_cbranch_execnz .LBB7_1
; GFX1250-NEXT:  ; %bb.2: ; %exit
; GFX1250-NEXT:    s_or_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    flat_store_b64 v[2:3], v[4:5]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_loop_variant:
; GFX90A:       ; %bb.0: ; %entry
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_mov_b32 s6, 0
; GFX90A-NEXT:    s_mov_b64 s[4:5], 0
; GFX90A-NEXT:    v_and_b32_e32 v6, 1, v6
; GFX90A-NEXT:    v_mov_b32_e32 v8, 0
; GFX90A-NEXT:    v_mov_b32_e32 v12, 0xbff00000
; GFX90A-NEXT:    v_mov_b32_e32 v10, s6
; GFX90A-NEXT:  .LBB7_1: ; %loop
; GFX90A-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX90A-NEXT:    v_ashrrev_i32_e32 v11, 31, v10
; GFX90A-NEXT:    v_lshlrev_b64 v[14:15], 3, v[10:11]
; GFX90A-NEXT:    v_add_co_u32_e32 v14, vcc, v0, v14
; GFX90A-NEXT:    v_addc_co_u32_e32 v15, vcc, v1, v15, vcc
; GFX90A-NEXT:    flat_load_dwordx2 v[14:15], v[14:15]
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v6
; GFX90A-NEXT:    v_add_u32_e32 v10, 1, v10
; GFX90A-NEXT:    v_cndmask_b32_e32 v9, 0, v12, vcc
; GFX90A-NEXT:    v_cmp_eq_u32_e32 vcc, v7, v10
; GFX90A-NEXT:    s_or_b64 s[4:5], vcc, s[4:5]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_fmac_f64_e32 v[4:5], v[8:9], v[14:15]
; GFX90A-NEXT:    s_andn2_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    s_cbranch_execnz .LBB7_1
; GFX90A-NEXT:  ; %bb.2: ; %exit
; GFX90A-NEXT:    s_or_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    flat_store_dwordx2 v[2:3], v[4:5]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_loop_variant
; GFX90A-MIR: bb.0.entry:
; GFX90A-MIR-NEXT:   successors: %bb.1(0x80000000)
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY2:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   undef [[COPY3:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr5
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY4]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   undef [[COPY6:%[0-9]+]].sub0:vreg_64_align2 = COPY [[S_MOV_B32_]]
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.1.loop:
; GFX90A-MIR-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]].sub1:vreg_64_align2 = V_ASHRREV_I32_e32 31, [[COPY6]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_LSHLREV_B64_e64_:%[0-9]+]]:vreg_64_align2 = V_LSHLREV_B64_e64 3, [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_ADD_CO_U32_e64_:%[0-9]+]].sub0:vreg_64_align2, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY]], [[V_LSHLREV_B64_e64_]].sub0, 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]].sub1:vreg_64_align2, dead [[V_ADDC_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY1]], [[V_LSHLREV_B64_e64_]].sub1, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
; GFX90A-MIR-NEXT:   [[FLAT_LOAD_DWORDX2_:%[0-9]+]]:vreg_64_align2 = FLAT_LOAD_DWORDX2 [[V_ADD_CO_U32_e64_]], 0, 0, implicit $exec, implicit $flat_scr :: (load (s64) from %ir.ptr)
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[COPY6:%[0-9]+]].sub0:vreg_64_align2 = V_ADD_U32_e32 1, [[COPY6]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_EQ_U32_e64 [[COPY5]], [[COPY6]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_OR_B64 [[V_CMP_EQ_U32_e64_]], [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[FLAT_LOAD_DWORDX2_]], [[COPY3]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $exec = S_ANDN2_B64_term $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   S_CBRANCH_EXECNZ %bb.1, implicit $exec
; GFX90A-MIR-NEXT:   S_BRANCH %bb.2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.2.exit:
; GFX90A-MIR-NEXT:   $exec = S_OR_B64 $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY2]], [[COPY3]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out)
; GFX90A-MIR-NEXT:   SI_RETURN
; Interesting case where using FMA saves two 'v_cndmasks' in a loop compared to the
; original pattern because the condition is loop independent and 'v_cndmask' only
; selects the correction constant and doesn't depend on 'c'. Actually this can
; be beneficial for gfx12 too.
; TODO: This doesn't work in GlobalISel. It doesn't hoist v_cmp and v_cndmask
; out of the loop.
entry:
  br label %loop

loop:
  %i = phi i32 [ 0, %entry ], [ %i.next, %loop ]
  %acc = phi double [ %a, %entry ], [ %result, %loop ]

  %ptr = getelementptr double, ptr %in, i32 %i
  %c = load double, ptr %ptr
  %sel = select i1 %cond, double %c, double 0.0
  %result = fsub double %acc, %sel

  %i.next = add i32 %i, 1
  %exit.cond = icmp eq i32 %i.next, %n
  br i1 %exit.cond, label %exit, label %loop

exit:
  store double %result, ptr %out
  ret void
}

; Loop where condition depends on loop index
define void @cond_sub_loop_cond_variant(ptr %conds, ptr %out, double %a, double %c, i32 %n) {
; GFX1250-LABEL: cond_sub_loop_cond_variant:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_mov_b32 s0, 0
; GFX1250-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; GFX1250-NEXT:    v_mov_b32_e32 v9, s0
; GFX1250-NEXT:  .LBB8_1: ; %loop
; GFX1250-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_2) | instid1(VALU_DEP_3)
; GFX1250-NEXT:    v_ashrrev_i32_e32 v11, 31, v9
; GFX1250-NEXT:    v_add_co_u32 v10, vcc_lo, v0, v9
; GFX1250-NEXT:    v_add_nc_u32_e32 v9, 1, v9
; GFX1250-NEXT:    v_add_co_ci_u32_e64 v11, null, v1, v11, vcc_lo
; GFX1250-NEXT:    flat_load_u8 v10, v[10:11]
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v10, 1, v10
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_2) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v10
; GFX1250-NEXT:    v_dual_cndmask_b32 v10, 0, v6 :: v_dual_cndmask_b32 v11, 0, v7
; GFX1250-NEXT:    v_cmp_eq_u32_e32 vcc_lo, v8, v9
; GFX1250-NEXT:    v_add_f64_e64 v[4:5], v[4:5], -v[10:11]
; GFX1250-NEXT:    s_or_b32 s0, vcc_lo, s0
; GFX1250-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; GFX1250-NEXT:    s_and_not1_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    s_cbranch_execnz .LBB8_1
; GFX1250-NEXT:  ; %bb.2: ; %exit
; GFX1250-NEXT:    s_or_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    flat_store_b64 v[2:3], v[4:5]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_loop_cond_variant:
; GFX90A:       ; %bb.0: ; %entry
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_mov_b32 s6, 0
; GFX90A-NEXT:    s_mov_b64 s[4:5], 0
; GFX90A-NEXT:    v_mov_b32_e32 v10, 0
; GFX90A-NEXT:    v_mov_b32_e32 v9, 0xbff00000
; GFX90A-NEXT:    v_mov_b32_e32 v12, s6
; GFX90A-NEXT:  .LBB8_1: ; %loop
; GFX90A-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX90A-NEXT:    v_ashrrev_i32_e32 v11, 31, v12
; GFX90A-NEXT:    v_add_co_u32_e32 v14, vcc, v0, v12
; GFX90A-NEXT:    v_addc_co_u32_e32 v15, vcc, v1, v11, vcc
; GFX90A-NEXT:    flat_load_ubyte v11, v[14:15]
; GFX90A-NEXT:    v_add_u32_e32 v12, 1, v12
; GFX90A-NEXT:    v_cmp_eq_u32_e32 vcc, v8, v12
; GFX90A-NEXT:    s_or_b64 s[4:5], vcc, s[4:5]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v11, 1, v11
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v11
; GFX90A-NEXT:    v_cndmask_b32_e32 v11, 0, v9, vcc
; GFX90A-NEXT:    v_fmac_f64_e32 v[4:5], v[10:11], v[6:7]
; GFX90A-NEXT:    s_andn2_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    s_cbranch_execnz .LBB8_1
; GFX90A-NEXT:  ; %bb.2: ; %exit
; GFX90A-NEXT:    s_or_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    flat_store_dwordx2 v[2:3], v[4:5]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_loop_cond_variant
; GFX90A-MIR: bb.0.entry:
; GFX90A-MIR-NEXT:   successors: %bb.1(0x80000000)
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY2:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   undef [[COPY3:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr5
; GFX90A-MIR-NEXT:   undef [[COPY4:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr8
; GFX90A-MIR-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY [[S_MOV_B32_]]
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.1.loop:
; GFX90A-MIR-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[V_ASHRREV_I32_e32_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e32 31, [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_ADD_CO_U32_e64_:%[0-9]+]].sub0:vreg_64_align2, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY]], [[COPY6]], 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]].sub1:vreg_64_align2, dead [[V_ADDC_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY1]], [[V_ASHRREV_I32_e32_]], [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
; GFX90A-MIR-NEXT:   [[FLAT_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = FLAT_LOAD_UBYTE [[V_ADD_CO_U32_e64_]], 0, 0, implicit $exec, implicit $flat_scr :: (load (s8) from %ir.cond_ptr)
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 1, [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_EQ_U32_e64 [[COPY5]], [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_OR_B64 [[V_CMP_EQ_U32_e64_]], [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[FLAT_LOAD_UBYTE]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[COPY4]], [[COPY3]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $exec = S_ANDN2_B64_term $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   S_CBRANCH_EXECNZ %bb.1, implicit $exec
; GFX90A-MIR-NEXT:   S_BRANCH %bb.2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.2.exit:
; GFX90A-MIR-NEXT:   $exec = S_OR_B64 $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY2]], [[COPY3]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out)
; GFX90A-MIR-NEXT:   SI_RETURN
; Expect conversion to FMA.
entry:
  br label %loop

loop:
  %i = phi i32 [ 0, %entry ], [ %i.next, %loop ]
  %acc = phi double [ %a, %entry ], [ %result, %loop ]

  %cond_ptr = getelementptr i1, ptr %conds, i32 %i
  %cond = load i1, ptr %cond_ptr
  %sel = select i1 %cond, double %c, double 0.0
  %result = fsub double %acc, %sel

  %i.next = add i32 %i, 1
  %exit.cond = icmp eq i32 %i.next, %n
  br i1 %exit.cond, label %exit, label %loop

exit:
  store double %result, ptr %out
  ret void
}

; Loop where both condition and value depend on loop index
define void @cond_sub_loop_both_variant(ptr %conds, ptr %values, ptr %out, double %a, i32 %n) {
; GFX1250-LABEL: cond_sub_loop_both_variant:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_mov_b32 s0, 0
; GFX1250-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; GFX1250-NEXT:    v_mov_b32_e32 v10, s0
; GFX1250-NEXT:  .LBB9_1: ; %loop
; GFX1250-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_ashrrev_i32_e32 v11, 31, v10
; GFX1250-NEXT:    v_add_co_u32 v12, vcc_lo, v0, v10
; GFX1250-NEXT:    v_add_co_ci_u32_e64 v13, null, v1, v11, vcc_lo
; GFX1250-NEXT:    flat_load_u8 v9, v[12:13]
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v9, 1, v9
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    v_lshl_add_u64 v[12:13], v[10:11], 3, v[2:3]
; GFX1250-NEXT:    v_add_nc_u32_e32 v10, 1, v10
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v9
; GFX1250-NEXT:    flat_load_b64 v[12:13], v[12:13]
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    v_dual_cndmask_b32 v12, 0, v12 :: v_dual_cndmask_b32 v13, 0, v13
; GFX1250-NEXT:    v_cmp_eq_u32_e32 vcc_lo, v8, v10
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(SKIP_1) | instid1(SALU_CYCLE_1)
; GFX1250-NEXT:    v_add_f64_e64 v[6:7], v[6:7], -v[12:13]
; GFX1250-NEXT:    s_or_b32 s0, vcc_lo, s0
; GFX1250-NEXT:    s_and_not1_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    s_cbranch_execnz .LBB9_1
; GFX1250-NEXT:  ; %bb.2: ; %exit
; GFX1250-NEXT:    s_or_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    flat_store_b64 v[4:5], v[6:7]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_loop_both_variant:
; GFX90A:       ; %bb.0: ; %entry
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_mov_b32 s4, 0
; GFX90A-NEXT:    s_mov_b64 s[6:7], 0
; GFX90A-NEXT:    v_mov_b32_e32 v10, 0
; GFX90A-NEXT:    v_mov_b32_e32 v9, 0xbff00000
; GFX90A-NEXT:    v_mov_b32_e32 v12, s4
; GFX90A-NEXT:  .LBB9_1: ; %loop
; GFX90A-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX90A-NEXT:    v_ashrrev_i32_e32 v13, 31, v12
; GFX90A-NEXT:    v_add_co_u32_e32 v14, vcc, v0, v12
; GFX90A-NEXT:    v_addc_co_u32_e32 v15, vcc, v1, v13, vcc
; GFX90A-NEXT:    flat_load_ubyte v11, v[14:15]
; GFX90A-NEXT:    v_lshlrev_b64 v[14:15], 3, v[12:13]
; GFX90A-NEXT:    v_add_co_u32_e32 v14, vcc, v2, v14
; GFX90A-NEXT:    v_addc_co_u32_e32 v15, vcc, v3, v15, vcc
; GFX90A-NEXT:    flat_load_dwordx2 v[14:15], v[14:15]
; GFX90A-NEXT:    v_add_u32_e32 v12, 1, v12
; GFX90A-NEXT:    v_cmp_eq_u32_e32 vcc, v8, v12
; GFX90A-NEXT:    s_or_b64 s[6:7], vcc, s[6:7]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v11, 1, v11
; GFX90A-NEXT:    v_cmp_ne_u32_e64 s[4:5], 0, v11
; GFX90A-NEXT:    v_cndmask_b32_e64 v11, 0, v9, s[4:5]
; GFX90A-NEXT:    v_fmac_f64_e32 v[6:7], v[10:11], v[14:15]
; GFX90A-NEXT:    s_andn2_b64 exec, exec, s[6:7]
; GFX90A-NEXT:    s_cbranch_execnz .LBB9_1
; GFX90A-NEXT:  ; %bb.2: ; %exit
; GFX90A-NEXT:    s_or_b64 exec, exec, s[6:7]
; GFX90A-NEXT:    flat_store_dwordx2 v[4:5], v[6:7]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_loop_both_variant
; GFX90A-MIR: bb.0.entry:
; GFX90A-MIR-NEXT:   successors: %bb.1(0x80000000)
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr1
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr3
; GFX90A-MIR-NEXT:   undef [[COPY4:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr5
; GFX90A-MIR-NEXT:   undef [[COPY5:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr8
; GFX90A-MIR-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
; GFX90A-MIR-NEXT:   undef [[AV_MOV_:%[0-9]+]].sub0:vreg_64_align2 = AV_MOV_B32_IMM_PSEUDO 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 -1074790400, implicit $exec
; GFX90A-MIR-NEXT:   undef [[COPY7:%[0-9]+]].sub0:vreg_64_align2 = COPY [[S_MOV_B32_]]
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.1.loop:
; GFX90A-MIR-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY7:%[0-9]+]].sub1:vreg_64_align2 = V_ASHRREV_I32_e32 31, [[COPY7]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_ADD_CO_U32_e64_:%[0-9]+]].sub0:vreg_64_align2, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY]], [[COPY7]].sub0, 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]].sub1:vreg_64_align2, dead [[V_ADDC_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY1]], [[COPY7]].sub1, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
; GFX90A-MIR-NEXT:   [[FLAT_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = FLAT_LOAD_UBYTE [[V_ADD_CO_U32_e64_]], 0, 0, implicit $exec, implicit $flat_scr :: (load (s8) from %ir.cond_ptr)
; GFX90A-MIR-NEXT:   [[V_LSHLREV_B64_e64_:%[0-9]+]]:vreg_64_align2 = V_LSHLREV_B64_e64 3, [[COPY7]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_ADD_CO_U32_e64_2:%[0-9]+]].sub0:vreg_64_align2, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 [[COPY2]], [[V_LSHLREV_B64_e64_]].sub0, 0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]].sub1:vreg_64_align2, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY3]], [[V_LSHLREV_B64_e64_]].sub1, [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
; GFX90A-MIR-NEXT:   [[FLAT_LOAD_DWORDX2_:%[0-9]+]]:vreg_64_align2 = FLAT_LOAD_DWORDX2 [[V_ADD_CO_U32_e64_2]], 0, 0, implicit $exec, implicit $flat_scr :: (load (s64) from %ir.value_ptr)
; GFX90A-MIR-NEXT:   undef [[COPY7:%[0-9]+]].sub0:vreg_64_align2 = V_ADD_U32_e32 1, [[COPY7]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_EQ_U32_e64 [[COPY6]], [[COPY7]].sub0, implicit $exec
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[FLAT_LOAD_UBYTE]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[AV_MOV_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[V_MOV_B32_e32_]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_OR_B64 [[V_CMP_EQ_U32_e64_]], [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vreg_64_align2 = nofpexcept V_FMAC_F64_e32 [[AV_MOV_]], [[FLAT_LOAD_DWORDX2_]], [[COPY5]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $exec = S_ANDN2_B64_term $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   S_CBRANCH_EXECNZ %bb.1, implicit $exec
; GFX90A-MIR-NEXT:   S_BRANCH %bb.2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.2.exit:
; GFX90A-MIR-NEXT:   $exec = S_OR_B64 $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY4]], [[COPY5]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out)
; GFX90A-MIR-NEXT:   SI_RETURN
; Expect conversion to FMA.
entry:
  br label %loop

loop:
  %i = phi i32 [ 0, %entry ], [ %i.next, %loop ]
  %acc = phi double [ %a, %entry ], [ %result, %loop ]

  %cond_ptr = getelementptr i1, ptr %conds, i32 %i
  %cond = load i1, ptr %cond_ptr
  %value_ptr = getelementptr double, ptr %values, i32 %i
  %c = load double, ptr %value_ptr

  %sel = select i1 %cond, double %c, double 0.0
  %result = fsub double %acc, %sel

  %i.next = add i32 %i, 1
  %exit.cond = icmp eq i32 %i.next, %n
  br i1 %exit.cond, label %exit, label %loop

exit:
  store double %result, ptr %out
  ret void
}

; ============================================================================
; Multiple patterns in a loop
; ============================================================================
; Two conditional subtractions in same loop iteration
define void @cond_sub_loop_two_ops(ptr %out, double %a, double %c1, double %c2, i1 %cond1, i1 %cond2, i32 %n) {
; GFX1250-LABEL: cond_sub_loop_two_ops:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v8, 1, v8
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v8
; GFX1250-NEXT:    v_dual_cndmask_b32 v4, 0, v4, vcc_lo :: v_dual_bitop2_b32 v9, 1, v9 bitop3:0x40
; GFX1250-NEXT:    v_cmp_ne_u32_e64 s0, 0, v9
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GFX1250-NEXT:    v_dual_cndmask_b32 v5, 0, v5, vcc_lo :: v_dual_cndmask_b32 v6, 0, v6, s0
; GFX1250-NEXT:    v_cndmask_b32_e64 v7, 0, v7, s0
; GFX1250-NEXT:    s_mov_b32 s0, 0
; GFX1250-NEXT:  .LBB10_1: ; %loop
; GFX1250-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_dual_add_f64 v[2:3], v[2:3], -v[4:5] :: v_dual_add_nc_u32 v10, -1, v10
; GFX1250-NEXT:    v_cmp_eq_u32_e32 vcc_lo, 0, v10
; GFX1250-NEXT:    s_or_b32 s0, vcc_lo, s0
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GFX1250-NEXT:    v_add_f64_e64 v[2:3], v[2:3], -v[6:7]
; GFX1250-NEXT:    s_and_not1_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    s_cbranch_execnz .LBB10_1
; GFX1250-NEXT:  ; %bb.2: ; %exit
; GFX1250-NEXT:    s_or_b32 exec_lo, exec_lo, s0
; GFX1250-NEXT:    flat_store_b64 v[0:1], v[2:3]
; GFX1250-NEXT:    s_wait_dscnt 0x0
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_loop_two_ops:
; GFX90A:       ; %bb.0: ; %entry
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v8, 1, v8
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v8
; GFX90A-NEXT:    v_and_b32_e32 v8, 1, v9
; GFX90A-NEXT:    v_cndmask_b32_e32 v4, 0, v4, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v5, 0, v5, vcc
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v8
; GFX90A-NEXT:    v_cndmask_b32_e32 v6, 0, v6, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v7, 0, v7, vcc
; GFX90A-NEXT:    s_mov_b64 s[4:5], 0
; GFX90A-NEXT:  .LBB10_1: ; %loop
; GFX90A-NEXT:    ; =>This Inner Loop Header: Depth=1
; GFX90A-NEXT:    v_add_u32_e32 v10, -1, v10
; GFX90A-NEXT:    v_add_f64 v[2:3], v[2:3], -v[4:5]
; GFX90A-NEXT:    v_cmp_eq_u32_e32 vcc, 0, v10
; GFX90A-NEXT:    s_or_b64 s[4:5], vcc, s[4:5]
; GFX90A-NEXT:    v_add_f64 v[2:3], v[2:3], -v[6:7]
; GFX90A-NEXT:    s_andn2_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    s_cbranch_execnz .LBB10_1
; GFX90A-NEXT:  ; %bb.2: ; %exit
; GFX90A-NEXT:    s_or_b64 exec, exec, s[4:5]
; GFX90A-NEXT:    flat_store_dwordx2 v[0:1], v[2:3]
; GFX90A-NEXT:    s_waitcnt vmcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_loop_two_ops
; GFX90A-MIR: bb.0.entry:
; GFX90A-MIR-NEXT:   successors: %bb.1(0x80000000)
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   undef [[COPY1:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr5
; GFX90A-MIR-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr6
; GFX90A-MIR-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr7
; GFX90A-MIR-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr8
; GFX90A-MIR-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr9
; GFX90A-MIR-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr10
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY6]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY2]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY3]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_1:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY7]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_1]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_CNDMASK_B32_e64_1:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY4]], [[V_CMP_NE_U32_e64_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CNDMASK_B32_e64_1:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY5]], [[V_CMP_NE_U32_e64_1]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_MOV_B64 0
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.1.loop:
; GFX90A-MIR-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = nofpexcept V_ADD_F64_e64 0, [[COPY1]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 -1, [[COPY8]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_EQ_U32_e64 0, [[COPY8]], implicit $exec
; GFX90A-MIR-NEXT:   [[S_MOV_B64_:%[0-9]+]]:sreg_64_xexec = S_OR_B64 [[V_CMP_EQ_U32_e64_]], [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vreg_64_align2 = nofpexcept V_ADD_F64_e64 0, [[V_ADD_F64_e64_]], 1, [[V_CNDMASK_B32_e64_1]], 0, 0, implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $exec = S_ANDN2_B64_term $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   S_CBRANCH_EXECNZ %bb.1, implicit $exec
; GFX90A-MIR-NEXT:   S_BRANCH %bb.2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT: bb.2.exit:
; GFX90A-MIR-NEXT:   $exec = S_OR_B64 $exec, [[S_MOV_B64_]], implicit-def $scc
; GFX90A-MIR-NEXT:   FLAT_STORE_DWORDX2 [[COPY]], [[COPY1]], 0, 0, implicit $exec, implicit $flat_scr :: (store (s64) into %ir.out)
; GFX90A-MIR-NEXT:   SI_RETURN
; Applies the same comment as for cond_sub_loop_invariant. Expect no FMA conversion.
entry:
  br label %loop

loop:
  %i = phi i32 [ 0, %entry ], [ %i.next, %loop ]
  %acc = phi double [ %a, %entry ], [ %result2, %loop ]

  %sel1 = select i1 %cond1, double %c1, double 0.0
  %result1 = fsub double %acc, %sel1

  %sel2 = select i1 %cond2, double %c2, double 0.0
  %result2 = fsub double %result1, %sel2

  %i.next = add i32 %i, 1
  %exit.cond = icmp eq i32 %i.next, %n
  br i1 %exit.cond, label %exit, label %loop

exit:
  store double %result2, ptr %out
  ret void
}

; ============================================================================
; Negative tests - patterns that should NOT be optimized
; ============================================================================

; f32 should not be optimized (no FMAC f64 for f32)
define float @cond_sub_f32(float %a, float %c, i1 %cond) {
; GFX1250-LABEL: cond_sub_f32:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v2, 1, v2
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v2
; GFX1250-NEXT:    v_cndmask_b32_e32 v1, 0, v1, vcc_lo
; GFX1250-NEXT:    v_sub_f32_e32 v0, v0, v1
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_f32:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v2, 1, v2
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v2
; GFX90A-NEXT:    v_cndmask_b32_e32 v1, 0, v1, vcc
; GFX90A-NEXT:    v_sub_f32_e32 v0, v0, v1
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_f32
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr1
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY2]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]]:vgpr_32 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_SUB_F32_e32_:%[0-9]+]]:vgpr_32 = nofpexcept V_SUB_F32_e32 [[COPY]], [[V_CNDMASK_B32_e64_]], implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[V_SUB_F32_e32_]]
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0
  %sel = select i1 %cond, float %c, float 0.0
  %result = fsub float %a, %sel
  ret float %result
}

; False value is not 0.0
define double @cond_sub_wrong_false_value(double %a, double %c, i1 %cond) {
; GFX1250-LABEL: cond_sub_wrong_false_value:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(SKIP_2) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_cmp_ne_u32_e32 vcc_lo, 0, v4
; GFX1250-NEXT:    v_cndmask_b32_e32 v2, 0, v2, vcc_lo
; GFX1250-NEXT:    v_cndmask_b32_e32 v3, 0x3ff00000, v3, vcc_lo
; GFX1250-NEXT:    v_add_f64_e64 v[0:1], v[0:1], -v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX90A-LABEL: cond_sub_wrong_false_value:
; GFX90A:       ; %bb.0:
; GFX90A-NEXT:    s_waitcnt vmcnt(0) expcnt(0) lgkmcnt(0)
; GFX90A-NEXT:    v_and_b32_e32 v4, 1, v4
; GFX90A-NEXT:    v_mov_b32_e32 v5, 0x3ff00000
; GFX90A-NEXT:    v_cmp_ne_u32_e32 vcc, 0, v4
; GFX90A-NEXT:    v_cndmask_b32_e32 v2, 0, v2, vcc
; GFX90A-NEXT:    v_cndmask_b32_e32 v3, v5, v3, vcc
; GFX90A-NEXT:    v_add_f64 v[0:1], v[0:1], -v[2:3]
; GFX90A-NEXT:    s_setpc_b64 s[30:31]
; GFX90A-MIR-LABEL: name: cond_sub_wrong_false_value
; GFX90A-MIR: bb.0 (%ir-block.0):
; GFX90A-MIR-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
; GFX90A-MIR-NEXT: {{  $}}
; GFX90A-MIR-NEXT:   undef [[COPY:%[0-9]+]].sub0:vreg_64_align2 = COPY $vgpr0
; GFX90A-MIR-NEXT:   [[COPY:%[0-9]+]].sub1:vreg_64_align2 = COPY $vgpr1
; GFX90A-MIR-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr2
; GFX90A-MIR-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr3
; GFX90A-MIR-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr4
; GFX90A-MIR-NEXT:   [[V_AND_B32_e32_:%[0-9]+]]:vgpr_32 = V_AND_B32_e32 1, [[COPY3]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 1072693248, implicit $exec
; GFX90A-MIR-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_64_xexec = V_CMP_NE_U32_e64 0, [[V_AND_B32_e32_]], implicit $exec
; GFX90A-MIR-NEXT:   undef [[V_CNDMASK_B32_e64_:%[0-9]+]].sub0:vreg_64_align2 = V_CNDMASK_B32_e64 0, 0, 0, [[COPY1]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_CNDMASK_B32_e64_:%[0-9]+]].sub1:vreg_64_align2 = V_CNDMASK_B32_e64 0, [[V_MOV_B32_e32_]], 0, [[COPY2]], [[V_CMP_NE_U32_e64_]], implicit $exec
; GFX90A-MIR-NEXT:   [[V_ADD_F64_e64_:%[0-9]+]]:vreg_64_align2 = nofpexcept V_ADD_F64_e64 0, [[COPY]], 1, [[V_CNDMASK_B32_e64_]], 0, 0, implicit $mode, implicit $exec
; GFX90A-MIR-NEXT:   $vgpr0 = COPY [[V_ADD_F64_e64_]].sub0
; GFX90A-MIR-NEXT:   $vgpr1 = COPY [[V_ADD_F64_e64_]].sub1
; GFX90A-MIR-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
  %sel = select i1 %cond, double %c, double 1.0
  %result = fsub double %a, %sel
  ret double %result
}
