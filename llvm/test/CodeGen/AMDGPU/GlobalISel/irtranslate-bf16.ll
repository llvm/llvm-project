; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 4
; RUN: llc < %s -global-isel -stop-after=irtranslator -mtriple=amdgcn -mcpu=gfx900 | FileCheck %s -check-prefixes=GFX9

; tests bf16 argument & return values lowering.

define <3 x bfloat> @v3bf16(<3 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v3bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<3 x i16>) = G_TRUNC [[BUILD_VECTOR]](<3 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<3 x bf16>) = G_BITCAST [[TRUNC]](<3 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<3 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<3 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<3 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(bf16), [[UV5:%[0-9]+]]:_(bf16), [[UV6:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<3 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV4]](bf16)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV5]](bf16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT4]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT5]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
  %res = shufflevector <3 x bfloat> %arg0, <3 x bfloat> zeroinitializer, <3 x i32> <i32 3, i32 1, i32 2>
  ret <3 x bfloat> %res
}

define <4 x bfloat> @v4bf16(<4 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v4bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<4 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<4 x i16>) = G_TRUNC [[BUILD_VECTOR]](<4 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<4 x bf16>) = G_BITCAST [[TRUNC]](<4 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<4 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<4 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<4 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2, 0)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(bf16), [[UV5:%[0-9]+]]:_(bf16), [[UV6:%[0-9]+]]:_(bf16), [[UV7:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<4 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV4]](bf16)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV5]](bf16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT4]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT5]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1
  %res = shufflevector <4 x bfloat> %arg0, <4 x bfloat> zeroinitializer, <4 x i32> <i32 3, i32 1, i32 2, i32 0>
  ret <4 x bfloat> %res
}

define <5 x bfloat> @v5bf16(<5 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v5bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<5 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<5 x i16>) = G_TRUNC [[BUILD_VECTOR]](<5 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<5 x bf16>) = G_BITCAST [[TRUNC]](<5 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<5 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<5 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<5 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2, 0, 4)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(bf16), [[UV7:%[0-9]+]]:_(bf16), [[UV8:%[0-9]+]]:_(bf16), [[UV9:%[0-9]+]]:_(bf16), [[UV10:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<5 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV6]](bf16)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV7]](bf16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV8]](bf16)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT6]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT7]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT8]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2
  %res = shufflevector <5 x bfloat> %arg0, <5 x bfloat> zeroinitializer, <5 x i32> <i32 3, i32 1, i32 2, i32 0, i32 4>
  ret <5 x bfloat> %res
}

define <6 x bfloat> @v6bf16(<6 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v6bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<6 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32), [[ANYEXT5]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<6 x i16>) = G_TRUNC [[BUILD_VECTOR]](<6 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<6 x bf16>) = G_BITCAST [[TRUNC]](<6 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<6 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<6 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<6 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2, 0, 4, 5)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(bf16), [[UV7:%[0-9]+]]:_(bf16), [[UV8:%[0-9]+]]:_(bf16), [[UV9:%[0-9]+]]:_(bf16), [[UV10:%[0-9]+]]:_(bf16), [[UV11:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<6 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV6]](bf16)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV7]](bf16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV8]](bf16)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT6]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT7]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT8]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2
  %res = shufflevector <6 x bfloat> %arg0, <6 x bfloat> zeroinitializer, <6 x i32> <i32 3, i32 1, i32 2, i32 0, i32 4, i32 5>
  ret <6 x bfloat> %res
}

define <7 x bfloat> @v7bf16(<7 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v7bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(i16), [[UV7:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY3]](i32)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[UV6]](i16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[UV7]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<7 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32), [[ANYEXT5]](i32), [[ANYEXT6]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<7 x i16>) = G_TRUNC [[BUILD_VECTOR]](<7 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<7 x bf16>) = G_BITCAST [[TRUNC]](<7 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<7 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<7 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<7 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2, 0, 4, 5, 6)
  ; GFX9-NEXT:   [[UV8:%[0-9]+]]:_(bf16), [[UV9:%[0-9]+]]:_(bf16), [[UV10:%[0-9]+]]:_(bf16), [[UV11:%[0-9]+]]:_(bf16), [[UV12:%[0-9]+]]:_(bf16), [[UV13:%[0-9]+]]:_(bf16), [[UV14:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<7 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV8]](bf16)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV9]](bf16)
  ; GFX9-NEXT:   [[ANYEXT9:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV10]](bf16)
  ; GFX9-NEXT:   [[ANYEXT10:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   [[BITCAST4:%[0-9]+]]:_(i16) = G_BITCAST [[UV11]](bf16)
  ; GFX9-NEXT:   [[ANYEXT11:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST4]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT8]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT9]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT10]](i32)
  ; GFX9-NEXT:   $vgpr3 = COPY [[ANYEXT11]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2, implicit $vgpr3
  %res = shufflevector <7 x bfloat> %arg0, <7 x bfloat> zeroinitializer, <7 x i32> <i32 3, i32 1, i32 2, i32 0, i32 4, i32 5, i32 6>
  ret <7 x bfloat> %res
}

define <8 x bfloat> @v8bf16(<8 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v8bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(i16), [[UV7:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY3]](i32)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[UV6]](i16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[UV7]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<8 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32), [[ANYEXT5]](i32), [[ANYEXT6]](i32), [[ANYEXT7]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<8 x i16>) = G_TRUNC [[BUILD_VECTOR]](<8 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<8 x bf16>) = G_BITCAST [[TRUNC]](<8 x i16>)
  ; GFX9-NEXT:   [[C:%[0-9]+]]:_(bf16) = G_FCONSTANT bfloat 0xR0000
  ; GFX9-NEXT:   [[BUILD_VECTOR1:%[0-9]+]]:_(<8 x bf16>) = G_BUILD_VECTOR [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16), [[C]](bf16)
  ; GFX9-NEXT:   [[SHUF:%[0-9]+]]:_(<8 x bf16>) = G_SHUFFLE_VECTOR [[BITCAST]](<8 x bf16>), [[BUILD_VECTOR1]], shufflemask(3, 1, 2, 0, 4, 5, 6, 7)
  ; GFX9-NEXT:   [[UV8:%[0-9]+]]:_(bf16), [[UV9:%[0-9]+]]:_(bf16), [[UV10:%[0-9]+]]:_(bf16), [[UV11:%[0-9]+]]:_(bf16), [[UV12:%[0-9]+]]:_(bf16), [[UV13:%[0-9]+]]:_(bf16), [[UV14:%[0-9]+]]:_(bf16), [[UV15:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[SHUF]](<8 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV8]](bf16)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV9]](bf16)
  ; GFX9-NEXT:   [[ANYEXT9:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV10]](bf16)
  ; GFX9-NEXT:   [[ANYEXT10:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   [[BITCAST4:%[0-9]+]]:_(i16) = G_BITCAST [[UV11]](bf16)
  ; GFX9-NEXT:   [[ANYEXT11:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST4]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT8]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT9]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT10]](i32)
  ; GFX9-NEXT:   $vgpr3 = COPY [[ANYEXT11]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2, implicit $vgpr3
  %res = shufflevector <8 x bfloat> %arg0, <8 x bfloat> zeroinitializer, <8 x i32> <i32 3, i32 1, i32 2, i32 0, i32 4, i32 5, i32 6, i32 7>
  ret <8 x bfloat> %res
}

define <16 x bfloat> @v16bf16(<16 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v16bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX9-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX9-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX9-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr6
  ; GFX9-NEXT:   [[COPY7:%[0-9]+]]:_(i32) = COPY $vgpr7
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(i16), [[UV7:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY3]](i32)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[UV6]](i16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[UV7]](i16)
  ; GFX9-NEXT:   [[UV8:%[0-9]+]]:_(i16), [[UV9:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY4]](i32)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[UV8]](i16)
  ; GFX9-NEXT:   [[ANYEXT9:%[0-9]+]]:_(i32) = G_ANYEXT [[UV9]](i16)
  ; GFX9-NEXT:   [[UV10:%[0-9]+]]:_(i16), [[UV11:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY5]](i32)
  ; GFX9-NEXT:   [[ANYEXT10:%[0-9]+]]:_(i32) = G_ANYEXT [[UV10]](i16)
  ; GFX9-NEXT:   [[ANYEXT11:%[0-9]+]]:_(i32) = G_ANYEXT [[UV11]](i16)
  ; GFX9-NEXT:   [[UV12:%[0-9]+]]:_(i16), [[UV13:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY6]](i32)
  ; GFX9-NEXT:   [[ANYEXT12:%[0-9]+]]:_(i32) = G_ANYEXT [[UV12]](i16)
  ; GFX9-NEXT:   [[ANYEXT13:%[0-9]+]]:_(i32) = G_ANYEXT [[UV13]](i16)
  ; GFX9-NEXT:   [[UV14:%[0-9]+]]:_(i16), [[UV15:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY7]](i32)
  ; GFX9-NEXT:   [[ANYEXT14:%[0-9]+]]:_(i32) = G_ANYEXT [[UV14]](i16)
  ; GFX9-NEXT:   [[ANYEXT15:%[0-9]+]]:_(i32) = G_ANYEXT [[UV15]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<16 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32), [[ANYEXT5]](i32), [[ANYEXT6]](i32), [[ANYEXT7]](i32), [[ANYEXT8]](i32), [[ANYEXT9]](i32), [[ANYEXT10]](i32), [[ANYEXT11]](i32), [[ANYEXT12]](i32), [[ANYEXT13]](i32), [[ANYEXT14]](i32), [[ANYEXT15]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<16 x i16>) = G_TRUNC [[BUILD_VECTOR]](<16 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<16 x bf16>) = G_BITCAST [[TRUNC]](<16 x i16>)
  ; GFX9-NEXT:   [[UV16:%[0-9]+]]:_(bf16), [[UV17:%[0-9]+]]:_(bf16), [[UV18:%[0-9]+]]:_(bf16), [[UV19:%[0-9]+]]:_(bf16), [[UV20:%[0-9]+]]:_(bf16), [[UV21:%[0-9]+]]:_(bf16), [[UV22:%[0-9]+]]:_(bf16), [[UV23:%[0-9]+]]:_(bf16), [[UV24:%[0-9]+]]:_(bf16), [[UV25:%[0-9]+]]:_(bf16), [[UV26:%[0-9]+]]:_(bf16), [[UV27:%[0-9]+]]:_(bf16), [[UV28:%[0-9]+]]:_(bf16), [[UV29:%[0-9]+]]:_(bf16), [[UV30:%[0-9]+]]:_(bf16), [[UV31:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[BITCAST]](<16 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV16]](bf16)
  ; GFX9-NEXT:   [[ANYEXT16:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV17]](bf16)
  ; GFX9-NEXT:   [[ANYEXT17:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV18]](bf16)
  ; GFX9-NEXT:   [[ANYEXT18:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   [[BITCAST4:%[0-9]+]]:_(i16) = G_BITCAST [[UV19]](bf16)
  ; GFX9-NEXT:   [[ANYEXT19:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST4]](i16)
  ; GFX9-NEXT:   [[BITCAST5:%[0-9]+]]:_(i16) = G_BITCAST [[UV20]](bf16)
  ; GFX9-NEXT:   [[ANYEXT20:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST5]](i16)
  ; GFX9-NEXT:   [[BITCAST6:%[0-9]+]]:_(i16) = G_BITCAST [[UV21]](bf16)
  ; GFX9-NEXT:   [[ANYEXT21:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST6]](i16)
  ; GFX9-NEXT:   [[BITCAST7:%[0-9]+]]:_(i16) = G_BITCAST [[UV22]](bf16)
  ; GFX9-NEXT:   [[ANYEXT22:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST7]](i16)
  ; GFX9-NEXT:   [[BITCAST8:%[0-9]+]]:_(i16) = G_BITCAST [[UV23]](bf16)
  ; GFX9-NEXT:   [[ANYEXT23:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST8]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT16]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT17]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT18]](i32)
  ; GFX9-NEXT:   $vgpr3 = COPY [[ANYEXT19]](i32)
  ; GFX9-NEXT:   $vgpr4 = COPY [[ANYEXT20]](i32)
  ; GFX9-NEXT:   $vgpr5 = COPY [[ANYEXT21]](i32)
  ; GFX9-NEXT:   $vgpr6 = COPY [[ANYEXT22]](i32)
  ; GFX9-NEXT:   $vgpr7 = COPY [[ANYEXT23]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2, implicit $vgpr3, implicit $vgpr4, implicit $vgpr5, implicit $vgpr6, implicit $vgpr7
  ret <16 x bfloat> %arg0
}

define <32 x bfloat> @v32bf16(<32 x bfloat> %arg0) {
  ; GFX9-LABEL: name: v32bf16
  ; GFX9: bb.1 (%ir-block.0):
  ; GFX9-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13, $vgpr14, $vgpr15
  ; GFX9-NEXT: {{  $}}
  ; GFX9-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX9-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX9-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX9-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX9-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX9-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX9-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr6
  ; GFX9-NEXT:   [[COPY7:%[0-9]+]]:_(i32) = COPY $vgpr7
  ; GFX9-NEXT:   [[COPY8:%[0-9]+]]:_(i32) = COPY $vgpr8
  ; GFX9-NEXT:   [[COPY9:%[0-9]+]]:_(i32) = COPY $vgpr9
  ; GFX9-NEXT:   [[COPY10:%[0-9]+]]:_(i32) = COPY $vgpr10
  ; GFX9-NEXT:   [[COPY11:%[0-9]+]]:_(i32) = COPY $vgpr11
  ; GFX9-NEXT:   [[COPY12:%[0-9]+]]:_(i32) = COPY $vgpr12
  ; GFX9-NEXT:   [[COPY13:%[0-9]+]]:_(i32) = COPY $vgpr13
  ; GFX9-NEXT:   [[COPY14:%[0-9]+]]:_(i32) = COPY $vgpr14
  ; GFX9-NEXT:   [[COPY15:%[0-9]+]]:_(i32) = COPY $vgpr15
  ; GFX9-NEXT:   [[UV:%[0-9]+]]:_(i16), [[UV1:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY]](i32)
  ; GFX9-NEXT:   [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[UV]](i16)
  ; GFX9-NEXT:   [[ANYEXT1:%[0-9]+]]:_(i32) = G_ANYEXT [[UV1]](i16)
  ; GFX9-NEXT:   [[UV2:%[0-9]+]]:_(i16), [[UV3:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY1]](i32)
  ; GFX9-NEXT:   [[ANYEXT2:%[0-9]+]]:_(i32) = G_ANYEXT [[UV2]](i16)
  ; GFX9-NEXT:   [[ANYEXT3:%[0-9]+]]:_(i32) = G_ANYEXT [[UV3]](i16)
  ; GFX9-NEXT:   [[UV4:%[0-9]+]]:_(i16), [[UV5:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY2]](i32)
  ; GFX9-NEXT:   [[ANYEXT4:%[0-9]+]]:_(i32) = G_ANYEXT [[UV4]](i16)
  ; GFX9-NEXT:   [[ANYEXT5:%[0-9]+]]:_(i32) = G_ANYEXT [[UV5]](i16)
  ; GFX9-NEXT:   [[UV6:%[0-9]+]]:_(i16), [[UV7:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY3]](i32)
  ; GFX9-NEXT:   [[ANYEXT6:%[0-9]+]]:_(i32) = G_ANYEXT [[UV6]](i16)
  ; GFX9-NEXT:   [[ANYEXT7:%[0-9]+]]:_(i32) = G_ANYEXT [[UV7]](i16)
  ; GFX9-NEXT:   [[UV8:%[0-9]+]]:_(i16), [[UV9:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY4]](i32)
  ; GFX9-NEXT:   [[ANYEXT8:%[0-9]+]]:_(i32) = G_ANYEXT [[UV8]](i16)
  ; GFX9-NEXT:   [[ANYEXT9:%[0-9]+]]:_(i32) = G_ANYEXT [[UV9]](i16)
  ; GFX9-NEXT:   [[UV10:%[0-9]+]]:_(i16), [[UV11:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY5]](i32)
  ; GFX9-NEXT:   [[ANYEXT10:%[0-9]+]]:_(i32) = G_ANYEXT [[UV10]](i16)
  ; GFX9-NEXT:   [[ANYEXT11:%[0-9]+]]:_(i32) = G_ANYEXT [[UV11]](i16)
  ; GFX9-NEXT:   [[UV12:%[0-9]+]]:_(i16), [[UV13:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY6]](i32)
  ; GFX9-NEXT:   [[ANYEXT12:%[0-9]+]]:_(i32) = G_ANYEXT [[UV12]](i16)
  ; GFX9-NEXT:   [[ANYEXT13:%[0-9]+]]:_(i32) = G_ANYEXT [[UV13]](i16)
  ; GFX9-NEXT:   [[UV14:%[0-9]+]]:_(i16), [[UV15:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY7]](i32)
  ; GFX9-NEXT:   [[ANYEXT14:%[0-9]+]]:_(i32) = G_ANYEXT [[UV14]](i16)
  ; GFX9-NEXT:   [[ANYEXT15:%[0-9]+]]:_(i32) = G_ANYEXT [[UV15]](i16)
  ; GFX9-NEXT:   [[UV16:%[0-9]+]]:_(i16), [[UV17:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY8]](i32)
  ; GFX9-NEXT:   [[ANYEXT16:%[0-9]+]]:_(i32) = G_ANYEXT [[UV16]](i16)
  ; GFX9-NEXT:   [[ANYEXT17:%[0-9]+]]:_(i32) = G_ANYEXT [[UV17]](i16)
  ; GFX9-NEXT:   [[UV18:%[0-9]+]]:_(i16), [[UV19:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY9]](i32)
  ; GFX9-NEXT:   [[ANYEXT18:%[0-9]+]]:_(i32) = G_ANYEXT [[UV18]](i16)
  ; GFX9-NEXT:   [[ANYEXT19:%[0-9]+]]:_(i32) = G_ANYEXT [[UV19]](i16)
  ; GFX9-NEXT:   [[UV20:%[0-9]+]]:_(i16), [[UV21:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY10]](i32)
  ; GFX9-NEXT:   [[ANYEXT20:%[0-9]+]]:_(i32) = G_ANYEXT [[UV20]](i16)
  ; GFX9-NEXT:   [[ANYEXT21:%[0-9]+]]:_(i32) = G_ANYEXT [[UV21]](i16)
  ; GFX9-NEXT:   [[UV22:%[0-9]+]]:_(i16), [[UV23:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY11]](i32)
  ; GFX9-NEXT:   [[ANYEXT22:%[0-9]+]]:_(i32) = G_ANYEXT [[UV22]](i16)
  ; GFX9-NEXT:   [[ANYEXT23:%[0-9]+]]:_(i32) = G_ANYEXT [[UV23]](i16)
  ; GFX9-NEXT:   [[UV24:%[0-9]+]]:_(i16), [[UV25:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY12]](i32)
  ; GFX9-NEXT:   [[ANYEXT24:%[0-9]+]]:_(i32) = G_ANYEXT [[UV24]](i16)
  ; GFX9-NEXT:   [[ANYEXT25:%[0-9]+]]:_(i32) = G_ANYEXT [[UV25]](i16)
  ; GFX9-NEXT:   [[UV26:%[0-9]+]]:_(i16), [[UV27:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY13]](i32)
  ; GFX9-NEXT:   [[ANYEXT26:%[0-9]+]]:_(i32) = G_ANYEXT [[UV26]](i16)
  ; GFX9-NEXT:   [[ANYEXT27:%[0-9]+]]:_(i32) = G_ANYEXT [[UV27]](i16)
  ; GFX9-NEXT:   [[UV28:%[0-9]+]]:_(i16), [[UV29:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY14]](i32)
  ; GFX9-NEXT:   [[ANYEXT28:%[0-9]+]]:_(i32) = G_ANYEXT [[UV28]](i16)
  ; GFX9-NEXT:   [[ANYEXT29:%[0-9]+]]:_(i32) = G_ANYEXT [[UV29]](i16)
  ; GFX9-NEXT:   [[UV30:%[0-9]+]]:_(i16), [[UV31:%[0-9]+]]:_(i16) = G_UNMERGE_VALUES [[COPY15]](i32)
  ; GFX9-NEXT:   [[ANYEXT30:%[0-9]+]]:_(i32) = G_ANYEXT [[UV30]](i16)
  ; GFX9-NEXT:   [[ANYEXT31:%[0-9]+]]:_(i32) = G_ANYEXT [[UV31]](i16)
  ; GFX9-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<32 x i32>) = G_BUILD_VECTOR [[ANYEXT]](i32), [[ANYEXT1]](i32), [[ANYEXT2]](i32), [[ANYEXT3]](i32), [[ANYEXT4]](i32), [[ANYEXT5]](i32), [[ANYEXT6]](i32), [[ANYEXT7]](i32), [[ANYEXT8]](i32), [[ANYEXT9]](i32), [[ANYEXT10]](i32), [[ANYEXT11]](i32), [[ANYEXT12]](i32), [[ANYEXT13]](i32), [[ANYEXT14]](i32), [[ANYEXT15]](i32), [[ANYEXT16]](i32), [[ANYEXT17]](i32), [[ANYEXT18]](i32), [[ANYEXT19]](i32), [[ANYEXT20]](i32), [[ANYEXT21]](i32), [[ANYEXT22]](i32), [[ANYEXT23]](i32), [[ANYEXT24]](i32), [[ANYEXT25]](i32), [[ANYEXT26]](i32), [[ANYEXT27]](i32), [[ANYEXT28]](i32), [[ANYEXT29]](i32), [[ANYEXT30]](i32), [[ANYEXT31]](i32)
  ; GFX9-NEXT:   [[TRUNC:%[0-9]+]]:_(<32 x i16>) = G_TRUNC [[BUILD_VECTOR]](<32 x i32>)
  ; GFX9-NEXT:   [[BITCAST:%[0-9]+]]:_(<32 x bf16>) = G_BITCAST [[TRUNC]](<32 x i16>)
  ; GFX9-NEXT:   [[UV32:%[0-9]+]]:_(bf16), [[UV33:%[0-9]+]]:_(bf16), [[UV34:%[0-9]+]]:_(bf16), [[UV35:%[0-9]+]]:_(bf16), [[UV36:%[0-9]+]]:_(bf16), [[UV37:%[0-9]+]]:_(bf16), [[UV38:%[0-9]+]]:_(bf16), [[UV39:%[0-9]+]]:_(bf16), [[UV40:%[0-9]+]]:_(bf16), [[UV41:%[0-9]+]]:_(bf16), [[UV42:%[0-9]+]]:_(bf16), [[UV43:%[0-9]+]]:_(bf16), [[UV44:%[0-9]+]]:_(bf16), [[UV45:%[0-9]+]]:_(bf16), [[UV46:%[0-9]+]]:_(bf16), [[UV47:%[0-9]+]]:_(bf16), [[UV48:%[0-9]+]]:_(bf16), [[UV49:%[0-9]+]]:_(bf16), [[UV50:%[0-9]+]]:_(bf16), [[UV51:%[0-9]+]]:_(bf16), [[UV52:%[0-9]+]]:_(bf16), [[UV53:%[0-9]+]]:_(bf16), [[UV54:%[0-9]+]]:_(bf16), [[UV55:%[0-9]+]]:_(bf16), [[UV56:%[0-9]+]]:_(bf16), [[UV57:%[0-9]+]]:_(bf16), [[UV58:%[0-9]+]]:_(bf16), [[UV59:%[0-9]+]]:_(bf16), [[UV60:%[0-9]+]]:_(bf16), [[UV61:%[0-9]+]]:_(bf16), [[UV62:%[0-9]+]]:_(bf16), [[UV63:%[0-9]+]]:_(bf16) = G_UNMERGE_VALUES [[BITCAST]](<32 x bf16>)
  ; GFX9-NEXT:   [[BITCAST1:%[0-9]+]]:_(i16) = G_BITCAST [[UV32]](bf16)
  ; GFX9-NEXT:   [[ANYEXT32:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST1]](i16)
  ; GFX9-NEXT:   [[BITCAST2:%[0-9]+]]:_(i16) = G_BITCAST [[UV33]](bf16)
  ; GFX9-NEXT:   [[ANYEXT33:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST2]](i16)
  ; GFX9-NEXT:   [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[UV34]](bf16)
  ; GFX9-NEXT:   [[ANYEXT34:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
  ; GFX9-NEXT:   [[BITCAST4:%[0-9]+]]:_(i16) = G_BITCAST [[UV35]](bf16)
  ; GFX9-NEXT:   [[ANYEXT35:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST4]](i16)
  ; GFX9-NEXT:   [[BITCAST5:%[0-9]+]]:_(i16) = G_BITCAST [[UV36]](bf16)
  ; GFX9-NEXT:   [[ANYEXT36:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST5]](i16)
  ; GFX9-NEXT:   [[BITCAST6:%[0-9]+]]:_(i16) = G_BITCAST [[UV37]](bf16)
  ; GFX9-NEXT:   [[ANYEXT37:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST6]](i16)
  ; GFX9-NEXT:   [[BITCAST7:%[0-9]+]]:_(i16) = G_BITCAST [[UV38]](bf16)
  ; GFX9-NEXT:   [[ANYEXT38:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST7]](i16)
  ; GFX9-NEXT:   [[BITCAST8:%[0-9]+]]:_(i16) = G_BITCAST [[UV39]](bf16)
  ; GFX9-NEXT:   [[ANYEXT39:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST8]](i16)
  ; GFX9-NEXT:   [[BITCAST9:%[0-9]+]]:_(i16) = G_BITCAST [[UV40]](bf16)
  ; GFX9-NEXT:   [[ANYEXT40:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST9]](i16)
  ; GFX9-NEXT:   [[BITCAST10:%[0-9]+]]:_(i16) = G_BITCAST [[UV41]](bf16)
  ; GFX9-NEXT:   [[ANYEXT41:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST10]](i16)
  ; GFX9-NEXT:   [[BITCAST11:%[0-9]+]]:_(i16) = G_BITCAST [[UV42]](bf16)
  ; GFX9-NEXT:   [[ANYEXT42:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST11]](i16)
  ; GFX9-NEXT:   [[BITCAST12:%[0-9]+]]:_(i16) = G_BITCAST [[UV43]](bf16)
  ; GFX9-NEXT:   [[ANYEXT43:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST12]](i16)
  ; GFX9-NEXT:   [[BITCAST13:%[0-9]+]]:_(i16) = G_BITCAST [[UV44]](bf16)
  ; GFX9-NEXT:   [[ANYEXT44:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST13]](i16)
  ; GFX9-NEXT:   [[BITCAST14:%[0-9]+]]:_(i16) = G_BITCAST [[UV45]](bf16)
  ; GFX9-NEXT:   [[ANYEXT45:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST14]](i16)
  ; GFX9-NEXT:   [[BITCAST15:%[0-9]+]]:_(i16) = G_BITCAST [[UV46]](bf16)
  ; GFX9-NEXT:   [[ANYEXT46:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST15]](i16)
  ; GFX9-NEXT:   [[BITCAST16:%[0-9]+]]:_(i16) = G_BITCAST [[UV47]](bf16)
  ; GFX9-NEXT:   [[ANYEXT47:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST16]](i16)
  ; GFX9-NEXT:   $vgpr0 = COPY [[ANYEXT32]](i32)
  ; GFX9-NEXT:   $vgpr1 = COPY [[ANYEXT33]](i32)
  ; GFX9-NEXT:   $vgpr2 = COPY [[ANYEXT34]](i32)
  ; GFX9-NEXT:   $vgpr3 = COPY [[ANYEXT35]](i32)
  ; GFX9-NEXT:   $vgpr4 = COPY [[ANYEXT36]](i32)
  ; GFX9-NEXT:   $vgpr5 = COPY [[ANYEXT37]](i32)
  ; GFX9-NEXT:   $vgpr6 = COPY [[ANYEXT38]](i32)
  ; GFX9-NEXT:   $vgpr7 = COPY [[ANYEXT39]](i32)
  ; GFX9-NEXT:   $vgpr8 = COPY [[ANYEXT40]](i32)
  ; GFX9-NEXT:   $vgpr9 = COPY [[ANYEXT41]](i32)
  ; GFX9-NEXT:   $vgpr10 = COPY [[ANYEXT42]](i32)
  ; GFX9-NEXT:   $vgpr11 = COPY [[ANYEXT43]](i32)
  ; GFX9-NEXT:   $vgpr12 = COPY [[ANYEXT44]](i32)
  ; GFX9-NEXT:   $vgpr13 = COPY [[ANYEXT45]](i32)
  ; GFX9-NEXT:   $vgpr14 = COPY [[ANYEXT46]](i32)
  ; GFX9-NEXT:   $vgpr15 = COPY [[ANYEXT47]](i32)
  ; GFX9-NEXT:   SI_RETURN implicit $vgpr0, implicit $vgpr1, implicit $vgpr2, implicit $vgpr3, implicit $vgpr4, implicit $vgpr5, implicit $vgpr6, implicit $vgpr7, implicit $vgpr8, implicit $vgpr9, implicit $vgpr10, implicit $vgpr11, implicit $vgpr12, implicit $vgpr13, implicit $vgpr14, implicit $vgpr15
  ret <32 x bfloat> %arg0
}
