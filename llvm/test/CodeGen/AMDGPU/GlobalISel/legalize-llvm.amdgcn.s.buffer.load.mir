# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=tahiti -run-pass=legalizer %s -o - | FileCheck -check-prefixes=GCN,GFX67 %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=hawaii -run-pass=legalizer %s -o - | FileCheck -check-prefixes=GCN,GFX67 %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=gfx1200 -run-pass=legalizer %s -o - | FileCheck -check-prefixes=GCN,GFX12 %s

---
name: s_buffer_load_s32
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GCN-LABEL: name: s_buffer_load_s32
    ; GCN: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GCN-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(i32) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i32))
    ; GCN-NEXT: S_ENDPGM 0, implicit [[AMDGPU_S_BUFFER_LOAD]](i32)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(i32)

...

---
name: s_buffer_load_v3s32
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GFX67-LABEL: name: s_buffer_load_v3s32
    ; GFX67: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: {{  $}}
    ; GFX67-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX67-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX67-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i32>)
    ; GFX67-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32)
    ; GFX67-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i32>)
    ;
    ; GFX12-LABEL: name: s_buffer_load_v3s32
    ; GFX12: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX12-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<3 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[AMDGPU_S_BUFFER_LOAD]](<3 x i32>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<3 x i32>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<3 x i32>)

...

---
name: s_buffer_load_v3p3
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GFX67-LABEL: name: s_buffer_load_v3p3
    ; GFX67: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: {{  $}}
    ; GFX67-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX67-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX67-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i32>)
    ; GFX67-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32)
    ; GFX67-NEXT: [[BITCAST:%[0-9]+]]:_(<3 x p3>) = G_BITCAST [[BUILD_VECTOR]](<3 x i32>)
    ; GFX67-NEXT: S_ENDPGM 0, implicit [[BITCAST]](<3 x p3>)
    ;
    ; GFX12-LABEL: name: s_buffer_load_v3p3
    ; GFX12: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX12-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<3 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX12-NEXT: [[BITCAST:%[0-9]+]]:_(<3 x p3>) = G_BITCAST [[AMDGPU_S_BUFFER_LOAD]](<3 x i32>)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[BITCAST]](<3 x p3>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<3 x p3>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<3 x p3>)

...

---
name: s_buffer_load_v6s16
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GFX67-LABEL: name: s_buffer_load_v6s16
    ; GFX67: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: {{  $}}
    ; GFX67-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX67-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX67-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i32>)
    ; GFX67-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32)
    ; GFX67-NEXT: [[BITCAST:%[0-9]+]]:_(<6 x i16>) = G_BITCAST [[BUILD_VECTOR]](<3 x i32>)
    ; GFX67-NEXT: S_ENDPGM 0, implicit [[BITCAST]](<6 x i16>)
    ;
    ; GFX12-LABEL: name: s_buffer_load_v6s16
    ; GFX12: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX12-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<3 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX12-NEXT: [[BITCAST:%[0-9]+]]:_(<6 x i16>) = G_BITCAST [[AMDGPU_S_BUFFER_LOAD]](<3 x i32>)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[BITCAST]](<6 x i16>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<6 x i16>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<6 x i16>)

...

---
name: s_buffer_load_v6s32
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GCN-LABEL: name: s_buffer_load_v6s32
    ; GCN: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GCN-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<8 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i192), align 32)
    ; GCN-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32), [[UV4:%[0-9]+]]:_(i32), [[UV5:%[0-9]+]]:_(i32), [[UV6:%[0-9]+]]:_(i32), [[UV7:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<8 x i32>)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<6 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32), [[UV3]](i32), [[UV4]](i32), [[UV5]](i32)
    ; GCN-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<6 x i32>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<6 x i32>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<6 x i32>)

...

---
name: s_buffer_load_v3s64
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GCN-LABEL: name: s_buffer_load_v3s64
    ; GCN: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GCN-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GCN-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i64>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i192), align 32)
    ; GCN-NEXT: [[UV:%[0-9]+]]:_(i64), [[UV1:%[0-9]+]]:_(i64), [[UV2:%[0-9]+]]:_(i64), [[UV3:%[0-9]+]]:_(i64) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i64>)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i64>) = G_BUILD_VECTOR [[UV]](i64), [[UV1]](i64), [[UV2]](i64)
    ; GCN-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i64>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<3 x i64>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<3 x i64>)

...

---
name: s_buffer_load_v12s8
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GFX67-LABEL: name: s_buffer_load_v12s8
    ; GFX67: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: {{  $}}
    ; GFX67-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX67-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX67-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i32>)
    ; GFX67-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 8
    ; GFX67-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C1]](i32)
    ; GFX67-NEXT: [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX67-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C2]](i32)
    ; GFX67-NEXT: [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 24
    ; GFX67-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C3]](i32)
    ; GFX67-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C1]](i32)
    ; GFX67-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C2]](i32)
    ; GFX67-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C3]](i32)
    ; GFX67-NEXT: [[LSHR6:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C1]](i32)
    ; GFX67-NEXT: [[LSHR7:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C2]](i32)
    ; GFX67-NEXT: [[LSHR8:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C3]](i32)
    ; GFX67-NEXT: [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 65535
    ; GFX67-NEXT: [[AND:%[0-9]+]]:_(i32) = G_AND [[UV]], [[C4]]
    ; GFX67-NEXT: [[AND1:%[0-9]+]]:_(i32) = G_AND [[LSHR]], [[C4]]
    ; GFX67-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[AND1]], [[C2]](i32)
    ; GFX67-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[AND]], [[SHL]]
    ; GFX67-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; GFX67-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[LSHR2]], [[C2]](i32)
    ; GFX67-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[LSHR1]], [[SHL1]]
    ; GFX67-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; GFX67-NEXT: [[AND2:%[0-9]+]]:_(i32) = G_AND [[UV1]], [[C4]]
    ; GFX67-NEXT: [[AND3:%[0-9]+]]:_(i32) = G_AND [[LSHR3]], [[C4]]
    ; GFX67-NEXT: [[SHL2:%[0-9]+]]:_(i32) = G_SHL [[AND3]], [[C2]](i32)
    ; GFX67-NEXT: [[OR2:%[0-9]+]]:_(i32) = G_OR [[AND2]], [[SHL2]]
    ; GFX67-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR2]](i32)
    ; GFX67-NEXT: [[SHL3:%[0-9]+]]:_(i32) = G_SHL [[LSHR5]], [[C2]](i32)
    ; GFX67-NEXT: [[OR3:%[0-9]+]]:_(i32) = G_OR [[LSHR4]], [[SHL3]]
    ; GFX67-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR3]](i32)
    ; GFX67-NEXT: [[AND4:%[0-9]+]]:_(i32) = G_AND [[UV2]], [[C4]]
    ; GFX67-NEXT: [[AND5:%[0-9]+]]:_(i32) = G_AND [[LSHR6]], [[C4]]
    ; GFX67-NEXT: [[SHL4:%[0-9]+]]:_(i32) = G_SHL [[AND5]], [[C2]](i32)
    ; GFX67-NEXT: [[OR4:%[0-9]+]]:_(i32) = G_OR [[AND4]], [[SHL4]]
    ; GFX67-NEXT: [[BITCAST4:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR4]](i32)
    ; GFX67-NEXT: [[SHL5:%[0-9]+]]:_(i32) = G_SHL [[LSHR8]], [[C2]](i32)
    ; GFX67-NEXT: [[OR5:%[0-9]+]]:_(i32) = G_OR [[LSHR7]], [[SHL5]]
    ; GFX67-NEXT: [[BITCAST5:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR5]](i32)
    ; GFX67-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<12 x i16>) = G_CONCAT_VECTORS [[BITCAST]](<2 x i16>), [[BITCAST1]](<2 x i16>), [[BITCAST2]](<2 x i16>), [[BITCAST3]](<2 x i16>), [[BITCAST4]](<2 x i16>), [[BITCAST5]](<2 x i16>)
    ; GFX67-NEXT: S_ENDPGM 0, implicit [[CONCAT_VECTORS]](<12 x i16>)
    ;
    ; GFX12-LABEL: name: s_buffer_load_v12s8
    ; GFX12: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX12-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<3 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX12-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<3 x i32>)
    ; GFX12-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 8
    ; GFX12-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C1]](i32)
    ; GFX12-NEXT: [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX12-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C2]](i32)
    ; GFX12-NEXT: [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 24
    ; GFX12-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[UV]], [[C3]](i32)
    ; GFX12-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[UV]](i32)
    ; GFX12-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX12-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX12-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; GFX12-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C1]](i32)
    ; GFX12-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C2]](i32)
    ; GFX12-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[UV1]], [[C3]](i32)
    ; GFX12-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[UV1]](i32)
    ; GFX12-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; GFX12-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; GFX12-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR5]](i32)
    ; GFX12-NEXT: [[LSHR6:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C1]](i32)
    ; GFX12-NEXT: [[LSHR7:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C2]](i32)
    ; GFX12-NEXT: [[LSHR8:%[0-9]+]]:_(i32) = G_LSHR [[UV2]], [[C3]](i32)
    ; GFX12-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[UV2]](i32)
    ; GFX12-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR6]](i32)
    ; GFX12-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR7]](i32)
    ; GFX12-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR8]](i32)
    ; GFX12-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC]](i16), [[TRUNC1]](i16)
    ; GFX12-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC2]](i16), [[TRUNC3]](i16)
    ; GFX12-NEXT: [[BUILD_VECTOR2:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC4]](i16), [[TRUNC5]](i16)
    ; GFX12-NEXT: [[BUILD_VECTOR3:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC6]](i16), [[TRUNC7]](i16)
    ; GFX12-NEXT: [[BUILD_VECTOR4:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC8]](i16), [[TRUNC9]](i16)
    ; GFX12-NEXT: [[BUILD_VECTOR5:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC10]](i16), [[TRUNC11]](i16)
    ; GFX12-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<12 x i16>) = G_CONCAT_VECTORS [[BUILD_VECTOR]](<2 x i16>), [[BUILD_VECTOR1]](<2 x i16>), [[BUILD_VECTOR2]](<2 x i16>), [[BUILD_VECTOR3]](<2 x i16>), [[BUILD_VECTOR4]](<2 x i16>), [[BUILD_VECTOR5]](<2 x i16>)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[CONCAT_VECTORS]](<12 x i16>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<12 x i8>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    %3:_(<12 x i16>) = G_ANYEXT %2(<12 x i8>)
    S_ENDPGM 0, implicit %3(<12 x i16>)

...

---
name: s_buffer_load_s96
body:             |
  bb.0:
    liveins: $sgpr0_sgpr1_sgpr2_sgpr3

    ; GFX67-LABEL: name: s_buffer_load_s96
    ; GFX67: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: {{  $}}
    ; GFX67-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX67-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX67-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<4 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX67-NEXT: [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32), [[UV3:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[AMDGPU_S_BUFFER_LOAD]](<4 x i32>)
    ; GFX67-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32)
    ; GFX67-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i32>)
    ;
    ; GFX12-LABEL: name: s_buffer_load_s96
    ; GFX12: liveins: $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    ; GFX12-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX12-NEXT: [[AMDGPU_S_BUFFER_LOAD:%[0-9]+]]:_(<3 x i32>) = G_AMDGPU_S_BUFFER_LOAD [[COPY]](<4 x i32>), [[C]](i32), 0 :: (dereferenceable invariant load (i96), align 16)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[AMDGPU_S_BUFFER_LOAD]](<3 x i32>)
    %0:_(<4 x i32>) = COPY $sgpr0_sgpr1_sgpr2_sgpr3
    %1:_(i32) = G_CONSTANT i32 0
    %2:_(<3 x i32>) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.buffer.load), %0(<4 x i32>), %1(i32), 0
    S_ENDPGM 0, implicit %2(<3 x i32>)

...
