# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=amdgcn -mcpu=fiji -run-pass=regbankselect -regbankselect-fast -verify-machineinstrs -o - %s | FileCheck %s
# RUN: llc -mtriple=amdgcn -mcpu=fiji -run-pass=regbankselect -regbankselect-greedy -verify-machineinstrs -o - %s | FileCheck %s

...

# Generate the 3 operand vector bitfield extract instructions for 32-bit
# operations only.
---
name:            test_sbfx_s32_vvv
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0, $vgpr1, $vgpr2

    ; CHECK-LABEL: name: test_sbfx_s32_vvv
    ; CHECK: liveins: $vgpr0, $vgpr1, $vgpr2
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY $vgpr1
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY $vgpr2
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[COPY]], [[COPY1]](i32), [[COPY2]]
    ; CHECK-NEXT: $vgpr0 = COPY [[SBFX]](i32)
    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr2
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $vgpr0 = COPY %3(i32)
...

---
name:            test_sbfx_s32_vii
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0

    ; CHECK-LABEL: name: test_sbfx_s32_vii
    ; CHECK: liveins: $vgpr0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY [[C]](i32)
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY [[C1]](i32)
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[COPY]], [[COPY1]](i32), [[COPY2]]
    ; CHECK-NEXT: $vgpr0 = COPY [[SBFX]](i32)
    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = G_CONSTANT i32 10
    %2:_(i32) = G_CONSTANT i32 4
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $vgpr0 = COPY %3(i32)
...

---
name:            test_sbfx_s32_vss
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0, $sgpr0, $sgpr1

    ; CHECK-LABEL: name: test_sbfx_s32_vss
    ; CHECK: liveins: $vgpr0, $sgpr0, $sgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:sgpr(i32) = COPY $sgpr0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:sgpr(i32) = COPY $sgpr1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:vgpr(i32) = COPY [[COPY1]](i32)
    ; CHECK-NEXT: [[COPY4:%[0-9]+]]:vgpr(i32) = COPY [[COPY2]](i32)
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[COPY]], [[COPY3]](i32), [[COPY4]]
    ; CHECK-NEXT: $vgpr0 = COPY [[SBFX]](i32)
    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $sgpr0
    %2:_(i32) = COPY $sgpr1
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $vgpr0 = COPY %3(i32)
...

# Expand to a sequence that implements the 64-bit bitfield extract using
# shifts and masks.
---
name:            test_sbfx_s64_vvv
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0_vgpr1, $vgpr2, $vgpr3

    ; CHECK-LABEL: name: test_sbfx_s64_vvv
    ; CHECK: liveins: $vgpr0_vgpr1, $vgpr2, $vgpr3
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i64) = COPY $vgpr0_vgpr1
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY $vgpr2
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY $vgpr3
    ; CHECK-NEXT: [[ASHR:%[0-9]+]]:vgpr(i64) = G_ASHR [[COPY]], [[COPY1]](i32)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[ASHR]](i64)
    ; CHECK-NEXT: [[C:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 64
    ; CHECK-NEXT: [[SUB:%[0-9]+]]:vgpr(i32) = G_SUB [[C]], [[COPY2]]
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:vgpr(i64) = G_SHL [[ASHR]], [[SUB]](i32)
    ; CHECK-NEXT: [[ASHR1:%[0-9]+]]:vgpr(i64) = G_ASHR [[SHL]], [[SUB]](i32)
    ; CHECK-NEXT: $vgpr0_vgpr1 = COPY %3:vgpr(i64)
    %0:_(i64) = COPY $vgpr0_vgpr1
    %1:_(i32) = COPY $vgpr2
    %2:_(i32) = COPY $vgpr3
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $vgpr0_vgpr1 = COPY %3(i64)
...

---
name:            test_sbfx_s64_vss
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0_vgpr1, $sgpr0, $sgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_vss
    ; CHECK: liveins: $vgpr0_vgpr1, $sgpr0, $sgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i64) = COPY $vgpr0_vgpr1
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY $vgpr1
    ; CHECK-NEXT: [[ASHR:%[0-9]+]]:vgpr(i64) = G_ASHR [[COPY]], [[COPY1]](i32)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[ASHR]](i64)
    ; CHECK-NEXT: [[C:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 64
    ; CHECK-NEXT: [[SUB:%[0-9]+]]:vgpr(i32) = G_SUB [[C]], [[COPY2]]
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:vgpr(i64) = G_SHL [[ASHR]], [[SUB]](i32)
    ; CHECK-NEXT: [[ASHR1:%[0-9]+]]:vgpr(i64) = G_ASHR [[SHL]], [[SUB]](i32)
    ; CHECK-NEXT: $vgpr0_vgpr1 = COPY %3:vgpr(i64)
    %0:_(i64) = COPY $vgpr0_vgpr1
    %1:_(i32) = COPY $vgpr0
    %2:_(i32) = COPY $vgpr1
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $vgpr0_vgpr1 = COPY %3(i64)
...

# If the offset and width are constants, use the 32-bit bitfield extract,
# and merge to create a 64-bit result.
---
name:            test_sbfx_s64_vii_small
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0_vgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_vii_small
    ; CHECK: liveins: $vgpr0_vgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i64) = COPY $vgpr0_vgpr1
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 31
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 4
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY [[C]](i32)
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY [[C1]](i32)
    ; CHECK-NEXT: [[ASHR:%[0-9]+]]:vgpr(i64) = G_ASHR [[COPY]], [[COPY1]](i32)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[ASHR]](i64)
    ; CHECK-NEXT: [[C2:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[UV]], [[C2]](i32), [[COPY2]]
    ; CHECK-NEXT: [[C3:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 31
    ; CHECK-NEXT: [[ASHR1:%[0-9]+]]:vgpr(i32) = G_ASHR [[SBFX]], [[C3]](i32)
    ; CHECK-NEXT: [[MV:%[0-9]+]]:vgpr(i64) = G_MERGE_VALUES [[SBFX]](i32), [[ASHR1]](i32)
    ; CHECK-NEXT: $vgpr0_vgpr1 = COPY [[MV]](i64)
    %0:_(i64) = COPY $vgpr0_vgpr1
    %1:_(i32) = G_CONSTANT i32 31
    %2:_(i32) = G_CONSTANT i32 4
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $vgpr0_vgpr1 = COPY %3(i64)
...

---
name:            test_sbfx_s64_vii_big
legalized: true

body: |
  bb.0.entry:
    liveins: $vgpr0_vgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_vii_big
    ; CHECK: liveins: $vgpr0_vgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:vgpr(i64) = COPY $vgpr0_vgpr1
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 40
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY [[C]](i32)
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY [[C1]](i32)
    ; CHECK-NEXT: [[ASHR:%[0-9]+]]:vgpr(i64) = G_ASHR [[COPY]], [[COPY1]](i32)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[ASHR]](i64)
    ; CHECK-NEXT: [[C2:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 0
    ; CHECK-NEXT: [[C3:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 8
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[UV1]], [[C2]](i32), [[C3]]
    ; CHECK-NEXT: [[MV:%[0-9]+]]:vgpr(i64) = G_MERGE_VALUES [[UV]](i32), [[SBFX]](i32)
    ; CHECK-NEXT: $vgpr0_vgpr1 = COPY [[MV]](i64)
    %0:_(i64) = COPY $vgpr0_vgpr1
    %1:_(i32) = G_CONSTANT i32 8
    %2:_(i32) = G_CONSTANT i32 40
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $vgpr0_vgpr1 = COPY %3(i64)
...

---
name:            test_sbfx_s64_svv
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0_sgpr1, $vgpr0, $vgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_svv
    ; CHECK: liveins: $sgpr0_sgpr1, $vgpr0, $vgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sgpr(i64) = COPY $sgpr0_sgpr1
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY $vgpr1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:vgpr(i64) = COPY [[COPY]](i64)
    ; CHECK-NEXT: [[ASHR:%[0-9]+]]:vgpr(i64) = G_ASHR [[COPY3]], [[COPY1]](i32)
    ; CHECK-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[ASHR]](i64)
    ; CHECK-NEXT: [[C:%[0-9]+]]:vgpr(i32) = G_CONSTANT i32 64
    ; CHECK-NEXT: [[SUB:%[0-9]+]]:vgpr(i32) = G_SUB [[C]], [[COPY2]]
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:vgpr(i64) = G_SHL [[ASHR]], [[SUB]](i32)
    ; CHECK-NEXT: [[ASHR1:%[0-9]+]]:vgpr(i64) = G_ASHR [[SHL]], [[SUB]](i32)
    ; CHECK-NEXT: $vgpr0_vgpr1 = COPY %3:vgpr(i64)
    %0:_(i64) = COPY $sgpr0_sgpr1
    %1:_(i32) = COPY $vgpr0
    %2:_(i32) = COPY $vgpr1
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $vgpr0_vgpr1 = COPY %3(i64)
...

# Expand to a sequence that combines the offset and width for the two operand
# version of the 32-bit instruction.
---
name:            test_sbfx_s32_svv
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0, $vgpr0, $vgpr1

    ; CHECK-LABEL: name: test_sbfx_s32_svv
    ; CHECK: liveins: $sgpr0, $vgpr0, $vgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sgpr(i32) = COPY $sgpr0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:vgpr(i32) = COPY $vgpr0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:vgpr(i32) = COPY $vgpr1
    ; CHECK-NEXT: [[COPY3:%[0-9]+]]:vgpr(i32) = COPY [[COPY]](i32)
    ; CHECK-NEXT: [[SBFX:%[0-9]+]]:vgpr(i32) = G_SBFX [[COPY3]], [[COPY1]](i32), [[COPY2]]
    ; CHECK-NEXT: $vgpr0 = COPY [[SBFX]](i32)
    %0:_(i32) = COPY $sgpr0
    %1:_(i32) = COPY $vgpr0
    %2:_(i32) = COPY $vgpr1
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $vgpr0 = COPY %3(i32)
...

---
name:            test_sbfx_s32_sss
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0, $sgpr1, $sgpr3

    ; CHECK-LABEL: name: test_sbfx_s32_sss
    ; CHECK: liveins: $sgpr0, $sgpr1, $sgpr3
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sreg_32(i32) = COPY $sgpr0
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:sgpr(i32) = COPY $sgpr1
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:sgpr(i32) = COPY $sgpr2
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 63
    ; CHECK-NEXT: [[AND:%[0-9]+]]:sgpr(i32) = G_AND [[COPY1]], [[C]]
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 16
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:sgpr(i32) = G_SHL [[COPY2]], [[C1]](i32)
    ; CHECK-NEXT: [[OR:%[0-9]+]]:sreg_32(i32) = G_OR [[AND]], [[SHL]]
    ; CHECK-NEXT: [[S_BFE_I32_:%[0-9]+]]:sreg_32(i32) = S_BFE_I32 [[COPY]](i32), [[OR]](i32), implicit-def $scc
    ; CHECK-NEXT: $sgpr0 = COPY [[S_BFE_I32_]](i32)
    %0:_(i32) = COPY $sgpr0
    %1:_(i32) = COPY $sgpr1
    %2:_(i32) = COPY $sgpr2
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $sgpr0 = COPY %3(i32)
...

---
name:            test_sbfx_s32_sii
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0

    ; CHECK-LABEL: name: test_sbfx_s32_sii
    ; CHECK: liveins: $sgpr0
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sreg_32(i32) = COPY $sgpr0
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C2:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 63
    ; CHECK-NEXT: [[AND:%[0-9]+]]:sgpr(i32) = G_AND [[C]], [[C2]]
    ; CHECK-NEXT: [[C3:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 16
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:sgpr(i32) = G_SHL [[C1]], [[C3]](i32)
    ; CHECK-NEXT: [[OR:%[0-9]+]]:sreg_32(i32) = G_OR [[AND]], [[SHL]]
    ; CHECK-NEXT: [[S_BFE_I32_:%[0-9]+]]:sreg_32(i32) = S_BFE_I32 [[COPY]](i32), [[OR]](i32), implicit-def $scc
    ; CHECK-NEXT: $sgpr0 = COPY [[S_BFE_I32_]](i32)
    %0:_(i32) = COPY $sgpr0
    %1:_(i32) = G_CONSTANT i32 1
    %2:_(i32) = G_CONSTANT i32 10
    %3:_(i32) = G_SBFX %0, %1(i32), %2
    $sgpr0 = COPY %3(i32)
...

# Expand to a sequence that combines the offset and width for the two operand
# version of the 64-bit scalar instruction.
---
name:            test_sbfx_s64_sss
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0_sgpr1, $sgpr0, $sgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_sss
    ; CHECK: liveins: $sgpr0_sgpr1, $sgpr0, $sgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sreg_64(i64) = COPY $sgpr0_sgpr1
    ; CHECK-NEXT: [[COPY1:%[0-9]+]]:sgpr(i32) = COPY $sgpr0
    ; CHECK-NEXT: [[COPY2:%[0-9]+]]:sgpr(i32) = COPY $sgpr1
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 63
    ; CHECK-NEXT: [[AND:%[0-9]+]]:sgpr(i32) = G_AND [[COPY1]], [[C]]
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 16
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:sgpr(i32) = G_SHL [[COPY2]], [[C1]](i32)
    ; CHECK-NEXT: [[OR:%[0-9]+]]:sreg_32(i32) = G_OR [[AND]], [[SHL]]
    ; CHECK-NEXT: [[S_BFE_I64_:%[0-9]+]]:sreg_64(i64) = S_BFE_I64 [[COPY]](i64), [[OR]](i32), implicit-def $scc
    ; CHECK-NEXT: $sgpr0_sgpr1 = COPY [[S_BFE_I64_]](i64)
    %0:_(i64) = COPY $sgpr0_sgpr1
    %1:_(i32) = COPY $sgpr0
    %2:_(i32) = COPY $sgpr1
    %3:_(i64) = G_SBFX %0, %1(i32), %2
    $sgpr0_sgpr1 = COPY %3(i64)
...

---
name:            test_sbfx_s64_sii
legalized: true

body: |
  bb.0.entry:
    liveins: $sgpr0_sgpr1

    ; CHECK-LABEL: name: test_sbfx_s64_sii
    ; CHECK: liveins: $sgpr0_sgpr1
    ; CHECK-NEXT: {{  $}}
    ; CHECK-NEXT: [[COPY:%[0-9]+]]:sreg_64(i64) = COPY $sgpr0_sgpr1
    ; CHECK-NEXT: [[C:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 1
    ; CHECK-NEXT: [[C1:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 10
    ; CHECK-NEXT: [[C2:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 63
    ; CHECK-NEXT: [[AND:%[0-9]+]]:sgpr(i32) = G_AND [[C]], [[C2]]
    ; CHECK-NEXT: [[C3:%[0-9]+]]:sgpr(i32) = G_CONSTANT i32 16
    ; CHECK-NEXT: [[SHL:%[0-9]+]]:sgpr(i32) = G_SHL [[C1]], [[C3]](i32)
    ; CHECK-NEXT: [[OR:%[0-9]+]]:sreg_32(i32) = G_OR [[AND]], [[SHL]]
    ; CHECK-NEXT: [[S_BFE_I64_:%[0-9]+]]:sreg_64(i64) = S_BFE_I64 [[COPY]](i64), [[OR]](i32), implicit-def $scc
    %0:_(i64) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_CONSTANT i32 1
    %2:_(i32) = G_CONSTANT i32 10
    %3:_(i64) = G_SBFX %0, %1(i32), %2
...
