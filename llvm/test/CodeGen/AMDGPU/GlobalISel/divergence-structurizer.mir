# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 4
# RUN: llc -global-isel -mtriple=amdgcn-mesa-amdpal -mcpu=gfx1010 -run-pass=amdgpu-global-isel-divergence-lowering -verify-machineinstrs %s -o - | FileCheck -check-prefix=GFX10 %s

---
name: divergent_i1_phi_if_then
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_if_then
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 6
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(uge), [[COPY2]](i32), [[C]]
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(eq), [[COPY3]](i32), [[C1]]
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP]](i1)
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[COPY4]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP1]](i1), %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.1
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[COPY2]](i32), [[C2]]
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY5]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY6]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[COPY4]](i1), %bb.0, [[S_OR_B32_]](i1), %bb.1
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF]](i32)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY7]](i1), [[C4]], [[C3]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x40000000), %bb.2(0x40000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(i32) = G_CONSTANT i32 6
    %6:_(i1) = G_ICMP intpred(uge), %3(i32), %5
    %7:_(i32) = G_CONSTANT i32 0
    %8:sreg_32_xm0_xexec(i1) = G_ICMP intpred(eq), %4(i32), %7
    %9:sreg_32_xm0_xexec(i32) = SI_IF %8(i1), %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.1

  bb.1:
    successors: %bb.2(0x80000000)

    %10:_(i32) = G_CONSTANT i32 1
    %11:_(i1) = G_ICMP intpred(ult), %3(i32), %10

  bb.2:
    %12:_(i1) = G_PHI %6(i1), %bb.0, %11(i1), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %9(i32)
    %13:_(i32) = G_CONSTANT i32 2
    %14:_(i32) = G_CONSTANT i32 1
    %15:_(i32) = G_SELECT %12(i1), %14, %13
    G_STORE %15(i32), %2(p1) :: (store (i32), addrspace 1)
    S_ENDPGM 0




...

---
name: divergent_i1_phi_if_else
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_if_else
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.3(0x40000000), %bb.1(0x40000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i1) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[COPY3]](i32), [[C]]
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[DEF]](i1)
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[COPY4]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP]](i1), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x40000000), %bb.4(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[COPY4]](i1), %bb.0, %20(i1), %bb.3
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[COPY6]](i1)
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[COPY7]](i1)
  ; GFX10-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_ELSE [[SI_IF]](i32), %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(uge), [[COPY2]](i32), [[C1]]
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP1]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY8]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY9]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[COPY2]](i32), [[C2]]
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY5]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY10]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.1
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:sreg_32(i1) = PHI [[COPY7]](i1), %bb.1, [[S_OR_B32_]](i1), %bb.2
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[PHI1]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_ELSE]](i32)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY11]](i1), [[C3]], [[C4]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.3(0x40000000), %bb.1(0x40000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(i1) = G_IMPLICIT_DEF
    %6:_(i32) = G_CONSTANT i32 0
    %7:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %4(i32), %6
    %8:sreg_32_xm0_xexec(i32) = SI_IF %7(i1), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.3

  bb.1:
    successors: %bb.2(0x40000000), %bb.4(0x40000000)

    %9:_(i1) = G_PHI %10(i1), %bb.3, %5(i1), %bb.0
    %11:sreg_32_xm0_xexec(i32) = SI_ELSE %8(i32), %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2

  bb.2:
    successors: %bb.4(0x80000000)

    %12:_(i32) = G_CONSTANT i32 1
    %13:_(i1) = G_ICMP intpred(uge), %3(i32), %12
    G_BR %bb.4

  bb.3:
    successors: %bb.1(0x80000000)

    %14:_(i32) = G_CONSTANT i32 2
    %10:_(i1) = G_ICMP intpred(ult), %3(i32), %14
    G_BR %bb.1

  bb.4:
    %15:_(i1) = G_PHI %9(i1), %bb.1, %13(i1), %bb.2
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %11(i32)
    %16:_(i32) = G_CONSTANT i32 1
    %17:_(i32) = G_CONSTANT i32 2
    %18:_(i32) = G_SELECT %15(i1), %16, %17
    G_STORE %18(i32), %2(p1) :: (store (i32), addrspace 1)
    S_ENDPGM 0








...

---
name: loop_with_1break
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: loop_with_1break
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY2]](i32), [[COPY3]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i32) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF1:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x40000000), %bb.3(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[DEF1]](i1), %bb.0, %35(i1), %bb.3
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:_(i32) = G_PHI %9(i32), %bb.3, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.0, %11(i32), %bb.3
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[C1]](i1)
  ; GFX10-NEXT:   [[SEXT:%[0-9]+]]:_(i64) = G_SEXT [[PHI2]](i32)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C2]](i32)
  ; GFX10-NEXT:   [[PTR_ADD:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV1]], [[SHL]](i64)
  ; GFX10-NEXT:   [[LOAD:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD]](i32), [[C3]]
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY4]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY5]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP]](i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL1:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C4]](i32)
  ; GFX10-NEXT:   [[PTR_ADD1:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV]], [[SHL1]](i64)
  ; GFX10-NEXT:   [[LOAD1:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD1]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[LOAD1]], [[C5]]
  ; GFX10-NEXT:   G_STORE [[ADD]](i32), [[PTR_ADD1]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   [[ADD1:%[0-9]+]]:_(i32) = G_ADD [[PHI2]], [[C5]]
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i32) = G_CONSTANT i32 100
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[PHI2]](i32), [[C6]]
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP1]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY6]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY7]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.4(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:sreg_32(i1) = PHI [[S_OR_B32_]](i1), %bb.1, [[S_OR_B32_1]](i1), %bb.2
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI [[ADD1]](i32), %bb.2, [[DEF]](i32), %bb.1
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[PHI3]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF]](i32)
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[COPY8]](i1), [[PHI1]](i32)
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.3
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI5]](i32)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(p1) = G_MERGE_VALUES %3(i32), %4(i32)
    %6:_(i32) = G_CONSTANT i32 0
    %7:_(i32) = G_IMPLICIT_DEF

  bb.1:
    successors: %bb.2(0x40000000), %bb.3(0x40000000)

    %8:_(i32) = G_PHI %9(i32), %bb.3, %6(i32), %bb.0
    %10:_(i32) = G_PHI %6(i32), %bb.0, %11(i32), %bb.3
    %12:_(i1) = G_CONSTANT i1 true
    %13:_(i64) = G_SEXT %10(i32)
    %14:_(i32) = G_CONSTANT i32 2
    %15:_(i64) = G_SHL %13, %14(i32)
    %16:_(p1) = G_PTR_ADD %5, %15(i64)
    %17:_(i32) = G_LOAD %16(p1) :: (load (i32), addrspace 1)
    %18:_(i32) = G_CONSTANT i32 0
    %19:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %17(i32), %18
    %20:sreg_32_xm0_xexec(i32) = SI_IF %19(i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2

  bb.2:
    successors: %bb.3(0x80000000)

    %21:_(i32) = G_CONSTANT i32 2
    %22:_(i64) = G_SHL %13, %21(i32)
    %23:_(p1) = G_PTR_ADD %2, %22(i64)
    %24:_(i32) = G_LOAD %23(p1) :: (load (i32), addrspace 1)
    %25:_(i32) = G_CONSTANT i32 1
    %26:_(i32) = G_ADD %24, %25
    G_STORE %26(i32), %23(p1) :: (store (i32), addrspace 1)
    %27:_(i32) = G_ADD %10, %25
    %28:_(i32) = G_CONSTANT i32 100
    %29:_(i1) = G_ICMP intpred(ult), %10(i32), %28

  bb.3:
    successors: %bb.4(0x04000000), %bb.1(0x7c000000)

    %11:_(i32) = G_PHI %27(i32), %bb.2, %7(i32), %bb.1
    %30:_(i1) = G_PHI %29(i1), %bb.2, %12(i1), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %20(i32)
    %9:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %30(i1), %8(i32)
    SI_LOOP %9(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.4

  bb.4:
    %31:_(i32) = G_PHI %9(i32), %bb.3
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %31(i32)
    S_ENDPGM 0








...

---
name: loop_with_2breaks
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: loop_with_2breaks
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY2]](i32), [[COPY3]](i32)
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX10-NEXT:   [[MV2:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY4]](i32), [[COPY5]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i32) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF1:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x40000000), %bb.3(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[DEF1]](i1), %bb.0, %48(i1), %bb.3
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:_(i32) = G_PHI %12(i32), %bb.3, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.0, %14(i32), %bb.3
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[C1]](i1)
  ; GFX10-NEXT:   [[SEXT:%[0-9]+]]:_(i64) = G_SEXT [[PHI2]](i32)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C2]](i32)
  ; GFX10-NEXT:   [[PTR_ADD:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV1]], [[SHL]](i64)
  ; GFX10-NEXT:   [[LOAD:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD]](i32), [[C3]]
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY6]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY7]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP]](i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x40000000), %bb.5(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL1:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C5]](i32)
  ; GFX10-NEXT:   [[PTR_ADD1:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV2]], [[SHL1]](i64)
  ; GFX10-NEXT:   [[LOAD1:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD1]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD1]](i32), [[C6]]
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[C4]](i1)
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32(i1) = COPY [[COPY9]](i1)
  ; GFX10-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP1]](i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.6(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:sreg_32(i1) = PHI [[S_OR_B32_]](i1), %bb.1, %47(i1), %bb.5
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI %32(i32), %bb.5, [[DEF]](i32), %bb.1
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[PHI3]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF]](i32)
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[COPY11]](i1), [[PHI1]](i32)
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   successors: %bb.5(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C7:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL2:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C7]](i32)
  ; GFX10-NEXT:   [[PTR_ADD2:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV]], [[SHL2]](i64)
  ; GFX10-NEXT:   [[LOAD2:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD2]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C8:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[LOAD2]], [[C8]]
  ; GFX10-NEXT:   G_STORE [[ADD]](i32), [[PTR_ADD2]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   [[ADD1:%[0-9]+]]:_(i32) = G_ADD [[PHI2]], [[C8]]
  ; GFX10-NEXT:   [[C9:%[0-9]+]]:_(i32) = G_CONSTANT i32 100
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[PHI2]](i32), [[C9]]
  ; GFX10-NEXT:   [[COPY12:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY10]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY12]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:sreg_32(i1) = PHI [[COPY9]](i1), %bb.2, [[S_OR_B32_1]](i1), %bb.4
  ; GFX10-NEXT:   [[PHI6:%[0-9]+]]:_(i32) = G_PHI [[ADD1]](i32), %bb.4, [[DEF]](i32), %bb.2
  ; GFX10-NEXT:   [[COPY13:%[0-9]+]]:sreg_32(i1) = COPY [[PHI5]](i1)
  ; GFX10-NEXT:   [[COPY14:%[0-9]+]]:sreg_32(i1) = COPY [[COPY13]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF1]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_2:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY8]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY14]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_2:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_2]](i1), [[S_AND_B32_2]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   [[PHI7:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.3
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI7]](i32)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(p1) = G_MERGE_VALUES %3(i32), %4(i32)
    %6:_(i32) = COPY $vgpr4
    %7:_(i32) = COPY $vgpr5
    %8:_(p1) = G_MERGE_VALUES %6(i32), %7(i32)
    %9:_(i32) = G_CONSTANT i32 0
    %10:_(i32) = G_IMPLICIT_DEF

  bb.1:
    successors: %bb.2(0x40000000), %bb.3(0x40000000)

    %11:_(i32) = G_PHI %12(i32), %bb.3, %9(i32), %bb.0
    %13:_(i32) = G_PHI %9(i32), %bb.0, %14(i32), %bb.3
    %15:_(i1) = G_CONSTANT i1 true
    %16:_(i64) = G_SEXT %13(i32)
    %17:_(i32) = G_CONSTANT i32 2
    %18:_(i64) = G_SHL %16, %17(i32)
    %19:_(p1) = G_PTR_ADD %5, %18(i64)
    %20:_(i32) = G_LOAD %19(p1) :: (load (i32), addrspace 1)
    %21:_(i32) = G_CONSTANT i32 0
    %22:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %20(i32), %21
    %23:sreg_32_xm0_xexec(i32) = SI_IF %22(i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2

  bb.2:
    successors: %bb.4(0x40000000), %bb.5(0x40000000)

    %24:_(i1) = G_CONSTANT i1 true
    %25:_(i32) = G_CONSTANT i32 2
    %26:_(i64) = G_SHL %16, %25(i32)
    %27:_(p1) = G_PTR_ADD %8, %26(i64)
    %28:_(i32) = G_LOAD %27(p1) :: (load (i32), addrspace 1)
    %29:_(i32) = G_CONSTANT i32 0
    %30:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %28(i32), %29
    %31:sreg_32_xm0_xexec(i32) = SI_IF %30(i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.4

  bb.3:
    successors: %bb.6(0x04000000), %bb.1(0x7c000000)

    %14:_(i32) = G_PHI %32(i32), %bb.5, %10(i32), %bb.1
    %33:_(i1) = G_PHI %34(i1), %bb.5, %15(i1), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %23(i32)
    %12:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %33(i1), %11(i32)
    SI_LOOP %12(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.6

  bb.4:
    successors: %bb.5(0x80000000)

    %35:_(i32) = G_CONSTANT i32 2
    %36:_(i64) = G_SHL %16, %35(i32)
    %37:_(p1) = G_PTR_ADD %2, %36(i64)
    %38:_(i32) = G_LOAD %37(p1) :: (load (i32), addrspace 1)
    %39:_(i32) = G_CONSTANT i32 1
    %40:_(i32) = G_ADD %38, %39
    G_STORE %40(i32), %37(p1) :: (store (i32), addrspace 1)
    %41:_(i32) = G_ADD %13, %39
    %42:_(i32) = G_CONSTANT i32 100
    %43:_(i1) = G_ICMP intpred(ult), %13(i32), %42

  bb.5:
    successors: %bb.3(0x80000000)

    %32:_(i32) = G_PHI %41(i32), %bb.4, %10(i32), %bb.2
    %34:_(i1) = G_PHI %43(i1), %bb.4, %24(i1), %bb.2
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %31(i32)
    G_BR %bb.3

  bb.6:
    %44:_(i32) = G_PHI %12(i32), %bb.3
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %44(i32)
    S_ENDPGM 0












...

---
name: loop_with_3breaks
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: loop_with_3breaks
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY2]](i32), [[COPY3]](i32)
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX10-NEXT:   [[MV2:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY4]](i32), [[COPY5]](i32)
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr6
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:_(i32) = COPY $vgpr7
  ; GFX10-NEXT:   [[MV3:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY6]](i32), [[COPY7]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i32) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF1:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x40000000), %bb.3(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[DEF1]](i1), %bb.0, %61(i1), %bb.3
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:_(i32) = G_PHI %15(i32), %bb.3, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.0, %17(i32), %bb.3
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[C1]](i1)
  ; GFX10-NEXT:   [[SEXT:%[0-9]+]]:_(i64) = G_SEXT [[PHI2]](i32)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C2]](i32)
  ; GFX10-NEXT:   [[PTR_ADD:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV1]], [[SHL]](i64)
  ; GFX10-NEXT:   [[LOAD:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD]](i32), [[C3]]
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY8]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY9]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP]](i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x40000000), %bb.5(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL1:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C5]](i32)
  ; GFX10-NEXT:   [[PTR_ADD1:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV2]], [[SHL1]](i64)
  ; GFX10-NEXT:   [[LOAD1:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD1]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD1]](i32), [[C6]]
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[C4]](i1)
  ; GFX10-NEXT:   [[COPY12:%[0-9]+]]:sreg_32(i1) = COPY [[COPY11]](i1)
  ; GFX10-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP1]](i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.8(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:sreg_32(i1) = PHI [[S_OR_B32_]](i1), %bb.1, %60(i1), %bb.5
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI %35(i32), %bb.5, [[DEF]](i32), %bb.1
  ; GFX10-NEXT:   [[COPY13:%[0-9]+]]:sreg_32(i1) = COPY [[PHI3]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF]](i32)
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[COPY13]](i1), [[PHI1]](i32)
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.8
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   successors: %bb.6(0x40000000), %bb.7(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C7:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[C8:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL2:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C8]](i32)
  ; GFX10-NEXT:   [[PTR_ADD2:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV3]], [[SHL2]](i64)
  ; GFX10-NEXT:   [[LOAD2:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD2]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C9:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD2]](i32), [[C9]]
  ; GFX10-NEXT:   [[COPY14:%[0-9]+]]:sreg_32(i1) = COPY [[C7]](i1)
  ; GFX10-NEXT:   [[COPY15:%[0-9]+]]:sreg_32(i1) = COPY [[COPY14]](i1)
  ; GFX10-NEXT:   [[SI_IF2:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP2]](i1), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:sreg_32(i1) = PHI [[COPY11]](i1), %bb.2, %72(i1), %bb.7
  ; GFX10-NEXT:   [[PHI6:%[0-9]+]]:_(i32) = G_PHI %46(i32), %bb.7, [[DEF]](i32), %bb.2
  ; GFX10-NEXT:   [[COPY16:%[0-9]+]]:sreg_32(i1) = COPY [[PHI5]](i1)
  ; GFX10-NEXT:   [[COPY17:%[0-9]+]]:sreg_32(i1) = COPY [[COPY16]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF1]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY10]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY17]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   successors: %bb.7(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C10:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL3:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C10]](i32)
  ; GFX10-NEXT:   [[PTR_ADD3:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV]], [[SHL3]](i64)
  ; GFX10-NEXT:   [[LOAD3:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD3]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C11:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[LOAD3]], [[C11]]
  ; GFX10-NEXT:   G_STORE [[ADD]](i32), [[PTR_ADD3]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   [[ADD1:%[0-9]+]]:_(i32) = G_ADD [[PHI2]], [[C11]]
  ; GFX10-NEXT:   [[C12:%[0-9]+]]:_(i32) = G_CONSTANT i32 100
  ; GFX10-NEXT:   [[ICMP3:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[PHI2]](i32), [[C12]]
  ; GFX10-NEXT:   [[COPY18:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP3]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_2:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY15]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY18]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_2:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_2]](i1), [[S_AND_B32_2]](i1), implicit-def $scc
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.7:
  ; GFX10-NEXT:   successors: %bb.5(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI7:%[0-9]+]]:sreg_32(i1) = PHI [[COPY14]](i1), %bb.4, [[S_OR_B32_2]](i1), %bb.6
  ; GFX10-NEXT:   [[PHI8:%[0-9]+]]:_(i32) = G_PHI [[ADD1]](i32), %bb.6, [[DEF]](i32), %bb.4
  ; GFX10-NEXT:   [[COPY19:%[0-9]+]]:sreg_32(i1) = COPY [[PHI7]](i1)
  ; GFX10-NEXT:   [[COPY20:%[0-9]+]]:sreg_32(i1) = COPY [[COPY19]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF2]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_3:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY12]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_3:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY20]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_3:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_3]](i1), [[S_AND_B32_3]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.8:
  ; GFX10-NEXT:   [[PHI9:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.3
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI9]](i32)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(p1) = G_MERGE_VALUES %3(i32), %4(i32)
    %6:_(i32) = COPY $vgpr4
    %7:_(i32) = COPY $vgpr5
    %8:_(p1) = G_MERGE_VALUES %6(i32), %7(i32)
    %9:_(i32) = COPY $vgpr6
    %10:_(i32) = COPY $vgpr7
    %11:_(p1) = G_MERGE_VALUES %9(i32), %10(i32)
    %12:_(i32) = G_CONSTANT i32 0
    %13:_(i32) = G_IMPLICIT_DEF

  bb.1:
    successors: %bb.2(0x40000000), %bb.3(0x40000000)

    %14:_(i32) = G_PHI %15(i32), %bb.3, %12(i32), %bb.0
    %16:_(i32) = G_PHI %12(i32), %bb.0, %17(i32), %bb.3
    %18:_(i1) = G_CONSTANT i1 true
    %19:_(i64) = G_SEXT %16(i32)
    %20:_(i32) = G_CONSTANT i32 2
    %21:_(i64) = G_SHL %19, %20(i32)
    %22:_(p1) = G_PTR_ADD %5, %21(i64)
    %23:_(i32) = G_LOAD %22(p1) :: (load (i32), addrspace 1)
    %24:_(i32) = G_CONSTANT i32 0
    %25:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %23(i32), %24
    %26:sreg_32_xm0_xexec(i32) = SI_IF %25(i1), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2

  bb.2:
    successors: %bb.4(0x40000000), %bb.5(0x40000000)

    %27:_(i1) = G_CONSTANT i1 true
    %28:_(i32) = G_CONSTANT i32 2
    %29:_(i64) = G_SHL %19, %28(i32)
    %30:_(p1) = G_PTR_ADD %8, %29(i64)
    %31:_(i32) = G_LOAD %30(p1) :: (load (i32), addrspace 1)
    %32:_(i32) = G_CONSTANT i32 0
    %33:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %31(i32), %32
    %34:sreg_32_xm0_xexec(i32) = SI_IF %33(i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.4

  bb.3:
    successors: %bb.8(0x04000000), %bb.1(0x7c000000)

    %17:_(i32) = G_PHI %35(i32), %bb.5, %13(i32), %bb.1
    %36:_(i1) = G_PHI %37(i1), %bb.5, %18(i1), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %26(i32)
    %15:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %36(i1), %14(i32)
    SI_LOOP %15(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.8

  bb.4:
    successors: %bb.6(0x40000000), %bb.7(0x40000000)

    %38:_(i1) = G_CONSTANT i1 true
    %39:_(i32) = G_CONSTANT i32 2
    %40:_(i64) = G_SHL %19, %39(i32)
    %41:_(p1) = G_PTR_ADD %11, %40(i64)
    %42:_(i32) = G_LOAD %41(p1) :: (load (i32), addrspace 1)
    %43:_(i32) = G_CONSTANT i32 0
    %44:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %42(i32), %43
    %45:sreg_32_xm0_xexec(i32) = SI_IF %44(i1), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.6

  bb.5:
    successors: %bb.3(0x80000000)

    %35:_(i32) = G_PHI %46(i32), %bb.7, %13(i32), %bb.2
    %37:_(i1) = G_PHI %47(i1), %bb.7, %27(i1), %bb.2
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %34(i32)
    G_BR %bb.3

  bb.6:
    successors: %bb.7(0x80000000)

    %48:_(i32) = G_CONSTANT i32 2
    %49:_(i64) = G_SHL %19, %48(i32)
    %50:_(p1) = G_PTR_ADD %2, %49(i64)
    %51:_(i32) = G_LOAD %50(p1) :: (load (i32), addrspace 1)
    %52:_(i32) = G_CONSTANT i32 1
    %53:_(i32) = G_ADD %51, %52
    G_STORE %53(i32), %50(p1) :: (store (i32), addrspace 1)
    %54:_(i32) = G_ADD %16, %52
    %55:_(i32) = G_CONSTANT i32 100
    %56:_(i1) = G_ICMP intpred(ult), %16(i32), %55

  bb.7:
    successors: %bb.5(0x80000000)

    %46:_(i32) = G_PHI %54(i32), %bb.6, %13(i32), %bb.4
    %47:_(i1) = G_PHI %56(i1), %bb.6, %38(i1), %bb.4
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %45(i32)
    G_BR %bb.5

  bb.8:
    %57:_(i32) = G_PHI %15(i32), %bb.3
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %57(i32)
    S_ENDPGM 0
















...

---
name: loop_with_div_break_with_body
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: loop_with_div_break_with_body
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY2]](i32), [[COPY3]](i32)
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX10-NEXT:   [[MV2:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY4]](i32), [[COPY5]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i32) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF1:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF2:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF3:%[0-9]+]]:sreg_32_xm0_xexec(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.3(0x40000000), %bb.5(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32_xm0_xexec(i1) = PHI [[DEF3]](i1), %bb.0, %67(i1), %bb.5
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:sreg_32(i1) = PHI [[DEF2]](i1), %bb.0, %56(i1), %bb.5
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:sreg_32(i1) = PHI [[DEF1]](i1), %bb.0, %43(i1), %bb.5
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:_(i32) = G_PHI %12(i32), %bb.5, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.0, %14(i32), %bb.5
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[PHI1]](i1)
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[PHI2]](i1)
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[C1]](i1)
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32(i1) = COPY [[C1]](i1)
  ; GFX10-NEXT:   [[SEXT:%[0-9]+]]:_(i64) = G_SEXT [[PHI4]](i32)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C2]](i32)
  ; GFX10-NEXT:   [[PTR_ADD:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV1]], [[SHL]](i64)
  ; GFX10-NEXT:   [[LOAD:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), [[LOAD]](i32), [[C3]]
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY8]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY10]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY7]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY9]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY12:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_1]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[ICMP]](i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 10
  ; GFX10-NEXT:   G_STORE [[C4]](i32), [[MV2]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.5(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i1) = G_CONSTANT i1 false
  ; GFX10-NEXT:   [[COPY13:%[0-9]+]]:sreg_32(i1) = COPY [[C5]](i1)
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL1:%[0-9]+]]:_(i64) = G_SHL [[SEXT]], [[C6]](i32)
  ; GFX10-NEXT:   [[PTR_ADD1:%[0-9]+]]:_(p1) = G_PTR_ADD [[MV]], [[SHL1]](i64)
  ; GFX10-NEXT:   [[LOAD1:%[0-9]+]]:_(i32) = G_LOAD [[PTR_ADD1]](p1) :: (load (i32), addrspace 1)
  ; GFX10-NEXT:   [[C7:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[LOAD1]], [[C7]]
  ; GFX10-NEXT:   G_STORE [[ADD]](i32), [[PTR_ADD1]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   [[ADD1:%[0-9]+]]:_(i32) = G_ADD [[PHI4]], [[C7]]
  ; GFX10-NEXT:   [[C8:%[0-9]+]]:_(i32) = G_CONSTANT i32 100
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[PHI4]](i32), [[C8]]
  ; GFX10-NEXT:   [[COPY14:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP1]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_2:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY11]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY13]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_2:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_2]](i1), [[S_AND_B32_2]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_ANDN2_B32_3:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY12]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_3:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY14]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_3:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_3]](i1), [[S_AND_B32_3]](i1), implicit-def $scc
  ; GFX10-NEXT:   G_BR %bb.5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %35(i32)
  ; GFX10-NEXT:   S_ENDPGM 0
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   successors: %bb.6(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:sreg_32(i1) = PHI [[S_OR_B32_1]](i1), %bb.1, [[S_OR_B32_3]](i1), %bb.3
  ; GFX10-NEXT:   [[PHI6:%[0-9]+]]:sreg_32(i1) = PHI [[S_OR_B32_]](i1), %bb.1, [[S_OR_B32_2]](i1), %bb.3
  ; GFX10-NEXT:   [[PHI7:%[0-9]+]]:_(i32) = G_PHI [[ADD1]](i32), %bb.3, [[DEF]](i32), %bb.1
  ; GFX10-NEXT:   [[COPY15:%[0-9]+]]:sreg_32(i1) = COPY [[PHI5]](i1)
  ; GFX10-NEXT:   [[COPY16:%[0-9]+]]:sreg_32(i1) = COPY [[PHI6]](i1)
  ; GFX10-NEXT:   [[COPY17:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[COPY16]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[SI_IF]](i32)
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[COPY15]](i1), [[PHI3]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_4:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_ANDN2_B32 [[COPY6]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_4:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_AND_B32 $exec_lo, [[COPY17]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_4:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_OR_B32 [[S_ANDN2_B32_4]](i1), [[S_AND_B32_4]](i1), implicit-def $scc
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   successors: %bb.2(0x40000000), %bb.4(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI8:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.5
  ; GFX10-NEXT:   [[COPY18:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[S_OR_B32_4]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI8]](i32)
  ; GFX10-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[COPY18]](i1), %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $vgpr3
    %5:_(p1) = G_MERGE_VALUES %3(i32), %4(i32)
    %6:_(i32) = COPY $vgpr4
    %7:_(i32) = COPY $vgpr5
    %8:_(p1) = G_MERGE_VALUES %6(i32), %7(i32)
    %9:_(i32) = G_CONSTANT i32 0
    %10:_(i32) = G_IMPLICIT_DEF

  bb.1:
    successors: %bb.3(0x40000000), %bb.5(0x40000000)

    %11:_(i32) = G_PHI %12(i32), %bb.5, %9(i32), %bb.0
    %13:_(i32) = G_PHI %9(i32), %bb.0, %14(i32), %bb.5
    %15:_(i1) = G_CONSTANT i1 true
    %16:_(i64) = G_SEXT %13(i32)
    %17:_(i32) = G_CONSTANT i32 2
    %18:_(i64) = G_SHL %16, %17(i32)
    %19:_(p1) = G_PTR_ADD %5, %18(i64)
    %20:_(i32) = G_LOAD %19(p1) :: (load (i32), addrspace 1)
    %21:_(i32) = G_CONSTANT i32 0
    %22:sreg_32_xm0_xexec(i1) = G_ICMP intpred(ne), %20(i32), %21
    %23:sreg_32_xm0_xexec(i32) = SI_IF %22(i1), %bb.5, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.3

  bb.2:
    successors: %bb.4(0x80000000)

    %24:_(i32) = G_CONSTANT i32 10
    G_STORE %24(i32), %8(p1) :: (store (i32), addrspace 1)
    G_BR %bb.4

  bb.3:
    successors: %bb.5(0x80000000)

    %25:_(i1) = G_CONSTANT i1 false
    %26:_(i32) = G_CONSTANT i32 2
    %27:_(i64) = G_SHL %16, %26(i32)
    %28:_(p1) = G_PTR_ADD %2, %27(i64)
    %29:_(i32) = G_LOAD %28(p1) :: (load (i32), addrspace 1)
    %30:_(i32) = G_CONSTANT i32 1
    %31:_(i32) = G_ADD %29, %30
    G_STORE %31(i32), %28(p1) :: (store (i32), addrspace 1)
    %32:_(i32) = G_ADD %13, %30
    %33:_(i32) = G_CONSTANT i32 100
    %34:_(i1) = G_ICMP intpred(ult), %13(i32), %33
    G_BR %bb.5

  bb.4:
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %35(i32)
    S_ENDPGM 0

  bb.5:
    successors: %bb.6(0x04000000), %bb.1(0x7c000000)

    %14:_(i32) = G_PHI %32(i32), %bb.3, %10(i32), %bb.1
    %36:_(i1) = G_PHI %25(i1), %bb.3, %15(i1), %bb.1
    %37:_(i1) = G_PHI %34(i1), %bb.3, %15(i1), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %23(i32)
    %12:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %37(i1), %11(i32)
    SI_LOOP %12(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.6

  bb.6:
    successors: %bb.2(0x40000000), %bb.4(0x40000000)

    %38:sreg_32_xm0_xexec(i1) = G_PHI %36(i1), %bb.5
    %39:_(i32) = G_PHI %12(i32), %bb.5
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %39(i32)
    %35:sreg_32_xm0_xexec(i32) = SI_IF %38(i1), %bb.4, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2












...

---
name: irreducible_cfg
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: irreducible_cfg
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.7(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i1) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(sgt), [[COPY4]](i32), [[COPY1]]
  ; GFX10-NEXT:   [[DEF1:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   [[DEF2:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP]](i1)
  ; GFX10-NEXT:   G_BR %bb.7
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(sle), [[COPY4]](i32), [[COPY]]
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x40000000), %bb.7(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32_xm0_xexec(i1) = PHI %53(i1), %bb.6, %57(i1), %bb.7
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:sreg_32(i1) = PHI %35(i1), %bb.6, %34(i1), %bb.7
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i1) = G_PHI %12(i1), %bb.6, [[DEF]](i1), %bb.7
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[PHI2]](i1)
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[PHI1]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %15(i32)
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[COPY9]](i1), %17(i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_ANDN2_B32 [[COPY8]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_AND_B32 $exec_lo, [[COPY7]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.6(0x04000000), %bb.3(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:_(i32) = G_PHI [[C1]](i32), %bb.1, %19(i32), %bb.3
  ; GFX10-NEXT:   [[INT1:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[ICMP1]](i1), [[PHI3]](i32)
  ; GFX10-NEXT:   SI_LOOP [[INT1]](i32), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   successors: %bb.5(0x04000000), %bb.7(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[INT]](i32)
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(sgt), [[COPY5]](i32), [[COPY]]
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY12:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[C2]](i1)
  ; GFX10-NEXT:   [[XOR:%[0-9]+]]:_(i1) = G_XOR [[ICMP]], [[C2]]
  ; GFX10-NEXT:   [[OR:%[0-9]+]]:_(i1) = G_OR [[ICMP2]], [[XOR]]
  ; GFX10-NEXT:   [[INT2:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[OR]](i1), %25(i32)
  ; GFX10-NEXT:   [[DEF3:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 %49(i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY11]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_1:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_1]](i1), [[S_AND_B32_1]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_ANDN2_B32_2:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_ANDN2_B32 [[COPY10]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_2:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_AND_B32 $exec_lo, [[COPY12]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_2:%[0-9]+]]:sreg_32_xm0_xexec(i1) = S_OR_B32 [[S_ANDN2_B32_2]](i1), [[S_AND_B32_2]](i1), implicit-def $scc
  ; GFX10-NEXT:   SI_LOOP [[INT2]](i32), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI [[INT2]](i32), %bb.4
  ; GFX10-NEXT:   [[COPY13:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_1]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI4]](i32)
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY13]](i1), [[COPY3]], [[COPY2]]
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[SELECT]](i32)
  ; GFX10-NEXT:   $sgpr0 = COPY [[INTRINSIC_CONVERGENT]](i32)
  ; GFX10-NEXT:   SI_RETURN_TO_EPILOG implicit $sgpr0
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:_(i32) = G_PHI [[INT1]](i32), %bb.3
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i1) = G_CONSTANT i1 false
  ; GFX10-NEXT:   [[COPY14:%[0-9]+]]:sreg_32(i1) = COPY [[C3]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI5]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_3:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 %42(i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_3:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY14]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_3:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_3]](i1), [[S_AND_B32_3]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[DEF4:%[0-9]+]]:sreg_32_xm0_xexec(i1) = IMPLICIT_DEF
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.7:
  ; GFX10-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI6:%[0-9]+]]:sreg_32_xm0_xexec(i1) = PHI [[COPY6]](i1), %bb.0, [[S_OR_B32_]](i1), %bb.2, [[S_OR_B32_2]](i1), %bb.4
  ; GFX10-NEXT:   [[PHI7:%[0-9]+]]:sreg_32(i1) = PHI [[DEF2]](i1), %bb.0, [[PHI7]](i1), %bb.2, [[S_OR_B32_1]](i1), %bb.4
  ; GFX10-NEXT:   [[PHI8:%[0-9]+]]:sreg_32(i1) = PHI [[DEF1]](i1), %bb.0, [[PHI1]](i1), %bb.2, [[DEF3]](i1), %bb.4
  ; GFX10-NEXT:   [[PHI9:%[0-9]+]]:_(i32) = G_PHI [[INT2]](i32), %bb.4, [[PHI9]](i32), %bb.2, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI10:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.4, [[INT]](i32), %bb.2, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[COPY15:%[0-9]+]]:sreg_32_xm0_xexec(i1) = COPY [[PHI6]](i1)
  ; GFX10-NEXT:   [[COPY16:%[0-9]+]]:sreg_32(i1) = COPY [[PHI7]](i1)
  ; GFX10-NEXT:   [[COPY17:%[0-9]+]]:sreg_32(i1) = COPY [[PHI8]](i1)
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[COPY18:%[0-9]+]]:sreg_32(i1) = COPY [[C4]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_4:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY17]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_4:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY18]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_4:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_4]](i1), [[S_AND_B32_4]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[COPY19:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_4]](i1)
  ; GFX10-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32_xm0_xexec(i32) = SI_IF [[COPY15]](i1), %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.1
  bb.0:
    successors: %bb.7(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr2
    %3:_(i32) = COPY $vgpr3
    %4:_(i32) = COPY $vgpr4
    %5:_(i32) = COPY $vgpr5
    %6:_(i32) = G_CONSTANT i32 0
    %7:_(i1) = G_IMPLICIT_DEF
    %8:_(i1) = G_ICMP intpred(sgt), %4(i32), %1
    G_BR %bb.7

  bb.1:
    successors: %bb.3(0x80000000)

    %9:_(i32) = G_CONSTANT i32 0
    %10:_(i1) = G_ICMP intpred(sle), %4(i32), %0
    G_BR %bb.3

  bb.2:
    successors: %bb.4(0x40000000), %bb.7(0x40000000)

    %11:_(i1) = G_PHI %12(i1), %bb.6, %7(i1), %bb.7
    %13:_(i1) = G_PHI %12(i1), %bb.6, %14(i1), %bb.7
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %15(i32)
    %16:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %13(i1), %17(i32)
    SI_LOOP %16(i32), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.4

  bb.3:
    successors: %bb.6(0x04000000), %bb.3(0x7c000000)

    %18:_(i32) = G_PHI %9(i32), %bb.1, %19(i32), %bb.3
    %19:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %10(i1), %18(i32)
    SI_LOOP %19(i32), %bb.3, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.6

  bb.4:
    successors: %bb.5(0x04000000), %bb.7(0x7c000000)

    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %16(i32)
    %20:_(i1) = G_ICMP intpred(sgt), %5(i32), %0
    %21:_(i1) = G_CONSTANT i1 true
    %22:_(i1) = G_XOR %8, %21
    %23:_(i1) = G_OR %20, %22
    %24:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %23(i1), %25(i32)
    SI_LOOP %24(i32), %bb.7, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.5

  bb.5:
    %26:_(i1) = G_PHI %20(i1), %bb.4
    %27:_(i32) = G_PHI %24(i32), %bb.4
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %27(i32)
    %28:_(i32) = G_SELECT %26(i1), %3, %2
    %29:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), %28(i32)
    $sgpr0 = COPY %29(i32)
    SI_RETURN_TO_EPILOG implicit $sgpr0

  bb.6:
    successors: %bb.2(0x80000000)

    %30:_(i32) = G_PHI %19(i32), %bb.3
    %12:_(i1) = G_CONSTANT i1 false
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %30(i32)
    G_BR %bb.2

  bb.7:
    successors: %bb.1(0x40000000), %bb.2(0x40000000)

    %25:_(i32) = G_PHI %24(i32), %bb.4, %25(i32), %bb.2, %6(i32), %bb.0
    %17:_(i32) = G_PHI %6(i32), %bb.4, %16(i32), %bb.2, %6(i32), %bb.0
    %31:sreg_32_xm0_xexec(i1) = G_PHI %8(i1), %bb.0, %11(i1), %bb.2, %21(i1), %bb.4
    %14:_(i1) = G_CONSTANT i1 true
    %15:sreg_32_xm0_xexec(i32) = SI_IF %31(i1), %bb.2, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.1














...
