# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=fiji -run-pass=legalizer -verify-machineinstrs -o - %s | FileCheck -check-prefix=GFX8 %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=gfx900 -run-pass=legalizer -verify-machineinstrs -o - %s | FileCheck -check-prefix=GFX9 %s

--- |

  define <2 x i16> @and_v2i16(<2 x i16> %a, <2 x i16> %b) #0 {
    %and = and <2 x i16> %a, %b
    ret <2 x i16> %and
  }

  define <3 x i16> @add_v3i16(<3 x i16> %a, <3 x i16> %b) #0 {
    %add = add <3 x i16> %a, %b
    ret <3 x i16> %add
  }

  define <3 x i16> @shl_v3i16(<3 x i16> %a, <3 x i16> %b) #0 {
    %shl = shl <3 x i16> %a, %b
    ret <3 x i16> %shl
  }

  define <4 x half> @fma_v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c) {
    %fma = call <4 x half> @llvm.fma.v4f16(<4 x half> %a, <4 x half> %b, <4 x half> %c)
    ret <4 x half> %fma
  }

  define amdgpu_ps <5 x half> @maxnum_v5i16(<5 x half> %a, <5 x half> %b) {
    %fma = call <5 x half> @llvm.maxnum.v5f16(<5 x half> %a, <5 x half> %b)
    ret <5 x half> %fma
  }

  declare <4 x half> @llvm.fma.v4f16(<4 x half>, <4 x half>, <4 x half>)
  declare <5 x half> @llvm.maxnum.v5f16(<5 x half>, <5 x half>)
...

---
name: and_v2i16
body: |
  bb.1:
    liveins: $vgpr0, $vgpr1

    ; GFX8-LABEL: name: and_v2i16
    ; GFX8: liveins: $vgpr0, $vgpr1
    ; GFX8-NEXT: {{  $}}
    ; GFX8-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX8-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX8-NEXT: [[AND:%[0-9]+]]:_(<2 x i16>) = G_AND [[COPY]], [[COPY1]]
    ; GFX8-NEXT: $vgpr0 = COPY [[AND]](<2 x i16>)
    ; GFX8-NEXT: SI_RETURN implicit $vgpr0
    ;
    ; GFX9-LABEL: name: and_v2i16
    ; GFX9: liveins: $vgpr0, $vgpr1
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[AND:%[0-9]+]]:_(<2 x i16>) = G_AND [[COPY]], [[COPY1]]
    ; GFX9-NEXT: $vgpr0 = COPY [[AND]](<2 x i16>)
    ; GFX9-NEXT: SI_RETURN implicit $vgpr0
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<2 x i16>) = G_AND %0, %1
    $vgpr0 = COPY %2(<2 x i16>)
    SI_RETURN implicit $vgpr0
...

---
name: add_v3i16
body: |
  bb.1:
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3

    ; GFX8-LABEL: name: add_v3i16
    ; GFX8: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
    ; GFX8-NEXT: {{  $}}
    ; GFX8-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX8-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX8-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX8-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX8-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX8-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX8-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX8-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX8-NEXT: [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST2]](i32)
    ; GFX8-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST2]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX8-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST3]](i32)
    ; GFX8-NEXT: [[ADD:%[0-9]+]]:_(i16) = G_ADD [[TRUNC]], [[TRUNC3]]
    ; GFX8-NEXT: [[ADD1:%[0-9]+]]:_(i16) = G_ADD [[TRUNC1]], [[TRUNC4]]
    ; GFX8-NEXT: [[ADD2:%[0-9]+]]:_(i16) = G_ADD [[TRUNC2]], [[TRUNC5]]
    ; GFX8-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[ADD]](i16)
    ; GFX8-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[ADD1]](i16)
    ; GFX8-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; GFX8-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; GFX8-NEXT: [[BITCAST4:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; GFX8-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[ADD2]](i16)
    ; GFX8-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX8-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[C1]], [[C]](i32)
    ; GFX8-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; GFX8-NEXT: [[BITCAST5:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; GFX8-NEXT: $vgpr0 = COPY [[BITCAST4]](<2 x i16>)
    ; GFX8-NEXT: $vgpr1 = COPY [[BITCAST5]](<2 x i16>)
    ; GFX8-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    ;
    ; GFX9-LABEL: name: add_v3i16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX9-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX9-NEXT: [[ADD:%[0-9]+]]:_(<2 x i16>) = G_ADD [[COPY]], [[COPY2]]
    ; GFX9-NEXT: [[ADD1:%[0-9]+]]:_(<2 x i16>) = G_ADD [[COPY1]], [[COPY3]]
    ; GFX9-NEXT: $vgpr0 = COPY [[ADD]](<2 x i16>)
    ; GFX9-NEXT: $vgpr1 = COPY [[ADD1]](<2 x i16>)
    ; GFX9-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<4 x i16>) = G_CONCAT_VECTORS %0(<2 x i16>), %1(<2 x i16>)
    %3:_(i16), %4:_(i16), %5:_(i16), %6:_(i16) = G_UNMERGE_VALUES %2(<4 x i16>)
    %7:_(<3 x i16>) = G_BUILD_VECTOR %3(i16), %4(i16), %5(i16)
    %8:_(<2 x i16>) = COPY $vgpr2
    %9:_(<2 x i16>) = COPY $vgpr3
    %10:_(<4 x i16>) = G_CONCAT_VECTORS %8(<2 x i16>), %9(<2 x i16>)
    %11:_(i16), %12:_(i16), %13:_(i16), %14:_(i16) = G_UNMERGE_VALUES %10(<4 x i16>)
    %15:_(<3 x i16>) = G_BUILD_VECTOR %11(i16), %12(i16), %13(i16)
    %16:_(<3 x i16>) = G_ADD %7, %15
    %17:_(i16), %18:_(i16), %19:_(i16) = G_UNMERGE_VALUES %16(<3 x i16>)
    %20:_(i16) = G_IMPLICIT_DEF
    %21:_(<4 x i16>) = G_BUILD_VECTOR %17(i16), %18(i16), %19(i16), %20(i16)
    %22:_(<2 x i16>), %23:_(<2 x i16>) = G_UNMERGE_VALUES %21(<4 x i16>)
    $vgpr0 = COPY %22(<2 x i16>)
    $vgpr1 = COPY %23(<2 x i16>)
    SI_RETURN implicit $vgpr0, implicit $vgpr1
...

---
name: shl_v3i16
body: |
  bb.1:
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3

    ; GFX8-LABEL: name: shl_v3i16
    ; GFX8: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
    ; GFX8-NEXT: {{  $}}
    ; GFX8-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX8-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX8-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX8-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX8-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX8-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX8-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX8-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX8-NEXT: [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST2]](i32)
    ; GFX8-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST2]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX8-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST3]](i32)
    ; GFX8-NEXT: [[SHL:%[0-9]+]]:_(i16) = G_SHL [[TRUNC]], [[TRUNC3]](i16)
    ; GFX8-NEXT: [[SHL1:%[0-9]+]]:_(i16) = G_SHL [[TRUNC1]], [[TRUNC4]](i16)
    ; GFX8-NEXT: [[SHL2:%[0-9]+]]:_(i16) = G_SHL [[TRUNC2]], [[TRUNC5]](i16)
    ; GFX8-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[SHL]](i16)
    ; GFX8-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[SHL1]](i16)
    ; GFX8-NEXT: [[SHL3:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; GFX8-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL3]]
    ; GFX8-NEXT: [[BITCAST4:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; GFX8-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[SHL2]](i16)
    ; GFX8-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX8-NEXT: [[SHL4:%[0-9]+]]:_(i32) = G_SHL [[C1]], [[C]](i32)
    ; GFX8-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL4]]
    ; GFX8-NEXT: [[BITCAST5:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; GFX8-NEXT: $vgpr0 = COPY [[BITCAST4]](<2 x i16>)
    ; GFX8-NEXT: $vgpr1 = COPY [[BITCAST5]](<2 x i16>)
    ; GFX8-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    ;
    ; GFX9-LABEL: name: shl_v3i16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX9-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX9-NEXT: [[SHL:%[0-9]+]]:_(<2 x i16>) = G_SHL [[COPY]], [[COPY2]](<2 x i16>)
    ; GFX9-NEXT: [[SHL1:%[0-9]+]]:_(i16) = G_SHL [[TRUNC]], [[TRUNC1]](i16)
    ; GFX9-NEXT: [[DEF:%[0-9]+]]:_(i16) = G_IMPLICIT_DEF
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[SHL1]](i16), [[DEF]](i16)
    ; GFX9-NEXT: $vgpr0 = COPY [[SHL]](<2 x i16>)
    ; GFX9-NEXT: $vgpr1 = COPY [[BUILD_VECTOR]](<2 x i16>)
    ; GFX9-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<4 x i16>) = G_CONCAT_VECTORS %0(<2 x i16>), %1(<2 x i16>)
    %3:_(i16), %4:_(i16), %5:_(i16), %6:_(i16) = G_UNMERGE_VALUES %2(<4 x i16>)
    %7:_(<3 x i16>) = G_BUILD_VECTOR %3(i16), %4(i16), %5(i16)
    %8:_(<2 x i16>) = COPY $vgpr2
    %9:_(<2 x i16>) = COPY $vgpr3
    %10:_(<4 x i16>) = G_CONCAT_VECTORS %8(<2 x i16>), %9(<2 x i16>)
    %11:_(i16), %12:_(i16), %13:_(i16), %14:_(i16) = G_UNMERGE_VALUES %10(<4 x i16>)
    %15:_(<3 x i16>) = G_BUILD_VECTOR %11(i16), %12(i16), %13(i16)
    %16:_(<3 x i16>) = G_SHL %7, %15(<3 x i16>)
    %17:_(i16), %18:_(i16), %19:_(i16) = G_UNMERGE_VALUES %16(<3 x i16>)
    %20:_(i16) = G_IMPLICIT_DEF
    %21:_(<4 x i16>) = G_BUILD_VECTOR %17(i16), %18(i16), %19(i16), %20(i16)
    %22:_(<2 x i16>), %23:_(<2 x i16>) = G_UNMERGE_VALUES %21(<4 x i16>)
    $vgpr0 = COPY %22(<2 x i16>)
    $vgpr1 = COPY %23(<2 x i16>)
    SI_RETURN implicit $vgpr0, implicit $vgpr1
...

---
name: fma_v4f16
body: |
  bb.1:
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5

    ; GFX8-LABEL: name: fma_v4f16
    ; GFX8: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
    ; GFX8-NEXT: {{  $}}
    ; GFX8-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX8-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX8-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX8-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX8-NEXT: [[COPY4:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr4
    ; GFX8-NEXT: [[COPY5:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr5
    ; GFX8-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST %40(i16)
    ; GFX8-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST %46(i16)
    ; GFX8-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST %41(i16)
    ; GFX8-NEXT: [[BITCAST5:%[0-9]+]]:_(f16) = G_BITCAST %47(i16)
    ; GFX8-NEXT: [[BITCAST6:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST1]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST7:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST6]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST7]](i32)
    ; GFX8-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX8-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST7]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX8-NEXT: [[BITCAST8:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST9:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST8]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST9]](i32)
    ; GFX8-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST9]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX8-NEXT: [[BITCAST10:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST11:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST %51(i16)
    ; GFX8-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST %56(i16)
    ; GFX8-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST %52(i16)
    ; GFX8-NEXT: [[BITCAST15:%[0-9]+]]:_(f16) = G_BITCAST %57(i16)
    ; GFX8-NEXT: [[BITCAST16:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST11]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST17:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST16]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST17]](i32)
    ; GFX8-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST17]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; GFX8-NEXT: [[BITCAST18:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST10]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST19:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST18]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST19]](i32)
    ; GFX8-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST19]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; GFX8-NEXT: [[BITCAST20:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY4]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST21:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY5]](<2 x i16>)
    ; GFX8-NEXT: [[BITCAST22:%[0-9]+]]:_(f16) = G_BITCAST %61(i16)
    ; GFX8-NEXT: [[BITCAST23:%[0-9]+]]:_(f16) = G_BITCAST %66(i16)
    ; GFX8-NEXT: [[BITCAST24:%[0-9]+]]:_(f16) = G_BITCAST %62(i16)
    ; GFX8-NEXT: [[BITCAST25:%[0-9]+]]:_(f16) = G_BITCAST %67(i16)
    ; GFX8-NEXT: [[BITCAST26:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST21]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST27:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST26]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST27]](i32)
    ; GFX8-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST27]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; GFX8-NEXT: [[BITCAST28:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST20]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST29:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST28]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST29]](i32)
    ; GFX8-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST29]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR5]](i32)
    ; GFX8-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST2]], [[BITCAST12]], [[BITCAST22]]
    ; GFX8-NEXT: [[FMA1:%[0-9]+]]:_(f16) = G_FMA [[BITCAST4]], [[BITCAST14]], [[BITCAST24]]
    ; GFX8-NEXT: [[FMA2:%[0-9]+]]:_(f16) = G_FMA [[BITCAST3]], [[BITCAST13]], [[BITCAST23]]
    ; GFX8-NEXT: [[FMA3:%[0-9]+]]:_(f16) = G_FMA [[BITCAST5]], [[BITCAST15]], [[BITCAST25]]
    ; GFX8-NEXT: [[BITCAST30:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; GFX8-NEXT: [[BITCAST31:%[0-9]+]]:_(i16) = G_BITCAST [[FMA1]](f16)
    ; GFX8-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST30]](i16)
    ; GFX8-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST31]](i16)
    ; GFX8-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; GFX8-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; GFX8-NEXT: [[BITCAST32:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR]](i32)
    ; GFX8-NEXT: [[BITCAST33:%[0-9]+]]:_(i16) = G_BITCAST [[FMA2]](f16)
    ; GFX8-NEXT: [[BITCAST34:%[0-9]+]]:_(i16) = G_BITCAST [[FMA3]](f16)
    ; GFX8-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST33]](i16)
    ; GFX8-NEXT: [[ZEXT3:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST34]](i16)
    ; GFX8-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[ZEXT3]], [[C]](i32)
    ; GFX8-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; GFX8-NEXT: [[BITCAST35:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR1]](i32)
    ; GFX8-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<4 x f16>) = G_CONCAT_VECTORS [[BITCAST32]](<2 x f16>), [[BITCAST35]](<2 x f16>)
    ; GFX8-NEXT: [[BITCAST36:%[0-9]+]]:_(<4 x i16>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f16>)
    ; GFX8-NEXT: [[UV:%[0-9]+]]:_(<2 x i16>), [[UV1:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[BITCAST36]](<4 x i16>)
    ; GFX8-NEXT: $vgpr0 = COPY [[UV]](<2 x i16>)
    ; GFX8-NEXT: $vgpr1 = COPY [[UV1]](<2 x i16>)
    ; GFX8-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    ;
    ; GFX9-LABEL: name: fma_v4f16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX9-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX9-NEXT: [[COPY4:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr4
    ; GFX9-NEXT: [[COPY5:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr5
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST4:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY4]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST5:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY5]](<2 x i16>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(<2 x f16>) = G_FMA [[BITCAST]], [[BITCAST2]], [[BITCAST4]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(<2 x f16>) = G_FMA [[BITCAST1]], [[BITCAST3]], [[BITCAST5]]
    ; GFX9-NEXT: [[BITCAST6:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMA]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST7:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMA1]](<2 x f16>)
    ; GFX9-NEXT: $vgpr0 = COPY [[BITCAST6]](<2 x i16>)
    ; GFX9-NEXT: $vgpr1 = COPY [[BITCAST7]](<2 x i16>)
    ; GFX9-NEXT: SI_RETURN implicit $vgpr0, implicit $vgpr1
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<4 x i16>) = G_CONCAT_VECTORS %0(<2 x i16>), %1(<2 x i16>)
    %3:_(<2 x i16>) = COPY $vgpr2
    %4:_(<2 x i16>) = COPY $vgpr3
    %5:_(<4 x i16>) = G_CONCAT_VECTORS %3(<2 x i16>), %4(<2 x i16>)
    %6:_(<2 x i16>) = COPY $vgpr4
    %7:_(<2 x i16>) = COPY $vgpr5
    %8:_(<4 x i16>) = G_CONCAT_VECTORS %6(<2 x i16>), %7(<2 x i16>)
    %9:_(<4 x f16>) = G_BITCAST %2(<4 x i16>)
    %10:_(<4 x f16>) = G_BITCAST %5(<4 x i16>)
    %11:_(<4 x f16>) = G_BITCAST %8(<4 x i16>)
    %12:_(<4 x f16>) = G_FMA %9, %10, %11
    %13:_(<4 x i16>) = G_BITCAST %12(<4 x f16>)
    %14:_(<2 x i16>), %15:_(<2 x i16>) = G_UNMERGE_VALUES %13(<4 x i16>)
    $vgpr0 = COPY %14(<2 x i16>)
    $vgpr1 = COPY %15(<2 x i16>)
    SI_RETURN implicit $vgpr0, implicit $vgpr1
...

---
name: maxnum_v5i16
body: |
  bb.1:
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5

    ; GFX8-LABEL: name: maxnum_v5i16
    ; GFX8: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
    ; GFX8-NEXT: {{  $}}
    ; GFX8-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX8-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX8-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX8-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX8-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX8-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX8-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX8-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST1]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX8-NEXT: [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST2]](i32)
    ; GFX8-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX8-NEXT: [[COPY4:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr4
    ; GFX8-NEXT: [[COPY5:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr5
    ; GFX8-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST3]](i32)
    ; GFX8-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST3]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; GFX8-NEXT: [[BITCAST4:%[0-9]+]]:_(i32) = G_BITCAST [[COPY4]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST4]](i32)
    ; GFX8-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST4]], [[C]](i32)
    ; GFX8-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; GFX8-NEXT: [[BITCAST5:%[0-9]+]]:_(i32) = G_BITCAST [[COPY5]](<2 x i16>)
    ; GFX8-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST5]](i32)
    ; GFX8-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; GFX8-NEXT: [[BITCAST7:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; GFX8-NEXT: [[BITCAST8:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; GFX8-NEXT: [[BITCAST9:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC3]](i16)
    ; GFX8-NEXT: [[BITCAST10:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC4]](i16)
    ; GFX8-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC5]](i16)
    ; GFX8-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC6]](i16)
    ; GFX8-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC7]](i16)
    ; GFX8-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC8]](i16)
    ; GFX8-NEXT: [[BITCAST15:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC9]](i16)
    ; GFX8-NEXT: [[FCANONICALIZE:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST6]]
    ; GFX8-NEXT: [[FCANONICALIZE1:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST11]]
    ; GFX8-NEXT: [[FMAXNUM_IEEE:%[0-9]+]]:_(f16) = G_FMAXNUM_IEEE [[FCANONICALIZE]], [[FCANONICALIZE1]]
    ; GFX8-NEXT: [[FCANONICALIZE2:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST7]]
    ; GFX8-NEXT: [[FCANONICALIZE3:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST12]]
    ; GFX8-NEXT: [[FMAXNUM_IEEE1:%[0-9]+]]:_(f16) = G_FMAXNUM_IEEE [[FCANONICALIZE2]], [[FCANONICALIZE3]]
    ; GFX8-NEXT: [[FCANONICALIZE4:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST8]]
    ; GFX8-NEXT: [[FCANONICALIZE5:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST13]]
    ; GFX8-NEXT: [[FMAXNUM_IEEE2:%[0-9]+]]:_(f16) = G_FMAXNUM_IEEE [[FCANONICALIZE4]], [[FCANONICALIZE5]]
    ; GFX8-NEXT: [[FCANONICALIZE6:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST9]]
    ; GFX8-NEXT: [[FCANONICALIZE7:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST14]]
    ; GFX8-NEXT: [[FMAXNUM_IEEE3:%[0-9]+]]:_(f16) = G_FMAXNUM_IEEE [[FCANONICALIZE6]], [[FCANONICALIZE7]]
    ; GFX8-NEXT: [[FCANONICALIZE8:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST10]]
    ; GFX8-NEXT: [[FCANONICALIZE9:%[0-9]+]]:_(f16) = G_FCANONICALIZE [[BITCAST15]]
    ; GFX8-NEXT: [[FMAXNUM_IEEE4:%[0-9]+]]:_(f16) = G_FMAXNUM_IEEE [[FCANONICALIZE8]], [[FCANONICALIZE9]]
    ; GFX8-NEXT: [[BITCAST16:%[0-9]+]]:_(i16) = G_BITCAST [[FMAXNUM_IEEE]](f16)
    ; GFX8-NEXT: [[BITCAST17:%[0-9]+]]:_(i16) = G_BITCAST [[FMAXNUM_IEEE1]](f16)
    ; GFX8-NEXT: [[BITCAST18:%[0-9]+]]:_(i16) = G_BITCAST [[FMAXNUM_IEEE2]](f16)
    ; GFX8-NEXT: [[BITCAST19:%[0-9]+]]:_(i16) = G_BITCAST [[FMAXNUM_IEEE3]](f16)
    ; GFX8-NEXT: [[BITCAST20:%[0-9]+]]:_(i16) = G_BITCAST [[FMAXNUM_IEEE4]](f16)
    ; GFX8-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST16]](i16)
    ; GFX8-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST17]](i16)
    ; GFX8-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; GFX8-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; GFX8-NEXT: [[BITCAST21:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; GFX8-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST18]](i16)
    ; GFX8-NEXT: [[ZEXT3:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST19]](i16)
    ; GFX8-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[ZEXT3]], [[C]](i32)
    ; GFX8-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; GFX8-NEXT: [[BITCAST22:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; GFX8-NEXT: [[ZEXT4:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST20]](i16)
    ; GFX8-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
    ; GFX8-NEXT: [[SHL2:%[0-9]+]]:_(i32) = G_SHL [[C1]], [[C]](i32)
    ; GFX8-NEXT: [[OR2:%[0-9]+]]:_(i32) = G_OR [[ZEXT4]], [[SHL2]]
    ; GFX8-NEXT: [[BITCAST23:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR2]](i32)
    ; GFX8-NEXT: $vgpr0 = COPY [[BITCAST21]](<2 x i16>)
    ; GFX8-NEXT: $vgpr1 = COPY [[BITCAST22]](<2 x i16>)
    ; GFX8-NEXT: $vgpr2 = COPY [[BITCAST23]](<2 x i16>)
    ; GFX8-NEXT: SI_RETURN_TO_EPILOG implicit $vgpr0, implicit $vgpr1, implicit $vgpr2
    ;
    ; GFX9-LABEL: name: maxnum_v5i16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX9-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX9-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX9-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST1]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST2]](i32)
    ; GFX9-NEXT: [[COPY3:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr3
    ; GFX9-NEXT: [[COPY4:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr4
    ; GFX9-NEXT: [[COPY5:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr5
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[COPY3]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST3]](i32)
    ; GFX9-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST3]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; GFX9-NEXT: [[BITCAST4:%[0-9]+]]:_(i32) = G_BITCAST [[COPY4]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST4]](i32)
    ; GFX9-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST4]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; GFX9-NEXT: [[BITCAST5:%[0-9]+]]:_(i32) = G_BITCAST [[COPY5]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST5]](i32)
    ; GFX9-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; GFX9-NEXT: [[BITCAST7:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; GFX9-NEXT: [[BITCAST8:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; GFX9-NEXT: [[BITCAST9:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC3]](i16)
    ; GFX9-NEXT: [[BITCAST10:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC4]](i16)
    ; GFX9-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC5]](i16)
    ; GFX9-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC6]](i16)
    ; GFX9-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC7]](i16)
    ; GFX9-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC8]](i16)
    ; GFX9-NEXT: [[BITCAST15:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC9]](i16)
    ; GFX9-NEXT: [[DEF:%[0-9]+]]:_(f16) = G_IMPLICIT_DEF
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST6]](f16), [[BITCAST7]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST8]](f16), [[BITCAST9]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR2:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST10]](f16), [[DEF]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR3:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST11]](f16), [[BITCAST12]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR4:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST13]](f16), [[BITCAST14]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR5:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST15]](f16), [[DEF]](f16)
    ; GFX9-NEXT: [[FCANONICALIZE:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR]]
    ; GFX9-NEXT: [[FCANONICALIZE1:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR3]]
    ; GFX9-NEXT: [[FMAXNUM_IEEE:%[0-9]+]]:_(<2 x f16>) = G_FMAXNUM_IEEE [[FCANONICALIZE]], [[FCANONICALIZE1]]
    ; GFX9-NEXT: [[FCANONICALIZE2:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR1]]
    ; GFX9-NEXT: [[FCANONICALIZE3:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR4]]
    ; GFX9-NEXT: [[FMAXNUM_IEEE1:%[0-9]+]]:_(<2 x f16>) = G_FMAXNUM_IEEE [[FCANONICALIZE2]], [[FCANONICALIZE3]]
    ; GFX9-NEXT: [[FCANONICALIZE4:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR2]]
    ; GFX9-NEXT: [[FCANONICALIZE5:%[0-9]+]]:_(<2 x f16>) = G_FCANONICALIZE [[BUILD_VECTOR5]]
    ; GFX9-NEXT: [[FMAXNUM_IEEE2:%[0-9]+]]:_(<2 x f16>) = G_FMAXNUM_IEEE [[FCANONICALIZE4]], [[FCANONICALIZE5]]
    ; GFX9-NEXT: [[BITCAST16:%[0-9]+]]:_(f16) = G_BITCAST %123(i16)
    ; GFX9-NEXT: [[BITCAST17:%[0-9]+]]:_(f16) = G_BITCAST %128(i16)
    ; GFX9-NEXT: [[BITCAST18:%[0-9]+]]:_(f16) = G_BITCAST %124(i16)
    ; GFX9-NEXT: [[BITCAST19:%[0-9]+]]:_(f16) = G_BITCAST %129(i16)
    ; GFX9-NEXT: [[BITCAST20:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMAXNUM_IEEE1]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST21:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST20]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST21]](i32)
    ; GFX9-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST21]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; GFX9-NEXT: [[BITCAST22:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMAXNUM_IEEE]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST23:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST22]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC12:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST23]](i32)
    ; GFX9-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST23]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC13:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR5]](i32)
    ; GFX9-NEXT: [[BITCAST24:%[0-9]+]]:_(f16) = G_BITCAST %133(i16)
    ; GFX9-NEXT: [[BITCAST25:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST16]](f16)
    ; GFX9-NEXT: [[BITCAST26:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMAXNUM_IEEE2]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST27:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST26]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC14:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST27]](i32)
    ; GFX9-NEXT: [[BITCAST28:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST18]](f16)
    ; GFX9-NEXT: [[BITCAST29:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST17]](f16)
    ; GFX9-NEXT: [[BITCAST30:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST19]](f16)
    ; GFX9-NEXT: [[BITCAST31:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST24]](f16)
    ; GFX9-NEXT: [[DEF1:%[0-9]+]]:_(i16) = G_IMPLICIT_DEF
    ; GFX9-NEXT: [[BUILD_VECTOR6:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[BITCAST25]](i16), [[BITCAST28]](i16)
    ; GFX9-NEXT: [[BUILD_VECTOR7:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[BITCAST29]](i16), [[BITCAST30]](i16)
    ; GFX9-NEXT: [[BUILD_VECTOR8:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[BITCAST31]](i16), [[DEF1]](i16)
    ; GFX9-NEXT: $vgpr0 = COPY [[BUILD_VECTOR6]](<2 x i16>)
    ; GFX9-NEXT: $vgpr1 = COPY [[BUILD_VECTOR7]](<2 x i16>)
    ; GFX9-NEXT: $vgpr2 = COPY [[BUILD_VECTOR8]](<2 x i16>)
    ; GFX9-NEXT: SI_RETURN_TO_EPILOG implicit $vgpr0, implicit $vgpr1, implicit $vgpr2
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<2 x i16>) = COPY $vgpr2
    %3:_(<6 x i16>) = G_CONCAT_VECTORS %0(<2 x i16>), %1(<2 x i16>), %2(<2 x i16>)
    %4:_(i16), %5:_(i16), %6:_(i16), %7:_(i16), %8:_(i16), %9:_(i16) = G_UNMERGE_VALUES %3(<6 x i16>)
    %10:_(<5 x i16>) = G_BUILD_VECTOR %4(i16), %5(i16), %6(i16), %7(i16), %8(i16)
    %11:_(<2 x i16>) = COPY $vgpr3
    %12:_(<2 x i16>) = COPY $vgpr4
    %13:_(<2 x i16>) = COPY $vgpr5
    %14:_(<6 x i16>) = G_CONCAT_VECTORS %11(<2 x i16>), %12(<2 x i16>), %13(<2 x i16>)
    %15:_(i16), %16:_(i16), %17:_(i16), %18:_(i16), %19:_(i16), %20:_(i16) = G_UNMERGE_VALUES %14(<6 x i16>)
    %21:_(<5 x i16>) = G_BUILD_VECTOR %15(i16), %16(i16), %17(i16), %18(i16), %19(i16)
    %22:_(<5 x f16>) = G_BITCAST %10(<5 x i16>)
    %23:_(<5 x f16>) = G_BITCAST %21(<5 x i16>)
    %24:_(<5 x f16>) = G_FMAXNUM %22, %23
    %25:_(<5 x i16>) = G_BITCAST %24(<5 x f16>)
    %26:_(i16), %27:_(i16), %28:_(i16), %29:_(i16), %30:_(i16) = G_UNMERGE_VALUES %25(<5 x i16>)
    %31:_(i16) = G_IMPLICIT_DEF
    %32:_(<6 x i16>) = G_BUILD_VECTOR %26(i16), %27(i16), %28(i16), %29(i16), %30(i16), %31(i16)
    %33:_(<2 x i16>), %34:_(<2 x i16>), %35:_(<2 x i16>) = G_UNMERGE_VALUES %32(<6 x i16>)
    $vgpr0 = COPY %33(<2 x i16>)
    $vgpr1 = COPY %34(<2 x i16>)
    $vgpr2 = COPY %35(<2 x i16>)
    SI_RETURN_TO_EPILOG implicit $vgpr0, implicit $vgpr1, implicit $vgpr2
...
