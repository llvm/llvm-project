; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 2
; RUN: llc --global-isel=1 -mtriple=amdgcn--amdpal -mcpu=gfx1100 -stop-after=irtranslator -verify-machineinstrs %s -o - | FileCheck %s --check-prefix=GFX11
; RUN: llc --global-isel=1 -mtriple=amdgcn--amdpal -mcpu=gfx1030 -stop-after=irtranslator -verify-machineinstrs %s -o - | FileCheck %s --check-prefix=GFX10

declare amdgpu_cs_chain void @callee(<3 x i32> inreg, { i32, ptr addrspace(5), i32, i32 })
declare amdgpu_cs_chain_preserve void @callee_preserve(<3 x i32> inreg, { i32, ptr addrspace(5), i32, i32 })
declare void @llvm.amdgcn.cs.chain(ptr, i32, <3 x i32> inreg, { i32, ptr addrspace(5), i32, i32 }, i32, ...) noreturn

define amdgpu_cs_chain void @chain_call(<3 x i32> inreg %sgpr, { i32, ptr addrspace(5), i32, i32 } %vgpr) {
  ; GFX11-LABEL: name: chain_call
  ; GFX11: bb.1 (%ir-block.0):
  ; GFX11-NEXT:   liveins: $sgpr0, $sgpr1, $sgpr2, $vgpr8, $vgpr9, $vgpr10, $vgpr11
  ; GFX11-NEXT: {{  $}}
  ; GFX11-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX11-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $sgpr1
  ; GFX11-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $sgpr2
  ; GFX11-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[COPY]](i32), [[COPY1]](i32), [[COPY2]](i32)
  ; GFX11-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr8
  ; GFX11-NEXT:   [[COPY4:%[0-9]+]]:_(p5) = COPY $vgpr9
  ; GFX11-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr10
  ; GFX11-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr11
  ; GFX11-NEXT:   [[GV:%[0-9]+]]:_(p0) = G_GLOBAL_VALUE @callee
  ; GFX11-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX11-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX11-NEXT:   [[GV1:%[0-9]+]]:ccr_sgpr_64(p0) = G_GLOBAL_VALUE @callee
  ; GFX11-NEXT:   [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[BUILD_VECTOR]](<3 x i32>)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV]](i32)
  ; GFX11-NEXT:   $sgpr0 = COPY [[INTRINSIC_CONVERGENT]](i32)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT1:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV1]](i32)
  ; GFX11-NEXT:   $sgpr1 = COPY [[INTRINSIC_CONVERGENT1]](i32)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT2:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV2]](i32)
  ; GFX11-NEXT:   $sgpr2 = COPY [[INTRINSIC_CONVERGENT2]](i32)
  ; GFX11-NEXT:   $vgpr8 = COPY [[COPY3]](i32)
  ; GFX11-NEXT:   $vgpr9 = COPY [[COPY4]](p5)
  ; GFX11-NEXT:   $vgpr10 = COPY [[COPY5]](i32)
  ; GFX11-NEXT:   $vgpr11 = COPY [[COPY6]](i32)
  ; GFX11-NEXT:   SI_CS_CHAIN_TC_W32 [[GV1]](p0), @callee, 0, -1, amdgpu_allvgprs, implicit $sgpr0, implicit $sgpr1, implicit $sgpr2, implicit $vgpr8, implicit $vgpr9, implicit $vgpr10, implicit $vgpr11
  ;
  ; GFX10-LABEL: name: chain_call
  ; GFX10: bb.1 (%ir-block.0):
  ; GFX10-NEXT:   liveins: $sgpr0, $sgpr1, $sgpr2, $vgpr8, $vgpr9, $vgpr10, $vgpr11
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $sgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $sgpr2
  ; GFX10-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[COPY]](i32), [[COPY1]](i32), [[COPY2]](i32)
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr8
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(p5) = COPY $vgpr9
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr10
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr11
  ; GFX10-NEXT:   [[GV:%[0-9]+]]:_(p0) = G_GLOBAL_VALUE @callee
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[GV1:%[0-9]+]]:ccr_sgpr_64(p0) = G_GLOBAL_VALUE @callee
  ; GFX10-NEXT:   [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[BUILD_VECTOR]](<3 x i32>)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV]](i32)
  ; GFX10-NEXT:   $sgpr0 = COPY [[INTRINSIC_CONVERGENT]](i32)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT1:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV1]](i32)
  ; GFX10-NEXT:   $sgpr1 = COPY [[INTRINSIC_CONVERGENT1]](i32)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT2:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV2]](i32)
  ; GFX10-NEXT:   $sgpr2 = COPY [[INTRINSIC_CONVERGENT2]](i32)
  ; GFX10-NEXT:   $vgpr8 = COPY [[COPY3]](i32)
  ; GFX10-NEXT:   $vgpr9 = COPY [[COPY4]](p5)
  ; GFX10-NEXT:   $vgpr10 = COPY [[COPY5]](i32)
  ; GFX10-NEXT:   $vgpr11 = COPY [[COPY6]](i32)
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr48_sgpr49_sgpr50_sgpr51
  ; GFX10-NEXT:   $sgpr48_sgpr49_sgpr50_sgpr51 = COPY [[COPY7]](<4 x i32>)
  ; GFX10-NEXT:   SI_CS_CHAIN_TC_W32 [[GV1]](p0), @callee, 0, -1, amdgpu_allvgprs, implicit $sgpr0, implicit $sgpr1, implicit $sgpr2, implicit $vgpr8, implicit $vgpr9, implicit $vgpr10, implicit $vgpr11, implicit $sgpr48_sgpr49_sgpr50_sgpr51
  call void(ptr, i32, <3 x i32>, { i32, ptr addrspace(5), i32, i32 }, i32, ...) @llvm.amdgcn.cs.chain(ptr @callee, i32 -1, <3 x i32> inreg %sgpr, { i32, ptr addrspace(5), i32, i32 } %vgpr, i32 0)
  unreachable
}

define amdgpu_cs_chain void @chain_preserve_call(<3 x i32> inreg %sgpr, { i32, ptr addrspace(5), i32, i32 } %vgpr) {
  ; GFX11-LABEL: name: chain_preserve_call
  ; GFX11: bb.1 (%ir-block.0):
  ; GFX11-NEXT:   liveins: $sgpr0, $sgpr1, $sgpr2, $vgpr8, $vgpr9, $vgpr10, $vgpr11
  ; GFX11-NEXT: {{  $}}
  ; GFX11-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX11-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $sgpr1
  ; GFX11-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $sgpr2
  ; GFX11-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[COPY]](i32), [[COPY1]](i32), [[COPY2]](i32)
  ; GFX11-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr8
  ; GFX11-NEXT:   [[COPY4:%[0-9]+]]:_(p5) = COPY $vgpr9
  ; GFX11-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr10
  ; GFX11-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr11
  ; GFX11-NEXT:   [[GV:%[0-9]+]]:_(p0) = G_GLOBAL_VALUE @callee_preserve
  ; GFX11-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX11-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX11-NEXT:   [[GV1:%[0-9]+]]:ccr_sgpr_64(p0) = G_GLOBAL_VALUE @callee_preserve
  ; GFX11-NEXT:   [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[BUILD_VECTOR]](<3 x i32>)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV]](i32)
  ; GFX11-NEXT:   $sgpr0 = COPY [[INTRINSIC_CONVERGENT]](i32)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT1:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV1]](i32)
  ; GFX11-NEXT:   $sgpr1 = COPY [[INTRINSIC_CONVERGENT1]](i32)
  ; GFX11-NEXT:   [[INTRINSIC_CONVERGENT2:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV2]](i32)
  ; GFX11-NEXT:   $sgpr2 = COPY [[INTRINSIC_CONVERGENT2]](i32)
  ; GFX11-NEXT:   $vgpr8 = COPY [[COPY3]](i32)
  ; GFX11-NEXT:   $vgpr9 = COPY [[COPY4]](p5)
  ; GFX11-NEXT:   $vgpr10 = COPY [[COPY5]](i32)
  ; GFX11-NEXT:   $vgpr11 = COPY [[COPY6]](i32)
  ; GFX11-NEXT:   SI_CS_CHAIN_TC_W32 [[GV1]](p0), @callee_preserve, 0, -1, amdgpu_allvgprs, implicit $sgpr0, implicit $sgpr1, implicit $sgpr2, implicit $vgpr8, implicit $vgpr9, implicit $vgpr10, implicit $vgpr11
  ;
  ; GFX10-LABEL: name: chain_preserve_call
  ; GFX10: bb.1 (%ir-block.0):
  ; GFX10-NEXT:   liveins: $sgpr0, $sgpr1, $sgpr2, $vgpr8, $vgpr9, $vgpr10, $vgpr11
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $sgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $sgpr2
  ; GFX10-NEXT:   [[BUILD_VECTOR:%[0-9]+]]:_(<3 x i32>) = G_BUILD_VECTOR [[COPY]](i32), [[COPY1]](i32), [[COPY2]](i32)
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr8
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(p5) = COPY $vgpr9
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr10
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr11
  ; GFX10-NEXT:   [[GV:%[0-9]+]]:_(p0) = G_GLOBAL_VALUE @callee_preserve
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[GV1:%[0-9]+]]:ccr_sgpr_64(p0) = G_GLOBAL_VALUE @callee_preserve
  ; GFX10-NEXT:   [[UV:%[0-9]+]]:_(i32), [[UV1:%[0-9]+]]:_(i32), [[UV2:%[0-9]+]]:_(i32) = G_UNMERGE_VALUES [[BUILD_VECTOR]](<3 x i32>)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV]](i32)
  ; GFX10-NEXT:   $sgpr0 = COPY [[INTRINSIC_CONVERGENT]](i32)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT1:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV1]](i32)
  ; GFX10-NEXT:   $sgpr1 = COPY [[INTRINSIC_CONVERGENT1]](i32)
  ; GFX10-NEXT:   [[INTRINSIC_CONVERGENT2:%[0-9]+]]:_(i32) = G_INTRINSIC_CONVERGENT intrinsic(@llvm.amdgcn.readfirstlane), [[UV2]](i32)
  ; GFX10-NEXT:   $sgpr2 = COPY [[INTRINSIC_CONVERGENT2]](i32)
  ; GFX10-NEXT:   $vgpr8 = COPY [[COPY3]](i32)
  ; GFX10-NEXT:   $vgpr9 = COPY [[COPY4]](p5)
  ; GFX10-NEXT:   $vgpr10 = COPY [[COPY5]](i32)
  ; GFX10-NEXT:   $vgpr11 = COPY [[COPY6]](i32)
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:_(<4 x i32>) = COPY $sgpr48_sgpr49_sgpr50_sgpr51
  ; GFX10-NEXT:   $sgpr48_sgpr49_sgpr50_sgpr51 = COPY [[COPY7]](<4 x i32>)
  ; GFX10-NEXT:   SI_CS_CHAIN_TC_W32 [[GV1]](p0), @callee_preserve, 0, -1, amdgpu_allvgprs, implicit $sgpr0, implicit $sgpr1, implicit $sgpr2, implicit $vgpr8, implicit $vgpr9, implicit $vgpr10, implicit $vgpr11, implicit $sgpr48_sgpr49_sgpr50_sgpr51
  call void(ptr, i32, <3 x i32>, { i32, ptr addrspace(5), i32, i32 }, i32, ...) @llvm.amdgcn.cs.chain(ptr @callee_preserve, i32 -1, <3 x i32> inreg %sgpr, { i32, ptr addrspace(5), i32, i32 } %vgpr, i32 0)
  unreachable
}


