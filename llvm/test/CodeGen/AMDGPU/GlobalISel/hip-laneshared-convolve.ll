; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
; RUN: llc -global-isel -mtriple=amdgcn-- -mcpu=gfx1300 -amdgpu-promote-lane-shared=false -stop-after=finalize-isel -verify-machineinstrs -o - %s | FileCheck %s
; RUN: llc -global-isel -mtriple=amdgcn-- -mcpu=gfx1300 -stop-after=finalize-isel -o - %s | FileCheck -check-prefix=VIDX %s
target datalayout = "A5"

@weights = external local_unnamed_addr addrspace(10) global <9 x i32>, align 64
@col_center = external local_unnamed_addr addrspace(10) global <3 x i32>, align 16
@col_left = external local_unnamed_addr addrspace(10) global <3 x i32>, align 16
@col_right = external local_unnamed_addr addrspace(10) global <3 x i32>, align 16
@out = external local_unnamed_addr addrspace(10) global <8 x i16>, align 16

; Function Attrs: convergent mustprogress nofree norecurse nosync nounwind memory(readwrite, argmem: none, inaccessiblemem: none)
define dso_local amdgpu_kernel void @_Z36test_amdgcn_convolve_f16_fp8_fp8_3x3_4x4v() local_unnamed_addr {
  ; CHECK-LABEL: name: _Z36test_amdgcn_convolve_f16_fp8_fp8_3x3_4x4v
  ; CHECK: bb.1.entry:
  ; CHECK-NEXT:   liveins: $sgpr4_sgpr5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sgpr_128 = REG_SEQUENCE [[S_MOV_B32_]], %subreg.sub0, [[S_MOV_B32_]], %subreg.sub1, [[S_MOV_B32_]], %subreg.sub2, [[S_MOV_B32_]], %subreg.sub3
  ; CHECK-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 160
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_1]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @col_center, align 16, addrspace 10)
  ; CHECK-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 144
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR1:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_2]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @col_left, align 16, addrspace 10)
  ; CHECK-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 128
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR2:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_3]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @col_right, align 16, addrspace 10)
  ; CHECK-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 64
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR3:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_4]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @weights, align 64, addrspace 10)
  ; CHECK-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 76
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR4:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_5]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @weights + 12, align 4, basealign 64, addrspace 10)
  ; CHECK-NEXT:   [[S_MOV_B32_6:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 88
  ; CHECK-NEXT:   [[SCRATCH_LOAD_DWORDX3_SADDR5:%[0-9]+]]:vreg_96_align2 = SCRATCH_LOAD_DWORDX3_SADDR [[S_MOV_B32_6]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (<3 x s32>) from @weights + 24, align 8, basealign 64, addrspace 10)
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_288_align2 = REG_SEQUENCE [[SCRATCH_LOAD_DWORDX3_SADDR3]], %subreg.sub0_sub1_sub2, [[SCRATCH_LOAD_DWORDX3_SADDR4]], %subreg.sub3_sub4_sub5, [[SCRATCH_LOAD_DWORDX3_SADDR5]], %subreg.sub6_sub7_sub8
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vreg_128_align2 = COPY [[REG_SEQUENCE]]
  ; CHECK-NEXT:   [[V_CONVOLVE_F16_FP8_FP8_3x3_4x4_:%[0-9]+]]:vreg_128_align2 = contract V_CONVOLVE_F16_FP8_FP8_3x3_4x4 [[COPY]], [[REG_SEQUENCE1]], [[SCRATCH_LOAD_DWORDX3_SADDR]], [[SCRATCH_LOAD_DWORDX3_SADDR1]], [[SCRATCH_LOAD_DWORDX3_SADDR2]], 42, -1, 0, 0, implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_7:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 0
  ; CHECK-NEXT:   SCRATCH_STORE_DWORDX4_SADDR [[V_CONVOLVE_F16_FP8_FP8_3x3_4x4_]], [[S_MOV_B32_7]], 0, 0, implicit $exec, implicit $flat_scr :: (store (<8 x s16>) into @out, !tbaa !0, addrspace 10)
  ; CHECK-NEXT:   S_ENDPGM 0
  ;
  ; VIDX-LABEL: name: _Z36test_amdgcn_convolve_f16_fp8_fp8_3x3_4x4v
  ; VIDX: bb.1.entry:
  ; VIDX-NEXT:   liveins: $sgpr4_sgpr5
  ; VIDX-NEXT: {{  $}}
  ; VIDX-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; VIDX-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:sgpr_128 = REG_SEQUENCE [[S_MOV_B32_]], %subreg.sub0, [[S_MOV_B32_]], %subreg.sub1, [[S_MOV_B32_]], %subreg.sub2, [[S_MOV_B32_]], %subreg.sub3
  ; VIDX-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 112
  ; VIDX-NEXT:   [[S_LSHR_B32_:%[0-9]+]]:sreg_32_xexec_hi = S_LSHR_B32 [[S_MOV_B32_1]], 2, implicit-def dead $scc
  ; VIDX-NEXT:   [[V_LOAD_IDX:%[0-9]+]]:vreg_96_align2 = V_LOAD_IDX [[S_LSHR_B32_]], 0, implicit $exec :: (dereferenceable load (<3 x s32>) from @col_center, align 16, addrspace 10)
  ; VIDX-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 96
  ; VIDX-NEXT:   [[S_LSHR_B32_1:%[0-9]+]]:sreg_32_xexec_hi = S_LSHR_B32 [[S_MOV_B32_2]], 2, implicit-def dead $scc
  ; VIDX-NEXT:   [[V_LOAD_IDX1:%[0-9]+]]:vreg_96_align2 = V_LOAD_IDX [[S_LSHR_B32_1]], 0, implicit $exec :: (dereferenceable load (<3 x s32>) from @col_left, align 16, addrspace 10)
  ; VIDX-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 80
  ; VIDX-NEXT:   [[S_LSHR_B32_2:%[0-9]+]]:sreg_32_xexec_hi = S_LSHR_B32 [[S_MOV_B32_3]], 2, implicit-def dead $scc
  ; VIDX-NEXT:   [[V_LOAD_IDX2:%[0-9]+]]:vreg_96_align2 = V_LOAD_IDX [[S_LSHR_B32_2]], 0, implicit $exec :: (dereferenceable load (<3 x s32>) from @col_right, align 16, addrspace 10)
  ; VIDX-NEXT:   [[S_MOV_B32_4:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 16
  ; VIDX-NEXT:   [[S_LSHR_B32_3:%[0-9]+]]:sreg_32_xexec_hi = S_LSHR_B32 [[S_MOV_B32_4]], 2, implicit-def dead $scc
  ; VIDX-NEXT:   [[V_LOAD_IDX3:%[0-9]+]]:vreg_288_align2 = V_LOAD_IDX [[S_LSHR_B32_3]], 0, implicit $exec :: (dereferenceable load (<9 x s32>) from @weights, align 64, addrspace 10)
  ; VIDX-NEXT:   [[COPY:%[0-9]+]]:vreg_128_align2 = COPY [[REG_SEQUENCE]]
  ; VIDX-NEXT:   [[V_CONVOLVE_F16_FP8_FP8_3x3_4x4_:%[0-9]+]]:vreg_128_align2 = contract V_CONVOLVE_F16_FP8_FP8_3x3_4x4 [[COPY]], [[V_LOAD_IDX3]], [[V_LOAD_IDX]], [[V_LOAD_IDX1]], [[V_LOAD_IDX2]], 42, -1, 0, 0, implicit $exec
  ; VIDX-NEXT:   [[S_MOV_B32_5:%[0-9]+]]:sreg_32_xexec_hi = S_MOV_B32 0
  ; VIDX-NEXT:   [[S_LSHR_B32_4:%[0-9]+]]:sreg_32_xexec_hi = S_LSHR_B32 [[S_MOV_B32_5]], 2, implicit-def dead $scc
  ; VIDX-NEXT:   V_STORE_IDX [[V_CONVOLVE_F16_FP8_FP8_3x3_4x4_]], [[S_LSHR_B32_4]], 0, implicit $exec :: (store (<8 x s16>) into @out, !tbaa !0, addrspace 10)
  ; VIDX-NEXT:   S_ENDPGM 0
entry:
  %vec30 = load <3 x i32>, ptr addrspace(10) @col_center, align 16
  %vec31 = load <3 x i32>, ptr addrspace(10) @col_left, align 16
  %vec32 = load <3 x i32>, ptr addrspace(10) @col_right, align 16
  %wei = load <9 x i32>, ptr addrspace(10) @weights, align 64
  %0 = tail call contract <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half> zeroinitializer, <9 x i32> %wei, <3 x i32> %vec30, <3 x i32> %vec31, <3 x i32> %vec32, i32 42, i1 true)
  store <8 x half> %0, ptr addrspace(10) @out, align 16, !tbaa !4
  ret void
}

; Function Attrs: convergent mustprogress nocallback nofree nosync nounwind willreturn memory(none)
declare <8 x half> @llvm.amdgcn.convolve.f16.fp8.fp8.3x3.v8f16.v8f16.v9i32.v3i32(<8 x half>, <9 x i32>, <3 x i32>, <3 x i32>, <3 x i32>, i32 immarg, i1 immarg) #1

!4 = !{!5, !5, i64 0}
!5 = !{!"omnipotent char", !6, i64 0}
!6 = !{!"Simple C++ TBAA"}
