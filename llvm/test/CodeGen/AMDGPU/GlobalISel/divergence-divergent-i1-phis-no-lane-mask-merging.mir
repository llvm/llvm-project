# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 4
# RUN: llc -global-isel -mtriple=amdgcn-mesa-amdpal -mcpu=gfx1010 -run-pass=amdgpu-global-isel-divergence-lowering -verify-machineinstrs %s -o - | FileCheck -check-prefix=GFX10 %s

--- |
  define void @divergent_i1_phi_uniform_branch() {ret void}
  define void @divergent_i1_phi_uniform_branch_simple() {ret void}
  define void @divergent_i1_phi_used_inside_loop() {ret void}
  define void @divergent_i1_phi_used_inside_loop_bigger_loop_body() {ret void}
  define void @_amdgpu_cs_main() #0 {ret void}

  attributes #0 = {"amdgpu-flat-work-group-size"="1,1"}
...

---
name: divergent_i1_phi_uniform_branch
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_uniform_branch
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x30000000), %bb.2(0x50000000)
  ; GFX10-NEXT:   liveins: $sgpr0, $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY4]](i32), [[COPY5]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 6
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(uge), [[COPY2]](i32), [[C]]
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(ne), [[COPY3]](i32), [[C1]]
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP]](i1)
  ; GFX10-NEXT:   G_BRCOND [[ICMP1]](i1), %bb.2
  ; GFX10-NEXT:   G_BR %bb.1
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 123
  ; GFX10-NEXT:   G_STORE [[C2]](i32), [[MV1]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.4(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[COPY6]](i1), %bb.0, %20(i1), %bb.3
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[COPY2]](i32), [[C3]]
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY7]](i1), [[C5]], [[C4]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x30000000), %bb.2(0x50000000)
    liveins: $sgpr0, $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $sgpr0
    %5:_(i32) = COPY $vgpr3
    %6:_(i32) = COPY $vgpr4
    %7:_(p1) = G_MERGE_VALUES %5(i32), %6(i32)
    %8:_(i32) = G_CONSTANT i32 6
    %9:_(i1) = G_ICMP intpred(uge), %3(i32), %8
    %10:_(i32) = G_CONSTANT i32 0
    %11:_(i1) = G_ICMP intpred(ne), %4(i32), %10
    G_BRCOND %11(i1), %bb.2
    G_BR %bb.1

  bb.1:
    successors: %bb.3(0x80000000)

    %12:_(i32) = G_CONSTANT i32 123
    G_STORE %12(i32), %7(p1) :: (store (i32), addrspace 1)
    G_BR %bb.3

  bb.2:
    successors: %bb.4(0x80000000)

    %13:_(i1) = G_PHI %14(i1), %bb.3, %9(i1), %bb.0
    G_BR %bb.4

  bb.3:
    successors: %bb.2(0x80000000)

    %15:_(i32) = G_CONSTANT i32 1
    %14:_(i1) = G_ICMP intpred(ult), %3(i32), %15
    G_BR %bb.2

  bb.4:
    %16:_(i32) = G_CONSTANT i32 2
    %17:_(i32) = G_CONSTANT i32 1
    %18:_(i32) = G_SELECT %13(i1), %17, %16
    G_STORE %18(i32), %2(p1) :: (store (i32), addrspace 1)
    S_ENDPGM 0
...

---
name: divergent_i1_phi_uniform_branch_simple
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_uniform_branch_simple
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x30000000), %bb.2(0x50000000)
  ; GFX10-NEXT:   liveins: $sgpr0, $vgpr0, $vgpr1, $vgpr2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p1) = G_MERGE_VALUES [[COPY]](i32), [[COPY1]](i32)
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 6
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(uge), [[COPY2]](i32), [[C]]
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(ne), [[COPY3]](i32), [[C1]]
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP]](i1)
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[COPY4]](i1)
  ; GFX10-NEXT:   G_BRCOND [[ICMP1]](i1), %bb.2
  ; GFX10-NEXT:   G_BR %bb.1
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(ult), [[COPY2]](i32), [[C2]]
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:sreg_32(i1) = COPY [[ICMP2]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY5]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY6]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[COPY4]](i1), %bb.0, [[S_OR_B32_]](i1), %bb.1
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY7]](i1), [[C4]], [[C3]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p1) :: (store (i32), addrspace 1)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x30000000), %bb.2(0x50000000)
    liveins: $sgpr0, $vgpr0, $vgpr1, $vgpr2

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(p1) = G_MERGE_VALUES %0(i32), %1(i32)
    %3:_(i32) = COPY $vgpr2
    %4:_(i32) = COPY $sgpr0
    %5:_(i32) = G_CONSTANT i32 6
    %6:_(i1) = G_ICMP intpred(uge), %3(i32), %5
    %7:_(i32) = G_CONSTANT i32 0
    %8:_(i1) = G_ICMP intpred(ne), %4(i32), %7
    G_BRCOND %8(i1), %bb.2
    G_BR %bb.1

  bb.1:
    successors: %bb.2(0x80000000)

    %9:_(i32) = G_CONSTANT i32 1
    %10:_(i1) = G_ICMP intpred(ult), %3(i32), %9

  bb.2:
    %11:_(i1) = G_PHI %6(i1), %bb.0, %10(i1), %bb.1
    %12:_(i32) = G_CONSTANT i32 2
    %13:_(i32) = G_CONSTANT i32 1
    %14:_(i32) = G_SELECT %11(i1), %13, %12
    G_STORE %14(i32), %2(p1) :: (store (i32), addrspace 1)
    S_ENDPGM 0
...

---
name: divergent_i1_phi_used_inside_loop
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_used_inside_loop
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p0) = G_MERGE_VALUES [[COPY1]](i32), [[COPY2]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.2(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[DEF]](i1), %bb.0, %25(i1), %bb.1
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:_(i32) = G_PHI %7(i32), %bb.1, [[C1]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI [[C1]](i32), %bb.0, %9(i32), %bb.1
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:_(i1) = G_PHI [[C]](i1), %bb.0, %11(i1), %bb.1
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[XOR:%[0-9]+]]:_(i1) = G_XOR [[PHI3]], [[C2]]
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[XOR]](i1)
  ; GFX10-NEXT:   [[UITOFP:%[0-9]+]]:_(f32) = G_UITOFP [[PHI2]](i32)
  ; GFX10-NEXT:   [[BITCAST:%[0-9]+]]:_(f32) = G_BITCAST [[COPY]](i32)
  ; GFX10-NEXT:   [[FCMP:%[0-9]+]]:_(i1) = G_FCMP floatpred(ogt), [[UITOFP]](f32), [[BITCAST]]
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[PHI2]], [[C3]]
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[FCMP]](i1), [[PHI1]](i32)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY3]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY4]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.1
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI4]](i32)
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(f32) = G_FCONSTANT float 0.000000e+00
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(f32) = G_FCONSTANT float 1.000000e+00
  ; GFX10-NEXT:   [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[C5]](f32)
  ; GFX10-NEXT:   [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[C4]](f32)
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY5]](i1), [[BITCAST1]], [[BITCAST2]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p0) :: (store (i32))
  ; GFX10-NEXT:   SI_RETURN
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr2
    %3:_(p0) = G_MERGE_VALUES %1(i32), %2(i32)
    %4:_(i1) = G_CONSTANT i1 true
    %5:_(i32) = G_CONSTANT i32 0

  bb.1:
    successors: %bb.2(0x04000000), %bb.1(0x7c000000)

    %6:_(i32) = G_PHI %7(i32), %bb.1, %5(i32), %bb.0
    %8:_(i32) = G_PHI %5(i32), %bb.0, %9(i32), %bb.1
    %10:_(i1) = G_PHI %4(i1), %bb.0, %11(i1), %bb.1
    %12:_(i1) = G_CONSTANT i1 true
    %11:_(i1) = G_XOR %10, %12
    %13:_(f32) = G_UITOFP %8(i32)
    %14:_(f32) = G_BITCAST %0(i32)
    %15:_(i1) = G_FCMP floatpred(ogt), %13(f32), %14
    %16:_(i32) = G_CONSTANT i32 1
    %9:_(i32) = G_ADD %8, %16
    %7:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %15(i1), %6(i32)
    SI_LOOP %7(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.2

  bb.2:
    %17:_(i1) = G_PHI %11(i1), %bb.1
    %18:_(i32) = G_PHI %7(i32), %bb.1
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %18(i32)
    %19:_(f32) = G_FCONSTANT float 0.000000e+00
    %20:_(f32) = G_FCONSTANT float 1.000000e+00
    %21:_(i32) = G_BITCAST %20(f32)
    %22:_(i32) = G_BITCAST %19(f32)
    %23:_(i32) = G_SELECT %17(i1), %21, %22
    G_STORE %23(i32), %3(p0) :: (store (i32))
    SI_RETURN
...

---
name: divergent_i1_phi_used_inside_loop_bigger_loop_body
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: divergent_i1_phi_used_inside_loop_bigger_loop_body
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x80000000)
  ; GFX10-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:_(i32) = COPY $vgpr3
  ; GFX10-NEXT:   [[MV:%[0-9]+]]:_(p0) = G_MERGE_VALUES [[COPY2]](i32), [[COPY3]](i32)
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:_(i32) = COPY $vgpr4
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:_(i32) = COPY $vgpr5
  ; GFX10-NEXT:   [[MV1:%[0-9]+]]:_(p0) = G_MERGE_VALUES [[COPY4]](i32), [[COPY5]](i32)
  ; GFX10-NEXT:   [[COPY6:%[0-9]+]]:_(i32) = COPY $vgpr6
  ; GFX10-NEXT:   [[COPY7:%[0-9]+]]:_(i32) = COPY $vgpr7
  ; GFX10-NEXT:   [[MV2:%[0-9]+]]:_(p0) = G_MERGE_VALUES [[COPY6]](i32), [[COPY7]](i32)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(f32) = G_FCONSTANT float 1.000000e+00
  ; GFX10-NEXT:   [[BITCAST:%[0-9]+]]:_(f32) = G_BITCAST [[COPY1]](i32)
  ; GFX10-NEXT:   [[FCMP:%[0-9]+]]:_(i1) = G_FCMP floatpred(ogt), [[BITCAST]](f32), [[C1]]
  ; GFX10-NEXT:   [[COPY8:%[0-9]+]]:sreg_32(i1) = COPY [[FCMP]](i1)
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:sreg_32(i1) = IMPLICIT_DEF
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.4(0x40000000), %bb.2(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[DEF]](i1), %bb.0, %46(i1), %bb.5
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:sreg_32(i1) = PHI [[COPY8]](i1), %bb.0, %43(i1), %bb.5
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI %16(i32), %bb.5, [[C]](i32), %bb.0
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:_(i32) = G_PHI [[C]](i32), %bb.0, %18(i32), %bb.5
  ; GFX10-NEXT:   [[COPY9:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   [[COPY10:%[0-9]+]]:sreg_32(i1) = COPY [[PHI1]](i1)
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 1000
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(sle), [[PHI3]](i32), [[C3]]
  ; GFX10-NEXT:   G_BRCOND [[ICMP]](i1), %bb.4
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.3(0x40000000), %bb.5(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i1) = G_PHI %25(i1), %bb.4, [[C2]](i1), %bb.1
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[XOR:%[0-9]+]]:_(i1) = G_XOR [[PHI4]], [[C4]]
  ; GFX10-NEXT:   G_BRCOND [[XOR]](i1), %bb.5
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.5(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i32) = G_CONSTANT i32 1000
  ; GFX10-NEXT:   G_STORE [[C5]](i32), [[MV1]](p0) :: (store (i32))
  ; GFX10-NEXT:   G_BR %bb.5
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i1) = G_CONSTANT i1 false
  ; GFX10-NEXT:   [[C7:%[0-9]+]]:_(i32) = G_CONSTANT i32 1000
  ; GFX10-NEXT:   G_STORE [[C7]](i32), [[MV2]](p0) :: (store (i32))
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   successors: %bb.6(0x04000000), %bb.1(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C8:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[XOR1:%[0-9]+]]:_(i1) = G_XOR [[COPY10]], [[C8]]
  ; GFX10-NEXT:   [[COPY11:%[0-9]+]]:sreg_32(i1) = COPY [[XOR1]](i1)
  ; GFX10-NEXT:   [[UITOFP:%[0-9]+]]:_(f32) = G_UITOFP [[PHI3]](i32)
  ; GFX10-NEXT:   [[BITCAST1:%[0-9]+]]:_(f32) = G_BITCAST [[COPY]](i32)
  ; GFX10-NEXT:   [[FCMP1:%[0-9]+]]:_(i1) = G_FCMP floatpred(ogt), [[UITOFP]](f32), [[BITCAST1]]
  ; GFX10-NEXT:   [[C9:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[PHI3]], [[C9]]
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), [[FCMP1]](i1), [[PHI2]](i32)
  ; GFX10-NEXT:   [[COPY12:%[0-9]+]]:sreg_32(i1) = COPY [[XOR1]](i1)
  ; GFX10-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32(i1) = S_ANDN2_B32 [[COPY9]](i1), $exec_lo, implicit-def $scc
  ; GFX10-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32(i1) = S_AND_B32 $exec_lo, [[COPY11]](i1), implicit-def $scc
  ; GFX10-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32(i1) = S_OR_B32 [[S_ANDN2_B32_]](i1), [[S_AND_B32_]](i1), implicit-def $scc
  ; GFX10-NEXT:   SI_LOOP [[INT]](i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:_(i32) = G_PHI [[INT]](i32), %bb.5
  ; GFX10-NEXT:   [[COPY13:%[0-9]+]]:sreg_32(i1) = COPY [[S_OR_B32_]](i1)
  ; GFX10-NEXT:   G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), [[PHI5]](i32)
  ; GFX10-NEXT:   [[C10:%[0-9]+]]:_(f32) = G_FCONSTANT float 0.000000e+00
  ; GFX10-NEXT:   [[C11:%[0-9]+]]:_(f32) = G_FCONSTANT float 1.000000e+00
  ; GFX10-NEXT:   [[BITCAST2:%[0-9]+]]:_(i32) = G_BITCAST [[C11]](f32)
  ; GFX10-NEXT:   [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[C10]](f32)
  ; GFX10-NEXT:   [[SELECT:%[0-9]+]]:_(i32) = G_SELECT [[COPY13]](i1), [[BITCAST2]], [[BITCAST3]]
  ; GFX10-NEXT:   G_STORE [[SELECT]](i32), [[MV]](p0) :: (store (i32))
  ; GFX10-NEXT:   SI_RETURN
  bb.0:
    successors: %bb.1(0x80000000)
    liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7

    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr2
    %3:_(i32) = COPY $vgpr3
    %4:_(p0) = G_MERGE_VALUES %2(i32), %3(i32)
    %5:_(i32) = COPY $vgpr4
    %6:_(i32) = COPY $vgpr5
    %7:_(p0) = G_MERGE_VALUES %5(i32), %6(i32)
    %8:_(i32) = COPY $vgpr6
    %9:_(i32) = COPY $vgpr7
    %10:_(p0) = G_MERGE_VALUES %8(i32), %9(i32)
    %11:_(i32) = G_CONSTANT i32 0
    %12:_(f32) = G_FCONSTANT float 1.000000e+00
    %13:_(f32) = G_BITCAST %1(i32)
    %14:_(i1) = G_FCMP floatpred(ogt), %13(f32), %12

  bb.1:
    successors: %bb.4(0x40000000), %bb.2(0x40000000)
    %15:_(i32) = G_PHI %16(i32), %bb.5, %11(i32), %bb.0
    %17:_(i32) = G_PHI %11(i32), %bb.0, %18(i32), %bb.5
    %19:_(i1) = G_PHI %14(i1), %bb.0, %20(i1), %bb.5
    %21:_(i1) = G_CONSTANT i1 true
    %22:_(i32) = G_CONSTANT i32 1000
    %23:_(i1) = G_ICMP intpred(sle), %17(i32), %22
    G_BRCOND %23(i1), %bb.4

    G_BR %bb.2

  bb.2:
    successors: %bb.3(0x40000000), %bb.5(0x40000000)
    %24:_(i1) = G_PHI %25(i1), %bb.4, %21(i1), %bb.1
    %26:_(i1) = G_CONSTANT i1 true
    %27:_(i1) = G_XOR %24, %26
    G_BRCOND %27(i1), %bb.5

    G_BR %bb.3

  bb.3:
    successors: %bb.5(0x80000000)

    %28:_(i32) = G_CONSTANT i32 1000
    G_STORE %28(i32), %7(p0) :: (store (i32))
    G_BR %bb.5

  bb.4:
    successors: %bb.2(0x80000000)

    %25:_(i1) = G_CONSTANT i1 false
    %29:_(i32) = G_CONSTANT i32 1000
    G_STORE %29(i32), %10(p0) :: (store (i32))
    G_BR %bb.2

  bb.5:
    successors: %bb.6(0x04000000), %bb.1(0x7c000000)

    %30:_(i1) = G_CONSTANT i1 true
    %20:_(i1) = G_XOR %19, %30
    %31:_(f32) = G_UITOFP %17(i32)
    %32:_(f32) = G_BITCAST %0(i32)
    %33:_(i1) = G_FCMP floatpred(ogt), %31(f32), %32
    %34:_(i32) = G_CONSTANT i32 1
    %18:_(i32) = G_ADD %17, %34
    %16:sreg_32_xm0_xexec(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.if.break), %33(i1), %15(i32)
    SI_LOOP %16(i32), %bb.1, implicit-def $exec, implicit-def $scc, implicit $exec
    G_BR %bb.6

  bb.6:
    %35:_(i1) = G_PHI %20(i1), %bb.5
    %36:_(i32) = G_PHI %16(i32), %bb.5
    G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.end.cf), %36(i32)
    %37:_(f32) = G_FCONSTANT float 0.000000e+00
    %38:_(f32) = G_FCONSTANT float 1.000000e+00
    %39:_(i32) = G_BITCAST %38(f32)
    %40:_(i32) = G_BITCAST %37(f32)
    %41:_(i32) = G_SELECT %35(i1), %39, %40
    G_STORE %41(i32), %4(p0) :: (store (i32))
    SI_RETURN
...

---
name: _amdgpu_cs_main
legalized: true
tracksRegLiveness: true
body: |
  ; GFX10-LABEL: name: _amdgpu_cs_main
  ; GFX10: bb.0:
  ; GFX10-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; GFX10-NEXT:   liveins: $sgpr0, $sgpr1, $sgpr2, $sgpr3, $vgpr0, $vgpr1, $vgpr2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[COPY:%[0-9]+]]:_(i32) = COPY $sgpr0
  ; GFX10-NEXT:   [[COPY1:%[0-9]+]]:_(i32) = COPY $sgpr1
  ; GFX10-NEXT:   [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr0
  ; GFX10-NEXT:   [[DEF:%[0-9]+]]:_(i32) = G_IMPLICIT_DEF
  ; GFX10-NEXT:   [[INT:%[0-9]+]]:_(i64) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.getpc)
  ; GFX10-NEXT:   [[C:%[0-9]+]]:_(i64) = G_CONSTANT i64 -4294967296
  ; GFX10-NEXT:   [[AND:%[0-9]+]]:_(i64) = G_AND [[INT]], [[C]]
  ; GFX10-NEXT:   [[ZEXT:%[0-9]+]]:_(i64) = G_ZEXT [[COPY]](i32)
  ; GFX10-NEXT:   [[OR:%[0-9]+]]:_(i64) = G_OR [[AND]], [[ZEXT]]
  ; GFX10-NEXT:   [[INTTOPTR:%[0-9]+]]:_(p4) = G_INTTOPTR [[OR]](i64)
  ; GFX10-NEXT:   [[LOAD:%[0-9]+]]:_(<8 x i32>) = G_LOAD [[INTTOPTR]](p4) :: (load (<8 x i32>))
  ; GFX10-NEXT:   [[BITCAST:%[0-9]+]]:_(i256) = G_BITCAST [[LOAD]](<8 x i32>)
  ; GFX10-NEXT:   [[TRUNC:%[0-9]+]]:_(i128) = G_TRUNC [[BITCAST]](i256)
  ; GFX10-NEXT:   [[BITCAST1:%[0-9]+]]:_(<4 x i32>) = G_BITCAST [[TRUNC]](i128)
  ; GFX10-NEXT:   [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[C2:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX10-NEXT:   [[INT1:%[0-9]+]]:_(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.mbcnt.lo), [[C2]](i32), [[C1]](i32)
  ; GFX10-NEXT:   [[INT2:%[0-9]+]]:_(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.mbcnt.hi), [[C2]](i32), [[INT1]](i32)
  ; GFX10-NEXT:   [[FREEZE:%[0-9]+]]:_(i32) = G_FREEZE [[INT2]]
  ; GFX10-NEXT:   [[C3:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL:%[0-9]+]]:_(i32) = G_SHL [[FREEZE]], [[C3]](i32)
  ; GFX10-NEXT:   [[AMDGPU_BUFFER_LOAD:%[0-9]+]]:_(i32) = G_AMDGPU_BUFFER_LOAD [[BITCAST1]](<4 x i32>), [[C1]](i32), [[SHL]], [[C1]], 0, 0, 0 :: (load (i32), align 1, addrspace 8)
  ; GFX10-NEXT:   [[ICMP:%[0-9]+]]:_(i1) = G_ICMP intpred(eq), [[AMDGPU_BUFFER_LOAD]](i32), [[C1]]
  ; GFX10-NEXT:   [[C4:%[0-9]+]]:_(i32) = G_CONSTANT i32 1
  ; GFX10-NEXT:   [[AND1:%[0-9]+]]:_(i32) = G_AND [[FREEZE]], [[C4]]
  ; GFX10-NEXT:   [[TRUNC1:%[0-9]+]]:_(i1) = G_TRUNC [[AND1]](i32)
  ; GFX10-NEXT:   [[C5:%[0-9]+]]:_(i1) = G_CONSTANT i1 true
  ; GFX10-NEXT:   [[XOR:%[0-9]+]]:_(i1) = G_XOR [[TRUNC1]], [[C5]]
  ; GFX10-NEXT:   [[COPY3:%[0-9]+]]:sreg_32(i1) = COPY [[C5]](i1)
  ; GFX10-NEXT:   G_BRCOND [[XOR]](i1), %bb.2
  ; GFX10-NEXT:   G_BR %bb.1
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.1:
  ; GFX10-NEXT:   successors: %bb.3(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[C6:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   G_BR %bb.3
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.2:
  ; GFX10-NEXT:   successors: %bb.5(0x40000000), %bb.6(0x40000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI:%[0-9]+]]:sreg_32(i1) = PHI [[COPY3]](i1), %bb.0, %58(i1), %bb.4
  ; GFX10-NEXT:   [[PHI1:%[0-9]+]]:_(i32) = G_PHI %30(i32), %bb.4, [[DEF]](i32), %bb.0
  ; GFX10-NEXT:   [[COPY4:%[0-9]+]]:sreg_32(i1) = COPY [[PHI]](i1)
  ; GFX10-NEXT:   G_BRCOND [[COPY4]](i1), %bb.5
  ; GFX10-NEXT:   G_BR %bb.6
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.3:
  ; GFX10-NEXT:   successors: %bb.4(0x04000000), %bb.3(0x7c000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI2:%[0-9]+]]:_(i32) = G_PHI %34(i32), %bb.3, [[C6]](i32), %bb.1
  ; GFX10-NEXT:   [[PHI3:%[0-9]+]]:_(i32) = G_PHI %36(i32), %bb.3, [[FREEZE]](i32), %bb.1
  ; GFX10-NEXT:   [[PHI4:%[0-9]+]]:_(i32) = G_PHI %38(i32), %bb.3, [[C6]](i32), %bb.1
  ; GFX10-NEXT:   [[C7:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   [[AMDGPU_BUFFER_LOAD1:%[0-9]+]]:_(i32) = G_AMDGPU_BUFFER_LOAD [[BITCAST1]](<4 x i32>), [[C7]](i32), [[PHI2]], [[C7]], 0, 0, 0 :: (load (i32), align 1, addrspace 8)
  ; GFX10-NEXT:   [[ADD:%[0-9]+]]:_(i32) = G_ADD [[AMDGPU_BUFFER_LOAD1]], [[PHI4]]
  ; GFX10-NEXT:   [[C8:%[0-9]+]]:_(i32) = G_CONSTANT i32 -1
  ; GFX10-NEXT:   [[ADD1:%[0-9]+]]:_(i32) = G_ADD [[PHI3]], [[C8]]
  ; GFX10-NEXT:   [[C9:%[0-9]+]]:_(i32) = G_CONSTANT i32 4
  ; GFX10-NEXT:   [[ADD2:%[0-9]+]]:_(i32) = G_ADD [[PHI2]], [[C9]]
  ; GFX10-NEXT:   [[ICMP1:%[0-9]+]]:_(i1) = G_ICMP intpred(ne), [[ADD1]](i32), [[C7]]
  ; GFX10-NEXT:   G_BRCOND [[ICMP1]](i1), %bb.3
  ; GFX10-NEXT:   G_BR %bb.4
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.4:
  ; GFX10-NEXT:   successors: %bb.2(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[PHI5:%[0-9]+]]:_(i32) = G_PHI [[ADD]](i32), %bb.3
  ; GFX10-NEXT:   [[C10:%[0-9]+]]:_(i1) = G_CONSTANT i1 false
  ; GFX10-NEXT:   [[ICMP2:%[0-9]+]]:_(i1) = G_ICMP intpred(eq), [[PHI5]](i32), [[AMDGPU_BUFFER_LOAD]]
  ; GFX10-NEXT:   [[OR1:%[0-9]+]]:_(i1) = G_OR [[ICMP]], [[ICMP2]]
  ; GFX10-NEXT:   [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[OR1]](i1)
  ; GFX10-NEXT:   [[COPY5:%[0-9]+]]:sreg_32(i1) = COPY [[C10]](i1)
  ; GFX10-NEXT:   G_BR %bb.2
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.5:
  ; GFX10-NEXT:   successors: %bb.6(0x80000000)
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT:   [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[ICMP]](i1)
  ; GFX10-NEXT:   [[C11:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[OR2:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[C11]]
  ; GFX10-NEXT: {{  $}}
  ; GFX10-NEXT: bb.6:
  ; GFX10-NEXT:   [[PHI6:%[0-9]+]]:_(i32) = G_PHI [[PHI1]](i32), %bb.2, [[OR2]](i32), %bb.5
  ; GFX10-NEXT:   [[UV:%[0-9]+]]:_(<4 x i32>), [[UV1:%[0-9]+]]:_(<4 x i32>) = G_UNMERGE_VALUES [[LOAD]](<8 x i32>)
  ; GFX10-NEXT:   [[ADD3:%[0-9]+]]:_(i32) = G_ADD [[COPY2]], [[COPY1]]
  ; GFX10-NEXT:   [[C12:%[0-9]+]]:_(i32) = G_CONSTANT i32 2
  ; GFX10-NEXT:   [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[ADD3]], [[C12]](i32)
  ; GFX10-NEXT:   [[C13:%[0-9]+]]:_(i32) = G_CONSTANT i32 0
  ; GFX10-NEXT:   G_AMDGPU_BUFFER_STORE [[PHI6]](i32), [[UV1]](<4 x i32>), [[C13]](i32), [[SHL1]], [[C13]], 0, 0, 0 :: (store (i32), align 1, addrspace 8)
  ; GFX10-NEXT:   S_ENDPGM 0
  bb.0:
    successors: %bb.1(0x40000000), %bb.2(0x40000000)
    liveins: $sgpr0, $sgpr1, $sgpr2, $sgpr3, $vgpr0, $vgpr1, $vgpr2

    %0:_(i32) = COPY $sgpr0
    %1:_(i32) = COPY $sgpr1
    %2:_(i32) = COPY $vgpr0
    %3:_(i32) = G_IMPLICIT_DEF
    %4:_(i64) = G_INTRINSIC intrinsic(@llvm.amdgcn.s.getpc)
    %5:_(i64) = G_CONSTANT i64 -4294967296
    %6:_(i64) = G_AND %4, %5
    %7:_(i64) = G_ZEXT %0(i32)
    %8:_(i64) = G_OR %6, %7
    %9:_(p4) = G_INTTOPTR %8(i64)
    %10:_(<8 x i32>) = G_LOAD %9(p4) :: (load (<8 x i32>))
    %11:_(i256) = G_BITCAST %10(<8 x i32>)
    %12:_(i128) = G_TRUNC %11(i256)
    %13:_(<4 x i32>) = G_BITCAST %12(i128)
    %14:_(i32) = G_CONSTANT i32 0
    %15:_(i32) = G_CONSTANT i32 -1
    %16:_(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.mbcnt.lo), %15(i32), %14(i32)
    %17:_(i32) = G_INTRINSIC intrinsic(@llvm.amdgcn.mbcnt.hi), %15(i32), %16(i32)
    %18:_(i32) = G_FREEZE %17
    %19:_(i32) = G_CONSTANT i32 2
    %20:_(i32) = G_SHL %18, %19(i32)
    %21:_(i32) = G_AMDGPU_BUFFER_LOAD %13(<4 x i32>), %14(i32), %20, %14, 0, 0, 0 :: (load (i32), align 1, addrspace 8)
    %22:_(i1) = G_ICMP intpred(eq), %21(i32), %14
    %23:_(i32) = G_CONSTANT i32 1
    %24:_(i32) = G_AND %18, %23
    %25:_(i1) = G_TRUNC %24(i32)
    %26:_(i1) = G_CONSTANT i1 true
    %27:_(i1) = G_XOR %25, %26
    G_BRCOND %27(i1), %bb.2
    G_BR %bb.1

  bb.1:
    successors: %bb.3(0x80000000)

    %28:_(i32) = G_CONSTANT i32 0
    G_BR %bb.3

  bb.2:
    successors: %bb.5(0x40000000), %bb.6(0x40000000)

    %29:_(i32) = G_PHI %30(i32), %bb.4, %3(i32), %bb.0
    %31:_(i1) = G_PHI %32(i1), %bb.4, %26(i1), %bb.0
    G_BRCOND %31(i1), %bb.5
    G_BR %bb.6

  bb.3:
    successors: %bb.4(0x04000000), %bb.3(0x7c000000)

    %33:_(i32) = G_PHI %34(i32), %bb.3, %28(i32), %bb.1
    %35:_(i32) = G_PHI %36(i32), %bb.3, %18(i32), %bb.1
    %37:_(i32) = G_PHI %38(i32), %bb.3, %28(i32), %bb.1
    %39:_(i32) = G_CONSTANT i32 0
    %40:_(i32) = G_AMDGPU_BUFFER_LOAD %13(<4 x i32>), %39(i32), %33, %39, 0, 0, 0 :: (load (i32), align 1, addrspace 8)
    %38:_(i32) = G_ADD %40, %37
    %41:_(i32) = G_CONSTANT i32 -1
    %36:_(i32) = G_ADD %35, %41
    %42:_(i32) = G_CONSTANT i32 4
    %34:_(i32) = G_ADD %33, %42
    %43:_(i1) = G_ICMP intpred(ne), %36(i32), %39
    G_BRCOND %43(i1), %bb.3
    G_BR %bb.4

  bb.4:
    successors: %bb.2(0x80000000)

    %44:_(i32) = G_PHI %38(i32), %bb.3
    %32:_(i1) = G_CONSTANT i1 false
    %45:_(i1) = G_ICMP intpred(eq), %44(i32), %21
    %46:_(i1) = G_OR %22, %45
    %30:_(i32) = G_ZEXT %46(i1)
    G_BR %bb.2

  bb.5:
    successors: %bb.6(0x80000000)

    %47:_(i32) = G_ZEXT %22(i1)
    %48:_(i32) = G_CONSTANT i32 2
    %49:_(i32) = G_OR %47, %48

  bb.6:
    %50:_(i32) = G_PHI %29(i32), %bb.2, %49(i32), %bb.5
    %51:_(<4 x i32>), %52:_(<4 x i32>) = G_UNMERGE_VALUES %10(<8 x i32>)
    %53:_(i32) = G_ADD %2, %1
    %54:_(i32) = G_CONSTANT i32 2
    %55:_(i32) = G_SHL %53, %54(i32)
    %56:_(i32) = G_CONSTANT i32 0
    G_AMDGPU_BUFFER_STORE %50(i32), %52(<4 x i32>), %56(i32), %55, %56, 0, 0, 0 :: (store (i32), align 1, addrspace 8)
    S_ENDPGM 0
...
