# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -amdgpu-global-isel-new-legality -mtriple=amdgcn-amd-amdhsa -mcpu=hawaii -run-pass="amdgpu-regbankselect,amdgpu-regbanklegalize" %s -verify-machineinstrs -o - | FileCheck %s -check-prefixes=GCN,GFX7
# RUN: llc -amdgpu-global-isel-new-legality -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1200 -run-pass="amdgpu-regbankselect,amdgpu-regbanklegalize" %s -verify-machineinstrs -o - | FileCheck %s -check-prefixes=GCN,GFX12

--- |
  define amdgpu_kernel void @load_global_v8i32_non_uniform(ptr addrspace(1) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %global.not.uniform.v8i32 = getelementptr <8 x i32>, ptr addrspace(1) %in, i32 %tmp0
    %tmp2 = load <8 x i32>, ptr addrspace(1) %global.not.uniform.v8i32
    ret void
  }

  define amdgpu_kernel void @load_global_v4i64_non_uniform(ptr addrspace(1) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %global.not.uniform.v4i64 = getelementptr <4 x i64>, ptr addrspace(1) %in, i32 %tmp0
    %tmp2 = load <4 x i64>, ptr addrspace(1) %global.not.uniform.v4i64
    ret void
  }
  define amdgpu_kernel void @load_global_v16i32_non_uniform(ptr addrspace(1) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %global.not.uniform.v16i32 = getelementptr <16 x i32>, ptr addrspace(1) %in, i32 %tmp0
    %tmp2 = load <16 x i32>, ptr addrspace(1) %global.not.uniform.v16i32
    ret void
  }
  define amdgpu_kernel void @load_global_v8i64_non_uniform(ptr addrspace(1) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %global.not.uniform.v8i64 = getelementptr <8 x i64>, ptr addrspace(1) %in, i32 %tmp0
    %tmp2 = load <8 x i64>, ptr addrspace(1) %global.not.uniform.v8i64
    ret void
  }
  define amdgpu_kernel void @load_global_v8i32_uniform() {ret void}
  define amdgpu_kernel void @load_global_v4i64_uniform() {ret void}
  define amdgpu_kernel void @load_global_v16i32_uniform() {ret void}
  define amdgpu_kernel void @load_global_v8i64_uniform() {ret void}
  define amdgpu_kernel void @load_constant_v8i32_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform.v8i32 = getelementptr <8 x i32>, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load <8 x i32>, ptr addrspace(4) %constant.not.uniform.v8i32
    ret void
  }

  define amdgpu_kernel void @load_constant_i256_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform = getelementptr i256, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load i256, ptr addrspace(4) %constant.not.uniform
    ret void
  }

  define amdgpu_kernel void @load_constant_v16i16_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform = getelementptr <16 x i16>, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load <16 x i16>, ptr addrspace(4) %constant.not.uniform
    ret void
  }

  define amdgpu_kernel void @load_constant_v4i64_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform.v4i64 = getelementptr <4 x i64>, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load <4 x i64>, ptr addrspace(4) %constant.not.uniform.v4i64
    ret void
  }
  define amdgpu_kernel void @load_constant_v16i32_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform.v16i32 = getelementptr <16 x i32>, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load <16 x i32>, ptr addrspace(4) %constant.not.uniform.v16i32
    ret void
  }
  define amdgpu_kernel void @load_constant_v8i64_non_uniform(ptr addrspace(4) %in) {
    %tmp0 = call i32 @llvm.amdgcn.workitem.id.x() #0
    %constant.not.uniform.v8i64 = getelementptr <8 x i64>, ptr addrspace(4) %in, i32 %tmp0
    %tmp2 = load <8 x i64>, ptr addrspace(4) %constant.not.uniform.v8i64
    ret void
  }

  define amdgpu_kernel void @load_constant_v8i32_uniform() {ret void}
  define amdgpu_kernel void @load_constant_v16i16_uniform() {ret void}
  define amdgpu_kernel void @load_constant_v4i64_uniform() {ret void}
  define amdgpu_kernel void @load_constant_v16i32_uniform() {ret void}
  define amdgpu_kernel void @load_constant_v8i64_uniform() {ret void}
  define amdgpu_kernel void @load_local_uniform() { ret void }
  define amdgpu_kernel void @load_region_uniform() { ret void }
  define amdgpu_kernel void @extload_constant_i8_to_i32_uniform() { ret void }
  define amdgpu_kernel void @extload_global_i8_to_i32_uniform() { ret void }
  define amdgpu_kernel void @extload_constant_i16_to_i32_uniform() { ret void }
  define amdgpu_kernel void @extload_global_i16_to_i32_uniform() { ret void }
  define amdgpu_kernel void @load_constant_i32_uniform_align4() {ret void}
  define amdgpu_kernel void @load_constant_i32_uniform_align2() {ret void}
  define amdgpu_kernel void @load_constant_i32_uniform_align1() {ret void}
  define amdgpu_kernel void @load_private_uniform_sgpr_i32() {ret void}
  define amdgpu_kernel void @load_constant_v8i32_vgpr_crash() { ret void }
  define amdgpu_kernel void @load_constant_v8i32_vgpr_crash_loop_phi() { ret void }

  define amdgpu_kernel void @load_constant_v3i32_align4() { ret void }
  define amdgpu_kernel void @load_constant_v3i32_align8() { ret void }
  define amdgpu_kernel void @load_constant_v3i32_align16() { ret void }

  define amdgpu_kernel void @load_constant_v6i16_align4() { ret void }
  define amdgpu_kernel void @load_constant_v6i16_align8() { ret void }
  define amdgpu_kernel void @load_constant_v6i16_align16() { ret void }

  define amdgpu_kernel void @load_constant_i96_align4() { ret void }
  define amdgpu_kernel void @load_constant_i96_align8() { ret void }
  define amdgpu_kernel void @load_constant_i96_align16() { ret void }

  declare i32 @llvm.amdgcn.workitem.id.x() #0
  attributes #0 = { nounwind readnone }
...

---
name: load_global_v8i32_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v8i32_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p1) = COPY [[COPY]](p1)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[COPY1]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v8i32, align 32, addrspace 1)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v8i32 + 16, basealign 32, addrspace 1)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32), [[UV2:%[0-9]+]]:vgpr(i32), [[UV3:%[0-9]+]]:vgpr(i32), [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32), [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[CONCAT_VECTORS]](<8 x i32>)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<8 x i32>) = G_BUILD_VECTOR [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32), [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32), [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32), [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<8 x i32>) = G_LOAD %0(p1) :: (load (<8 x i32>) from %ir.global.not.uniform.v8i32, addrspace 1)
...

---
name: load_global_v4i64_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: load_global_v4i64_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p1) = COPY [[COPY]](p1)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[COPY1]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v4i64, align 32, addrspace 1)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v4i64 + 16, basealign 32, addrspace 1)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<4 x f64>) = G_CONCAT_VECTORS [[LOAD]](<2 x f64>), [[LOAD1]](<2 x f64>)
    ; GCN-NEXT: [[BITCAST:%[0-9]+]]:vgpr(<4 x i64>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f64>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i64), [[UV1:%[0-9]+]]:vgpr(i64), [[UV2:%[0-9]+]]:vgpr(i64), [[UV3:%[0-9]+]]:vgpr(i64) = G_UNMERGE_VALUES [[BITCAST]](<4 x i64>)
    ; GCN-NEXT: [[BITCAST1:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV]](i64)
    ; GCN-NEXT: [[BITCAST2:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST1]](f64)
    ; GCN-NEXT: [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST2]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[MV:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32)
    ; GCN-NEXT: [[BITCAST3:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV]](i64)
    ; GCN-NEXT: [[BITCAST4:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV1]](i64)
    ; GCN-NEXT: [[BITCAST5:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST4]](f64)
    ; GCN-NEXT: [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST5]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[MV1:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32)
    ; GCN-NEXT: [[BITCAST6:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV1]](i64)
    ; GCN-NEXT: [[BITCAST7:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV2]](i64)
    ; GCN-NEXT: [[BITCAST8:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST7]](f64)
    ; GCN-NEXT: [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST8]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[MV2:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32)
    ; GCN-NEXT: [[BITCAST9:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV2]](i64)
    ; GCN-NEXT: [[BITCAST10:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV3]](i64)
    ; GCN-NEXT: [[BITCAST11:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST10]](f64)
    ; GCN-NEXT: [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST11]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[MV3:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    ; GCN-NEXT: [[BITCAST12:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV3]](i64)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<4 x f64>) = G_BUILD_VECTOR [[BITCAST3]](f64), [[BITCAST6]](f64), [[BITCAST9]](f64), [[BITCAST12]](f64)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<4 x f64>) = G_LOAD %0(p1) :: (load (<4 x f64>) from %ir.global.not.uniform.v4i64, addrspace 1)
...

---
name: load_global_v16i32_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v16i32_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p1) = COPY [[COPY]](p1)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[COPY1]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v16i32, align 64, addrspace 1)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v16i32 + 16, basealign 64, addrspace 1)
    ; GCN-NEXT: [[C1:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 32
    ; GCN-NEXT: [[PTR_ADD1:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C1]](i64)
    ; GCN-NEXT: [[LOAD2:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD1]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v16i32 + 32, align 32, basealign 64, addrspace 1)
    ; GCN-NEXT: [[C2:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 48
    ; GCN-NEXT: [[PTR_ADD2:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C2]](i64)
    ; GCN-NEXT: [[LOAD3:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD2]](p1) :: (load (<4 x i32>) from %ir.global.not.uniform.v16i32 + 48, basealign 64, addrspace 1)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<16 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>), [[LOAD2]](<4 x i32>), [[LOAD3]](<4 x i32>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32), [[UV2:%[0-9]+]]:vgpr(i32), [[UV3:%[0-9]+]]:vgpr(i32), [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32), [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32), [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32), [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32), [[UV12:%[0-9]+]]:vgpr(i32), [[UV13:%[0-9]+]]:vgpr(i32), [[UV14:%[0-9]+]]:vgpr(i32), [[UV15:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[CONCAT_VECTORS]](<16 x i32>)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE8:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE9:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE10:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE11:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE12:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV12]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE13:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV13]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE14:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV14]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE15:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV15]]
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<16 x i32>) = G_BUILD_VECTOR [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32), [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32), [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32), [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32), [[AMDGPU_READANYLANE8]](i32), [[AMDGPU_READANYLANE9]](i32), [[AMDGPU_READANYLANE10]](i32), [[AMDGPU_READANYLANE11]](i32), [[AMDGPU_READANYLANE12]](i32), [[AMDGPU_READANYLANE13]](i32), [[AMDGPU_READANYLANE14]](i32), [[AMDGPU_READANYLANE15]](i32)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<16 x i32>) = G_LOAD %0(p1) :: (load (<16 x i32>) from %ir.global.not.uniform.v16i32, addrspace 1)
...

---
name: load_global_v8i64_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v8i64_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p1) = COPY [[COPY]](p1)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[COPY1]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v8i64, align 64, addrspace 1)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v8i64 + 16, basealign 64, addrspace 1)
    ; GCN-NEXT: [[C1:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 32
    ; GCN-NEXT: [[PTR_ADD1:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C1]](i64)
    ; GCN-NEXT: [[LOAD2:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD1]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v8i64 + 32, align 32, basealign 64, addrspace 1)
    ; GCN-NEXT: [[C2:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 48
    ; GCN-NEXT: [[PTR_ADD2:%[0-9]+]]:vgpr(p1) = G_PTR_ADD [[COPY1]], [[C2]](i64)
    ; GCN-NEXT: [[LOAD3:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD2]](p1) :: (load (<2 x f64>) from %ir.global.not.uniform.v8i64 + 48, basealign 64, addrspace 1)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x f64>) = G_CONCAT_VECTORS [[LOAD]](<2 x f64>), [[LOAD1]](<2 x f64>), [[LOAD2]](<2 x f64>), [[LOAD3]](<2 x f64>)
    ; GCN-NEXT: [[BITCAST:%[0-9]+]]:vgpr(<8 x i64>) = G_BITCAST [[CONCAT_VECTORS]](<8 x f64>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i64), [[UV1:%[0-9]+]]:vgpr(i64), [[UV2:%[0-9]+]]:vgpr(i64), [[UV3:%[0-9]+]]:vgpr(i64), [[UV4:%[0-9]+]]:vgpr(i64), [[UV5:%[0-9]+]]:vgpr(i64), [[UV6:%[0-9]+]]:vgpr(i64), [[UV7:%[0-9]+]]:vgpr(i64) = G_UNMERGE_VALUES [[BITCAST]](<8 x i64>)
    ; GCN-NEXT: [[BITCAST1:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV]](i64)
    ; GCN-NEXT: [[BITCAST2:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST1]](f64)
    ; GCN-NEXT: [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST2]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[MV:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32)
    ; GCN-NEXT: [[BITCAST3:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV]](i64)
    ; GCN-NEXT: [[BITCAST4:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV1]](i64)
    ; GCN-NEXT: [[BITCAST5:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST4]](f64)
    ; GCN-NEXT: [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST5]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[MV1:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32)
    ; GCN-NEXT: [[BITCAST6:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV1]](i64)
    ; GCN-NEXT: [[BITCAST7:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV2]](i64)
    ; GCN-NEXT: [[BITCAST8:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST7]](f64)
    ; GCN-NEXT: [[UV12:%[0-9]+]]:vgpr(i32), [[UV13:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST8]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV12]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV13]]
    ; GCN-NEXT: [[MV2:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32)
    ; GCN-NEXT: [[BITCAST9:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV2]](i64)
    ; GCN-NEXT: [[BITCAST10:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV3]](i64)
    ; GCN-NEXT: [[BITCAST11:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST10]](f64)
    ; GCN-NEXT: [[UV14:%[0-9]+]]:vgpr(i32), [[UV15:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST11]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV14]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV15]]
    ; GCN-NEXT: [[MV3:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    ; GCN-NEXT: [[BITCAST12:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV3]](i64)
    ; GCN-NEXT: [[BITCAST13:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV4]](i64)
    ; GCN-NEXT: [[BITCAST14:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST13]](f64)
    ; GCN-NEXT: [[UV16:%[0-9]+]]:vgpr(i32), [[UV17:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST14]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE8:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV16]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE9:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV17]]
    ; GCN-NEXT: [[MV4:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE8]](i32), [[AMDGPU_READANYLANE9]](i32)
    ; GCN-NEXT: [[BITCAST15:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV4]](i64)
    ; GCN-NEXT: [[BITCAST16:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV5]](i64)
    ; GCN-NEXT: [[BITCAST17:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST16]](f64)
    ; GCN-NEXT: [[UV18:%[0-9]+]]:vgpr(i32), [[UV19:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST17]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE10:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV18]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE11:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV19]]
    ; GCN-NEXT: [[MV5:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE10]](i32), [[AMDGPU_READANYLANE11]](i32)
    ; GCN-NEXT: [[BITCAST18:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV5]](i64)
    ; GCN-NEXT: [[BITCAST19:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV6]](i64)
    ; GCN-NEXT: [[BITCAST20:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST19]](f64)
    ; GCN-NEXT: [[UV20:%[0-9]+]]:vgpr(i32), [[UV21:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST20]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE12:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV20]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE13:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV21]]
    ; GCN-NEXT: [[MV6:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE12]](i32), [[AMDGPU_READANYLANE13]](i32)
    ; GCN-NEXT: [[BITCAST21:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV6]](i64)
    ; GCN-NEXT: [[BITCAST22:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV7]](i64)
    ; GCN-NEXT: [[BITCAST23:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST22]](f64)
    ; GCN-NEXT: [[UV22:%[0-9]+]]:vgpr(i32), [[UV23:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST23]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE14:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV22]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE15:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV23]]
    ; GCN-NEXT: [[MV7:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE14]](i32), [[AMDGPU_READANYLANE15]](i32)
    ; GCN-NEXT: [[BITCAST24:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV7]](i64)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<8 x f64>) = G_BUILD_VECTOR [[BITCAST3]](f64), [[BITCAST6]](f64), [[BITCAST9]](f64), [[BITCAST12]](f64), [[BITCAST15]](f64), [[BITCAST18]](f64), [[BITCAST21]](f64), [[BITCAST24]](f64)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<8 x f64>) = G_LOAD %0(p1) :: (load (<8 x f64>) from %ir.global.not.uniform.v8i64, addrspace 1)
...

---
name: load_global_v8i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v8i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<8 x i32>) = G_LOAD [[COPY]](p1) :: (invariant load (<8 x i32>), addrspace 1)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<8 x i32>) = G_LOAD %0(p1) :: (invariant load (<8 x i32>), addrspace 1)
...

---
name: load_global_v4i64_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v4i64_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<4 x f64>) = G_LOAD [[COPY]](p1) :: (invariant load (<4 x f64>), addrspace 1)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<4 x f64>) = G_LOAD %0(p1) :: (invariant load (<4 x f64>), addrspace 1)
...

---
name: load_global_v16i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v16i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<16 x i32>) = G_LOAD [[COPY]](p1) :: (invariant load (<16 x i32>), addrspace 1)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<16 x i32>) = G_LOAD %0(p1) :: (invariant load (<16 x i32>), addrspace 1)
...

---
name: load_global_v8i64_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_global_v8i64_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p1) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<8 x f64>) = G_LOAD [[COPY]](p1) :: (invariant load (<8 x f64>), addrspace 1)
    %0:_(p1) = COPY $sgpr0_sgpr1
    %1:_(<8 x f64>) = G_LOAD %0(p1) :: (invariant load (<8 x f64>), addrspace 1)
...

---
name: load_constant_v8i32_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v8i32_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[COPY1]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v8i32, align 32, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v8i32 + 16, basealign 32, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32), [[UV2:%[0-9]+]]:vgpr(i32), [[UV3:%[0-9]+]]:vgpr(i32), [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32), [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[CONCAT_VECTORS]](<8 x i32>)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<8 x i32>) = G_BUILD_VECTOR [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32), [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32), [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32), [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<8 x i32>) = G_LOAD %0(p4) :: (load (<8 x i32>) from %ir.constant.not.uniform.v8i32, addrspace 4)
...

---
name: load_constant_i256_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_i256_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i128) = G_LOAD [[COPY1]](p4) :: (load (i128) from %ir.constant.not.uniform, align 32, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(i128) = G_LOAD [[PTR_ADD]](p4) :: (load (i128) from %ir.constant.not.uniform + 16, basealign 32, addrspace 4)
    ; GCN-NEXT: [[MV:%[0-9]+]]:vgpr(i256) = G_MERGE_VALUES [[LOAD]](i128), [[LOAD1]](i128)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32), [[UV2:%[0-9]+]]:vgpr(i32), [[UV3:%[0-9]+]]:vgpr(i32), [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32), [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[MV]](i256)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[MV1:%[0-9]+]]:sgpr(i256) = G_MERGE_VALUES [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32), [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32), [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32), [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i256) = G_LOAD %0(p4) :: (load (i256) from %ir.constant.not.uniform, addrspace 4)
...

---
name: load_constant_v16i16_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: load_constant_v16i16_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<8 x i16>) = G_LOAD [[COPY1]](p4) :: (load (<8 x i16>) from %ir.constant.not.uniform, align 32, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<8 x i16>) = G_LOAD [[PTR_ADD]](p4) :: (load (<8 x i16>) from %ir.constant.not.uniform + 16, basealign 32, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<16 x i16>) = G_CONCAT_VECTORS [[LOAD]](<8 x i16>), [[LOAD1]](<8 x i16>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(<2 x i16>), [[UV1:%[0-9]+]]:vgpr(<2 x i16>), [[UV2:%[0-9]+]]:vgpr(<2 x i16>), [[UV3:%[0-9]+]]:vgpr(<2 x i16>), [[UV4:%[0-9]+]]:vgpr(<2 x i16>), [[UV5:%[0-9]+]]:vgpr(<2 x i16>), [[UV6:%[0-9]+]]:vgpr(<2 x i16>), [[UV7:%[0-9]+]]:vgpr(<2 x i16>) = G_UNMERGE_VALUES [[CONCAT_VECTORS]](<16 x i16>)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(<2 x i16>) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[CONCAT_VECTORS1:%[0-9]+]]:sgpr(<16 x i16>) = G_CONCAT_VECTORS [[AMDGPU_READANYLANE]](<2 x i16>), [[AMDGPU_READANYLANE1]](<2 x i16>), [[AMDGPU_READANYLANE2]](<2 x i16>), [[AMDGPU_READANYLANE3]](<2 x i16>), [[AMDGPU_READANYLANE4]](<2 x i16>), [[AMDGPU_READANYLANE5]](<2 x i16>), [[AMDGPU_READANYLANE6]](<2 x i16>), [[AMDGPU_READANYLANE7]](<2 x i16>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<16 x i16>) = G_LOAD %0(p4) :: (load (<16 x i16>) from %ir.constant.not.uniform, addrspace 4)
...

---
name: load_constant_v4i64_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v4i64_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[COPY1]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v4i64, align 32, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v4i64 + 16, basealign 32, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<4 x f64>) = G_CONCAT_VECTORS [[LOAD]](<2 x f64>), [[LOAD1]](<2 x f64>)
    ; GCN-NEXT: [[BITCAST:%[0-9]+]]:vgpr(<4 x i64>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f64>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i64), [[UV1:%[0-9]+]]:vgpr(i64), [[UV2:%[0-9]+]]:vgpr(i64), [[UV3:%[0-9]+]]:vgpr(i64) = G_UNMERGE_VALUES [[BITCAST]](<4 x i64>)
    ; GCN-NEXT: [[BITCAST1:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV]](i64)
    ; GCN-NEXT: [[BITCAST2:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST1]](f64)
    ; GCN-NEXT: [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST2]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[MV:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32)
    ; GCN-NEXT: [[BITCAST3:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV]](i64)
    ; GCN-NEXT: [[BITCAST4:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV1]](i64)
    ; GCN-NEXT: [[BITCAST5:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST4]](f64)
    ; GCN-NEXT: [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST5]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[MV1:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32)
    ; GCN-NEXT: [[BITCAST6:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV1]](i64)
    ; GCN-NEXT: [[BITCAST7:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV2]](i64)
    ; GCN-NEXT: [[BITCAST8:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST7]](f64)
    ; GCN-NEXT: [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST8]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[MV2:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32)
    ; GCN-NEXT: [[BITCAST9:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV2]](i64)
    ; GCN-NEXT: [[BITCAST10:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV3]](i64)
    ; GCN-NEXT: [[BITCAST11:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST10]](f64)
    ; GCN-NEXT: [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST11]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[MV3:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    ; GCN-NEXT: [[BITCAST12:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV3]](i64)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<4 x f64>) = G_BUILD_VECTOR [[BITCAST3]](f64), [[BITCAST6]](f64), [[BITCAST9]](f64), [[BITCAST12]](f64)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<4 x f64>) = G_LOAD %0(p4) :: (load (<4 x f64>) from %ir.constant.not.uniform.v4i64, addrspace 4)
...

---
name: load_constant_v16i32_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v16i32_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[COPY1]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v16i32, align 64, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v16i32 + 16, basealign 64, addrspace 4)
    ; GCN-NEXT: [[C1:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 32
    ; GCN-NEXT: [[PTR_ADD1:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C1]](i64)
    ; GCN-NEXT: [[LOAD2:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD1]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v16i32 + 32, align 32, basealign 64, addrspace 4)
    ; GCN-NEXT: [[C2:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 48
    ; GCN-NEXT: [[PTR_ADD2:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C2]](i64)
    ; GCN-NEXT: [[LOAD3:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD2]](p4) :: (load (<4 x i32>) from %ir.constant.not.uniform.v16i32 + 48, basealign 64, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<16 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>), [[LOAD2]](<4 x i32>), [[LOAD3]](<4 x i32>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i32), [[UV1:%[0-9]+]]:vgpr(i32), [[UV2:%[0-9]+]]:vgpr(i32), [[UV3:%[0-9]+]]:vgpr(i32), [[UV4:%[0-9]+]]:vgpr(i32), [[UV5:%[0-9]+]]:vgpr(i32), [[UV6:%[0-9]+]]:vgpr(i32), [[UV7:%[0-9]+]]:vgpr(i32), [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32), [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32), [[UV12:%[0-9]+]]:vgpr(i32), [[UV13:%[0-9]+]]:vgpr(i32), [[UV14:%[0-9]+]]:vgpr(i32), [[UV15:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[CONCAT_VECTORS]](<16 x i32>)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV1]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV2]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV3]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV4]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV5]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV6]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV7]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE8:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE9:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE10:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE11:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE12:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV12]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE13:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV13]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE14:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV14]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE15:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV15]]
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<16 x i32>) = G_BUILD_VECTOR [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32), [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32), [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32), [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32), [[AMDGPU_READANYLANE8]](i32), [[AMDGPU_READANYLANE9]](i32), [[AMDGPU_READANYLANE10]](i32), [[AMDGPU_READANYLANE11]](i32), [[AMDGPU_READANYLANE12]](i32), [[AMDGPU_READANYLANE13]](i32), [[AMDGPU_READANYLANE14]](i32), [[AMDGPU_READANYLANE15]](i32)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<16 x i32>) = G_LOAD %0(p4) :: (load (<16 x i32>) from %ir.constant.not.uniform.v16i32, addrspace 4)
...

---
name: load_constant_v8i64_non_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v8i64_non_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[COPY1]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v8i64, align 64, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v8i64 + 16, basealign 64, addrspace 4)
    ; GCN-NEXT: [[C1:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 32
    ; GCN-NEXT: [[PTR_ADD1:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C1]](i64)
    ; GCN-NEXT: [[LOAD2:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD1]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v8i64 + 32, align 32, basealign 64, addrspace 4)
    ; GCN-NEXT: [[C2:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 48
    ; GCN-NEXT: [[PTR_ADD2:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY1]], [[C2]](i64)
    ; GCN-NEXT: [[LOAD3:%[0-9]+]]:vgpr(<2 x f64>) = G_LOAD [[PTR_ADD2]](p4) :: (load (<2 x f64>) from %ir.constant.not.uniform.v8i64 + 48, basealign 64, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x f64>) = G_CONCAT_VECTORS [[LOAD]](<2 x f64>), [[LOAD1]](<2 x f64>), [[LOAD2]](<2 x f64>), [[LOAD3]](<2 x f64>)
    ; GCN-NEXT: [[BITCAST:%[0-9]+]]:vgpr(<8 x i64>) = G_BITCAST [[CONCAT_VECTORS]](<8 x f64>)
    ; GCN-NEXT: [[UV:%[0-9]+]]:vgpr(i64), [[UV1:%[0-9]+]]:vgpr(i64), [[UV2:%[0-9]+]]:vgpr(i64), [[UV3:%[0-9]+]]:vgpr(i64), [[UV4:%[0-9]+]]:vgpr(i64), [[UV5:%[0-9]+]]:vgpr(i64), [[UV6:%[0-9]+]]:vgpr(i64), [[UV7:%[0-9]+]]:vgpr(i64) = G_UNMERGE_VALUES [[BITCAST]](<8 x i64>)
    ; GCN-NEXT: [[BITCAST1:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV]](i64)
    ; GCN-NEXT: [[BITCAST2:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST1]](f64)
    ; GCN-NEXT: [[UV8:%[0-9]+]]:vgpr(i32), [[UV9:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST2]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV8]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE1:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV9]]
    ; GCN-NEXT: [[MV:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE]](i32), [[AMDGPU_READANYLANE1]](i32)
    ; GCN-NEXT: [[BITCAST3:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV]](i64)
    ; GCN-NEXT: [[BITCAST4:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV1]](i64)
    ; GCN-NEXT: [[BITCAST5:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST4]](f64)
    ; GCN-NEXT: [[UV10:%[0-9]+]]:vgpr(i32), [[UV11:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST5]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE2:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV10]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE3:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV11]]
    ; GCN-NEXT: [[MV1:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE2]](i32), [[AMDGPU_READANYLANE3]](i32)
    ; GCN-NEXT: [[BITCAST6:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV1]](i64)
    ; GCN-NEXT: [[BITCAST7:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV2]](i64)
    ; GCN-NEXT: [[BITCAST8:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST7]](f64)
    ; GCN-NEXT: [[UV12:%[0-9]+]]:vgpr(i32), [[UV13:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST8]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE4:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV12]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE5:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV13]]
    ; GCN-NEXT: [[MV2:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE4]](i32), [[AMDGPU_READANYLANE5]](i32)
    ; GCN-NEXT: [[BITCAST9:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV2]](i64)
    ; GCN-NEXT: [[BITCAST10:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV3]](i64)
    ; GCN-NEXT: [[BITCAST11:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST10]](f64)
    ; GCN-NEXT: [[UV14:%[0-9]+]]:vgpr(i32), [[UV15:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST11]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE6:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV14]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE7:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV15]]
    ; GCN-NEXT: [[MV3:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE6]](i32), [[AMDGPU_READANYLANE7]](i32)
    ; GCN-NEXT: [[BITCAST12:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV3]](i64)
    ; GCN-NEXT: [[BITCAST13:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV4]](i64)
    ; GCN-NEXT: [[BITCAST14:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST13]](f64)
    ; GCN-NEXT: [[UV16:%[0-9]+]]:vgpr(i32), [[UV17:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST14]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE8:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV16]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE9:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV17]]
    ; GCN-NEXT: [[MV4:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE8]](i32), [[AMDGPU_READANYLANE9]](i32)
    ; GCN-NEXT: [[BITCAST15:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV4]](i64)
    ; GCN-NEXT: [[BITCAST16:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV5]](i64)
    ; GCN-NEXT: [[BITCAST17:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST16]](f64)
    ; GCN-NEXT: [[UV18:%[0-9]+]]:vgpr(i32), [[UV19:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST17]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE10:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV18]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE11:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV19]]
    ; GCN-NEXT: [[MV5:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE10]](i32), [[AMDGPU_READANYLANE11]](i32)
    ; GCN-NEXT: [[BITCAST18:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV5]](i64)
    ; GCN-NEXT: [[BITCAST19:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV6]](i64)
    ; GCN-NEXT: [[BITCAST20:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST19]](f64)
    ; GCN-NEXT: [[UV20:%[0-9]+]]:vgpr(i32), [[UV21:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST20]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE12:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV20]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE13:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV21]]
    ; GCN-NEXT: [[MV6:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE12]](i32), [[AMDGPU_READANYLANE13]](i32)
    ; GCN-NEXT: [[BITCAST21:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV6]](i64)
    ; GCN-NEXT: [[BITCAST22:%[0-9]+]]:vgpr(f64) = G_BITCAST [[UV7]](i64)
    ; GCN-NEXT: [[BITCAST23:%[0-9]+]]:vgpr(i64) = G_BITCAST [[BITCAST22]](f64)
    ; GCN-NEXT: [[UV22:%[0-9]+]]:vgpr(i32), [[UV23:%[0-9]+]]:vgpr(i32) = G_UNMERGE_VALUES [[BITCAST23]](i64)
    ; GCN-NEXT: [[AMDGPU_READANYLANE14:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV22]]
    ; GCN-NEXT: [[AMDGPU_READANYLANE15:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[UV23]]
    ; GCN-NEXT: [[MV7:%[0-9]+]]:sgpr(i64) = G_MERGE_VALUES [[AMDGPU_READANYLANE14]](i32), [[AMDGPU_READANYLANE15]](i32)
    ; GCN-NEXT: [[BITCAST24:%[0-9]+]]:sgpr(f64) = G_BITCAST [[MV7]](i64)
    ; GCN-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<8 x f64>) = G_BUILD_VECTOR [[BITCAST3]](f64), [[BITCAST6]](f64), [[BITCAST9]](f64), [[BITCAST12]](f64), [[BITCAST15]](f64), [[BITCAST18]](f64), [[BITCAST21]](f64), [[BITCAST24]](f64)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<8 x f64>) = G_LOAD %0(p4) :: (load (<8 x f64>) from %ir.constant.not.uniform.v8i64, addrspace 4)
...

---
name: load_constant_v8i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v8i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<8 x i32>) = G_LOAD [[COPY]](p4) :: (load (<8 x i32>), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<8 x i32>) = G_LOAD %0(p4) :: (load (<8 x i32>), addrspace 4)
...

---
name: load_constant_v16i16_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v16i16_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<16 x i16>) = G_LOAD [[COPY]](p4) :: (load (<16 x i16>), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<16 x i16>) = G_LOAD %0(p4) :: (load (<16 x i16>), addrspace 4)
...

---
name: load_constant_v4i64_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v4i64_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<4 x f64>) = G_LOAD [[COPY]](p4) :: (load (<4 x f64>), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<4 x f64>) = G_LOAD %0(p4) :: (load (<4 x f64>), addrspace 4)
...

---
name: load_constant_v16i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v16i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<16 x i32>) = G_LOAD [[COPY]](p4) :: (load (<16 x i32>), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<16 x i32>) = G_LOAD %0(p4) :: (load (<16 x i32>), addrspace 4)
...

---
name: load_constant_v8i64_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_v8i64_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(<8 x f64>) = G_LOAD [[COPY]](p4) :: (load (<8 x f64>), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<8 x f64>) = G_LOAD %0(p4) :: (load (<8 x f64>), addrspace 4)
...

---
name: load_local_uniform
legalized: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $sgpr0

    ; GCN-LABEL: name: load_local_uniform
    ; GCN: liveins: $sgpr0
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p3) = COPY $sgpr0
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p3) = COPY [[COPY]](p3)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p3) :: (load (i32), addrspace 3)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    %0:_(p3) = COPY $sgpr0
    %1:_(i32) = G_LOAD %0(p3) :: (load (i32), addrspace 3)

...
---
name: load_region_uniform
legalized: true
tracksRegLiveness: true
body: |
  bb.0:
    liveins: $sgpr0

    ; GCN-LABEL: name: load_region_uniform
    ; GCN: liveins: $sgpr0
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p3) = COPY $sgpr0
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p3) = COPY [[COPY]](p3)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p3) :: (load (i32), addrspace 5)
    %0:_(p3) = COPY $sgpr0
    %1:_(i32) = G_LOAD %0(p3) :: (load (i32), addrspace 5)

...

---
name: extload_constant_i8_to_i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: extload_constant_i8_to_i32_uniform
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i8), addrspace 4)
    ; GFX7-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    ;
    ; GFX12-LABEL: name: extload_constant_i8_to_i32_uniform
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(i32) = G_LOAD [[COPY]](p4) :: (load (i8), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i8), addrspace 4)
...

---
name: extload_global_i8_to_i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: extload_global_i8_to_i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i8), addrspace 1)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i8), addrspace 1)
...

---
name: extload_constant_i16_to_i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GFX7-LABEL: name: extload_constant_i16_to_i32_uniform
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i16), addrspace 4)
    ; GFX7-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    ;
    ; GFX12-LABEL: name: extload_constant_i16_to_i32_uniform
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(i32) = G_LOAD [[COPY]](p4) :: (load (i16), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i16), addrspace 4)
...

---
name: extload_global_i16_to_i32_uniform
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: extload_global_i16_to_i32_uniform
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i16), addrspace 1)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i16), addrspace 1)
...

---
name: load_constant_i32_uniform_align4
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GCN-LABEL: name: load_constant_i32_uniform_align4
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:sgpr(i32) = G_LOAD [[COPY]](p4) :: (load (i32), addrspace 4)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i32), addrspace 4)
...

---
name: load_constant_i32_uniform_align2
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: load_constant_i32_uniform_align2
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i32), align 2, addrspace 4)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i32), align 2, addrspace 4)
...

---
name: load_constant_i32_uniform_align1
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1

    ; GCN-LABEL: name: load_constant_i32_uniform_align1
    ; GCN: liveins: $sgpr0_sgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p4) = COPY [[COPY]](p4)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p4) :: (load (i32), align 1, addrspace 4)
    ; GCN-NEXT: [[AMDGPU_READANYLANE:%[0-9]+]]:sgpr(i32) = G_AMDGPU_READANYLANE [[LOAD]]
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i32) = G_LOAD %0(p4) :: (load (i32), align 1, addrspace 4)
...

---
name: load_private_uniform_sgpr_i32
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0

    ; GCN-LABEL: name: load_private_uniform_sgpr_i32
    ; GCN: liveins: $sgpr0
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:sgpr(p5) = COPY $sgpr0
    ; GCN-NEXT: [[COPY1:%[0-9]+]]:vgpr(p5) = COPY [[COPY]](p5)
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(i32) = G_LOAD [[COPY1]](p5) :: (load (i32), addrspace 5)
    %0:_(p5) = COPY $sgpr0
    %1:_(i32) = G_LOAD %0(p5) :: (load (i32), addrspace 5)
...

---
name: load_constant_v8i32_vgpr_crash
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $vgpr0_vgpr1

    ; GCN-LABEL: name: load_constant_v8i32_vgpr_crash
    ; GCN: liveins: $vgpr0_vgpr1
    ; GCN-NEXT: {{  $}}
    ; GCN-NEXT: [[COPY:%[0-9]+]]:vgpr(p4) = COPY $vgpr0_vgpr1
    ; GCN-NEXT: [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[COPY]](p4) :: (load (<4 x i32>), align 32, addrspace 4)
    ; GCN-NEXT: [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
    ; GCN-NEXT: [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GCN-NEXT: [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p4) :: (load (<4 x i32>) from unknown-address + 16, addrspace 4)
    ; GCN-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>)
    %0:_(p4) = COPY $vgpr0_vgpr1
    %1:_(<8 x i32>) = G_LOAD %0(p4) :: (load (<8 x i32>), addrspace 4)
...

---
name: load_constant_v8i32_vgpr_crash_loop_phi
legalized: true
tracksRegLiveness: true

body: |
  ; GCN-LABEL: name: load_constant_v8i32_vgpr_crash_loop_phi
  ; GCN: bb.0:
  ; GCN-NEXT:   successors: %bb.1(0x80000000)
  ; GCN-NEXT:   liveins: $vgpr0_vgpr1, $vgpr2_vgpr3
  ; GCN-NEXT: {{  $}}
  ; GCN-NEXT:   [[COPY:%[0-9]+]]:vgpr(p4) = COPY $vgpr0_vgpr1
  ; GCN-NEXT:   [[COPY1:%[0-9]+]]:vgpr(p4) = COPY $vgpr2_vgpr3
  ; GCN-NEXT:   G_BR %bb.1
  ; GCN-NEXT: {{  $}}
  ; GCN-NEXT: bb.1:
  ; GCN-NEXT:   successors: %bb.1(0x80000000)
  ; GCN-NEXT: {{  $}}
  ; GCN-NEXT:   [[PHI:%[0-9]+]]:vgpr(p4) = G_PHI [[COPY]](p4), %bb.0, %3(p4), %bb.1
  ; GCN-NEXT:   [[LOAD:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PHI]](p4) :: (load (<4 x i32>), align 32, addrspace 4)
  ; GCN-NEXT:   [[C:%[0-9]+]]:vgpr(i64) = G_CONSTANT i64 16
  ; GCN-NEXT:   [[PTR_ADD:%[0-9]+]]:vgpr(p4) = G_PTR_ADD [[PHI]], [[C]](i64)
  ; GCN-NEXT:   [[LOAD1:%[0-9]+]]:vgpr(<4 x i32>) = G_LOAD [[PTR_ADD]](p4) :: (load (<4 x i32>) from unknown-address + 16, addrspace 4)
  ; GCN-NEXT:   [[CONCAT_VECTORS:%[0-9]+]]:vgpr(<8 x i32>) = G_CONCAT_VECTORS [[LOAD]](<4 x i32>), [[LOAD1]](<4 x i32>)
  ; GCN-NEXT:   [[COPY2:%[0-9]+]]:vgpr(p4) = COPY [[COPY1]](p4)
  ; GCN-NEXT:   G_BR %bb.1
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3


    %0:_(p4) = COPY $vgpr0_vgpr1
    %1:_(p4) = COPY $vgpr2_vgpr3
    G_BR %bb.1

  bb.1:
    successors: %bb.1(0x80000000)

    %2:_(p4) = G_PHI %0(p4), %bb.0, %3(p4), %bb.1
    %4:_(<8 x i32>) = G_LOAD %2(p4) :: (load (<8 x i32>), addrspace 4)
    %3:_(p4) = COPY %1(p4)
    G_BR %bb.1



...

---
name: load_constant_v3i32_align4
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v3i32_align4
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<2 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<2 x i32>), align 4, addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(i32) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (i32) from unknown-address + 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(i32), [[UV1:%[0-9]+]]:sgpr(i32) = G_UNMERGE_VALUES [[LOAD]](<2 x i32>)
    ; GFX7-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[LOAD1]](i32)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i32>)
    ;
    ; GFX12-LABEL: name: load_constant_v3i32_align4
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<3 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<3 x i32>), align 4, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<3 x i32>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<3 x i32>) = G_LOAD %0(p4) :: (invariant load (<3 x i32>), align 4, addrspace 4)
    S_ENDPGM 0, implicit %1(<3 x i32>)
...

---
name: load_constant_v3i32_align8
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v3i32_align8
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<2 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<2 x i32>), addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(i32) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (i32) from unknown-address + 8, align 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(i32), [[UV1:%[0-9]+]]:sgpr(i32) = G_UNMERGE_VALUES [[LOAD]](<2 x i32>)
    ; GFX7-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[LOAD1]](i32)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i32>)
    ;
    ; GFX12-LABEL: name: load_constant_v3i32_align8
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<3 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<3 x i32>), align 8, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<3 x i32>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<3 x i32>) = G_LOAD %0(p4) :: (invariant load (<3 x i32>), align 8, addrspace 4)
    S_ENDPGM 0, implicit %1(<3 x i32>)
...

---
name: load_constant_v3i32_align16
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v3i32_align16
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<4 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<4 x i32>), addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(i32), [[UV1:%[0-9]+]]:sgpr(i32), [[UV2:%[0-9]+]]:sgpr(i32), [[UV3:%[0-9]+]]:sgpr(i32) = G_UNMERGE_VALUES [[LOAD]](<4 x i32>)
    ; GFX7-NEXT: [[BUILD_VECTOR:%[0-9]+]]:sgpr(<3 x i32>) = G_BUILD_VECTOR [[UV]](i32), [[UV1]](i32), [[UV2]](i32)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[BUILD_VECTOR]](<3 x i32>)
    ;
    ; GFX12-LABEL: name: load_constant_v3i32_align16
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<3 x i32>) = G_LOAD [[COPY]](p4) :: (invariant load (<3 x i32>), align 16, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<3 x i32>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<3 x i32>) = G_LOAD %0(p4) :: (invariant load (<3 x i32>), align 16, addrspace 4)
    S_ENDPGM 0, implicit %1(<3 x i32>)
...

---
name: load_constant_v6i16_align4
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v6i16_align4
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<4 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<4 x i16>), align 4, addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(<2 x i16>) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (<2 x i16>) from unknown-address + 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(<2 x i16>), [[UV1:%[0-9]+]]:sgpr(<2 x i16>) = G_UNMERGE_VALUES [[LOAD]](<4 x i16>)
    ; GFX7-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:sgpr(<6 x i16>) = G_CONCAT_VECTORS [[UV]](<2 x i16>), [[UV1]](<2 x i16>), [[LOAD1]](<2 x i16>)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[CONCAT_VECTORS]](<6 x i16>)
    ;
    ; GFX12-LABEL: name: load_constant_v6i16_align4
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<6 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<6 x i16>), align 4, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<6 x i16>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<6 x i16>) = G_LOAD %0(p4) :: (invariant load (<6 x i16>), align 4, addrspace 4)
    S_ENDPGM 0, implicit %1(<6 x i16>)
...

---
name: load_constant_v6i16_align8
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v6i16_align8
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<4 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<4 x i16>), addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(<2 x i16>) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (<2 x i16>) from unknown-address + 8, align 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(<2 x i16>), [[UV1:%[0-9]+]]:sgpr(<2 x i16>) = G_UNMERGE_VALUES [[LOAD]](<4 x i16>)
    ; GFX7-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:sgpr(<6 x i16>) = G_CONCAT_VECTORS [[UV]](<2 x i16>), [[UV1]](<2 x i16>), [[LOAD1]](<2 x i16>)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[CONCAT_VECTORS]](<6 x i16>)
    ;
    ; GFX12-LABEL: name: load_constant_v6i16_align8
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<6 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<6 x i16>), align 8, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<6 x i16>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<6 x i16>) = G_LOAD %0(p4) :: (invariant load (<6 x i16>), align 8, addrspace 4)
    S_ENDPGM 0, implicit %1(<6 x i16>)
...

---
name: load_constant_v6i16_align16
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_v6i16_align16
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(<8 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<8 x i16>), addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(<2 x i16>), [[UV1:%[0-9]+]]:sgpr(<2 x i16>), [[UV2:%[0-9]+]]:sgpr(<2 x i16>), [[UV3:%[0-9]+]]:sgpr(<2 x i16>) = G_UNMERGE_VALUES [[LOAD]](<8 x i16>)
    ; GFX7-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:sgpr(<6 x i16>) = G_CONCAT_VECTORS [[UV]](<2 x i16>), [[UV1]](<2 x i16>), [[UV2]](<2 x i16>)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[CONCAT_VECTORS]](<6 x i16>)
    ;
    ; GFX12-LABEL: name: load_constant_v6i16_align16
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(<6 x i16>) = G_LOAD [[COPY]](p4) :: (invariant load (<6 x i16>), align 16, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](<6 x i16>)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(<6 x i16>) = G_LOAD %0(p4) :: (invariant load (<6 x i16>), align 16, addrspace 4)
    S_ENDPGM 0, implicit %1(<6 x i16>)
...

---
name: load_constant_i96_align4
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_i96_align4
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(i64) = G_LOAD [[COPY]](p4) :: (invariant load (i64), align 4, addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(i32) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (i32) from unknown-address + 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(i32), [[UV1:%[0-9]+]]:sgpr(i32) = G_UNMERGE_VALUES [[LOAD]](i64)
    ; GFX7-NEXT: [[MV:%[0-9]+]]:sgpr(i96) = G_MERGE_VALUES [[UV]](i32), [[UV1]](i32), [[LOAD1]](i32)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[MV]](i96)
    ;
    ; GFX12-LABEL: name: load_constant_i96_align4
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(i96) = G_LOAD [[COPY]](p4) :: (invariant load (i96), align 4, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](i96)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i96) = G_LOAD %0(p4) :: (invariant load (i96), align 4, addrspace 4)
    S_ENDPGM 0, implicit %1(i96)
...

---
name: load_constant_i96_align8
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_i96_align8
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(i64) = G_LOAD [[COPY]](p4) :: (invariant load (i64), addrspace 4)
    ; GFX7-NEXT: [[C:%[0-9]+]]:sgpr(i64) = G_CONSTANT i64 8
    ; GFX7-NEXT: [[PTR_ADD:%[0-9]+]]:sgpr(p4) = G_PTR_ADD [[COPY]], [[C]](i64)
    ; GFX7-NEXT: [[LOAD1:%[0-9]+]]:sgpr(i32) = G_LOAD [[PTR_ADD]](p4) :: (invariant load (i32) from unknown-address + 8, align 8, addrspace 4)
    ; GFX7-NEXT: [[UV:%[0-9]+]]:sgpr(i32), [[UV1:%[0-9]+]]:sgpr(i32) = G_UNMERGE_VALUES [[LOAD]](i64)
    ; GFX7-NEXT: [[MV:%[0-9]+]]:sgpr(i96) = G_MERGE_VALUES [[UV]](i32), [[UV1]](i32), [[LOAD1]](i32)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[MV]](i96)
    ;
    ; GFX12-LABEL: name: load_constant_i96_align8
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(i96) = G_LOAD [[COPY]](p4) :: (invariant load (i96), align 8, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](i96)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i96) = G_LOAD %0(p4) :: (invariant load (i96), align 8, addrspace 4)
    S_ENDPGM 0, implicit %1(i96)
...

---
name: load_constant_i96_align16
legalized: true
tracksRegLiveness: true

body: |
  bb.0:
    liveins: $sgpr0_sgpr1
    ; GFX7-LABEL: name: load_constant_i96_align16
    ; GFX7: liveins: $sgpr0_sgpr1
    ; GFX7-NEXT: {{  $}}
    ; GFX7-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX7-NEXT: [[LOAD:%[0-9]+]]:sgpr(i128) = G_LOAD [[COPY]](p4) :: (invariant load (i128), addrspace 4)
    ; GFX7-NEXT: [[TRUNC:%[0-9]+]]:sgpr(i96) = G_TRUNC [[LOAD]](i128)
    ; GFX7-NEXT: S_ENDPGM 0, implicit [[TRUNC]](i96)
    ;
    ; GFX12-LABEL: name: load_constant_i96_align16
    ; GFX12: liveins: $sgpr0_sgpr1
    ; GFX12-NEXT: {{  $}}
    ; GFX12-NEXT: [[COPY:%[0-9]+]]:sgpr(p4) = COPY $sgpr0_sgpr1
    ; GFX12-NEXT: [[LOAD:%[0-9]+]]:sgpr(i96) = G_LOAD [[COPY]](p4) :: (invariant load (i96), align 16, addrspace 4)
    ; GFX12-NEXT: S_ENDPGM 0, implicit [[LOAD]](i96)
    %0:_(p4) = COPY $sgpr0_sgpr1
    %1:_(i96) = G_LOAD %0(p4) :: (invariant load (i96), align 16, addrspace 4)
    S_ENDPGM 0, implicit %1(i96)
...
