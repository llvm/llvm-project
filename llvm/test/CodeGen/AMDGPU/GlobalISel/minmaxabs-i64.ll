; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -global-isel -mtriple=amdgcn-unknown-amdhsa -mcpu=gfx1250 < %s | FileCheck %s -check-prefixes=CHECK,GFX1250
; RUN: llc -global-isel -mtriple=amdgcn-unknown-amdhsa -mcpu=gfx1300 < %s | FileCheck %s -check-prefixes=CHECK,GFX13

declare i64 @llvm.umin.i64(i64, i64)
declare i64 @llvm.umax.i64(i64, i64)
declare i64 @llvm.smin.i64(i64, i64)
declare i64 @llvm.smax.i64(i64, i64)
declare i64 @llvm.abs.i64(i64, i1)

declare <4 x i64> @llvm.umin.v4i64(<4 x i64>, <4 x i64>)
declare <4 x i64> @llvm.umax.v4i64(<4 x i64>, <4 x i64>)
declare <4 x i64> @llvm.smin.v4i64(<4 x i64>, <4 x i64>)
declare <4 x i64> @llvm.smax.v4i64(<4 x i64>, <4 x i64>)

define i64 @test_umin_i64(i64 %a, i64 %b) {
; GFX1250-LABEL: test_umin_i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_min_u64 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_umin_i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_lt_u64_e32 vcc_lo, v[0:1], v[2:3]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v2, v0 :: v_dual_cndmask_b32 v1, v3, v1
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call i64 @llvm.umin.i64(i64 %a, i64 %b)
  ret i64 %r
}

define i64 @test_umax_i64(i64 %a, i64 %b) {
; GFX1250-LABEL: test_umax_i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_max_u64 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_umax_i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_gt_u64_e32 vcc_lo, v[0:1], v[2:3]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v2, v0 :: v_dual_cndmask_b32 v1, v3, v1
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call i64 @llvm.umax.i64(i64 %a, i64 %b)
  ret i64 %r
}

define i64 @test_smin_i64(i64 %a, i64 %b) {
; GFX1250-LABEL: test_smin_i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_min_i64 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_smin_i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_lt_i64_e32 vcc_lo, v[0:1], v[2:3]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v2, v0 :: v_dual_cndmask_b32 v1, v3, v1
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call i64 @llvm.smin.i64(i64 %a, i64 %b)
  ret i64 %r
}

define i64 @test_smax_i64(i64 %a, i64 %b) {
; GFX1250-LABEL: test_smax_i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_max_i64 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_smax_i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_gt_i64_e32 vcc_lo, v[0:1], v[2:3]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v2, v0 :: v_dual_cndmask_b32 v1, v3, v1
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call i64 @llvm.smax.i64(i64 %a, i64 %b)
  ret i64 %r
}

define <4 x i64> @test_umin_v4i64(<4 x i64> %a, <4 x i64> %b) {
; GFX1250-LABEL: test_umin_v4i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_min_u64 v[0:1], v[0:1], v[8:9]
; GFX1250-NEXT:    v_min_u64 v[2:3], v[2:3], v[10:11]
; GFX1250-NEXT:    v_min_u64 v[4:5], v[4:5], v[12:13]
; GFX1250-NEXT:    v_min_u64 v[6:7], v[6:7], v[14:15]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_umin_v4i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_lt_u64_e32 vcc_lo, v[0:1], v[8:9]
; GFX13-NEXT:    v_cmp_lt_u64_e64 s0, v[2:3], v[10:11]
; GFX13-NEXT:    v_cmp_lt_u64_e64 s1, v[4:5], v[12:13]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v8, v0 :: v_dual_cndmask_b32 v1, v9, v1
; GFX13-NEXT:    v_cmp_lt_u64_e32 vcc_lo, v[6:7], v[14:15]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(NEXT) | instid1(VALU_DEP_4)
; GFX13-NEXT:    v_dual_cndmask_b32 v2, v10, v2, s0 :: v_dual_cndmask_b32 v3, v11, v3, s0
; GFX13-NEXT:    v_dual_cndmask_b32 v4, v12, v4, s1 :: v_dual_cndmask_b32 v5, v13, v5, s1
; GFX13-NEXT:    v_dual_cndmask_b32 v6, v14, v6 :: v_dual_cndmask_b32 v7, v15, v7
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call <4 x i64> @llvm.umin.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %r
}

define <4 x i64> @test_umax_v4i64(<4 x i64> %a, <4 x i64> %b) {
; GFX1250-LABEL: test_umax_v4i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_max_u64 v[0:1], v[0:1], v[8:9]
; GFX1250-NEXT:    v_max_u64 v[2:3], v[2:3], v[10:11]
; GFX1250-NEXT:    v_max_u64 v[4:5], v[4:5], v[12:13]
; GFX1250-NEXT:    v_max_u64 v[6:7], v[6:7], v[14:15]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_umax_v4i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_gt_u64_e32 vcc_lo, v[0:1], v[8:9]
; GFX13-NEXT:    v_cmp_gt_u64_e64 s0, v[2:3], v[10:11]
; GFX13-NEXT:    v_cmp_gt_u64_e64 s1, v[4:5], v[12:13]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v8, v0 :: v_dual_cndmask_b32 v1, v9, v1
; GFX13-NEXT:    v_cmp_gt_u64_e32 vcc_lo, v[6:7], v[14:15]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(NEXT) | instid1(VALU_DEP_4)
; GFX13-NEXT:    v_dual_cndmask_b32 v2, v10, v2, s0 :: v_dual_cndmask_b32 v3, v11, v3, s0
; GFX13-NEXT:    v_dual_cndmask_b32 v4, v12, v4, s1 :: v_dual_cndmask_b32 v5, v13, v5, s1
; GFX13-NEXT:    v_dual_cndmask_b32 v6, v14, v6 :: v_dual_cndmask_b32 v7, v15, v7
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call <4 x i64> @llvm.umax.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %r
}

define <4 x i64> @test_smin_v4i64(<4 x i64> %a, <4 x i64> %b) {
; GFX1250-LABEL: test_smin_v4i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_min_i64 v[0:1], v[0:1], v[8:9]
; GFX1250-NEXT:    v_min_i64 v[2:3], v[2:3], v[10:11]
; GFX1250-NEXT:    v_min_i64 v[4:5], v[4:5], v[12:13]
; GFX1250-NEXT:    v_min_i64 v[6:7], v[6:7], v[14:15]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_smin_v4i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_lt_i64_e32 vcc_lo, v[0:1], v[8:9]
; GFX13-NEXT:    v_cmp_lt_i64_e64 s0, v[2:3], v[10:11]
; GFX13-NEXT:    v_cmp_lt_i64_e64 s1, v[4:5], v[12:13]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v8, v0 :: v_dual_cndmask_b32 v1, v9, v1
; GFX13-NEXT:    v_cmp_lt_i64_e32 vcc_lo, v[6:7], v[14:15]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(NEXT) | instid1(VALU_DEP_4)
; GFX13-NEXT:    v_dual_cndmask_b32 v2, v10, v2, s0 :: v_dual_cndmask_b32 v3, v11, v3, s0
; GFX13-NEXT:    v_dual_cndmask_b32 v4, v12, v4, s1 :: v_dual_cndmask_b32 v5, v13, v5, s1
; GFX13-NEXT:    v_dual_cndmask_b32 v6, v14, v6 :: v_dual_cndmask_b32 v7, v15, v7
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call <4 x i64> @llvm.smin.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %r
}

define <4 x i64> @test_smax_v4i64(<4 x i64> %a, <4 x i64> %b) {
; GFX1250-LABEL: test_smax_v4i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_max_i64 v[0:1], v[0:1], v[8:9]
; GFX1250-NEXT:    v_max_i64 v[2:3], v[2:3], v[10:11]
; GFX1250-NEXT:    v_max_i64 v[4:5], v[4:5], v[12:13]
; GFX1250-NEXT:    v_max_i64 v[6:7], v[6:7], v[14:15]
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_smax_v4i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_cmp_gt_i64_e32 vcc_lo, v[0:1], v[8:9]
; GFX13-NEXT:    v_cmp_gt_i64_e64 s0, v[2:3], v[10:11]
; GFX13-NEXT:    v_cmp_gt_i64_e64 s1, v[4:5], v[12:13]
; GFX13-NEXT:    v_dual_cndmask_b32 v0, v8, v0 :: v_dual_cndmask_b32 v1, v9, v1
; GFX13-NEXT:    v_cmp_gt_i64_e32 vcc_lo, v[6:7], v[14:15]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(NEXT) | instid1(VALU_DEP_4)
; GFX13-NEXT:    v_dual_cndmask_b32 v2, v10, v2, s0 :: v_dual_cndmask_b32 v3, v11, v3, s0
; GFX13-NEXT:    v_dual_cndmask_b32 v4, v12, v4, s1 :: v_dual_cndmask_b32 v5, v13, v5, s1
; GFX13-NEXT:    v_dual_cndmask_b32 v6, v14, v6 :: v_dual_cndmask_b32 v7, v15, v7
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call <4 x i64> @llvm.smax.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %r
}

define i64 @test_abs_i64(i64 %a) {
; GFX1250-LABEL: test_abs_i64:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    v_ashrrev_i32_e32 v2, 31, v1
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_1)
; GFX1250-NEXT:    v_mov_b32_e32 v3, v2
; GFX1250-NEXT:    v_add_nc_u64_e32 v[0:1], v[0:1], v[2:3]
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_xor_b32_e32 v0, v0, v2
; GFX1250-NEXT:    v_xor_b32_e32 v1, v1, v2
; GFX1250-NEXT:    s_set_pc_i64 s[30:31]
;
; GFX13-LABEL: test_abs_i64:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    s_wait_loadcnt_dscnt 0x0
; GFX13-NEXT:    s_wait_expcnt 0x0
; GFX13-NEXT:    s_wait_samplecnt 0x0
; GFX13-NEXT:    s_wait_rtscnt 0x0
; GFX13-NEXT:    s_wait_kmcnt 0x0
; GFX13-NEXT:    v_ashrrev_i32_e32 v2, 31, v1
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_1)
; GFX13-NEXT:    v_mov_b32_e32 v3, v2
; GFX13-NEXT:    v_add_nc_u64_e32 v[0:1], v[0:1], v[2:3]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX13-NEXT:    v_xor_b32_e32 v0, v0, v2
; GFX13-NEXT:    v_xor_b32_e32 v1, v1, v2
; GFX13-NEXT:    s_set_pc_i64 s[30:31]
  %r = call i64 @llvm.abs.i64(i64 %a, i1 0)
  ret i64 %r
}

define amdgpu_ps i64 @test_umin_i64_s(i64 inreg %a, i64 inreg %b) {
; GFX1250-LABEL: test_umin_i64_s:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    v_min_u64 v[0:1], s[0:1], s[2:3]
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_readfirstlane_b32 s0, v0
; GFX1250-NEXT:    v_readfirstlane_b32 s1, v1
; GFX1250-NEXT:    ; return to shader part epilog
;
; GFX13-LABEL: test_umin_i64_s:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    v_dual_mov_b32 v0, s2 :: v_dual_mov_b32 v1, s3
; GFX13-NEXT:    v_cmp_lt_u64_e64 s2, s[0:1], s[2:3]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_cndmask_b32_e64 v0, v0, s0, s2
; GFX13-NEXT:    v_cndmask_b32_e64 v1, v1, s1, s2
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_readfirstlane_b32 s1, v1
; GFX13-NEXT:    ; return to shader part epilog
  %r = call i64 @llvm.umin.i64(i64 %a, i64 %b)
  ret i64 %r
}

define amdgpu_ps i64 @test_umax_i64_s(i64 inreg %a, i64 inreg %b) {
; GFX1250-LABEL: test_umax_i64_s:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    v_max_u64 v[0:1], s[0:1], s[2:3]
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_readfirstlane_b32 s0, v0
; GFX1250-NEXT:    v_readfirstlane_b32 s1, v1
; GFX1250-NEXT:    ; return to shader part epilog
;
; GFX13-LABEL: test_umax_i64_s:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    v_dual_mov_b32 v0, s2 :: v_dual_mov_b32 v1, s3
; GFX13-NEXT:    v_cmp_gt_u64_e64 s2, s[0:1], s[2:3]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_cndmask_b32_e64 v0, v0, s0, s2
; GFX13-NEXT:    v_cndmask_b32_e64 v1, v1, s1, s2
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_readfirstlane_b32 s1, v1
; GFX13-NEXT:    ; return to shader part epilog
  %r = call i64 @llvm.umax.i64(i64 %a, i64 %b)
  ret i64 %r
}

define amdgpu_ps i64 @test_smin_i64_s(i64 inreg %a, i64 inreg %b) {
; GFX1250-LABEL: test_smin_i64_s:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    v_min_i64 v[0:1], s[0:1], s[2:3]
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_readfirstlane_b32 s0, v0
; GFX1250-NEXT:    v_readfirstlane_b32 s1, v1
; GFX1250-NEXT:    ; return to shader part epilog
;
; GFX13-LABEL: test_smin_i64_s:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    v_dual_mov_b32 v0, s2 :: v_dual_mov_b32 v1, s3
; GFX13-NEXT:    v_cmp_lt_i64_e64 s2, s[0:1], s[2:3]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_cndmask_b32_e64 v0, v0, s0, s2
; GFX13-NEXT:    v_cndmask_b32_e64 v1, v1, s1, s2
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_readfirstlane_b32 s1, v1
; GFX13-NEXT:    ; return to shader part epilog
  %r = call i64 @llvm.smin.i64(i64 %a, i64 %b)
  ret i64 %r
}

define amdgpu_ps i64 @test_smax_i64_s(i64 inreg %a, i64 inreg %b) {
; GFX1250-LABEL: test_smax_i64_s:
; GFX1250:       ; %bb.0:
; GFX1250-NEXT:    v_max_i64 v[0:1], s[0:1], s[2:3]
; GFX1250-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX1250-NEXT:    v_readfirstlane_b32 s0, v0
; GFX1250-NEXT:    v_readfirstlane_b32 s1, v1
; GFX1250-NEXT:    ; return to shader part epilog
;
; GFX13-LABEL: test_smax_i64_s:
; GFX13:       ; %bb.0:
; GFX13-NEXT:    v_dual_mov_b32 v0, s2 :: v_dual_mov_b32 v1, s3
; GFX13-NEXT:    v_cmp_gt_i64_e64 s2, s[0:1], s[2:3]
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_1) | instskip(NEXT) | instid1(VALU_DEP_3)
; GFX13-NEXT:    v_cndmask_b32_e64 v0, v0, s0, s2
; GFX13-NEXT:    v_cndmask_b32_e64 v1, v1, s1, s2
; GFX13-NEXT:    s_delay_alu instid0(VALU_DEP_2) | instskip(NEXT) | instid1(VALU_DEP_2)
; GFX13-NEXT:    v_readfirstlane_b32 s0, v0
; GFX13-NEXT:    v_readfirstlane_b32 s1, v1
; GFX13-NEXT:    ; return to shader part epilog
  %r = call i64 @llvm.smax.i64(i64 %a, i64 %b)
  ret i64 %r
}

define amdgpu_ps i64 @test_abs_i64_s(i64 inreg %a) {
; CHECK-LABEL: test_abs_i64_s:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_ashr_i32 s2, s1, 31
; CHECK-NEXT:    s_delay_alu instid0(SALU_CYCLE_1) | instskip(NEXT) | instid1(SALU_CYCLE_1)
; CHECK-NEXT:    s_mov_b32 s3, s2
; CHECK-NEXT:    s_add_nc_u64 s[0:1], s[0:1], s[2:3]
; CHECK-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; CHECK-NEXT:    s_xor_b64 s[0:1], s[0:1], s[2:3]
; CHECK-NEXT:    ; return to shader part epilog
  %r = call i64 @llvm.abs.i64(i64 %a, i1 0)
  ret i64 %r
}
