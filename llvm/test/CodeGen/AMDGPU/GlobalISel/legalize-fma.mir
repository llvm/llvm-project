# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=tahiti -run-pass=legalizer %s -o - | FileCheck -check-prefix=SI  %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=fiji -run-pass=legalizer %s -o - | FileCheck -check-prefix=VI %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=gfx900 -run-pass=legalizer %s -o - | FileCheck -check-prefix=GFX9  %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=gfx1010 -run-pass=legalizer %s -o - | FileCheck -check-prefix=GFX9  %s
# RUN: llc -mtriple=amdgcn-mesa-mesa3d -mcpu=gfx1100 -run-pass=legalizer %s -o - | FileCheck -check-prefix=GFX9  %s

---
name: test_fma_s32
body: |
  bb.0:
    liveins: $vgpr0, $vgpr1, $vgpr2

    ; SI-LABEL: name: test_fma_s32
    ; SI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(f32) = G_BITCAST [[COPY]](i32)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(f32) = G_BITCAST [[COPY1]](i32)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(f32) = G_BITCAST [[COPY2]](i32)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[FMA]](f32)
    ; SI-NEXT: $vgpr0 = COPY [[BITCAST3]](i32)
    ;
    ; VI-LABEL: name: test_fma_s32
    ; VI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(f32) = G_BITCAST [[COPY]](i32)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(f32) = G_BITCAST [[COPY1]](i32)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(f32) = G_BITCAST [[COPY2]](i32)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[FMA]](f32)
    ; VI-NEXT: $vgpr0 = COPY [[BITCAST3]](i32)
    ;
    ; GFX9-LABEL: name: test_fma_s32
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr2
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(f32) = G_BITCAST [[COPY]](i32)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(f32) = G_BITCAST [[COPY1]](i32)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(f32) = G_BITCAST [[COPY2]](i32)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(i32) = G_BITCAST [[FMA]](f32)
    ; GFX9-NEXT: $vgpr0 = COPY [[BITCAST3]](i32)
    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr2
    %3:_(f32) = G_BITCAST %0(i32)
    %4:_(f32) = G_BITCAST %1(i32)
    %5:_(f32) = G_BITCAST %2(i32)
    %6:_(f32) = G_FMA %3, %4, %5
    %7:_(i32) = G_BITCAST %6(f32)
    $vgpr0 = COPY %7(i32)
...
---
name: test_fma_s64
body: |
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3,  $vgpr4_vgpr5

    ; SI-LABEL: name: test_fma_s64
    ; SI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(i64) = COPY $vgpr0_vgpr1
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(i64) = COPY $vgpr2_vgpr3
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(i64) = COPY $vgpr4_vgpr5
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(f64) = G_BITCAST [[COPY]](i64)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(f64) = G_BITCAST [[COPY1]](i64)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(f64) = G_BITCAST [[COPY2]](i64)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(i64) = G_BITCAST [[FMA]](f64)
    ; SI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](i64)
    ;
    ; VI-LABEL: name: test_fma_s64
    ; VI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(i64) = COPY $vgpr0_vgpr1
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(i64) = COPY $vgpr2_vgpr3
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(i64) = COPY $vgpr4_vgpr5
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(f64) = G_BITCAST [[COPY]](i64)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(f64) = G_BITCAST [[COPY1]](i64)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(f64) = G_BITCAST [[COPY2]](i64)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(i64) = G_BITCAST [[FMA]](f64)
    ; VI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](i64)
    ;
    ; GFX9-LABEL: name: test_fma_s64
    ; GFX9: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(i64) = COPY $vgpr0_vgpr1
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(i64) = COPY $vgpr2_vgpr3
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(i64) = COPY $vgpr4_vgpr5
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(f64) = G_BITCAST [[COPY]](i64)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(f64) = G_BITCAST [[COPY1]](i64)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(f64) = G_BITCAST [[COPY2]](i64)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(i64) = G_BITCAST [[FMA]](f64)
    ; GFX9-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](i64)
    %0:_(i64) = COPY $vgpr0_vgpr1
    %1:_(i64) = COPY $vgpr2_vgpr3
    %2:_(i64) = COPY $vgpr4_vgpr5
    %3:_(f64) = G_BITCAST %0(i64)
    %4:_(f64) = G_BITCAST %1(i64)
    %5:_(f64) = G_BITCAST %2(i64)
    %6:_(f64) = G_FMA %3, %4, %5
    %7:_(i64) = G_BITCAST %6(f64)
    $vgpr0_vgpr1 = COPY %7(i64)
...

---
name: test_fma_s16
body: |
  bb.0:
    liveins: $vgpr0, $vgpr1, $vgpr2

    ; SI-LABEL: name: test_fma_s16
    ; SI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; SI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[COPY]](i32)
    ; SI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[COPY1]](i32)
    ; SI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[COPY2]](i32)
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; SI-NEXT: [[FPEXT:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST]](f16)
    ; SI-NEXT: [[FPEXT1:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST1]](f16)
    ; SI-NEXT: [[FPEXT2:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST2]](f16)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[FPEXT]], [[FPEXT1]], [[FPEXT2]]
    ; SI-NEXT: [[FPTRUNC:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA]](f32)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC]](f16)
    ; SI-NEXT: [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
    ; SI-NEXT: $vgpr0 = COPY [[ANYEXT]](i32)
    ;
    ; VI-LABEL: name: test_fma_s16
    ; VI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; VI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[COPY]](i32)
    ; VI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[COPY1]](i32)
    ; VI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[COPY2]](i32)
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; VI-NEXT: [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
    ; VI-NEXT: $vgpr0 = COPY [[ANYEXT]](i32)
    ;
    ; GFX9-LABEL: name: test_fma_s16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(i32) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(i32) = COPY $vgpr1
    ; GFX9-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[COPY]](i32)
    ; GFX9-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[COPY1]](i32)
    ; GFX9-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[COPY2]](i32)
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; GFX9-NEXT: [[ANYEXT:%[0-9]+]]:_(i32) = G_ANYEXT [[BITCAST3]](i16)
    ; GFX9-NEXT: $vgpr0 = COPY [[ANYEXT]](i32)
    %0:_(i32) = COPY $vgpr0
    %1:_(i32) = COPY $vgpr1
    %2:_(i32) = COPY $vgpr1
    %3:_(i16) = G_TRUNC %0(i32)
    %4:_(i16) = G_TRUNC %1(i32)
    %5:_(i16) = G_TRUNC %2(i32)
    %6:_(f16) = G_BITCAST %3(i16)
    %7:_(f16) = G_BITCAST %4(i16)
    %8:_(f16) = G_BITCAST %5(i16)
    %9:_(f16) = G_FMA %6, %7, %8
    %10:_(i16) = G_BITCAST %9(f16)
    %11:_(i32) = G_ANYEXT %10(i16)
    $vgpr0 = COPY %11(i32)

...

---
name: test_fma_v2s32
body: |
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5

    ; SI-LABEL: name: test_fma_v2s32
    ; SI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr0_vgpr1
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr2_vgpr3
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr4_vgpr5
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY]](<2 x i32>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY1]](<2 x i32>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY2]](<2 x i32>)
    ; SI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<2 x f32>)
    ; SI-NEXT: [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f32>)
    ; SI-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f32>)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; SI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i32>) = G_BITCAST [[BUILD_VECTOR]](<2 x f32>)
    ; SI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](<2 x i32>)
    ;
    ; VI-LABEL: name: test_fma_v2s32
    ; VI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr0_vgpr1
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr2_vgpr3
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr4_vgpr5
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY]](<2 x i32>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY1]](<2 x i32>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY2]](<2 x i32>)
    ; VI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<2 x f32>)
    ; VI-NEXT: [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f32>)
    ; VI-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f32>)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; VI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i32>) = G_BITCAST [[BUILD_VECTOR]](<2 x f32>)
    ; VI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](<2 x i32>)
    ;
    ; GFX9-LABEL: name: test_fma_v2s32
    ; GFX9: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr0_vgpr1
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr2_vgpr3
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i32>) = COPY $vgpr4_vgpr5
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY]](<2 x i32>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY1]](<2 x i32>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f32>) = G_BITCAST [[COPY2]](<2 x i32>)
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<2 x f32>)
    ; GFX9-NEXT: [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f32>)
    ; GFX9-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f32>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i32>) = G_BITCAST [[BUILD_VECTOR]](<2 x f32>)
    ; GFX9-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](<2 x i32>)
    %0:_(<2 x i32>) = COPY $vgpr0_vgpr1
    %1:_(<2 x i32>) = COPY $vgpr2_vgpr3
    %2:_(<2 x i32>) = COPY $vgpr4_vgpr5
    %3:_(<2 x f32>) = G_BITCAST %0(<2 x i32>)
    %4:_(<2 x f32>) = G_BITCAST %1(<2 x i32>)
    %5:_(<2 x f32>) = G_BITCAST %2(<2 x i32>)
    %6:_(<2 x f32>) = G_FMA %3, %4, %5
    %7:_(<2 x i32>) = G_BITCAST %6(<2 x f32>)
    $vgpr0_vgpr1 = COPY %7(<2 x i32>)
...

---
name: test_fma_v3s32
body: |
  bb.0:
    liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8

    ; SI-LABEL: name: test_fma_v3s32
    ; SI: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr0_vgpr1_vgpr2
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr3_vgpr4_vgpr5
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr6_vgpr7_vgpr8
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY]](<3 x i32>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY1]](<3 x i32>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY2]](<3 x i32>)
    ; SI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<3 x f32>)
    ; SI-NEXT: [[UV3:%[0-9]+]]:_(f32), [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<3 x f32>)
    ; SI-NEXT: [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32), [[UV8:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<3 x f32>)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV3]], [[UV6]]
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV4]], [[UV7]]
    ; SI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV5]], [[UV8]]
    ; SI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(<3 x i32>) = G_BITCAST [[BUILD_VECTOR]](<3 x f32>)
    ; SI-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[BITCAST3]](<3 x i32>)
    ;
    ; VI-LABEL: name: test_fma_v3s32
    ; VI: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr0_vgpr1_vgpr2
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr3_vgpr4_vgpr5
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr6_vgpr7_vgpr8
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY]](<3 x i32>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY1]](<3 x i32>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY2]](<3 x i32>)
    ; VI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<3 x f32>)
    ; VI-NEXT: [[UV3:%[0-9]+]]:_(f32), [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<3 x f32>)
    ; VI-NEXT: [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32), [[UV8:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<3 x f32>)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV3]], [[UV6]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV4]], [[UV7]]
    ; VI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV5]], [[UV8]]
    ; VI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(<3 x i32>) = G_BITCAST [[BUILD_VECTOR]](<3 x f32>)
    ; VI-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[BITCAST3]](<3 x i32>)
    ;
    ; GFX9-LABEL: name: test_fma_v3s32
    ; GFX9: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr0_vgpr1_vgpr2
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr3_vgpr4_vgpr5
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<3 x i32>) = COPY $vgpr6_vgpr7_vgpr8
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY]](<3 x i32>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY1]](<3 x i32>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<3 x f32>) = G_BITCAST [[COPY2]](<3 x i32>)
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<3 x f32>)
    ; GFX9-NEXT: [[UV3:%[0-9]+]]:_(f32), [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<3 x f32>)
    ; GFX9-NEXT: [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32), [[UV8:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<3 x f32>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV3]], [[UV6]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV4]], [[UV7]]
    ; GFX9-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV5]], [[UV8]]
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<3 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<3 x i32>) = G_BITCAST [[BUILD_VECTOR]](<3 x f32>)
    ; GFX9-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[BITCAST3]](<3 x i32>)
    %0:_(<3 x i32>) = COPY $vgpr0_vgpr1_vgpr2
    %1:_(<3 x i32>) = COPY $vgpr3_vgpr4_vgpr5
    %2:_(<3 x i32>) = COPY $vgpr6_vgpr7_vgpr8
    %3:_(<3 x f32>) = G_BITCAST %0(<3 x i32>)
    %4:_(<3 x f32>) = G_BITCAST %1(<3 x i32>)
    %5:_(<3 x f32>) = G_BITCAST %2(<3 x i32>)
    %6:_(<3 x f32>) = G_FMA %3, %4, %5
    %7:_(<3 x i32>) = G_BITCAST %6(<3 x f32>)
    $vgpr0_vgpr1_vgpr2 = COPY %7(<3 x i32>)
...

---
name: test_fma_v4s32
body: |
  bb.0:
    liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11

    ; SI-LABEL: name: test_fma_v4s32
    ; SI: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY]](<4 x i32>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY1]](<4 x i32>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY2]](<4 x i32>)
    ; SI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<4 x f32>)
    ; SI-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32), [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f32>)
    ; SI-NEXT: [[UV8:%[0-9]+]]:_(f32), [[UV9:%[0-9]+]]:_(f32), [[UV10:%[0-9]+]]:_(f32), [[UV11:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f32>)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV4]], [[UV8]]
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV5]], [[UV9]]
    ; SI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV6]], [[UV10]]
    ; SI-NEXT: [[FMA3:%[0-9]+]]:_(f32) = G_FMA [[UV3]], [[UV7]], [[UV11]]
    ; SI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<4 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32), [[FMA3]](f32)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(<4 x i32>) = G_BITCAST [[BUILD_VECTOR]](<4 x f32>)
    ; SI-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<4 x i32>)
    ;
    ; VI-LABEL: name: test_fma_v4s32
    ; VI: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY]](<4 x i32>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY1]](<4 x i32>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY2]](<4 x i32>)
    ; VI-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<4 x f32>)
    ; VI-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32), [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f32>)
    ; VI-NEXT: [[UV8:%[0-9]+]]:_(f32), [[UV9:%[0-9]+]]:_(f32), [[UV10:%[0-9]+]]:_(f32), [[UV11:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f32>)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV4]], [[UV8]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV5]], [[UV9]]
    ; VI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV6]], [[UV10]]
    ; VI-NEXT: [[FMA3:%[0-9]+]]:_(f32) = G_FMA [[UV3]], [[UV7]], [[UV11]]
    ; VI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<4 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32), [[FMA3]](f32)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(<4 x i32>) = G_BITCAST [[BUILD_VECTOR]](<4 x f32>)
    ; VI-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<4 x i32>)
    ;
    ; GFX9-LABEL: name: test_fma_v4s32
    ; GFX9: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i32>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY]](<4 x i32>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY1]](<4 x i32>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f32>) = G_BITCAST [[COPY2]](<4 x i32>)
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(f32), [[UV1:%[0-9]+]]:_(f32), [[UV2:%[0-9]+]]:_(f32), [[UV3:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST]](<4 x f32>)
    ; GFX9-NEXT: [[UV4:%[0-9]+]]:_(f32), [[UV5:%[0-9]+]]:_(f32), [[UV6:%[0-9]+]]:_(f32), [[UV7:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f32>)
    ; GFX9-NEXT: [[UV8:%[0-9]+]]:_(f32), [[UV9:%[0-9]+]]:_(f32), [[UV10:%[0-9]+]]:_(f32), [[UV11:%[0-9]+]]:_(f32) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f32>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[UV]], [[UV4]], [[UV8]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[UV1]], [[UV5]], [[UV9]]
    ; GFX9-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[UV2]], [[UV6]], [[UV10]]
    ; GFX9-NEXT: [[FMA3:%[0-9]+]]:_(f32) = G_FMA [[UV3]], [[UV7]], [[UV11]]
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<4 x f32>) = G_BUILD_VECTOR [[FMA]](f32), [[FMA1]](f32), [[FMA2]](f32), [[FMA3]](f32)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<4 x i32>) = G_BITCAST [[BUILD_VECTOR]](<4 x f32>)
    ; GFX9-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<4 x i32>)
    %0:_(<4 x i32>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    %1:_(<4 x i32>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    %2:_(<4 x i32>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    %3:_(<4 x f32>) = G_BITCAST %0(<4 x i32>)
    %4:_(<4 x f32>) = G_BITCAST %1(<4 x i32>)
    %5:_(<4 x f32>) = G_BITCAST %2(<4 x i32>)
    %6:_(<4 x f32>) = G_FMA %3, %4, %5
    %7:_(<4 x i32>) = G_BITCAST %6(<4 x f32>)
    $vgpr0_vgpr1_vgpr2_vgpr3 = COPY %7(<4 x i32>)
...

---
name: test_fma_v2s64
body: |
  bb.0:
    liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11

    ; SI-LABEL: name: test_fma_v2s64
    ; SI: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY]](<2 x i64>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY1]](<2 x i64>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY2]](<2 x i64>)
    ; SI-NEXT: [[UV:%[0-9]+]]:_(f64), [[UV1:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST]](<2 x f64>)
    ; SI-NEXT: [[UV2:%[0-9]+]]:_(f64), [[UV3:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f64>)
    ; SI-NEXT: [[UV4:%[0-9]+]]:_(f64), [[UV5:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f64>)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f64) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; SI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f64>) = G_BUILD_VECTOR [[FMA]](f64), [[FMA1]](f64)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i64>) = G_BITCAST [[BUILD_VECTOR]](<2 x f64>)
    ; SI-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<2 x i64>)
    ;
    ; VI-LABEL: name: test_fma_v2s64
    ; VI: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY]](<2 x i64>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY1]](<2 x i64>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY2]](<2 x i64>)
    ; VI-NEXT: [[UV:%[0-9]+]]:_(f64), [[UV1:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST]](<2 x f64>)
    ; VI-NEXT: [[UV2:%[0-9]+]]:_(f64), [[UV3:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f64>)
    ; VI-NEXT: [[UV4:%[0-9]+]]:_(f64), [[UV5:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f64>)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f64) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; VI-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f64>) = G_BUILD_VECTOR [[FMA]](f64), [[FMA1]](f64)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i64>) = G_BITCAST [[BUILD_VECTOR]](<2 x f64>)
    ; VI-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<2 x i64>)
    ;
    ; GFX9-LABEL: name: test_fma_v2s64
    ; GFX9: liveins: $vgpr0_vgpr1_vgpr2_vgpr3, $vgpr4_vgpr5_vgpr6_vgpr7, $vgpr8_vgpr9_vgpr10_vgpr11
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i64>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY]](<2 x i64>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY1]](<2 x i64>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f64>) = G_BITCAST [[COPY2]](<2 x i64>)
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(f64), [[UV1:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST]](<2 x f64>)
    ; GFX9-NEXT: [[UV2:%[0-9]+]]:_(f64), [[UV3:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST1]](<2 x f64>)
    ; GFX9-NEXT: [[UV4:%[0-9]+]]:_(f64), [[UV5:%[0-9]+]]:_(f64) = G_UNMERGE_VALUES [[BITCAST2]](<2 x f64>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(f64) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(f64) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f64>) = G_BUILD_VECTOR [[FMA]](f64), [[FMA1]](f64)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i64>) = G_BITCAST [[BUILD_VECTOR]](<2 x f64>)
    ; GFX9-NEXT: $vgpr0_vgpr1_vgpr2_vgpr3 = COPY [[BITCAST3]](<2 x i64>)
    %0:_(<2 x i64>) = COPY $vgpr0_vgpr1_vgpr2_vgpr3
    %1:_(<2 x i64>) = COPY $vgpr4_vgpr5_vgpr6_vgpr7
    %2:_(<2 x i64>) = COPY $vgpr8_vgpr9_vgpr10_vgpr11
    %3:_(<2 x f64>) = G_BITCAST %0(<2 x i64>)
    %4:_(<2 x f64>) = G_BITCAST %1(<2 x i64>)
    %5:_(<2 x f64>) = G_BITCAST %2(<2 x i64>)
    %6:_(<2 x f64>) = G_FMA %3, %4, %5
    %7:_(<2 x i64>) = G_BITCAST %6(<2 x f64>)
    $vgpr0_vgpr1_vgpr2_vgpr3 = COPY %7(<2 x i64>)
...

---
name: test_fma_v2s16
body: |
  bb.0:
    liveins: $vgpr0, $vgpr1, $vgpr2

    ; SI-LABEL: name: test_fma_v2s16
    ; SI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY]](<2 x i16>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY1]](<2 x i16>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY2]](<2 x i16>)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST %24(i16)
    ; SI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST %30(i16)
    ; SI-NEXT: [[BITCAST5:%[0-9]+]]:_(f16) = G_BITCAST %25(i16)
    ; SI-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST %31(i16)
    ; SI-NEXT: [[BITCAST7:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST1]](<2 x f16>)
    ; SI-NEXT: [[BITCAST8:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST7]](<2 x i16>)
    ; SI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST8]](i32)
    ; SI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; SI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST8]], [[C]](i32)
    ; SI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; SI-NEXT: [[BITCAST9:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST]](<2 x f16>)
    ; SI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST9]](<2 x i16>)
    ; SI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; SI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; SI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; SI-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST %35(i16)
    ; SI-NEXT: [[FPEXT:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST3]](f16)
    ; SI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST %36(i16)
    ; SI-NEXT: [[BITCAST13:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST2]](<2 x f16>)
    ; SI-NEXT: [[BITCAST14:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST13]](<2 x i16>)
    ; SI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST14]](i32)
    ; SI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST14]], [[C]](i32)
    ; SI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; SI-NEXT: [[FPEXT1:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST4]](f16)
    ; SI-NEXT: [[FPEXT2:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST11]](f16)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[FPEXT]], [[FPEXT1]], [[FPEXT2]]
    ; SI-NEXT: [[FPTRUNC:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA]](f32)
    ; SI-NEXT: [[FPEXT3:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST5]](f16)
    ; SI-NEXT: [[FPEXT4:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST6]](f16)
    ; SI-NEXT: [[FPEXT5:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST12]](f16)
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[FPEXT3]], [[FPEXT4]], [[FPEXT5]]
    ; SI-NEXT: [[FPTRUNC1:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA1]](f32)
    ; SI-NEXT: [[BITCAST15:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC]](f16)
    ; SI-NEXT: [[BITCAST16:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC1]](f16)
    ; SI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST15]](i16)
    ; SI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST16]](i16)
    ; SI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; SI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; SI-NEXT: [[BITCAST17:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR]](i32)
    ; SI-NEXT: [[BITCAST18:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST17]](<2 x f16>)
    ; SI-NEXT: $vgpr0 = COPY [[BITCAST18]](<2 x i16>)
    ;
    ; VI-LABEL: name: test_fma_v2s16
    ; VI: liveins: $vgpr0, $vgpr1, $vgpr2
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY]](<2 x i16>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY1]](<2 x i16>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY2]](<2 x i16>)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST %16(i16)
    ; VI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST %22(i16)
    ; VI-NEXT: [[BITCAST5:%[0-9]+]]:_(f16) = G_BITCAST %17(i16)
    ; VI-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST %23(i16)
    ; VI-NEXT: [[BITCAST7:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST1]](<2 x f16>)
    ; VI-NEXT: [[BITCAST8:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST7]](<2 x i16>)
    ; VI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST8]](i32)
    ; VI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; VI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST8]], [[C]](i32)
    ; VI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; VI-NEXT: [[BITCAST9:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST]](<2 x f16>)
    ; VI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST9]](<2 x i16>)
    ; VI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; VI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; VI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; VI-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST %27(i16)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST3]], [[BITCAST4]], [[BITCAST11]]
    ; VI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST %28(i16)
    ; VI-NEXT: [[BITCAST13:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST2]](<2 x f16>)
    ; VI-NEXT: [[BITCAST14:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST13]](<2 x i16>)
    ; VI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST14]](i32)
    ; VI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST14]], [[C]](i32)
    ; VI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f16) = G_FMA [[BITCAST5]], [[BITCAST6]], [[BITCAST12]]
    ; VI-NEXT: [[BITCAST15:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; VI-NEXT: [[BITCAST16:%[0-9]+]]:_(i16) = G_BITCAST [[FMA1]](f16)
    ; VI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST15]](i16)
    ; VI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST16]](i16)
    ; VI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; VI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; VI-NEXT: [[BITCAST17:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR]](i32)
    ; VI-NEXT: [[BITCAST18:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[BITCAST17]](<2 x f16>)
    ; VI-NEXT: $vgpr0 = COPY [[BITCAST18]](<2 x i16>)
    ;
    ; GFX9-LABEL: name: test_fma_v2s16
    ; GFX9: liveins: $vgpr0, $vgpr1, $vgpr2
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr0
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr1
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<2 x i16>) = COPY $vgpr2
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY1]](<2 x i16>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[COPY2]](<2 x i16>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(<2 x f16>) = G_FMA [[BITCAST]], [[BITCAST1]], [[BITCAST2]]
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMA]](<2 x f16>)
    ; GFX9-NEXT: $vgpr0 = COPY [[BITCAST3]](<2 x i16>)
    %0:_(<2 x i16>) = COPY $vgpr0
    %1:_(<2 x i16>) = COPY $vgpr1
    %2:_(<2 x i16>) = COPY $vgpr2
    %3:_(<2 x f16>) = G_BITCAST %0(<2 x i16>)
    %4:_(<2 x f16>) = G_BITCAST %1(<2 x i16>)
    %5:_(<2 x f16>) = G_BITCAST %2(<2 x i16>)
    %6:_(<2 x f16>) = G_FMA %3, %4, %5
    %7:_(<2 x i16>) = G_BITCAST %6(<2 x f16>)
    $vgpr0 = COPY %7(<2 x i16>)
...

---
name: test_fma_v3s16
body: |
  bb.0:
    liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8

    ; SI-LABEL: name: test_fma_v3s16
    ; SI: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr0_vgpr1_vgpr2
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr3_vgpr4_vgpr5
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr6_vgpr7_vgpr8
    ; SI-NEXT: [[UV:%[0-9]+]]:_(<2 x i16>), [[UV1:%[0-9]+]]:_(<2 x i16>), [[UV2:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY]](<6 x i16>)
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[UV]](<2 x i16>)
    ; SI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; SI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; SI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; SI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[UV1]](<2 x i16>)
    ; SI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; SI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; SI-NEXT: [[UV3:%[0-9]+]]:_(<2 x i16>), [[UV4:%[0-9]+]]:_(<2 x i16>), [[UV5:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY1]](<6 x i16>)
    ; SI-NEXT: [[BITCAST5:%[0-9]+]]:_(i32) = G_BITCAST [[UV3]](<2 x i16>)
    ; SI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST5]](i32)
    ; SI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST5]], [[C]](i32)
    ; SI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; SI-NEXT: [[BITCAST6:%[0-9]+]]:_(i32) = G_BITCAST [[UV4]](<2 x i16>)
    ; SI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST6]](i32)
    ; SI-NEXT: [[BITCAST7:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC3]](i16)
    ; SI-NEXT: [[BITCAST8:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC4]](i16)
    ; SI-NEXT: [[BITCAST9:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC5]](i16)
    ; SI-NEXT: [[UV6:%[0-9]+]]:_(<2 x i16>), [[UV7:%[0-9]+]]:_(<2 x i16>), [[UV8:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY2]](<6 x i16>)
    ; SI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[UV6]](<2 x i16>)
    ; SI-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; SI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; SI-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; SI-NEXT: [[BITCAST11:%[0-9]+]]:_(i32) = G_BITCAST [[UV7]](<2 x i16>)
    ; SI-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST11]](i32)
    ; SI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC6]](i16)
    ; SI-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC7]](i16)
    ; SI-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC8]](i16)
    ; SI-NEXT: [[FPEXT:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST2]](f16)
    ; SI-NEXT: [[FPEXT1:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST7]](f16)
    ; SI-NEXT: [[FPEXT2:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST12]](f16)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[FPEXT]], [[FPEXT1]], [[FPEXT2]]
    ; SI-NEXT: [[FPTRUNC:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA]](f32)
    ; SI-NEXT: [[FPEXT3:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST3]](f16)
    ; SI-NEXT: [[FPEXT4:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST8]](f16)
    ; SI-NEXT: [[FPEXT5:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST13]](f16)
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[FPEXT3]], [[FPEXT4]], [[FPEXT5]]
    ; SI-NEXT: [[FPTRUNC1:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA1]](f32)
    ; SI-NEXT: [[FPEXT6:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST4]](f16)
    ; SI-NEXT: [[FPEXT7:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST9]](f16)
    ; SI-NEXT: [[FPEXT8:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST14]](f16)
    ; SI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[FPEXT6]], [[FPEXT7]], [[FPEXT8]]
    ; SI-NEXT: [[FPTRUNC2:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA2]](f32)
    ; SI-NEXT: [[DEF:%[0-9]+]]:_(<4 x i16>) = G_IMPLICIT_DEF
    ; SI-NEXT: [[UV9:%[0-9]+]]:_(<2 x i16>), [[UV10:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[DEF]](<4 x i16>)
    ; SI-NEXT: [[BITCAST15:%[0-9]+]]:_(i32) = G_BITCAST [[UV9]](<2 x i16>)
    ; SI-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST15]], [[C]](i32)
    ; SI-NEXT: [[BITCAST16:%[0-9]+]]:_(i32) = G_BITCAST [[UV10]](<2 x i16>)
    ; SI-NEXT: [[BITCAST17:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC]](f16)
    ; SI-NEXT: [[BITCAST18:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC1]](f16)
    ; SI-NEXT: [[BITCAST19:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC2]](f16)
    ; SI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST17]](i16)
    ; SI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST18]](i16)
    ; SI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; SI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; SI-NEXT: [[BITCAST20:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; SI-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST19]](i16)
    ; SI-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 65535
    ; SI-NEXT: [[AND:%[0-9]+]]:_(i32) = G_AND [[BITCAST15]], [[C1]]
    ; SI-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[AND]], [[C]](i32)
    ; SI-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; SI-NEXT: [[BITCAST21:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; SI-NEXT: [[AND1:%[0-9]+]]:_(i32) = G_AND [[BITCAST16]], [[C1]]
    ; SI-NEXT: [[SHL2:%[0-9]+]]:_(i32) = G_SHL [[AND1]], [[C]](i32)
    ; SI-NEXT: [[OR2:%[0-9]+]]:_(i32) = G_OR [[LSHR3]], [[SHL2]]
    ; SI-NEXT: [[BITCAST22:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR2]](i32)
    ; SI-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<6 x i16>) = G_CONCAT_VECTORS [[BITCAST20]](<2 x i16>), [[BITCAST21]](<2 x i16>), [[BITCAST22]](<2 x i16>)
    ; SI-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[CONCAT_VECTORS]](<6 x i16>)
    ;
    ; VI-LABEL: name: test_fma_v3s16
    ; VI: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr0_vgpr1_vgpr2
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr3_vgpr4_vgpr5
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr6_vgpr7_vgpr8
    ; VI-NEXT: [[UV:%[0-9]+]]:_(<2 x i16>), [[UV1:%[0-9]+]]:_(<2 x i16>), [[UV2:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY]](<6 x i16>)
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[UV]](<2 x i16>)
    ; VI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; VI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; VI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; VI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[UV1]](<2 x i16>)
    ; VI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; VI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; VI-NEXT: [[UV3:%[0-9]+]]:_(<2 x i16>), [[UV4:%[0-9]+]]:_(<2 x i16>), [[UV5:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY1]](<6 x i16>)
    ; VI-NEXT: [[BITCAST5:%[0-9]+]]:_(i32) = G_BITCAST [[UV3]](<2 x i16>)
    ; VI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST5]](i32)
    ; VI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST5]], [[C]](i32)
    ; VI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; VI-NEXT: [[BITCAST6:%[0-9]+]]:_(i32) = G_BITCAST [[UV4]](<2 x i16>)
    ; VI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST6]](i32)
    ; VI-NEXT: [[BITCAST7:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC3]](i16)
    ; VI-NEXT: [[BITCAST8:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC4]](i16)
    ; VI-NEXT: [[BITCAST9:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC5]](i16)
    ; VI-NEXT: [[UV6:%[0-9]+]]:_(<2 x i16>), [[UV7:%[0-9]+]]:_(<2 x i16>), [[UV8:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY2]](<6 x i16>)
    ; VI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[UV6]](<2 x i16>)
    ; VI-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; VI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; VI-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; VI-NEXT: [[BITCAST11:%[0-9]+]]:_(i32) = G_BITCAST [[UV7]](<2 x i16>)
    ; VI-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST11]](i32)
    ; VI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC6]](i16)
    ; VI-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC7]](i16)
    ; VI-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC8]](i16)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST2]], [[BITCAST7]], [[BITCAST12]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f16) = G_FMA [[BITCAST3]], [[BITCAST8]], [[BITCAST13]]
    ; VI-NEXT: [[FMA2:%[0-9]+]]:_(f16) = G_FMA [[BITCAST4]], [[BITCAST9]], [[BITCAST14]]
    ; VI-NEXT: [[DEF:%[0-9]+]]:_(<4 x i16>) = G_IMPLICIT_DEF
    ; VI-NEXT: [[UV9:%[0-9]+]]:_(<2 x i16>), [[UV10:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[DEF]](<4 x i16>)
    ; VI-NEXT: [[BITCAST15:%[0-9]+]]:_(i32) = G_BITCAST [[UV9]](<2 x i16>)
    ; VI-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST15]], [[C]](i32)
    ; VI-NEXT: [[BITCAST16:%[0-9]+]]:_(i32) = G_BITCAST [[UV10]](<2 x i16>)
    ; VI-NEXT: [[BITCAST17:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; VI-NEXT: [[BITCAST18:%[0-9]+]]:_(i16) = G_BITCAST [[FMA1]](f16)
    ; VI-NEXT: [[BITCAST19:%[0-9]+]]:_(i16) = G_BITCAST [[FMA2]](f16)
    ; VI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST17]](i16)
    ; VI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST18]](i16)
    ; VI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; VI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; VI-NEXT: [[BITCAST20:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR]](i32)
    ; VI-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST19]](i16)
    ; VI-NEXT: [[C1:%[0-9]+]]:_(i32) = G_CONSTANT i32 65535
    ; VI-NEXT: [[AND:%[0-9]+]]:_(i32) = G_AND [[BITCAST15]], [[C1]]
    ; VI-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[AND]], [[C]](i32)
    ; VI-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; VI-NEXT: [[BITCAST21:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR1]](i32)
    ; VI-NEXT: [[AND1:%[0-9]+]]:_(i32) = G_AND [[BITCAST16]], [[C1]]
    ; VI-NEXT: [[SHL2:%[0-9]+]]:_(i32) = G_SHL [[AND1]], [[C]](i32)
    ; VI-NEXT: [[OR2:%[0-9]+]]:_(i32) = G_OR [[LSHR3]], [[SHL2]]
    ; VI-NEXT: [[BITCAST22:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[OR2]](i32)
    ; VI-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<6 x i16>) = G_CONCAT_VECTORS [[BITCAST20]](<2 x i16>), [[BITCAST21]](<2 x i16>), [[BITCAST22]](<2 x i16>)
    ; VI-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[CONCAT_VECTORS]](<6 x i16>)
    ;
    ; GFX9-LABEL: name: test_fma_v3s16
    ; GFX9: liveins: $vgpr0_vgpr1_vgpr2, $vgpr3_vgpr4_vgpr5, $vgpr6_vgpr7_vgpr8
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr0_vgpr1_vgpr2
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr3_vgpr4_vgpr5
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<6 x i16>) = COPY $vgpr6_vgpr7_vgpr8
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(<2 x i16>), [[UV1:%[0-9]+]]:_(<2 x i16>), [[UV2:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY]](<6 x i16>)
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(i32) = G_BITCAST [[UV]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST]](i32)
    ; GFX9-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; GFX9-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(i32) = G_BITCAST [[UV1]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST1]](i32)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC]](i16)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC1]](i16)
    ; GFX9-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC2]](i16)
    ; GFX9-NEXT: [[UV3:%[0-9]+]]:_(<2 x i16>), [[UV4:%[0-9]+]]:_(<2 x i16>), [[UV5:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY1]](<6 x i16>)
    ; GFX9-NEXT: [[BITCAST5:%[0-9]+]]:_(i32) = G_BITCAST [[UV3]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST5]](i32)
    ; GFX9-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST5]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; GFX9-NEXT: [[BITCAST6:%[0-9]+]]:_(i32) = G_BITCAST [[UV4]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST6]](i32)
    ; GFX9-NEXT: [[BITCAST7:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC3]](i16)
    ; GFX9-NEXT: [[BITCAST8:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC4]](i16)
    ; GFX9-NEXT: [[BITCAST9:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC5]](i16)
    ; GFX9-NEXT: [[UV6:%[0-9]+]]:_(<2 x i16>), [[UV7:%[0-9]+]]:_(<2 x i16>), [[UV8:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[COPY2]](<6 x i16>)
    ; GFX9-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[UV6]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; GFX9-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; GFX9-NEXT: [[BITCAST11:%[0-9]+]]:_(i32) = G_BITCAST [[UV7]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST11]](i32)
    ; GFX9-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC6]](i16)
    ; GFX9-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC7]](i16)
    ; GFX9-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST [[TRUNC8]](i16)
    ; GFX9-NEXT: [[DEF:%[0-9]+]]:_(f16) = G_IMPLICIT_DEF
    ; GFX9-NEXT: [[BUILD_VECTOR:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST2]](f16), [[BITCAST3]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR1:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST4]](f16), [[DEF]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR2:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST7]](f16), [[BITCAST8]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR3:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST9]](f16), [[DEF]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR4:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST12]](f16), [[BITCAST13]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR5:%[0-9]+]]:_(<2 x f16>) = G_BUILD_VECTOR [[BITCAST14]](f16), [[DEF]](f16)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(<2 x f16>) = G_FMA [[BUILD_VECTOR]], [[BUILD_VECTOR2]], [[BUILD_VECTOR4]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(<2 x f16>) = G_FMA [[BUILD_VECTOR1]], [[BUILD_VECTOR3]], [[BUILD_VECTOR5]]
    ; GFX9-NEXT: [[BITCAST15:%[0-9]+]]:_(f16) = G_BITCAST %106(i16)
    ; GFX9-NEXT: [[BITCAST16:%[0-9]+]]:_(f16) = G_BITCAST %112(i16)
    ; GFX9-NEXT: [[BITCAST17:%[0-9]+]]:_(f16) = G_BITCAST %107(i16)
    ; GFX9-NEXT: [[BITCAST18:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMA1]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST19:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST18]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST19]](i32)
    ; GFX9-NEXT: [[BITCAST20:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[FMA]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST21:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST20]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST21]](i32)
    ; GFX9-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST21]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; GFX9-NEXT: [[DEF1:%[0-9]+]]:_(<4 x i16>) = G_IMPLICIT_DEF
    ; GFX9-NEXT: [[UV9:%[0-9]+]]:_(<2 x i16>), [[UV10:%[0-9]+]]:_(<2 x i16>) = G_UNMERGE_VALUES [[DEF1]](<4 x i16>)
    ; GFX9-NEXT: [[BITCAST22:%[0-9]+]]:_(i32) = G_BITCAST [[UV9]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC12:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST22]](i32)
    ; GFX9-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST22]], [[C]](i32)
    ; GFX9-NEXT: [[TRUNC13:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; GFX9-NEXT: [[BITCAST23:%[0-9]+]]:_(i32) = G_BITCAST [[UV10]](<2 x i16>)
    ; GFX9-NEXT: [[TRUNC14:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST23]](i32)
    ; GFX9-NEXT: [[BITCAST24:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST15]](f16)
    ; GFX9-NEXT: [[BITCAST25:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST17]](f16)
    ; GFX9-NEXT: [[BITCAST26:%[0-9]+]]:_(i16) = G_BITCAST [[BITCAST16]](f16)
    ; GFX9-NEXT: [[BUILD_VECTOR6:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[BITCAST24]](i16), [[BITCAST25]](i16)
    ; GFX9-NEXT: [[BUILD_VECTOR7:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[BITCAST26]](i16), [[TRUNC12]](i16)
    ; GFX9-NEXT: [[BUILD_VECTOR8:%[0-9]+]]:_(<2 x i16>) = G_BUILD_VECTOR [[TRUNC13]](i16), [[TRUNC14]](i16)
    ; GFX9-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<6 x i16>) = G_CONCAT_VECTORS [[BUILD_VECTOR6]](<2 x i16>), [[BUILD_VECTOR7]](<2 x i16>), [[BUILD_VECTOR8]](<2 x i16>)
    ; GFX9-NEXT: $vgpr0_vgpr1_vgpr2 = COPY [[CONCAT_VECTORS]](<6 x i16>)
    %0:_(<6 x i16>) = COPY $vgpr0_vgpr1_vgpr2
    %1:_(<6 x i16>) = COPY $vgpr3_vgpr4_vgpr5
    %2:_(<6 x i16>) = COPY $vgpr6_vgpr7_vgpr8
    %3:_(<3 x i16>), %4:_(<3 x i16>) = G_UNMERGE_VALUES %0(<6 x i16>)
    %5:_(<3 x i16>), %6:_(<3 x i16>) = G_UNMERGE_VALUES %1(<6 x i16>)
    %7:_(<3 x i16>), %8:_(<3 x i16>) = G_UNMERGE_VALUES %2(<6 x i16>)
    %9:_(<3 x f16>) = G_BITCAST %3(<3 x i16>)
    %10:_(<3 x f16>) = G_BITCAST %5(<3 x i16>)
    %11:_(<3 x f16>) = G_BITCAST %7(<3 x i16>)
    %12:_(<3 x f16>) = G_FMA %9, %10, %11
    %13:_(<3 x i16>) = G_IMPLICIT_DEF
    %14:_(<3 x i16>) = G_BITCAST %12(<3 x f16>)
    %15:_(<6 x i16>) = G_CONCAT_VECTORS %14(<3 x i16>), %13(<3 x i16>)
    $vgpr0_vgpr1_vgpr2 = COPY %15(<6 x i16>)

...

---
name: test_fma_v4s16
body: |
  bb.0:
    liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5

    ; SI-LABEL: name: test_fma_v4s16
    ; SI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; SI-NEXT: {{  $}}
    ; SI-NEXT: [[COPY:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr0_vgpr1
    ; SI-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr2_vgpr3
    ; SI-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr4_vgpr5
    ; SI-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY]](<4 x i16>)
    ; SI-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY1]](<4 x i16>)
    ; SI-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY2]](<4 x i16>)
    ; SI-NEXT: [[UV:%[0-9]+]]:_(<2 x f16>), [[UV1:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST]](<4 x f16>)
    ; SI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST %48(i16)
    ; SI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST %54(i16)
    ; SI-NEXT: [[BITCAST5:%[0-9]+]]:_(f16) = G_BITCAST %49(i16)
    ; SI-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST %55(i16)
    ; SI-NEXT: [[BITCAST7:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV1]](<2 x f16>)
    ; SI-NEXT: [[BITCAST8:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST7]](<2 x i16>)
    ; SI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST8]](i32)
    ; SI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; SI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST8]], [[C]](i32)
    ; SI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; SI-NEXT: [[BITCAST9:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV]](<2 x f16>)
    ; SI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST9]](<2 x i16>)
    ; SI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; SI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; SI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; SI-NEXT: [[UV2:%[0-9]+]]:_(<2 x f16>), [[UV3:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f16>)
    ; SI-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST %59(i16)
    ; SI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST %64(i16)
    ; SI-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST %60(i16)
    ; SI-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST %65(i16)
    ; SI-NEXT: [[BITCAST15:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV3]](<2 x f16>)
    ; SI-NEXT: [[BITCAST16:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST15]](<2 x i16>)
    ; SI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST16]](i32)
    ; SI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST16]], [[C]](i32)
    ; SI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; SI-NEXT: [[BITCAST17:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV2]](<2 x f16>)
    ; SI-NEXT: [[BITCAST18:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST17]](<2 x i16>)
    ; SI-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST18]](i32)
    ; SI-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST18]], [[C]](i32)
    ; SI-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; SI-NEXT: [[UV4:%[0-9]+]]:_(<2 x f16>), [[UV5:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f16>)
    ; SI-NEXT: [[BITCAST19:%[0-9]+]]:_(f16) = G_BITCAST %69(i16)
    ; SI-NEXT: [[BITCAST20:%[0-9]+]]:_(f16) = G_BITCAST %74(i16)
    ; SI-NEXT: [[BITCAST21:%[0-9]+]]:_(f16) = G_BITCAST %70(i16)
    ; SI-NEXT: [[BITCAST22:%[0-9]+]]:_(f16) = G_BITCAST %75(i16)
    ; SI-NEXT: [[BITCAST23:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV5]](<2 x f16>)
    ; SI-NEXT: [[BITCAST24:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST23]](<2 x i16>)
    ; SI-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST24]](i32)
    ; SI-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST24]], [[C]](i32)
    ; SI-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; SI-NEXT: [[BITCAST25:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV4]](<2 x f16>)
    ; SI-NEXT: [[BITCAST26:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST25]](<2 x i16>)
    ; SI-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST26]](i32)
    ; SI-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST26]], [[C]](i32)
    ; SI-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR5]](i32)
    ; SI-NEXT: [[FPEXT:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST3]](f16)
    ; SI-NEXT: [[FPEXT1:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST11]](f16)
    ; SI-NEXT: [[FPEXT2:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST19]](f16)
    ; SI-NEXT: [[FMA:%[0-9]+]]:_(f32) = G_FMA [[FPEXT]], [[FPEXT1]], [[FPEXT2]]
    ; SI-NEXT: [[FPTRUNC:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA]](f32)
    ; SI-NEXT: [[FPEXT3:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST5]](f16)
    ; SI-NEXT: [[FPEXT4:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST13]](f16)
    ; SI-NEXT: [[FPEXT5:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST21]](f16)
    ; SI-NEXT: [[FMA1:%[0-9]+]]:_(f32) = G_FMA [[FPEXT3]], [[FPEXT4]], [[FPEXT5]]
    ; SI-NEXT: [[FPTRUNC1:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA1]](f32)
    ; SI-NEXT: [[FPEXT6:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST4]](f16)
    ; SI-NEXT: [[FPEXT7:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST12]](f16)
    ; SI-NEXT: [[FPEXT8:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST20]](f16)
    ; SI-NEXT: [[FMA2:%[0-9]+]]:_(f32) = G_FMA [[FPEXT6]], [[FPEXT7]], [[FPEXT8]]
    ; SI-NEXT: [[FPTRUNC2:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA2]](f32)
    ; SI-NEXT: [[FPEXT9:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST6]](f16)
    ; SI-NEXT: [[FPEXT10:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST14]](f16)
    ; SI-NEXT: [[FPEXT11:%[0-9]+]]:_(f32) = G_FPEXT [[BITCAST22]](f16)
    ; SI-NEXT: [[FMA3:%[0-9]+]]:_(f32) = G_FMA [[FPEXT9]], [[FPEXT10]], [[FPEXT11]]
    ; SI-NEXT: [[FPTRUNC3:%[0-9]+]]:_(f16) = G_FPTRUNC [[FMA3]](f32)
    ; SI-NEXT: [[BITCAST27:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC]](f16)
    ; SI-NEXT: [[BITCAST28:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC1]](f16)
    ; SI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST27]](i16)
    ; SI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST28]](i16)
    ; SI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; SI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; SI-NEXT: [[BITCAST29:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR]](i32)
    ; SI-NEXT: [[BITCAST30:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC2]](f16)
    ; SI-NEXT: [[BITCAST31:%[0-9]+]]:_(i16) = G_BITCAST [[FPTRUNC3]](f16)
    ; SI-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST30]](i16)
    ; SI-NEXT: [[ZEXT3:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST31]](i16)
    ; SI-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[ZEXT3]], [[C]](i32)
    ; SI-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; SI-NEXT: [[BITCAST32:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR1]](i32)
    ; SI-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<4 x f16>) = G_CONCAT_VECTORS [[BITCAST29]](<2 x f16>), [[BITCAST32]](<2 x f16>)
    ; SI-NEXT: [[BITCAST33:%[0-9]+]]:_(<4 x i16>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f16>)
    ; SI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST33]](<4 x i16>)
    ;
    ; VI-LABEL: name: test_fma_v4s16
    ; VI: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; VI-NEXT: {{  $}}
    ; VI-NEXT: [[COPY:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr0_vgpr1
    ; VI-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr2_vgpr3
    ; VI-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr4_vgpr5
    ; VI-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY]](<4 x i16>)
    ; VI-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY1]](<4 x i16>)
    ; VI-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY2]](<4 x i16>)
    ; VI-NEXT: [[UV:%[0-9]+]]:_(<2 x f16>), [[UV1:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST]](<4 x f16>)
    ; VI-NEXT: [[BITCAST3:%[0-9]+]]:_(f16) = G_BITCAST %32(i16)
    ; VI-NEXT: [[BITCAST4:%[0-9]+]]:_(f16) = G_BITCAST %38(i16)
    ; VI-NEXT: [[BITCAST5:%[0-9]+]]:_(f16) = G_BITCAST %33(i16)
    ; VI-NEXT: [[BITCAST6:%[0-9]+]]:_(f16) = G_BITCAST %39(i16)
    ; VI-NEXT: [[BITCAST7:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV1]](<2 x f16>)
    ; VI-NEXT: [[BITCAST8:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST7]](<2 x i16>)
    ; VI-NEXT: [[TRUNC:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST8]](i32)
    ; VI-NEXT: [[C:%[0-9]+]]:_(i32) = G_CONSTANT i32 16
    ; VI-NEXT: [[LSHR:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST8]], [[C]](i32)
    ; VI-NEXT: [[TRUNC1:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR]](i32)
    ; VI-NEXT: [[BITCAST9:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV]](<2 x f16>)
    ; VI-NEXT: [[BITCAST10:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST9]](<2 x i16>)
    ; VI-NEXT: [[TRUNC2:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST10]](i32)
    ; VI-NEXT: [[LSHR1:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST10]], [[C]](i32)
    ; VI-NEXT: [[TRUNC3:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR1]](i32)
    ; VI-NEXT: [[UV2:%[0-9]+]]:_(<2 x f16>), [[UV3:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f16>)
    ; VI-NEXT: [[BITCAST11:%[0-9]+]]:_(f16) = G_BITCAST %43(i16)
    ; VI-NEXT: [[BITCAST12:%[0-9]+]]:_(f16) = G_BITCAST %48(i16)
    ; VI-NEXT: [[BITCAST13:%[0-9]+]]:_(f16) = G_BITCAST %44(i16)
    ; VI-NEXT: [[BITCAST14:%[0-9]+]]:_(f16) = G_BITCAST %49(i16)
    ; VI-NEXT: [[BITCAST15:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV3]](<2 x f16>)
    ; VI-NEXT: [[BITCAST16:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST15]](<2 x i16>)
    ; VI-NEXT: [[TRUNC4:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST16]](i32)
    ; VI-NEXT: [[LSHR2:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST16]], [[C]](i32)
    ; VI-NEXT: [[TRUNC5:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR2]](i32)
    ; VI-NEXT: [[BITCAST17:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV2]](<2 x f16>)
    ; VI-NEXT: [[BITCAST18:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST17]](<2 x i16>)
    ; VI-NEXT: [[TRUNC6:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST18]](i32)
    ; VI-NEXT: [[LSHR3:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST18]], [[C]](i32)
    ; VI-NEXT: [[TRUNC7:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR3]](i32)
    ; VI-NEXT: [[UV4:%[0-9]+]]:_(<2 x f16>), [[UV5:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f16>)
    ; VI-NEXT: [[BITCAST19:%[0-9]+]]:_(f16) = G_BITCAST %53(i16)
    ; VI-NEXT: [[BITCAST20:%[0-9]+]]:_(f16) = G_BITCAST %58(i16)
    ; VI-NEXT: [[BITCAST21:%[0-9]+]]:_(f16) = G_BITCAST %54(i16)
    ; VI-NEXT: [[BITCAST22:%[0-9]+]]:_(f16) = G_BITCAST %59(i16)
    ; VI-NEXT: [[BITCAST23:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV5]](<2 x f16>)
    ; VI-NEXT: [[BITCAST24:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST23]](<2 x i16>)
    ; VI-NEXT: [[TRUNC8:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST24]](i32)
    ; VI-NEXT: [[LSHR4:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST24]], [[C]](i32)
    ; VI-NEXT: [[TRUNC9:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR4]](i32)
    ; VI-NEXT: [[BITCAST25:%[0-9]+]]:_(<2 x i16>) = G_BITCAST [[UV4]](<2 x f16>)
    ; VI-NEXT: [[BITCAST26:%[0-9]+]]:_(i32) = G_BITCAST [[BITCAST25]](<2 x i16>)
    ; VI-NEXT: [[TRUNC10:%[0-9]+]]:_(i16) = G_TRUNC [[BITCAST26]](i32)
    ; VI-NEXT: [[LSHR5:%[0-9]+]]:_(i32) = G_LSHR [[BITCAST26]], [[C]](i32)
    ; VI-NEXT: [[TRUNC11:%[0-9]+]]:_(i16) = G_TRUNC [[LSHR5]](i32)
    ; VI-NEXT: [[FMA:%[0-9]+]]:_(f16) = G_FMA [[BITCAST3]], [[BITCAST11]], [[BITCAST19]]
    ; VI-NEXT: [[FMA1:%[0-9]+]]:_(f16) = G_FMA [[BITCAST5]], [[BITCAST13]], [[BITCAST21]]
    ; VI-NEXT: [[FMA2:%[0-9]+]]:_(f16) = G_FMA [[BITCAST4]], [[BITCAST12]], [[BITCAST20]]
    ; VI-NEXT: [[FMA3:%[0-9]+]]:_(f16) = G_FMA [[BITCAST6]], [[BITCAST14]], [[BITCAST22]]
    ; VI-NEXT: [[BITCAST27:%[0-9]+]]:_(i16) = G_BITCAST [[FMA]](f16)
    ; VI-NEXT: [[BITCAST28:%[0-9]+]]:_(i16) = G_BITCAST [[FMA1]](f16)
    ; VI-NEXT: [[ZEXT:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST27]](i16)
    ; VI-NEXT: [[ZEXT1:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST28]](i16)
    ; VI-NEXT: [[SHL:%[0-9]+]]:_(i32) = G_SHL [[ZEXT1]], [[C]](i32)
    ; VI-NEXT: [[OR:%[0-9]+]]:_(i32) = G_OR [[ZEXT]], [[SHL]]
    ; VI-NEXT: [[BITCAST29:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR]](i32)
    ; VI-NEXT: [[BITCAST30:%[0-9]+]]:_(i16) = G_BITCAST [[FMA2]](f16)
    ; VI-NEXT: [[BITCAST31:%[0-9]+]]:_(i16) = G_BITCAST [[FMA3]](f16)
    ; VI-NEXT: [[ZEXT2:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST30]](i16)
    ; VI-NEXT: [[ZEXT3:%[0-9]+]]:_(i32) = G_ZEXT [[BITCAST31]](i16)
    ; VI-NEXT: [[SHL1:%[0-9]+]]:_(i32) = G_SHL [[ZEXT3]], [[C]](i32)
    ; VI-NEXT: [[OR1:%[0-9]+]]:_(i32) = G_OR [[ZEXT2]], [[SHL1]]
    ; VI-NEXT: [[BITCAST32:%[0-9]+]]:_(<2 x f16>) = G_BITCAST [[OR1]](i32)
    ; VI-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<4 x f16>) = G_CONCAT_VECTORS [[BITCAST29]](<2 x f16>), [[BITCAST32]](<2 x f16>)
    ; VI-NEXT: [[BITCAST33:%[0-9]+]]:_(<4 x i16>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f16>)
    ; VI-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST33]](<4 x i16>)
    ;
    ; GFX9-LABEL: name: test_fma_v4s16
    ; GFX9: liveins: $vgpr0_vgpr1, $vgpr2_vgpr3, $vgpr4_vgpr5
    ; GFX9-NEXT: {{  $}}
    ; GFX9-NEXT: [[COPY:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr0_vgpr1
    ; GFX9-NEXT: [[COPY1:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr2_vgpr3
    ; GFX9-NEXT: [[COPY2:%[0-9]+]]:_(<4 x i16>) = COPY $vgpr4_vgpr5
    ; GFX9-NEXT: [[BITCAST:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY]](<4 x i16>)
    ; GFX9-NEXT: [[BITCAST1:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY1]](<4 x i16>)
    ; GFX9-NEXT: [[BITCAST2:%[0-9]+]]:_(<4 x f16>) = G_BITCAST [[COPY2]](<4 x i16>)
    ; GFX9-NEXT: [[UV:%[0-9]+]]:_(<2 x f16>), [[UV1:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST]](<4 x f16>)
    ; GFX9-NEXT: [[UV2:%[0-9]+]]:_(<2 x f16>), [[UV3:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST1]](<4 x f16>)
    ; GFX9-NEXT: [[UV4:%[0-9]+]]:_(<2 x f16>), [[UV5:%[0-9]+]]:_(<2 x f16>) = G_UNMERGE_VALUES [[BITCAST2]](<4 x f16>)
    ; GFX9-NEXT: [[FMA:%[0-9]+]]:_(<2 x f16>) = G_FMA [[UV]], [[UV2]], [[UV4]]
    ; GFX9-NEXT: [[FMA1:%[0-9]+]]:_(<2 x f16>) = G_FMA [[UV1]], [[UV3]], [[UV5]]
    ; GFX9-NEXT: [[CONCAT_VECTORS:%[0-9]+]]:_(<4 x f16>) = G_CONCAT_VECTORS [[FMA]](<2 x f16>), [[FMA1]](<2 x f16>)
    ; GFX9-NEXT: [[BITCAST3:%[0-9]+]]:_(<4 x i16>) = G_BITCAST [[CONCAT_VECTORS]](<4 x f16>)
    ; GFX9-NEXT: $vgpr0_vgpr1 = COPY [[BITCAST3]](<4 x i16>)
    %0:_(<4 x i16>) = COPY $vgpr0_vgpr1
    %1:_(<4 x i16>) = COPY $vgpr2_vgpr3
    %2:_(<4 x i16>) = COPY $vgpr4_vgpr5
    %3:_(<4 x f16>) = G_BITCAST %0(<4 x i16>)
    %4:_(<4 x f16>) = G_BITCAST %1(<4 x i16>)
    %5:_(<4 x f16>) = G_BITCAST %2(<4 x i16>)
    %6:_(<4 x f16>) = G_FMA %3, %4, %5
    %7:_(<4 x i16>) = G_BITCAST %6(<4 x f16>)
    $vgpr0_vgpr1 = COPY %7(<4 x i16>)
...
