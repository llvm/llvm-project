# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx900 -run-pass=amdgpu-rebuild-ssa,amdgpu-ssa-spiller %s -verify-machineinstrs -o - | FileCheck %s

--- |
  source_filename = "test0.ll"
  target datalayout = "e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9"

  define amdgpu_kernel void @test0(ptr addrspace(1) %arg) #0 {
  S:
    %test0.kernarg.segment = call nonnull align 16 dereferenceable(264) ptr addrspace(4) @llvm.amdgcn.kernarg.segment.ptr()
    %arg.kernarg.offset = getelementptr inbounds i8, ptr addrspace(4) %test0.kernarg.segment, i64 36, !amdgpu.uniform !0
    %arg.load = load ptr addrspace(1), ptr addrspace(4) %arg.kernarg.offset, align 4, !invariant.load !0
    %tmp = tail call i32 @llvm.amdgcn.workitem.id.x()
    %idxX = mul i32 %tmp, 16
    %idxY = mul i32 %tmp, 19
    %idxprom = sext i32 %idxX to i64
    %gepX = getelementptr inbounds i32, ptr addrspace(1) %arg.load, i64 %idxprom
    %idxprom1 = sext i32 %idxY to i64
    %gepY = getelementptr inbounds i32, ptr addrspace(1) %arg.load, i64 %idxprom1
    %X = load i32, ptr addrspace(1) %gepX, align 4
    %Y = load i32, ptr addrspace(1) %gepY, align 4
    %gepC = getelementptr inbounds i32, ptr addrspace(1) %arg.load, i32 128, !amdgpu.uniform !0
    %C = load i32, ptr addrspace(1) %gepC, align 4, !amdgpu.noclobber !0
    %cmp0 = icmp sge i32 %C, %tmp
    %0 = call { i1, i64 } @llvm.amdgcn.if.i64(i1 %cmp0)
    %1 = extractvalue { i1, i64 } %0, 0
    %2 = extractvalue { i1, i64 } %0, 1
    br i1 %1, label %B, label %Flow9

  L:                                                ; preds = %Flow9
    %idxRX = add i32 %X, %tmp
    %gepD = getelementptr i32, ptr addrspace(1) %arg.load, i32 17, !amdgpu.uniform !0
    %3 = load <2 x i32>, ptr addrspace(1) %gepD, align 4, !amdgpu.noclobber !0
    %D5 = extractelement <2 x i32> %3, i32 0
    %F6 = extractelement <2 x i32> %3, i32 1
    %idxRY = sub i32 %X, %idxRX
    br label %E, !amdgpu.uniform !0

  B:                                                ; preds = %S
    %gepLI = getelementptr i32, ptr addrspace(1) %arg.load, i32 1, !amdgpu.uniform !0
    %LI0 = load i32, ptr addrspace(1) %gepLI, align 4, !amdgpu.noclobber !0
    %gepD1 = getelementptr i32, ptr addrspace(1) %arg.load, i32 27, !amdgpu.uniform !0
    %4 = load <2 x i32>, ptr addrspace(1) %gepD1, align 4, !amdgpu.noclobber !0
    %D17 = extractelement <2 x i32> %4, i32 0
    %F18 = extractelement <2 x i32> %4, i32 1
    %gepLB = getelementptr i32, ptr addrspace(1) %arg.load, i32 256, !amdgpu.uniform !0
    %LB = load i32, ptr addrspace(1) %gepLB, align 4, !amdgpu.noclobber !0
    br label %H, !amdgpu.uniform !0

  Flow9:                                            ; preds = %Flow, %S
    %5 = phi i32 [ %idxY, %Flow ], [ undef, %S ]
    %6 = phi i32 [ %idxX, %Flow ], [ undef, %S ]
    %7 = phi i32 [ %F3.lcssa, %Flow ], [ undef, %S ]
    %8 = phi i32 [ %D17, %Flow ], [ undef, %S ]
    %9 = call { i1, i64 } @llvm.amdgcn.else.i64.i64(i64 %2)
    %10 = extractvalue { i1, i64 } %9, 0
    %11 = extractvalue { i1, i64 } %9, 1
    br i1 %10, label %L, label %E

  H:                                                ; preds = %B, %H
    %I = phi i32 [ %II, %H ], [ %LI0, %B ]
    %F2 = phi i32 [ %F3, %H ], [ %F18, %B ]
    %idxL = shl i32 %X, %I
    %idxprom2 = sext i32 %idxL to i64
    %gepL = getelementptr i32, ptr addrspace(1) %arg.load, i64 %idxprom2
    %lval = load i32, ptr addrspace(1) %gepL, align 4
    %F3 = add nsw i32 %lval, %F2
    %II = add nsw i32 %I, 1
    %cmpl = icmp sgt i32 %II, %LB
    br i1 %cmpl, label %Flow, label %H, !amdgpu.uniform !0

  Flow:                                             ; preds = %H
    %F3.lcssa = phi i32 [ %F3, %H ]
    br label %Flow9, !amdgpu.uniform !0

  E:                                                ; preds = %L, %Flow9
    %D2 = phi i32 [ %8, %Flow9 ], [ %D5, %L ]
    %F4 = phi i32 [ %7, %Flow9 ], [ %F6, %L ]
    %idxRX1 = phi i32 [ %6, %Flow9 ], [ %idxRX, %L ]
    %idxRY1 = phi i32 [ %5, %Flow9 ], [ %idxRY, %L ]
    call void @llvm.amdgcn.end.cf.i64(i64 %11)
    %idxprom3 = sext i32 %idxRX1 to i64
    %gepRX = getelementptr i32, ptr addrspace(1) %arg.load, i64 %idxprom3
    %idxprom4 = sext i32 %idxRY1 to i64
    %gepRY = getelementptr i32, ptr addrspace(1) %arg.load, i64 %idxprom4
    %resX = mul i32 %X, %D2
    %resY = mul i32 %Y, %F4
    store i32 %resX, ptr addrspace(1) %gepRX, align 4
    store i32 %resY, ptr addrspace(1) %gepRY, align 4
    ret void
  }

  declare noundef i32 @llvm.amdgcn.workitem.id.x() #1

  declare noundef align 4 ptr addrspace(4) @llvm.amdgcn.kernarg.segment.ptr() #2

  declare { i1, i64 } @llvm.amdgcn.if.i64(i1) #3

  declare { i1, i64 } @llvm.amdgcn.else.i64.i64(i64) #3

  declare i64 @llvm.amdgcn.if.break.i64(i1, i64) #4

  declare i1 @llvm.amdgcn.loop.i64(i64) #3

  declare void @llvm.amdgcn.end.cf.i64(i64) #3

  attributes #0 = { nounwind "amdgpu-num-vgpr"="8" "amdgpu-wave-limiter"="true" "target-cpu"="gfx900" }
  attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) "target-cpu"="gfx900" }
  attributes #2 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
  attributes #3 = { nocallback nofree nounwind willreturn }
  attributes #4 = { nocallback nofree nounwind willreturn memory(none) }

  !0 = !{}

...
---
name:            test0
alignment:       1
exposesReturnsTwice: false
legalized:       false
regBankSelected: false
selected:        false
failedISel:      false
tracksRegLiveness: true
hasWinCFI:       false
callsEHReturn:   false
callsUnwindInit: false
hasEHCatchret:   false
hasEHScopes:     false
hasEHFunclets:   false
isOutlined:      false
debugInstrRef:   false
failsVerification: false
tracksDebugUserValues: false
registers:
  - { id: 0, class: sreg_64_xexec, preferred-register: '' }
  - { id: 1, class: vgpr_32, preferred-register: '' }
  - { id: 2, class: vgpr_32, preferred-register: '' }
  - { id: 3, class: vgpr_32, preferred-register: '' }
  - { id: 4, class: vgpr_32, preferred-register: '' }
  - { id: 5, class: vgpr_32, preferred-register: '' }
  - { id: 6, class: sreg_64, preferred-register: '' }
  - { id: 7, class: vgpr_32, preferred-register: '' }
  - { id: 8, class: sreg_32, preferred-register: '' }
  - { id: 9, class: sreg_32, preferred-register: '' }
  - { id: 10, class: vgpr_32, preferred-register: '' }
  - { id: 11, class: sreg_32_xm0_xexec, preferred-register: '' }
  - { id: 12, class: sreg_32, preferred-register: '' }
  - { id: 13, class: sreg_32, preferred-register: '' }
  - { id: 14, class: sreg_32, preferred-register: '' }
  - { id: 15, class: vgpr_32, preferred-register: '' }
  - { id: 16, class: vgpr_32, preferred-register: '' }
  - { id: 17, class: vgpr_32, preferred-register: '' }
  - { id: 18, class: sreg_32, preferred-register: '' }
  - { id: 19, class: sreg_64, preferred-register: '' }
  - { id: 20, class: sreg_32, preferred-register: '%23' }
  - { id: 21, class: vgpr_32, preferred-register: '' }
  - { id: 22, class: vgpr_32, preferred-register: '' }
  - { id: 23, class: sreg_32, preferred-register: '%20' }
  - { id: 24, class: vgpr_32, preferred-register: '' }
  - { id: 25, class: vgpr_32, preferred-register: '' }
  - { id: 26, class: vgpr_32, preferred-register: '' }
  - { id: 27, class: vgpr_32, preferred-register: '' }
  - { id: 28, class: vgpr_32, preferred-register: '' }
  - { id: 29, class: vgpr_32, preferred-register: '' }
  - { id: 30, class: vgpr_32, preferred-register: '' }
  - { id: 31, class: vgpr_32, preferred-register: '' }
  - { id: 32, class: sgpr_64, preferred-register: '' }
  - { id: 33, class: sgpr_64, preferred-register: '' }
  - { id: 34, class: sgpr_64, preferred-register: '' }
  - { id: 35, class: sgpr_32, preferred-register: '' }
  - { id: 36, class: sgpr_32, preferred-register: '' }
  - { id: 37, class: sgpr_32, preferred-register: '' }
  - { id: 38, class: sgpr_32, preferred-register: '' }
  - { id: 39, class: sreg_32, preferred-register: '' }
  - { id: 40, class: sreg_64_xexec, preferred-register: '' }
  - { id: 41, class: sreg_32, preferred-register: '' }
  - { id: 42, class: sreg_32, preferred-register: '' }
  - { id: 43, class: sreg_32, preferred-register: '' }
  - { id: 44, class: vgpr_32, preferred-register: '' }
  - { id: 45, class: sreg_32, preferred-register: '' }
  - { id: 46, class: vgpr_32, preferred-register: '' }
  - { id: 47, class: sreg_32_xm0_xexec, preferred-register: '' }
  - { id: 48, class: sreg_64, preferred-register: '$vcc' }
  - { id: 49, class: sreg_32_xm0_xexec, preferred-register: '' }
  - { id: 50, class: sreg_64_xexec, preferred-register: '' }
  - { id: 51, class: sreg_32_xm0_xexec, preferred-register: '' }
  - { id: 52, class: vgpr_32, preferred-register: '' }
  - { id: 53, class: vgpr_32, preferred-register: '' }
  - { id: 54, class: vgpr_32, preferred-register: '' }
  - { id: 55, class: vreg_64, preferred-register: '' }
  - { id: 56, class: sreg_32, preferred-register: '' }
  - { id: 57, class: vreg_64, preferred-register: '' }
  - { id: 58, class: vreg_64, preferred-register: '' }
  - { id: 59, class: vgpr_32, preferred-register: '' }
  - { id: 60, class: sreg_32, preferred-register: '' }
  - { id: 61, class: sreg_64_xexec, preferred-register: '' }
  - { id: 62, class: vgpr_32, preferred-register: '' }
  - { id: 63, class: vgpr_32, preferred-register: '' }
  - { id: 64, class: vreg_64, preferred-register: '' }
  - { id: 65, class: sreg_32, preferred-register: '' }
  - { id: 66, class: vreg_64, preferred-register: '' }
  - { id: 67, class: vreg_64, preferred-register: '' }
  - { id: 68, class: vgpr_32, preferred-register: '' }
  - { id: 69, class: vgpr_32, preferred-register: '' }
  - { id: 70, class: vreg_64, preferred-register: '' }
  - { id: 71, class: vreg_64, preferred-register: '' }
  - { id: 72, class: vreg_64, preferred-register: '' }
  - { id: 73, class: vgpr_32, preferred-register: '' }
  - { id: 74, class: vgpr_32, preferred-register: '' }
  - { id: 75, class: vgpr_32, preferred-register: '' }
  - { id: 76, class: vgpr_32, preferred-register: '' }
  - { id: 77, class: vgpr_32, preferred-register: '' }
  - { id: 78, class: vgpr_32, preferred-register: '' }
  - { id: 79, class: vgpr_32, preferred-register: '' }
  - { id: 80, class: vgpr_32, preferred-register: '' }
  - { id: 81, class: vgpr_32, preferred-register: '' }
  - { id: 82, class: vgpr_32, preferred-register: '' }
  - { id: 83, class: vgpr_32, preferred-register: '' }
  - { id: 84, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 85, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 86, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 87, class: vgpr_32, preferred-register: '' }
  - { id: 88, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 89, class: vgpr_32, preferred-register: '' }
  - { id: 90, class: vgpr_32, preferred-register: '' }
  - { id: 91, class: vgpr_32, preferred-register: '' }
  - { id: 92, class: vgpr_32, preferred-register: '' }
  - { id: 93, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 94, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 95, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 96, class: vgpr_32, preferred-register: '' }
  - { id: 97, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 98, class: vgpr_32, preferred-register: '' }
  - { id: 99, class: vgpr_32, preferred-register: '' }
  - { id: 100, class: vgpr_32, preferred-register: '' }
  - { id: 101, class: vgpr_32, preferred-register: '' }
  - { id: 102, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 103, class: sreg_64_xexec, preferred-register: '$vcc' }
  - { id: 104, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 105, class: vgpr_32, preferred-register: '' }
  - { id: 106, class: sreg_32_xexec_hi_and_sreg_32_xm0, preferred-register: '' }
  - { id: 107, class: vgpr_32, preferred-register: '' }
  - { id: 108, class: vgpr_32, preferred-register: '' }
  - { id: 109, class: vgpr_32, preferred-register: '' }
  - { id: 110, class: vgpr_32, preferred-register: '' }
  - { id: 111, class: vgpr_32, preferred-register: '' }
  - { id: 112, class: vgpr_32, preferred-register: '' }
  - { id: 113, class: vgpr_32, preferred-register: '' }
  - { id: 114, class: sreg_32, preferred-register: '' }
  - { id: 115, class: vgpr_32, preferred-register: '' }
  - { id: 116, class: sreg_32_xm0_xexec, preferred-register: '' }
  - { id: 117, class: vgpr_32, preferred-register: '' }
  - { id: 118, class: vgpr_32, preferred-register: '' }
  - { id: 119, class: vgpr_32, preferred-register: '' }
  - { id: 120, class: vgpr_32, preferred-register: '' }
  - { id: 121, class: vgpr_32, preferred-register: '' }
  - { id: 122, class: sreg_64, preferred-register: '' }
  - { id: 123, class: sreg_64, preferred-register: '' }
  - { id: 124, class: sreg_64, preferred-register: '' }
liveins:
  - { reg: '$vgpr0', virtual-reg: '%29' }
  - { reg: '$sgpr2_sgpr3', virtual-reg: '%33' }
frameInfo:
  isFrameAddressTaken: false
  isReturnAddressTaken: false
  hasStackMap:     false
  hasPatchPoint:   false
  stackSize:       0
  offsetAdjustment: 0
  maxAlignment:    1
  adjustsStack:    false
  hasCalls:        false
  stackProtector:  ''
  functionContext: ''
  maxCallFrameSize: 4294967295
  cvBytesOfCalleeSavedRegisters: 0
  hasOpaqueSPAdjustment: false
  hasVAStart:      false
  hasMustTailInVarArgFunc: false
  hasTailCall:     false
  isCalleeSavedInfoValid: false
  localFrameSize:  0
  savePoint:       ''
  restorePoint:    ''
fixedStack:      []
stack:           []
entry_values:    []
callSites:       []
debugValueSubstitutions: []
constants:       []
machineFunctionInfo:
  explicitKernArgSize: 8
  maxKernArgAlign: 8
  ldsSize:         0
  gdsSize:         0
  dynLDSAlign:     1
  isEntryFunction: true
  isChainFunction: false
  noSignedZerosFPMath: false
  memoryBound:     false
  waveLimiter:     true
  hasSpilledSGPRs: false
  hasSpilledVGPRs: false
  scratchRSrcReg:  '$sgpr96_sgpr97_sgpr98_sgpr99'
  frameOffsetReg:  '$fp_reg'
  stackPtrOffsetReg: '$sgpr32'
  bytesInStackArgArea: 0
  returnsVoid:     true
  argumentInfo:
    dispatchPtr:     { reg: '$sgpr0_sgpr1' }
    kernargSegmentPtr: { reg: '$sgpr2_sgpr3' }
    dispatchID:      { reg: '$sgpr4_sgpr5' }
    workGroupIDX:    { reg: '$sgpr6' }
    workGroupIDY:    { reg: '$sgpr7' }
    workGroupIDZ:    { reg: '$sgpr8' }
    privateSegmentWaveByteOffset: { reg: '$sgpr9' }
    workItemIDX:     { reg: '$vgpr0' }
    workItemIDY:     { reg: '$vgpr1' }
    workItemIDZ:     { reg: '$vgpr2' }
  psInputAddr:     0
  psInputEnable:   0
  mode:
    ieee:            true
    dx10-clamp:      true
    fp32-input-denormals: true
    fp32-output-denormals: true
    fp64-fp16-input-denormals: true
    fp64-fp16-output-denormals: true
  highBitsOf32BitAddress: 0
  occupancy:       8
  vgprForAGPRCopy: ''
  sgprForEXECCopy: '$sgpr100_sgpr101'
  longBranchReservedReg: ''
  hasInitWholeWave: false
body:             |
  ; CHECK-LABEL: name: test0
  ; CHECK: bb.0.S:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.3(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $sgpr2_sgpr3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:sgpr_64(p4) = COPY $sgpr2_sgpr3
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   early-clobber %0:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec [[COPY]](p4), 36, 0 :: (dereferenceable invariant load (s64) from %ir.arg.kernarg.offset, align 4, addrspace 4)
  ; CHECK-NEXT:   undef [[V_MUL_U32_U24_e32_:%[0-9]+]].sub0:vreg_64 = V_MUL_U32_U24_e32 19, [[COPY1]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e32_:%[0-9]+]]:vgpr_32 = nuw nsw V_LSHLREV_B32_e32 2, [[V_MUL_U32_U24_e32_]].sub0, implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR %0, [[V_LSHLREV_B32_e32_]], 0, 0, implicit $exec :: (load (s32) from %ir.gepY, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e32 6, [[COPY1]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD_SADDR1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR %0, [[V_LSHLREV_B32_e32_1]], 0, 0, implicit $exec :: (load (s32) from %ir.gepX, addrspace 1)
  ; CHECK-NEXT:   [[S_LOAD_DWORD_IMM:%[0-9]+]]:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 512, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepC, addrspace 1)
  ; CHECK-NEXT:   [[V_CMP_GE_I32_e64_:%[0-9]+]]:sreg_64 = V_CMP_GE_I32_e64 [[S_LOAD_DWORD_IMM]], [[COPY1]], implicit $exec
  ; CHECK-NEXT:   undef [[DEF:%[0-9]+]].sub0:sreg_64_xexec = IMPLICIT_DEF
  ; CHECK-NEXT:   [[DEF1:%[0-9]+]]:vgpr_32 = IMPLICIT_DEF
  ; CHECK-NEXT:   undef [[DEF2:%[0-9]+]].sub0:vreg_64 = IMPLICIT_DEF
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:sreg_64 = COPY $exec, implicit-def $exec
  ; CHECK-NEXT:   [[S_AND_B64_:%[0-9]+]]:sreg_64 = S_AND_B64 [[COPY2]], [[V_CMP_GE_I32_e64_]], implicit-def dead $scc
  ; CHECK-NEXT:   [[S_XOR_B64_:%[0-9]+]]:sreg_64 = S_XOR_B64 [[S_AND_B64_]], [[COPY2]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_MUL_U32_U24_e32_]].sub0, %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   $exec = S_MOV_B64_term [[S_AND_B64_]]
  ; CHECK-NEXT:   S_CBRANCH_EXECZ %bb.3, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.L:
  ; CHECK-NEXT:   successors: %bb.6(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   early-clobber %61:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec %0, 68, 0 :: ("amdgpu-noclobber" load (s64) from %ir.gepD, align 4, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e32_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 [[GLOBAL_LOAD_DWORD_SADDR1]], %125, implicit $exec
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY %61.sub0, implicit $exec
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY %61.sub1, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e32_:%[0-9]+]]:vgpr_32 = V_SUB_U32_e32 [[GLOBAL_LOAD_DWORD_SADDR1]], %141, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.6
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.B:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[S_LOAD_DWORD_IMM1:%[0-9]+]]:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 4, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepLI, addrspace 1)
  ; CHECK-NEXT:   early-clobber %132:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec %0, 108, 0 :: ("amdgpu-noclobber" load (s64) from %ir.gepD1, align 4, addrspace 1)
  ; CHECK-NEXT:   [[S_LOAD_DWORD_IMM2:%[0-9]+]]:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 1024, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepLB, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHLREV_B32_e32_2:%[0-9]+]]:vgpr_32 = V_LSHLREV_B32_e32 4, [[COPY1]], implicit $exec
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY %132.sub1, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.Flow9:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.6(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:vgpr_32 = PHI [[DEF2]].sub0, %bb.0, [[V_LSHLREV_B32_e32_2]], %bb.5
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[DEF1]], %bb.0, %137, %bb.5
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[DEF]].sub0, %bb.0, %132.sub0, %bb.5
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[COPY1]], %bb.0, %126, %bb.5
  ; CHECK-NEXT:   [[S_OR_SAVEEXEC_B64_:%[0-9]+]]:sreg_64 = S_OR_SAVEEXEC_B64 [[S_XOR_B64_]], implicit-def $exec, implicit-def $scc, implicit $exec
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY [[PHI2]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   $exec = S_XOR_B64_term $exec, [[S_OR_SAVEEXEC_B64_]], implicit-def $scc
  ; CHECK-NEXT:   S_CBRANCH_EXECZ %bb.6, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.H:
  ; CHECK-NEXT:   successors: %bb.5(0x04000000), %bb.4(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:sreg_32_xm0_xexec = PHI [[S_LOAD_DWORD_IMM1]], %bb.2, %148, %bb.4
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI [[COPY5]], %bb.2, %137, %bb.4
  ; CHECK-NEXT:   undef [[V_LSHLREV_B32_e32_3:%[0-9]+]].sub0:vreg_64 = V_LSHLREV_B32_e32 [[PHI4]], [[GLOBAL_LOAD_DWORD_SADDR1]], implicit $exec
  ; CHECK-NEXT:   [[V_ASHRREV_I32_e32_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e32 31, [[V_LSHLREV_B32_e32_3]].sub0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHLREV_B32_e32_3]].sub0, %subreg.sub0, [[V_ASHRREV_I32_e32_]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_e64_:%[0-9]+]]:vreg_64 = V_LSHLREV_B64_e64 2, [[REG_SEQUENCE]], implicit $exec
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY %0.sub1
  ; CHECK-NEXT:   undef [[V_ADD_CO_U32_e64_:%[0-9]+]].sub0:vreg_64, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, [[V_LSHLREV_B64_e64_]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_MUL_U32_U24_e32_]].sub0, %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_ADDC_U32_e64_:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_1:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY7]], [[V_LSHLREV_B64_e64_]].sub1, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]].sub0, %subreg.sub0, [[V_ADDC_U32_e64_]], %subreg.sub1
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (load (s32) from %ir.gepL, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e32_1:%[0-9]+]]:vgpr_32 = nsw V_ADD_U32_e32 [[GLOBAL_LOAD_DWORD]], [[PHI5]], implicit $exec
  ; CHECK-NEXT:   [[S_ADD_I32_:%[0-9]+]]:sreg_32_xm0_xexec = nsw S_ADD_I32 [[PHI4]], 1, implicit-def dead $scc
  ; CHECK-NEXT:   S_CMP_LE_I32 [[PHI4]], [[S_LOAD_DWORD_IMM2]], implicit-def $scc
  ; CHECK-NEXT:   S_CBRANCH_SCC1 %bb.4, implicit $scc
  ; CHECK-NEXT:   S_BRANCH %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.Flow:
  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DEF3:%[0-9]+]]:vgpr_32 = IMPLICIT_DEF
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.E:
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI [[COPY3]], %bb.1, [[COPY6]], %bb.3
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e32_]], %bb.1, [[PHI]], %bb.3
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI [[COPY4]], %bb.1, [[PHI1]], %bb.3
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:vgpr_32 = PHI [[V_SUB_U32_e32_]], %bb.1, [[SI_SPILL_V32_RESTORE]], %bb.3
  ; CHECK-NEXT:   $exec = S_OR_B64 $exec, [[S_OR_SAVEEXEC_B64_]], implicit-def $scc
  ; CHECK-NEXT:   [[V_ASHRREV_I32_e32_1:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e32 31, [[PHI7]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI7]], %subreg.sub0, [[V_ASHRREV_I32_e32_1]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_e64_1:%[0-9]+]]:vreg_64 = V_LSHLREV_B64_e64 2, [[REG_SEQUENCE2]], implicit $exec
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY %0.sub1
  ; CHECK-NEXT:   undef [[V_ADD_CO_U32_e64_2:%[0-9]+]].sub0:vreg_64, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, [[V_LSHLREV_B64_e64_1]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE killed [[PHI8]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[V_ADDC_U32_e64_2:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_3:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY8]], [[V_LSHLREV_B64_e64_1]].sub1, [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ASHRREV_I32_e32_2:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e32 31, [[PHI9]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI9]], %subreg.sub0, [[V_ASHRREV_I32_e32_2]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_e64_2:%[0-9]+]]:vreg_64 = V_LSHLREV_B64_e64 2, [[REG_SEQUENCE3]], implicit $exec
  ; CHECK-NEXT:   undef [[V_ADD_CO_U32_e64_4:%[0-9]+]].sub0:vreg_64, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, [[V_LSHLREV_B64_e64_2]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE killed [[GLOBAL_LOAD_DWORD_SADDR]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[V_ADDC_U32_e64_4:%[0-9]+]]:vgpr_32, dead [[V_ADDC_U32_e64_5:%[0-9]+]]:sreg_64_xexec = V_ADDC_U32_e64 [[COPY8]], [[V_LSHLREV_B64_e64_2]].sub1, [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[GLOBAL_LOAD_DWORD_SADDR1]], [[PHI6]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]].sub0, %subreg.sub0, [[V_ADDC_U32_e64_2]], %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE4]], [[V_MUL_LO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.gepRX, addrspace 1)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[SI_SPILL_V32_RESTORE1]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]].sub0, %subreg.sub0, [[V_ADDC_U32_e64_4]], %subreg.sub1
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE5]], [[V_MUL_LO_U32_e64_1]], 0, 0, implicit $exec :: (store (s32) into %ir.gepRY, addrspace 1)
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0.S:
    successors: %bb.2(0x40000000), %bb.3(0x40000000)
    liveins: $vgpr0, $sgpr2_sgpr3

    %33:sgpr_64(p4) = COPY $sgpr2_sgpr3
    %115:vgpr_32 = COPY $vgpr0
    early-clobber %0:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec %33(p4), 36, 0 :: (dereferenceable invariant load (s64) from %ir.arg.kernarg.offset, align 4, addrspace 4)
    undef %70.sub0:vreg_64 = V_MUL_U32_U24_e32 19, %115, implicit $exec
    %46:vgpr_32 = nuw nsw V_LSHLREV_B32_e32 2, %70.sub0, implicit $exec
    %5:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR %0, %46, 0, 0, implicit $exec :: (load (s32) from %ir.gepY, addrspace 1)
    %44:vgpr_32 = V_LSHLREV_B32_e32 6, %115, implicit $exec
    %4:vgpr_32 = GLOBAL_LOAD_DWORD_SADDR %0, %44, 0, 0, implicit $exec :: (load (s32) from %ir.gepX, addrspace 1)
    %47:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 512, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepC, addrspace 1)
    %48:sreg_64 = V_CMP_GE_I32_e64 %47, %115, implicit $exec
    undef %50.sub0:sreg_64_xexec = IMPLICIT_DEF
    %113:vgpr_32 = IMPLICIT_DEF
    undef %64.sub0:vreg_64 = IMPLICIT_DEF
    %122:sreg_64 = COPY $exec, implicit-def $exec
    %123:sreg_64 = S_AND_B64 %122, %48, implicit-def dead $scc
    %6:sreg_64 = S_XOR_B64 %123, %122, implicit-def dead $scc
    $exec = S_MOV_B64_term %123
    S_CBRANCH_EXECZ %bb.3, implicit $exec
    S_BRANCH %bb.2

  bb.1.L:
    successors: %bb.6(0x80000000)

    early-clobber %61:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec %0, 68, 0 :: ("amdgpu-noclobber" load (s64) from %ir.gepD, align 4, addrspace 1)
    undef %64.sub0:vreg_64 = V_ADD_U32_e32 %4, %115, implicit $exec
    %118:vgpr_32 = COPY %61.sub0, implicit $exec
    %113:vgpr_32 = COPY %61.sub1, implicit $exec
    undef %70.sub0:vreg_64 = V_SUB_U32_e32 %4, %64.sub0, implicit $exec
    S_BRANCH %bb.6

  bb.2.B:
    successors: %bb.4(0x80000000)

    %116:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 4, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepLI, addrspace 1)
    early-clobber %50:sreg_64_xexec = S_LOAD_DWORDX2_IMM_ec %0, 108, 0 :: ("amdgpu-noclobber" load (s64) from %ir.gepD1, align 4, addrspace 1)
    %51:sreg_32_xm0_xexec = S_LOAD_DWORD_IMM %0, 1024, 0 :: ("amdgpu-noclobber" load (s32) from %ir.gepLB, addrspace 1)
    undef %64.sub0:vreg_64 = V_LSHLREV_B32_e32 4, %115, implicit $exec
    %113:vgpr_32 = COPY %50.sub1, implicit $exec
    S_BRANCH %bb.4

  bb.3.Flow9:
    successors: %bb.1(0x40000000), %bb.6(0x40000000)

    %19:sreg_64 = S_OR_SAVEEXEC_B64 %6, implicit-def $exec, implicit-def $scc, implicit $exec
    %118:vgpr_32 = COPY %50.sub0, implicit $exec
    $exec = S_XOR_B64_term $exec, %19, implicit-def $scc
    S_CBRANCH_EXECZ %bb.6, implicit $exec
    S_BRANCH %bb.1

  bb.4.H:
    successors: %bb.5(0x04000000), %bb.4(0x7c000000)

    undef %55.sub0:vreg_64 = V_LSHLREV_B32_e32 %116, %4, implicit $exec
    %55.sub1:vreg_64 = V_ASHRREV_I32_e32 31, %55.sub0, implicit $exec
    %57:vreg_64 = V_LSHLREV_B64_e64 2, %55, implicit $exec
    %90:vgpr_32 = COPY %0.sub1
    undef %58.sub0:vreg_64, %84:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, %57.sub0, 0, implicit $exec
    %58.sub1:vreg_64, dead %85:sreg_64_xexec = V_ADDC_U32_e64 %90, %57.sub1, %84, 0, implicit $exec
    %59:vgpr_32 = GLOBAL_LOAD_DWORD %58, 0, 0, implicit $exec :: (load (s32) from %ir.gepL, addrspace 1)
    %113:vgpr_32 = nsw V_ADD_U32_e32 %59, %113, implicit $exec
    %116:sreg_32_xm0_xexec = nsw S_ADD_I32 %116, 1, implicit-def dead $scc
    S_CMP_LE_I32 %116, %51, implicit-def $scc
    S_CBRANCH_SCC1 %bb.4, implicit $scc
    S_BRANCH %bb.5

  bb.5.Flow:
    successors: %bb.3(0x80000000)

    %115:vgpr_32 = IMPLICIT_DEF
    S_BRANCH %bb.3

  bb.6.E:
    $exec = S_OR_B64 $exec, %19, implicit-def $scc
    %64.sub1:vreg_64 = V_ASHRREV_I32_e32 31, %64.sub0, implicit $exec
    %66:vreg_64 = V_LSHLREV_B64_e64 2, %64, implicit $exec
    %99:vgpr_32 = COPY %0.sub1
    undef %67.sub0:vreg_64, %93:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, %66.sub0, 0, implicit $exec
    %67.sub1:vreg_64, dead %94:sreg_64_xexec = V_ADDC_U32_e64 %99, %66.sub1, %93, 0, implicit $exec
    %70.sub1:vreg_64 = V_ASHRREV_I32_e32 31, %70.sub0, implicit $exec
    %71:vreg_64 = V_LSHLREV_B64_e64 2, %70, implicit $exec
    undef %72.sub0:vreg_64, %102:sreg_64_xexec = V_ADD_CO_U32_e64 %0.sub0, %71.sub0, 0, implicit $exec
    %72.sub1:vreg_64, dead %103:sreg_64_xexec = V_ADDC_U32_e64 %99, %71.sub1, %102, 0, implicit $exec
    %73:vgpr_32 = V_MUL_LO_U32_e64 %4, %118, implicit $exec
    GLOBAL_STORE_DWORD %67, %73, 0, 0, implicit $exec :: (store (s32) into %ir.gepRX, addrspace 1)
    %74:vgpr_32 = V_MUL_LO_U32_e64 %5, %113, implicit $exec
    GLOBAL_STORE_DWORD %72, %74, 0, 0, implicit $exec :: (store (s32) into %ir.gepRY, addrspace 1)
    S_ENDPGM 0

...
