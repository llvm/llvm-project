# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 5
# RUN: llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx900 -run-pass=amdgpu-ssa-spiller %s -verify-machineinstrs -o - | FileCheck %s

--- |
  target triple = "amdgcn-amd-amdhsa"
  target datalayout = "e-p:64:64"

  define amdgpu_kernel void @test_full_register_spill() #0 {
  entry:
    ret void
  }

  attributes #0 = {
    nounwind "amdgpu-num-vgpr"="8" "target-cpu"="gfx900"
  }

...
---
name:            test_full_register_spill
tracksRegLiveness: true
fixedStack:      []
stack:           []
entry_values:    []
callSites:       []
debugValueSubstitutions: []
constants:       []

machineFunctionInfo:
  explicitKernArgSize: 8
  maxKernArgAlign: 8
  ldsSize:         0
  gdsSize:         0
  dynLDSAlign:     1
  scratchRSrcReg:    '$sgpr96_sgpr97_sgpr98_sgpr99'
  frameOffsetReg:    '$fp_reg'
  stackPtrOffsetReg: '$sgpr32'
  bytesInStackArgArea: 0
  returnsVoid:         true
  argumentInfo:
    kernargSegmentPtr: { reg: '$sgpr2_sgpr3' }
body:             |
  ; CHECK-LABEL: name: test_full_register_spill
  ; CHECK: bb.0:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $sgpr0_sgpr1, $sgpr2_sgpr3, $sgpr4_sgpr5_sgpr6_sgpr7
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   %idx:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   dead %soffset:sgpr_64 = COPY $sgpr2_sgpr3
  ; CHECK-NEXT:   %rsrc:sgpr_128 = COPY $sgpr4_sgpr5_sgpr6_sgpr7
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 1, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_2:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 2, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_3:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 3, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_4:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 4, implicit $exec
  ; CHECK-NEXT:   dead [[V_MOV_B32_e32_5:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 5, implicit $exec
  ; CHECK-NEXT:   dead [[V_MOV_B32_e32_6:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 6, implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_7:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 7, implicit $exec
  ; CHECK-NEXT:   dead %offset:vgpr_32 = V_LSHLREV_B32_e32 4, %idx, implicit $exec
  ; CHECK-NEXT:   [[BUFFER_LOAD_DWORDX4_OFFSET:%[0-9]+]]:vreg_128 = BUFFER_LOAD_DWORDX4_OFFSET %rsrc, 0, 64, 0, 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE killed [[V_MOV_B32_e32_7]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.0, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE killed [[V_MOV_B32_e32_4]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE killed [[V_MOV_B32_e32_2]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   %cmp:sreg_64 = V_CMP_NE_U32_e64 %idx, [[V_MOV_B32_e32_3]], implicit $exec
  ; CHECK-NEXT:   %exec_mask:sreg_64 = S_AND_B64 $exec, %cmp, implicit-def $exec, implicit-def $scc
  ; CHECK-NEXT:   $exec = COPY %exec_mask
  ; CHECK-NEXT:   S_CBRANCH_EXECZ %bb.2, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1:
  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   dead [[V_ADD_U32_e32_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 [[V_MOV_B32_e32_]], [[V_MOV_B32_e32_1]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   dead [[V_ADD_U32_e32_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 [[SI_SPILL_V32_RESTORE]], [[SI_SPILL_V32_RESTORE1]], implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2:
  ; CHECK-NEXT:   successors: %bb.3(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY [[BUFFER_LOAD_DWORDX4_OFFSET]].sub1, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.0, addrspace 5)
  ; CHECK-NEXT:   dead [[V_ADD_U32_e32_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e32 [[COPY]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3:
  ; CHECK-NEXT:   $exec = S_OR_B64 $exec, %cmp, implicit-def $scc
  ; CHECK-NEXT:   S_ENDPGM 0
  bb.0:
    liveins: $vgpr0, $sgpr0_sgpr1, $sgpr2_sgpr3, $sgpr4_sgpr5_sgpr6_sgpr7

    %idx:vgpr_32 = COPY $vgpr0
    %soffset:sgpr_64 = COPY $sgpr2_sgpr3
    %rsrc:sgpr_128 = COPY $sgpr4_sgpr5_sgpr6_sgpr7

    %0:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
    %1:vgpr_32 = V_MOV_B32_e32 1, implicit $exec
    %2:vgpr_32 = V_MOV_B32_e32 2, implicit $exec
    %3:vgpr_32 = V_MOV_B32_e32 3, implicit $exec
    %4:vgpr_32 = V_MOV_B32_e32 4, implicit $exec
    %5:vgpr_32 = V_MOV_B32_e32 5, implicit $exec
    %6:vgpr_32 = V_MOV_B32_e32 6, implicit $exec
    %7:vgpr_32 = V_MOV_B32_e32 7, implicit $exec

    %offset:vgpr_32 = V_LSHLREV_B32_e32 4, %idx, implicit $exec

    %8:vreg_128 = BUFFER_LOAD_DWORDX4_OFFSET %rsrc, 0, 64, 0, 0, implicit $exec

    %cmp:sreg_64 = V_CMP_NE_U32_e64 %idx, %3, implicit $exec
    %exec_mask:sreg_64 = S_AND_B64 $exec, %cmp, implicit-def $exec, implicit-def $scc
    $exec = COPY %exec_mask


    S_CBRANCH_EXECZ %bb.2, implicit $exec
    S_BRANCH %bb.1

  bb.1:
    %9:vgpr_32 = V_ADD_U32_e32 %0, %1, implicit $exec
    %10:vgpr_32 = V_ADD_U32_e32 %2, %4, implicit $exec
    S_BRANCH %bb.3

  bb.2:
    %11:vgpr_32 = COPY %8.sub1, implicit $exec
    %12:vgpr_32 = V_ADD_U32_e32 %11, %7, implicit $exec

  bb.3:
    $exec = S_OR_B64 $exec, %cmp, implicit-def $scc
    S_ENDPGM 0
