; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=25 < %s 2>&1 | FileCheck %s

;           bb.0.entry
;            /    |
;       bb.3.bb3  |
;            \    |
;           bb.1.Flow12
;            /    |
;       bb.2.bb2  |
;            \    |
;           bb.4.bb4
;              |
;      bb.5.loop1.header<-------+
;              |                |
;      bb.6.loop2.header<-----+ |
;              |              | |
;      bb.7.loop3.header<---+ | |
;            /   |          | | |
;      bb.8.bb5  |          | | |
;            \   |          | | |
;      bb.9.loop3.latch-----+ | |
;              |              | |
;      bb.10.loop2.latch------+ |
;              |                |
;    bb.11.loop4.preheader      |
;              |                |
;         bb.12.loop4<----+     |
;              +----------+     |
;              |                |
;              |                |
;      bb.13.loop1.latch--------+
;              |
;          bb.14.bb6
;           /      |
;    bb.15.bb7     |
;           \      |
;    bb.16.loop5.preheader
;              |
;     +-->bb.17.loop5
;     +--------+
;              |
;          bb.18.exit
define amdgpu_ps i32 @test (ptr addrspace(1) %p1, ptr addrspace(1) %p2, ptr addrspace(1) %p3, ptr addrspace(1) %p4, ptr addrspace(1) %p5, ptr addrspace(1) %p6, ptr addrspace(1) %p7, ptr addrspace(1) %p8, ptr addrspace(1) %p9, ptr addrspace(1) %p10, ptr addrspace(1) %p11, i32 %TC1, i32 %TC2, i32 %TC3, i32 %TC4, i32 %TC5, i32 %Val1, i32 %Val2, i1 %cond1) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13, $vgpr14, $vgpr15, $vgpr16, $vgpr17, $vgpr18, $vgpr19, $vgpr20, $vgpr21, $vgpr22, $vgpr23, $vgpr24, $vgpr25, $vgpr26, $vgpr27
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr27
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr26
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr25
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr24
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr23
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr22
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr21
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr20
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr19
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY $vgpr18
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY $vgpr17
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY $vgpr16
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY $vgpr15
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY $vgpr14
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY $vgpr13
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vgpr_32 = COPY $vgpr12
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:vgpr_32 = COPY $vgpr11
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vgpr_32 = COPY $vgpr10
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]]:vgpr_32 = COPY $vgpr9
  ; CHECK-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY $vgpr8
  ; CHECK-NEXT:   [[COPY20:%[0-9]+]]:vgpr_32 = COPY $vgpr7
  ; CHECK-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY $vgpr6
  ; CHECK-NEXT:   [[COPY22:%[0-9]+]]:vgpr_32 = COPY $vgpr5
  ; CHECK-NEXT:   [[COPY23:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY24:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY3]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.0, addrspace 5)
  ; CHECK-NEXT:   [[COPY25:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY7]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[COPY26:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY1]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[COPY27:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY4]], %stack.3, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.3, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY6]], %stack.4, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY27]], %subreg.sub0, [[COPY26]], %subreg.sub1
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE1]], 8, [[GLOBAL_LOAD_UBYTE]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE3:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY5]], %stack.5, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.5, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE3]], 8, [[GLOBAL_LOAD_UBYTE2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_1]], 16, [[V_LSHL_OR_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_NE_U32_e64_]], %bb.1, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.Flow:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.4(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF]], %bb.4, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb2:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[COPY2]], [[V_LSHL_OR_B32_e64_2]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE]], [[V_MUL_LO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p1, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.bb3:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[COPY2]], [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[REG_SEQUENCE]], [[V_ADD_U32_e64_]], 2, 0, implicit $exec :: (store (s16) into %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[REG_SEQUENCE]], [[V_ADD_U32_e64_]], 0, 0, implicit $exec :: (store (s16) into %ir.p1, addrspace 1)
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.bb4:
  ; CHECK-NEXT:   successors: %bb.5(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY9]], %subreg.sub0, [[COPY8]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY11]], %subreg.sub0, [[COPY10]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY13]], %subreg.sub0, [[COPY12]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY15]], %subreg.sub0, [[COPY14]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY17]], %subreg.sub0, [[COPY16]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE6:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY19]], %subreg.sub0, [[COPY18]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE7:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY21]], %subreg.sub0, [[COPY20]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE8:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY23]], %subreg.sub0, [[COPY22]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE9:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY25]], %subreg.sub0, [[COPY24]], %subreg.sub1
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE4:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE5:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE5]], 8, [[GLOBAL_LOAD_UBYTE4]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE6:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE7:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE5]], %stack.6, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.6, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE7]], 8, [[GLOBAL_LOAD_UBYTE6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_4]], 16, [[V_LSHL_OR_B32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE3:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_5]], %stack.7, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.7, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_2]], %stack.8, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.8, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE4:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.5, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.5, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE5:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.8, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.8, addrspace 5)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.loop1.header:
  ; CHECK-NEXT:   successors: %bb.6(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.4, %67, %bb.13
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_1]], %bb.4, %66, %bb.13
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[SI_SPILL_V32_RESTORE5]], %bb.4, %65, %bb.13
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[V_MOV_B32_e32_]], %bb.4, %23, %bb.13
  ; CHECK-NEXT:   [[V_ASHRREV_I32_e64_:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e64 31, [[PHI2]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE10:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI2]], %subreg.sub0, [[V_ASHRREV_I32_e64_]], %subreg.sub1
  ; CHECK-NEXT:   [[V_LSHLREV_B64_pseudo_e64_:%[0-9]+]]:vreg_64 = nsw V_LSHLREV_B64_pseudo_e64 3, [[REG_SEQUENCE10]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[REG_SEQUENCE]].sub0, [[V_LSHLREV_B64_pseudo_e64_]].sub0, 0, implicit $exec
  ; CHECK-NEXT:   %250:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[REG_SEQUENCE]].sub1, [[V_LSHLREV_B64_pseudo_e64_]].sub1, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE11:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, %250, %subreg.sub1
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE8:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE11]], 0, 0, implicit $exec :: (load (s8) from %ir.gep1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE9:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE11]], 1, 0, implicit $exec :: (load (s8) from %ir.gep1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE9]], 8, [[GLOBAL_LOAD_UBYTE8]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE10:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE11]], 2, 0, implicit $exec :: (load (s8) from %ir.gep1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE11:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE11]], 3, 0, implicit $exec :: (load (s8) from %ir.gep1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE11]], 8, [[GLOBAL_LOAD_UBYTE10]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_8:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_7]], 16, [[V_LSHL_OR_B32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[COPY2]], [[PHI2]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[REG_SEQUENCE]], [[V_MUL_LO_U32_e64_1]], 0, 0, implicit $exec :: (store (s16) into %ir.p1, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[REG_SEQUENCE]], [[V_MUL_LO_U32_e64_1]], 2, 0, implicit $exec :: (store (s16) into %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_LSHL_OR_B32_e64_8]], [[PHI1]], implicit $exec
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.loop2.header:
  ; CHECK-NEXT:   successors: %bb.7(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.5, %40, %bb.10
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI [[V_MUL_LO_U32_e64_2]], %bb.5, %39, %bb.10
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_8]], %bb.5, %36, %bb.10
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vgpr_32 = PHI [[PHI2]], %bb.5, %25, %bb.10
  ; CHECK-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.7.loop3.header:
  ; CHECK-NEXT:   successors: %bb.8(0x40000000), %bb.9(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_1]], %bb.6, %29, %bb.9
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_1]], %bb.6, %28, %bb.9
  ; CHECK-NEXT:   [[V_ADD_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI9]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE12:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE9]], 0, 0, implicit $exec :: (load (s8) from %ir.p2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE13:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE9]], 1, 0, implicit $exec :: (load (s8) from %ir.p2 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_9:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE13]], 8, [[GLOBAL_LOAD_UBYTE12]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE14:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE9]], 2, 0, implicit $exec :: (load (s8) from %ir.p2 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE15:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE9]], 3, 0, implicit $exec :: (load (s8) from %ir.p2 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_10:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE15]], 8, [[GLOBAL_LOAD_UBYTE14]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_11:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_10]], 16, [[V_LSHL_OR_B32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_2]], [[V_LSHL_OR_B32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_GE_U32_e64_]], %bb.9, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.8
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.8.bb5:
  ; CHECK-NEXT:   successors: %bb.9(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI9]], [[PHI5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_3]], [[V_LSHL_OR_B32_e64_11]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_3]], [[V_MUL_LO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE4]], [[V_ADD_U32_e64_4]], 0, 0, implicit $exec :: (store (s32) into %ir.p7, addrspace 1)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.9.loop3.latch:
  ; CHECK-NEXT:   successors: %bb.10(0x04000000), %bb.7(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_8]], %bb.7, [[V_ADD_U32_e64_4]], %bb.8
  ; CHECK-NEXT:   [[PHI11:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_2]], %bb.7, [[V_MUL_LO_U32_e64_3]], %bb.8
  ; CHECK-NEXT:   [[PHI12:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_2]], %bb.7, [[PHI6]], %bb.8
  ; CHECK-NEXT:   SI_END_CF [[SI_IF1]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE3]], [[PHI12]], 0, 0, implicit $exec :: (store (s32) into %ir.p9, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE2]], 0, 0, implicit $exec :: (load (s32) from %ir.p10, addrspace 1)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_4:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI11]], [[PHI10]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE2]], [[V_MUL_LO_U32_e64_4]], 0, 0, implicit $exec :: (store (s32) into %ir.p10, addrspace 1)
  ; CHECK-NEXT:   [[S_ADD_I32_:%[0-9]+]]:sreg_32 = S_ADD_I32 [[PHI9]], 1, implicit-def dead $scc
  ; CHECK-NEXT:   [[V_ADD_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[S_ADD_I32_]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_1:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_5]], [[SI_SPILL_V32_RESTORE4]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_1]], [[PHI8]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK]], %bb.7, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.10
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.10.loop2.latch:
  ; CHECK-NEXT:   successors: %bb.11(0x04000000), %bb.6(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_6:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (load (s32) from %ir.p11, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_7:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_6]], [[PHI11]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE1]], [[V_ADD_U32_e64_7]], 0, 0, implicit $exec :: (store (s32) into %ir.p11, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_8:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI5]], [[PHI1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_2:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_6]], [[SI_SPILL_V32_RESTORE3]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK1:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_2]], [[PHI4]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK1]], %bb.6, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.11.loop4.preheader:
  ; CHECK-NEXT:   successors: %bb.12(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK1]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE12:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI10]], %subreg.sub0, undef %237:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %188:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[PHI10]], [[PHI11]], [[REG_SEQUENCE12]], 0, implicit $exec
  ; CHECK-NEXT:   [[COPY28:%[0-9]+]]:vgpr_32 = COPY %188.sub0
  ; CHECK-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.12.loop4:
  ; CHECK-NEXT:   successors: %bb.13(0x04000000), %bb.12(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI13:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_2]], %bb.11, %61, %bb.12
  ; CHECK-NEXT:   [[PHI14:%[0-9]+]]:vgpr_32 = PHI [[COPY28]], %bb.11, %60, %bb.12
  ; CHECK-NEXT:   [[PHI15:%[0-9]+]]:vgpr_32 = PHI [[V_MUL_LO_U32_e64_4]], %bb.11, %57, %bb.12
  ; CHECK-NEXT:   [[PHI16:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_7]], %bb.11, [[PHI3]], %bb.12
  ; CHECK-NEXT:   [[PHI17:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_DWORD]], %bb.11, %58, %bb.12
  ; CHECK-NEXT:   [[PHI18:%[0-9]+]]:vgpr_32 = PHI [[GLOBAL_LOAD_DWORD1]], %bb.11, %59, %bb.12
  ; CHECK-NEXT:   [[V_ADD_U32_e64_9:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI16]], [[PHI10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_5:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI18]], [[GLOBAL_LOAD_DWORD]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE8]], [[V_MUL_LO_U32_e64_5]], 0, 0, implicit $exec :: (store (s32) into %ir.p3, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_10:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI16]], [[PHI14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_6:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI7]], [[V_ADD_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_7:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI17]], [[GLOBAL_LOAD_DWORD1]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_11:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_7]], [[V_MUL_LO_U32_e64_6]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE7]], [[V_ADD_U32_e64_11]], 0, 0, implicit $exec :: (store (s32) into %ir.p4, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_12:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 4, [[PHI15]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_HI_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_HI_U32_e64 -1431655765, [[V_MUL_LO_U32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHRREV_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHRREV_B32_e64 1, [[V_MUL_HI_U32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHRREV_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHRREV_B32_e64 31, [[V_ADD_U32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_13:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_9]], [[V_LSHRREV_B32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ASHRREV_I32_e64_1:%[0-9]+]]:vgpr_32 = V_ASHRREV_I32_e64 1, [[V_ADD_U32_e64_13]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_14:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 4, [[PHI14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_3:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_12]], [[SI_SPILL_V32_RESTORE2]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK2:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_3]], [[PHI13]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK2]], %bb.12, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.13
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.13.loop1.latch:
  ; CHECK-NEXT:   successors: %bb.14(0x04000000), %bb.5(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK2]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_15:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_MUL_LO_U32_e64_7]], [[SI_SPILL_V32_RESTORE1]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE6]], [[V_ADD_U32_e64_15]], 0, 0, implicit $exec :: (store (s32) into %ir.p5, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_16:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[PHI2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_17:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[PHI1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_4:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_16]], [[SI_SPILL_V32_RESTORE]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK3:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_4]], [[PHI]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK3]], %bb.5, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.14
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.14.bb6:
  ; CHECK-NEXT:   successors: %bb.15(0x40000000), %bb.16(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK3]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE6:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.7, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.7, addrspace 5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_8:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 100, [[SI_SPILL_V32_RESTORE6]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GT_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GT_U32_e64 [[V_MUL_LO_U32_e64_8]], [[V_ADD_U32_e64_15]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF2:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_GT_U32_e64_]], %bb.16, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.15
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.15.bb7:
  ; CHECK-NEXT:   successors: %bb.16(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.6, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.6, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE]], [[V_MUL_LO_U32_e64_8]], 0, 0, implicit $exec :: (store (s32) into %ir.p6, addrspace 1)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.16.loop5.preheader:
  ; CHECK-NEXT:   successors: %bb.17(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI19:%[0-9]+]]:vgpr_32 = PHI [[V_MUL_LO_U32_e64_8]], %bb.14, [[V_ADD_U32_e64_15]], %bb.15
  ; CHECK-NEXT:   SI_END_CF [[SI_IF2]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_3:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE7:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.0, addrspace 5)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.17.loop5:
  ; CHECK-NEXT:   successors: %bb.18(0x04000000), %bb.17(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI20:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_3]], %bb.16, %76, %bb.17
  ; CHECK-NEXT:   [[PHI21:%[0-9]+]]:vgpr_32 = PHI [[PHI19]], %bb.16, %75, %bb.17
  ; CHECK-NEXT:   [[V_ADD_U32_e64_18:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 2, [[PHI21]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_5:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_18]], [[SI_SPILL_V32_RESTORE7]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK4:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_5]], [[PHI20]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK4]], %bb.17, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.18
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.18.exit:
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK4]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_19:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 100, [[SI_SPILL_V32_RESTORE5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_9:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_LO_U32_e64_8]], [[PHI21]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_10:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_19]], [[V_MUL_LO_U32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[V_READFIRSTLANE_B32_:%[0-9]+]]:sreg_32_xm0 = V_READFIRSTLANE_B32 [[V_MUL_LO_U32_e64_10]], implicit $exec
  ; CHECK-NEXT:   $sgpr0 = COPY [[V_READFIRSTLANE_B32_]]
  ; CHECK-NEXT:   SI_RETURN_TO_EPILOG killed $sgpr0
entry:
;        entry
;          |
;         bb1
;        /   \
;      bb2   bb3
;        \   /
;         bb4
;          |
;     loop1.header<-------+
;          |              |
;     loop2.header<-----+ |
;          |            | |
;     loop3.header<---+ | |
;         /   |       | | |
;       bb5   |       | | |
;         \   |       | | |
;     loop3.latch-----+ | |
;          |            | |
;     loop2.latch-------+ |
;          |              |
; +-->loop4|              |
; +--------+              |
;          |              |
;     loop1.latch---------+
;          |
;         bb6
;        /  |
;      bb17 |
;        |  |
;   +-->loop5
;   +-----+
;         |
;        exit

   %ld1 = load i32, ptr addrspace(1) %p1, align 1
   %add1 = add i32 %ld1, 100
   br label %bb1

bb1:
   br i1 %cond1, label %bb2, label %bb3

bb2:
   %mul1 = mul i32 %Val1, %ld1
   store i32 %mul1, ptr addrspace(1) %p1, align 4
   br label %bb4

bb3:
   %add2 = add i32 %Val1, %ld1
   store i32 %add2, ptr addrspace(1) %p1, align 2
   br label %bb4

bb4:
   %phi1 = phi i32 [ %mul1, %bb2 ], [ %add2, %bb3 ]
   %ld2 = load i32, ptr addrspace(1) %p1, align 1
   br label %loop1.header

loop1.header:
   %phi.inc1 = phi i32 [ %ld1, %bb4 ], [  %inc1, %loop1.latch ]
   %phi.phi = phi i32 [ 0, %bb4 ], [ %phi2, %loop1.latch ]
   %sext1 = sext i32 %phi.inc1 to i64
   %gep1 = getelementptr inbounds i64, ptr addrspace(1) %p1, i64 %sext1
   %ld3 = load i32, ptr addrspace(1) %gep1, align 1
   %mul2 = mul i32 %Val1, %phi.inc1
   store i32 %mul2, ptr addrspace(1) %p1, align 2
   br label %loop2.header

loop2.header:
   %phi.inc2 = phi i32 [ %ld3, %loop1.header ], [  %inc2, %loop2.latch ]
   %phi6 = phi i32 [ %phi.inc1, %loop1.header ], [ %phi5, %loop2.latch ]
   br label %loop3.header

loop3.header:
   %phi.inc3 = phi i32 [ %phi.inc2, %loop2.header ], [  %inc3, %loop3.latch ]
   %ld4 = load i32, ptr addrspace(1) %p2, align 1
   %cond2 = icmp uge i32 %phi.inc3, %ld4
   br i1 %cond2, label %bb5, label %loop3.latch

bb5:
   %mul3 = mul i32 %phi.inc1, %phi.inc2
   %add3 = add i32 %mul3, %phi.inc3
   %mul4 = mul i32 %add3, %ld4
   %add4 = add i32 %mul4, %mul2
   store i32 %add4, ptr addrspace(1) %p7
   br label %loop3.latch

loop3.latch:
   %phi2 = phi i32 [ %add4, %bb5 ], [ %ld3, %loop3.header ]
   %phi4 = phi i32 [ %mul4, %bb5 ], [ %phi.inc3, %loop3.header ]
   %phi5 = phi i32 [ %phi.inc2, %bb5 ], [ %phi.inc3, %loop3.header ]
   store i32 %phi5, ptr addrspace(1) %p9
   %inc3 = add i32 %phi.inc3, 1
   %ld10 = load i32, ptr addrspace(1) %p10
   %mul11 = mul i32 %phi4, %phi2
   store i32 %mul11, ptr addrspace(1) %p10
   %cond3 = icmp ult i32 %inc3, %TC3
   br i1 %cond3, label %loop3.header, label %loop2.latch

loop2.latch:
   %inc2 = add i32 %phi.inc2, 1
   %ld11 = load i32, ptr addrspace(1) %p11
   %add9 = add i32 %inc2, %phi4
   store i32 %add9, ptr addrspace(1) %p11
   %cond4 = icmp ult i32 %inc2, %TC2
   br i1 %cond4, label %loop2.header, label %loop4

loop4:
   %phi.inc4 = phi i32 [ %mul11, %loop2.latch ], [  %inc4, %loop4 ]
   %phi7 = phi i32 [ %add9, %loop2.latch ], [ %phi.phi, %loop4 ]
   %phi.div1 = phi i32 [ %ld10, %loop2.latch ], [ %div1, %loop4 ]
   %phi.div2 = phi i32 [ %ld11, %loop2.latch ], [ %div2, %loop4 ]
   %add5 = add i32 %phi7, %phi2
   %mul5 = mul i32 %phi.div2, %ld10
   store i32 %mul5, ptr addrspace(1) %p3
   %add6 = add i32 %add5, %phi.inc4
   %mul8 = mul i32 %phi6, %add6
   %mul9 = mul i32 %phi.div1, %ld11
   %add10 = add i32 %mul9, %mul8
   store i32 %add10, ptr addrspace(1) %p4
   %inc4 = add i32 %phi.inc4, 4
   %div1 = udiv i32 %mul8, 3
   %div2 = sdiv i32 %add5, 2
   %cond7 = icmp ult i32 %inc4, %TC4
   br i1 %cond7, label %loop4, label %loop1.latch

loop1.latch:
   %add7 = add i32 %mul9, %Val2
   store i32 %add7, ptr addrspace(1) %p5
   %inc1 = add i32 %phi.inc1, 1
   %cond5 = icmp ult i32 %inc1, %TC1
   br i1 %cond5, label %loop1.header, label %bb6

bb6:
   %mul6 = mul i32 %ld2, 100
   %cond8 = icmp ugt i32 %mul6, %add7
   br i1 %cond8, label %bb7, label %loop5

bb7:
   store i32 %mul6, ptr addrspace(1) %p6
   br label %loop5

loop5:
   %phi.inc5 = phi i32 [ %add7, %bb7 ], [ %mul6, %bb6 ], [  %inc5, %loop5 ]
   %add8 = mul i32 %mul6, %phi.inc5
   %inc5 = add i32 %phi.inc5, 2
   %cond9 = icmp ult i32 %inc5, %TC5
   br i1 %cond9, label %loop5, label %exit

exit:
   %mul7 = mul i32 %add1, %add8
   ret i32 %mul7
}
