; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; RUN: opt -S -mtriple=amdgcn -passes='instcombine' -mtriple=amdgcn-amd-amdhsa < %s | FileCheck %s --check-prefix=DEREF
; RUN: opt -S -mtriple=amdgcn -passes='loop-mssa(licm)' -mtriple=amdgcn-amd-amdhsa < %s | FileCheck %s --check-prefix=WITHOUT
; RUN: opt -S -mtriple=amdgcn -passes='instcombine,loop-mssa(licm)' -mtriple=amdgcn-amd-amdhsa < %s | FileCheck %s --check-prefix=WITH

; InstCombine added retattr dereferenceable(bytes) to a "call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()".
; Subsequently, licm could hoist load from its returned pointer in depth-2 loop's preheader to depth-1 loop's preheader.
; The load became safe to execute unconditionally as it load from a dereferenceable pointer.

target datalayout = "e-m:e-p:64:64-p1:64:64-p2:32:32-p3:32:32-p4:64:64-p5:32:32-p6:32:32-p7:160:256:256:32-p8:128:128:128:48-p9:192:256:256:32-i64:64-v16:16-v24:32-v32:32-v48:64-v96:128-v192:256-v256:256-v512:512-v1024:1024-v2048:2048-n32:64-S32-A5-G1-ni:7:8:9"

define protected amdgpu_kernel void @foo(ptr addrspace(1) noundef readonly captures(none) %d_a.coerce, ptr addrspace(1) noundef readonly captures(none) %d_b.coerce, ptr addrspace(1) noundef writeonly captures(none) %d_c.coerce, i32 noundef %count) local_unnamed_addr #0 {
; DEREF-LABEL: define protected amdgpu_kernel void @foo(
; DEREF-SAME: ptr addrspace(1) noundef readonly captures(none) [[D_A_COERCE:%.*]], ptr addrspace(1) noundef readonly captures(none) [[D_B_COERCE:%.*]], ptr addrspace(1) noundef writeonly captures(none) [[D_C_COERCE:%.*]], i32 noundef [[COUNT:%.*]]) local_unnamed_addr #[[ATTR0:[0-9]+]] {
; DEREF-NEXT:  [[ENTRY:.*:]]
; DEREF-NEXT:    [[TMP0:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
; DEREF-NEXT:    [[CMP11:%.*]] = icmp samesign ult i32 [[TMP0]], 4
; DEREF-NEXT:    br i1 [[CMP11]], label %[[FOR_BODY_LR_PH:.*]], label %[[FOR_COND_CLEANUP:.*]]
; DEREF:       [[FOR_BODY_LR_PH]]:
; DEREF-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.amdgcn.workgroup.id.x()
; DEREF-NEXT:    [[TMP2:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
; DEREF-NEXT:    [[CMP79:%.*]] = icmp samesign ult i32 [[TMP2]], 4
; DEREF-NEXT:    [[CONV:%.*]] = sext i32 [[TMP1]] to i64
; DEREF-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], 192
; DEREF-NEXT:    [[TMP3:%.*]] = tail call dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; DEREF-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 12
; DEREF-NEXT:    [[TMP5:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 14
; DEREF-NEXT:    [[DOTIN_I_I_I:%.*]] = load i16, ptr addrspace(4) [[TMP5]], align 2, !tbaa [[SHORT_TBAA6:![0-9]+]]
; DEREF-NEXT:    [[CONV_I_I:%.*]] = zext i16 [[DOTIN_I_I_I]] to i32
; DEREF-NEXT:    br label %[[FOR_BODY:.*]]
; DEREF:       [[FOR_COND_CLEANUP]]:
; DEREF-NEXT:    ret void
; DEREF:       [[FOR_BODY]]:
; DEREF-NEXT:    [[THREAD_Y_012:%.*]] = phi i32 [ [[TMP0]], %[[FOR_BODY_LR_PH]] ], [ [[ADD21:%.*]], %[[FOR_COND_CLEANUP8:.*]] ]
; DEREF-NEXT:    br i1 [[CMP79]], label %[[FOR_BODY9_LR_PH:.*]], label %[[FOR_COND_CLEANUP8]]
; DEREF:       [[FOR_BODY9_LR_PH]]:
; DEREF-NEXT:    [[MUL10:%.*]] = shl nuw nsw i32 [[THREAD_Y_012]], 2
; DEREF-NEXT:    [[CONV11:%.*]] = zext nneg i32 [[MUL10]] to i64
; DEREF-NEXT:    [[ADD:%.*]] = add nuw nsw i64 [[MUL]], [[CONV11]]
; DEREF-NEXT:    [[DOTIN_I_I_I7:%.*]] = load i16, ptr addrspace(4) [[TMP4]], align 4, !tbaa [[SHORT_TBAA6]]
; DEREF-NEXT:    [[CONV_I_I8:%.*]] = zext i16 [[DOTIN_I_I_I7]] to i32
; DEREF-NEXT:    br label %[[FOR_BODY9:.*]]
; DEREF:       [[FOR_COND_CLEANUP8]]:
; DEREF-NEXT:    [[ADD21]] = add nuw nsw i32 [[THREAD_Y_012]], [[CONV_I_I]]
; DEREF-NEXT:    [[CMP:%.*]] = icmp samesign ult i32 [[ADD21]], 4
; DEREF-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_COND_CLEANUP]], !llvm.loop [[LOOP10:![0-9]+]]
; DEREF:       [[FOR_BODY9]]:
; DEREF-NEXT:    [[THREAD_X_010:%.*]] = phi i32 [ [[TMP2]], %[[FOR_BODY9_LR_PH]] ], [ [[ADD18:%.*]], %[[FOR_BODY9]] ]
; DEREF-NEXT:    [[CONV12:%.*]] = zext nneg i32 [[THREAD_X_010]] to i64
; DEREF-NEXT:    [[ADD13:%.*]] = add nuw nsw i64 [[ADD]], [[CONV12]]
; DEREF-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_A_COERCE]], i64 [[ADD13]]
; DEREF-NEXT:    [[TMP6:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX]], align 8, !tbaa [[DOUBLE_TBAA12:![0-9]+]]
; DEREF-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_B_COERCE]], i64 [[ADD13]]
; DEREF-NEXT:    [[TMP7:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX14]], align 8, !tbaa [[DOUBLE_TBAA12]]
; DEREF-NEXT:    [[ADD15:%.*]] = fadd contract double [[TMP6]], [[TMP7]]
; DEREF-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_C_COERCE]], i64 [[ADD13]]
; DEREF-NEXT:    store double [[ADD15]], ptr addrspace(1) [[ARRAYIDX16]], align 8, !tbaa [[DOUBLE_TBAA12]]
; DEREF-NEXT:    [[ADD18]] = add nuw nsw i32 [[THREAD_X_010]], [[CONV_I_I8]]
; DEREF-NEXT:    [[CMP7:%.*]] = icmp samesign ult i32 [[ADD18]], 4
; DEREF-NEXT:    br i1 [[CMP7]], label %[[FOR_BODY9]], label %[[FOR_COND_CLEANUP8]], !llvm.loop [[LOOP14:![0-9]+]]
;
; WITHOUT-LABEL: define protected amdgpu_kernel void @foo(
; WITHOUT-SAME: ptr addrspace(1) noundef readonly captures(none) [[D_A_COERCE:%.*]], ptr addrspace(1) noundef readonly captures(none) [[D_B_COERCE:%.*]], ptr addrspace(1) noundef writeonly captures(none) [[D_C_COERCE:%.*]], i32 noundef [[COUNT:%.*]]) local_unnamed_addr #[[ATTR0:[0-9]+]] {
; WITHOUT-NEXT:  [[ENTRY:.*:]]
; WITHOUT-NEXT:    [[TMP0:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
; WITHOUT-NEXT:    [[CMP11:%.*]] = icmp samesign ult i32 [[TMP0]], 4
; WITHOUT-NEXT:    br i1 [[CMP11]], label %[[FOR_BODY_LR_PH:.*]], label %[[FOR_COND_CLEANUP:.*]]
; WITHOUT:       [[FOR_BODY_LR_PH]]:
; WITHOUT-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.amdgcn.workgroup.id.x()
; WITHOUT-NEXT:    [[TMP2:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
; WITHOUT-NEXT:    [[CMP79:%.*]] = icmp samesign ult i32 [[TMP2]], 4
; WITHOUT-NEXT:    [[CONV:%.*]] = sext i32 [[TMP1]] to i64
; WITHOUT-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], 192
; WITHOUT-NEXT:    [[TMP3:%.*]] = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; WITHOUT-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 12
; WITHOUT-NEXT:    [[TMP5:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 14
; WITHOUT-NEXT:    [[DOTIN_I_I_I:%.*]] = load i16, ptr addrspace(4) [[TMP5]], align 2, !tbaa [[SHORT_TBAA6:![0-9]+]]
; WITHOUT-NEXT:    [[CONV_I_I:%.*]] = zext i16 [[DOTIN_I_I_I]] to i32
; WITHOUT-NEXT:    br label %[[FOR_BODY:.*]]
; WITHOUT:       [[FOR_COND_CLEANUP_LOOPEXIT:.*]]:
; WITHOUT-NEXT:    br label %[[FOR_COND_CLEANUP]]
; WITHOUT:       [[FOR_COND_CLEANUP]]:
; WITHOUT-NEXT:    ret void
; WITHOUT:       [[FOR_BODY]]:
; WITHOUT-NEXT:    [[THREAD_Y_012:%.*]] = phi i32 [ [[TMP0]], %[[FOR_BODY_LR_PH]] ], [ [[ADD21:%.*]], %[[FOR_COND_CLEANUP8:.*]] ]
; WITHOUT-NEXT:    br i1 [[CMP79]], label %[[FOR_BODY9_LR_PH:.*]], label %[[FOR_COND_CLEANUP8]]
; WITHOUT:       [[FOR_BODY9_LR_PH]]:
; WITHOUT-NEXT:    [[MUL10:%.*]] = shl nuw nsw i32 [[THREAD_Y_012]], 2
; WITHOUT-NEXT:    [[CONV11:%.*]] = zext nneg i32 [[MUL10]] to i64
; WITHOUT-NEXT:    [[ADD:%.*]] = add nuw nsw i64 [[MUL]], [[CONV11]]
; WITHOUT-NEXT:    [[DOTIN_I_I_I7:%.*]] = load i16, ptr addrspace(4) [[TMP4]], align 4, !tbaa [[SHORT_TBAA6]]
; WITHOUT-NEXT:    [[CONV_I_I8:%.*]] = zext i16 [[DOTIN_I_I_I7]] to i32
; WITHOUT-NEXT:    br label %[[FOR_BODY9:.*]]
; WITHOUT:       [[FOR_COND_CLEANUP8_LOOPEXIT:.*]]:
; WITHOUT-NEXT:    br label %[[FOR_COND_CLEANUP8]]
; WITHOUT:       [[FOR_COND_CLEANUP8]]:
; WITHOUT-NEXT:    [[ADD21]] = add nuw nsw i32 [[THREAD_Y_012]], [[CONV_I_I]]
; WITHOUT-NEXT:    [[CMP:%.*]] = icmp samesign ult i32 [[ADD21]], 4
; WITHOUT-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_COND_CLEANUP_LOOPEXIT]], !llvm.loop [[LOOP10:![0-9]+]]
; WITHOUT:       [[FOR_BODY9]]:
; WITHOUT-NEXT:    [[THREAD_X_010:%.*]] = phi i32 [ [[TMP2]], %[[FOR_BODY9_LR_PH]] ], [ [[ADD18:%.*]], %[[FOR_BODY9]] ]
; WITHOUT-NEXT:    [[CONV12:%.*]] = zext nneg i32 [[THREAD_X_010]] to i64
; WITHOUT-NEXT:    [[ADD13:%.*]] = add nuw nsw i64 [[ADD]], [[CONV12]]
; WITHOUT-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_A_COERCE]], i64 [[ADD13]]
; WITHOUT-NEXT:    [[TMP6:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX]], align 8, !tbaa [[DOUBLE_TBAA12:![0-9]+]]
; WITHOUT-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_B_COERCE]], i64 [[ADD13]]
; WITHOUT-NEXT:    [[TMP7:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX14]], align 8, !tbaa [[DOUBLE_TBAA12]]
; WITHOUT-NEXT:    [[ADD15:%.*]] = fadd contract double [[TMP6]], [[TMP7]]
; WITHOUT-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_C_COERCE]], i64 [[ADD13]]
; WITHOUT-NEXT:    store double [[ADD15]], ptr addrspace(1) [[ARRAYIDX16]], align 8, !tbaa [[DOUBLE_TBAA12]]
; WITHOUT-NEXT:    [[ADD18]] = add nuw nsw i32 [[THREAD_X_010]], [[CONV_I_I8]]
; WITHOUT-NEXT:    [[CMP7:%.*]] = icmp samesign ult i32 [[ADD18]], 4
; WITHOUT-NEXT:    br i1 [[CMP7]], label %[[FOR_BODY9]], label %[[FOR_COND_CLEANUP8_LOOPEXIT]], !llvm.loop [[LOOP14:![0-9]+]]
;
; WITH-LABEL: define protected amdgpu_kernel void @foo(
; WITH-SAME: ptr addrspace(1) noundef readonly captures(none) [[D_A_COERCE:%.*]], ptr addrspace(1) noundef readonly captures(none) [[D_B_COERCE:%.*]], ptr addrspace(1) noundef writeonly captures(none) [[D_C_COERCE:%.*]], i32 noundef [[COUNT:%.*]]) local_unnamed_addr #[[ATTR0:[0-9]+]] {
; WITH-NEXT:  [[ENTRY:.*:]]
; WITH-NEXT:    [[TMP0:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
; WITH-NEXT:    [[CMP11:%.*]] = icmp samesign ult i32 [[TMP0]], 4
; WITH-NEXT:    br i1 [[CMP11]], label %[[FOR_BODY_LR_PH:.*]], label %[[FOR_COND_CLEANUP:.*]]
; WITH:       [[FOR_BODY_LR_PH]]:
; WITH-NEXT:    [[TMP1:%.*]] = tail call i32 @llvm.amdgcn.workgroup.id.x()
; WITH-NEXT:    [[TMP2:%.*]] = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
; WITH-NEXT:    [[CMP79:%.*]] = icmp samesign ult i32 [[TMP2]], 4
; WITH-NEXT:    [[CONV:%.*]] = sext i32 [[TMP1]] to i64
; WITH-NEXT:    [[MUL:%.*]] = mul nsw i64 [[CONV]], 192
; WITH-NEXT:    [[TMP3:%.*]] = tail call dereferenceable(256) ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
; WITH-NEXT:    [[TMP4:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 12
; WITH-NEXT:    [[TMP5:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(4) [[TMP3]], i64 14
; WITH-NEXT:    [[DOTIN_I_I_I:%.*]] = load i16, ptr addrspace(4) [[TMP5]], align 2, !tbaa [[SHORT_TBAA6:![0-9]+]]
; WITH-NEXT:    [[CONV_I_I:%.*]] = zext i16 [[DOTIN_I_I_I]] to i32
; WITH-NEXT:    [[DOTIN_I_I_I7:%.*]] = load i16, ptr addrspace(4) [[TMP4]], align 4
; WITH-NEXT:    [[CONV_I_I8:%.*]] = zext i16 [[DOTIN_I_I_I7]] to i32
; WITH-NEXT:    br label %[[FOR_BODY:.*]]
; WITH:       [[FOR_COND_CLEANUP_LOOPEXIT:.*]]:
; WITH-NEXT:    br label %[[FOR_COND_CLEANUP]]
; WITH:       [[FOR_COND_CLEANUP]]:
; WITH-NEXT:    ret void
; WITH:       [[FOR_BODY]]:
; WITH-NEXT:    [[THREAD_Y_012:%.*]] = phi i32 [ [[TMP0]], %[[FOR_BODY_LR_PH]] ], [ [[ADD21:%.*]], %[[FOR_COND_CLEANUP8:.*]] ]
; WITH-NEXT:    br i1 [[CMP79]], label %[[FOR_BODY9_LR_PH:.*]], label %[[FOR_COND_CLEANUP8]]
; WITH:       [[FOR_BODY9_LR_PH]]:
; WITH-NEXT:    [[MUL10:%.*]] = shl nuw nsw i32 [[THREAD_Y_012]], 2
; WITH-NEXT:    [[CONV11:%.*]] = zext nneg i32 [[MUL10]] to i64
; WITH-NEXT:    [[ADD:%.*]] = add nuw nsw i64 [[MUL]], [[CONV11]]
; WITH-NEXT:    br label %[[FOR_BODY9:.*]]
; WITH:       [[FOR_COND_CLEANUP8_LOOPEXIT:.*]]:
; WITH-NEXT:    br label %[[FOR_COND_CLEANUP8]]
; WITH:       [[FOR_COND_CLEANUP8]]:
; WITH-NEXT:    [[ADD21]] = add nuw nsw i32 [[THREAD_Y_012]], [[CONV_I_I]]
; WITH-NEXT:    [[CMP:%.*]] = icmp samesign ult i32 [[ADD21]], 4
; WITH-NEXT:    br i1 [[CMP]], label %[[FOR_BODY]], label %[[FOR_COND_CLEANUP_LOOPEXIT]], !llvm.loop [[LOOP10:![0-9]+]]
; WITH:       [[FOR_BODY9]]:
; WITH-NEXT:    [[THREAD_X_010:%.*]] = phi i32 [ [[TMP2]], %[[FOR_BODY9_LR_PH]] ], [ [[ADD18:%.*]], %[[FOR_BODY9]] ]
; WITH-NEXT:    [[CONV12:%.*]] = zext nneg i32 [[THREAD_X_010]] to i64
; WITH-NEXT:    [[ADD13:%.*]] = add nuw nsw i64 [[ADD]], [[CONV12]]
; WITH-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_A_COERCE]], i64 [[ADD13]]
; WITH-NEXT:    [[TMP6:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX]], align 8, !tbaa [[DOUBLE_TBAA12:![0-9]+]]
; WITH-NEXT:    [[ARRAYIDX14:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_B_COERCE]], i64 [[ADD13]]
; WITH-NEXT:    [[TMP7:%.*]] = load double, ptr addrspace(1) [[ARRAYIDX14]], align 8, !tbaa [[DOUBLE_TBAA12]]
; WITH-NEXT:    [[ADD15:%.*]] = fadd contract double [[TMP6]], [[TMP7]]
; WITH-NEXT:    [[ARRAYIDX16:%.*]] = getelementptr inbounds double, ptr addrspace(1) [[D_C_COERCE]], i64 [[ADD13]]
; WITH-NEXT:    store double [[ADD15]], ptr addrspace(1) [[ARRAYIDX16]], align 8, !tbaa [[DOUBLE_TBAA12]]
; WITH-NEXT:    [[ADD18]] = add nuw nsw i32 [[THREAD_X_010]], [[CONV_I_I8]]
; WITH-NEXT:    [[CMP7:%.*]] = icmp samesign ult i32 [[ADD18]], 4
; WITH-NEXT:    br i1 [[CMP7]], label %[[FOR_BODY9]], label %[[FOR_COND_CLEANUP8_LOOPEXIT]], !llvm.loop [[LOOP14:![0-9]+]]
;
entry:
  %0 = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y()
  %cmp11 = icmp samesign ult i32 %0, 4
  br i1 %cmp11, label %for.body.lr.ph, label %for.cond.cleanup

for.body.lr.ph:                                   ; preds = %entry
  %1 = tail call i32 @llvm.amdgcn.workgroup.id.x()
  %2 = tail call noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x()
  %cmp79 = icmp samesign ult i32 %2, 4
  %conv = sext i32 %1 to i64
  %mul = mul nsw i64 %conv, 192
  %3 = tail call ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr()
  %4 = getelementptr inbounds nuw i8, ptr addrspace(4) %3, i64 12
  %5 = getelementptr inbounds nuw i8, ptr addrspace(4) %3, i64 14
  %.in.i.i.i = load i16, ptr addrspace(4) %5, align 2, !tbaa !12
  %conv.i.i = zext i16 %.in.i.i.i to i32
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.cond.cleanup8, %entry
  ret void

for.body:                                         ; preds = %for.cond.cleanup8, %for.body.lr.ph
  %thread_y.012 = phi i32 [ %0, %for.body.lr.ph ], [ %add21, %for.cond.cleanup8 ]
  br i1 %cmp79, label %for.body9.lr.ph, label %for.cond.cleanup8

for.body9.lr.ph:                                  ; preds = %for.body
  %mul10 = shl nuw nsw i32 %thread_y.012, 2
  %conv11 = zext nneg i32 %mul10 to i64
  %add = add nuw nsw i64 %mul, %conv11
  %.in.i.i.i7 = load i16, ptr addrspace(4) %4, align 4, !tbaa !12
  %conv.i.i8 = zext i16 %.in.i.i.i7 to i32
  br label %for.body9

for.cond.cleanup8:                                ; preds = %for.body9, %for.body
  %add21 = add nuw nsw i32 %thread_y.012, %conv.i.i
  %cmp = icmp samesign ult i32 %add21, 4
  br i1 %cmp, label %for.body, label %for.cond.cleanup, !llvm.loop !16

for.body9:                                        ; preds = %for.body9, %for.body9.lr.ph
  %thread_x.010 = phi i32 [ %2, %for.body9.lr.ph ], [ %add18, %for.body9 ]
  %conv12 = zext nneg i32 %thread_x.010 to i64
  %add13 = add nuw nsw i64 %add, %conv12
  %arrayidx = getelementptr inbounds double, ptr addrspace(1) %d_a.coerce, i64 %add13
  %6 = load double, ptr addrspace(1) %arrayidx, align 8, !tbaa !18
  %arrayidx14 = getelementptr inbounds double, ptr addrspace(1) %d_b.coerce, i64 %add13
  %7 = load double, ptr addrspace(1) %arrayidx14, align 8, !tbaa !18
  %add15 = fadd contract double %6, %7
  %arrayidx16 = getelementptr inbounds double, ptr addrspace(1) %d_c.coerce, i64 %add13
  store double %add15, ptr addrspace(1) %arrayidx16, align 8, !tbaa !18
  %add18 = add nuw nsw i32 %thread_x.010, %conv.i.i8
  %cmp7 = icmp samesign ult i32 %add18, 4
  br i1 %cmp7, label %for.body9, label %for.cond.cleanup8, !llvm.loop !20
}

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.x() #1

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare noundef range(i32 0, 1024) i32 @llvm.amdgcn.workitem.id.y() #1

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare noundef align 4 ptr addrspace(4) @llvm.amdgcn.implicitarg.ptr() #1


; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare noundef i32 @llvm.amdgcn.workgroup.id.x() #1

attributes #0 = { mustprogress nofree norecurse nosync nounwind memory(argmem: readwrite) "amdgpu-agpr-alloc"="0" "amdgpu-flat-work-group-size"="1,192" "amdgpu-no-cluster-id-x" "amdgpu-no-cluster-id-y" "amdgpu-no-cluster-id-z" "amdgpu-no-completion-action" "amdgpu-no-default-queue" "amdgpu-no-dispatch-id" "amdgpu-no-dispatch-ptr" "amdgpu-no-flat-scratch-init" "amdgpu-no-heap-ptr" "amdgpu-no-hostcall-ptr" "amdgpu-no-lds-kernel-id" "amdgpu-no-multigrid-sync-arg" "amdgpu-no-queue-ptr" "amdgpu-no-workgroup-id-x" "amdgpu-no-workgroup-id-y" "amdgpu-no-workgroup-id-z" "amdgpu-no-workitem-id-x" "amdgpu-no-workitem-id-z" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="gfx942" "uniform-work-group-size"="true" }
attributes #1 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }

!llvm.module.flags = !{!0}
!llvm.errno.tbaa = !{!7}
!opencl.ocl.version = !{!11}

!0 = !{i32 1, !"amdhsa_code_object_version", i32 600}
!7 = !{!8, !8, i64 0}
!8 = !{!"int", !9, i64 0}
!9 = !{!"omnipotent char", !10, i64 0}
!10 = !{!"Simple C++ TBAA"}
!11 = !{i32 2, i32 0}
!12 = !{!13, !13, i64 0}
!13 = !{!"short", !14, i64 0}
!14 = !{!"omnipotent char", !15, i64 0}
!15 = !{!"Simple C/C++ TBAA"}
!16 = distinct !{!16, !17}
!17 = !{!"llvm.loop.mustprogress"}
!18 = !{!19, !19, i64 0}
!19 = !{!"double", !9, i64 0}
!20 = distinct !{!20, !17}
;.
; DEREF: [[META3:![0-9]+]] = !{!"omnipotent char", [[META4:![0-9]+]], i64 0}
; DEREF: [[META4]] = !{!"Simple C++ TBAA"}
; DEREF: [[SHORT_TBAA6]] = !{[[META7:![0-9]+]], [[META7]], i64 0}
; DEREF: [[META7]] = !{!"short", [[META8:![0-9]+]], i64 0}
; DEREF: [[META8]] = !{!"omnipotent char", [[META9:![0-9]+]], i64 0}
; DEREF: [[META9]] = !{!"Simple C/C++ TBAA"}
; DEREF: [[LOOP10]] = distinct !{[[LOOP10]], [[META11:![0-9]+]]}
; DEREF: [[META11]] = !{!"llvm.loop.mustprogress"}
; DEREF: [[DOUBLE_TBAA12]] = !{[[META13:![0-9]+]], [[META13]], i64 0}
; DEREF: [[META13]] = !{!"double", [[META3]], i64 0}
; DEREF: [[LOOP14]] = distinct !{[[LOOP14]], [[META11]]}
;.
; WITHOUT: [[META3:![0-9]+]] = !{!"omnipotent char", [[META4:![0-9]+]], i64 0}
; WITHOUT: [[META4]] = !{!"Simple C++ TBAA"}
; WITHOUT: [[SHORT_TBAA6]] = !{[[META7:![0-9]+]], [[META7]], i64 0}
; WITHOUT: [[META7]] = !{!"short", [[META8:![0-9]+]], i64 0}
; WITHOUT: [[META8]] = !{!"omnipotent char", [[META9:![0-9]+]], i64 0}
; WITHOUT: [[META9]] = !{!"Simple C/C++ TBAA"}
; WITHOUT: [[LOOP10]] = distinct !{[[LOOP10]], [[META11:![0-9]+]]}
; WITHOUT: [[META11]] = !{!"llvm.loop.mustprogress"}
; WITHOUT: [[DOUBLE_TBAA12]] = !{[[META13:![0-9]+]], [[META13]], i64 0}
; WITHOUT: [[META13]] = !{!"double", [[META3]], i64 0}
; WITHOUT: [[LOOP14]] = distinct !{[[LOOP14]], [[META11]]}
;.
; WITH: [[META3:![0-9]+]] = !{!"omnipotent char", [[META4:![0-9]+]], i64 0}
; WITH: [[META4]] = !{!"Simple C++ TBAA"}
; WITH: [[SHORT_TBAA6]] = !{[[META7:![0-9]+]], [[META7]], i64 0}
; WITH: [[META7]] = !{!"short", [[META8:![0-9]+]], i64 0}
; WITH: [[META8]] = !{!"omnipotent char", [[META9:![0-9]+]], i64 0}
; WITH: [[META9]] = !{!"Simple C/C++ TBAA"}
; WITH: [[LOOP10]] = distinct !{[[LOOP10]], [[META11:![0-9]+]]}
; WITH: [[META11]] = !{!"llvm.loop.mustprogress"}
; WITH: [[DOUBLE_TBAA12]] = !{[[META13:![0-9]+]], [[META13]], i64 0}
; WITH: [[META13]] = !{!"double", [[META3]], i64 0}
; WITH: [[LOOP14]] = distinct !{[[LOOP14]], [[META11]]}
;.
