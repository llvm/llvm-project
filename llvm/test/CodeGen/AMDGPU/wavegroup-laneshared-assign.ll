; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn -mcpu=gfx1300 < %s | FileCheck %s

@g1 = external addrspace(10) global i32
@g2 = external addrspace(10) global i32

define i32 @f1() {
; CHECK-LABEL: f1:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_set_gpr_idx_u32 idx1, 0
; CHECK-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; CHECK-NEXT:    v_mov_b32_e32 v0, g1[0]
; CHECK-NEXT:    s_set_vgpr_frames 0 ; vsrc0_idx=0 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
  %val = load i32, ptr addrspace(10) @g1
  ret i32 %val
}

define i32 @f2() {
; CHECK-LABEL: f2:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    s_wait_expcnt 0x0
; CHECK-NEXT:    s_wait_samplecnt 0x0
; CHECK-NEXT:    s_wait_rtscnt 0x0
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    s_set_gpr_idx_u32 idx1, 0
; CHECK-NEXT:    s_set_vgpr_frames 1 ; vsrc0_idx=1 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; CHECK-NEXT:    v_mov_b32_e32 v0, g1[1]
; CHECK-NEXT:    s_set_vgpr_frames 0 ; vsrc0_idx=0 vsrc1_idx=0 vsrc2_idx=0 vdst_idx=0 vsrc0_msb=0 vsrc1_msb=0 vsrc2_msb=0 vdst_msb=0
; CHECK-NEXT:    s_set_pc_i64 s[30:31]
  %val = load i32, ptr addrspace(10) @g2
  ret i32 %val
}

@table = internal global [2 x ptr] [ptr @f1, ptr @f2]

; Kernel indirectly calls functions that use lane-shared variables.
define amdgpu_kernel void @kernel(i32 %idx, ptr %out) "amdgpu-wavegroup-enable" !reqd_work_group_size !{i32 32, i32 8, i32 1} {
; CHECK-LABEL: kernel:
; CHECK:       ; %bb.0:
; CHECK-NEXT:    s_getreg_b32 s9, hwreg(HW_REG_WAVE_GROUP_INFO, 16, 4)
; CHECK-NEXT:    s_delay_alu instid0(SALU_CYCLE_1)
; CHECK-NEXT:    s_mul_i32 s10, s9, max(41, amdgpu.max_num_vgpr)
; CHECK-NEXT:    s_mul_i32 s33, s9, s8
; CHECK-NEXT:    s_add_co_u32 s10, s10, 2
; CHECK-NEXT:    s_add_co_u32 s32, s33, 0
; CHECK-NEXT:    s_set_gpr_idx_u32 idx0, s10
; CHECK-NEXT:    ; sched_barrier mask(0x00000000)
; CHECK-NEXT:    s_mov_b64 s[36:37], s[2:3]
; CHECK-NEXT:    s_clause 0x1
; CHECK-NEXT:    s_load_b32 s2, s[4:5], 0x24
; CHECK-NEXT:    s_load_b64 s[50:51], s[4:5], 0x2c
; CHECK-NEXT:    v_mov_b32_e32 v31, v0
; CHECK-NEXT:    s_mov_b64 s[38:39], s[0:1]
; CHECK-NEXT:    s_get_pc_i64 s[0:1]
; CHECK-NEXT:    s_add_nc_u64 s[0:1], s[0:1], table@rel64+4
; CHECK-NEXT:    v_mov_b32_e32 v40, 0
; CHECK-NEXT:    s_mov_b64 s[34:35], s[6:7]
; CHECK-NEXT:    s_add_nc_u64 s[48:49], s[4:5], 52
; CHECK-NEXT:    s_mov_b32 s52, exec_lo
; CHECK-NEXT:    s_wait_kmcnt 0x0
; CHECK-NEXT:    v_mov_b32_e32 v0, s2
; CHECK-NEXT:    flat_load_b64 v[1:2], v0, s[0:1] scale_offset
; CHECK-NEXT:  .LBB2_1: ; =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    s_wait_loadcnt_dscnt 0x0
; CHECK-NEXT:    v_readfirstlane_b32 s0, v1
; CHECK-NEXT:    v_readfirstlane_b32 s1, v2
; CHECK-NEXT:    s_mov_b32 s53, exec_lo
; CHECK-NEXT:    v_cmpx_eq_u64_e64 s[0:1], v[1:2]
; CHECK-NEXT:    s_mov_b64 s[4:5], s[38:39]
; CHECK-NEXT:    s_mov_b64 s[6:7], s[36:37]
; CHECK-NEXT:    s_mov_b64 s[8:9], s[48:49]
; CHECK-NEXT:    s_mov_b64 s[10:11], s[34:35]
; CHECK-NEXT:    s_swap_pc_i64 s[30:31], s[0:1]
; CHECK-NEXT:    ; implicit-def: $vgpr1_vgpr2
; CHECK-NEXT:    ; implicit-def: $vgpr31
; CHECK-NEXT:    s_xor_b32 exec_lo, exec_lo, s53
; CHECK-NEXT:    s_cbranch_execnz .LBB2_1
; CHECK-NEXT:  ; %bb.2:
; CHECK-NEXT:    s_mov_b32 exec_lo, s52
; CHECK-NEXT:    flat_store_b32 v40, v0, s[50:51] scope:SCOPE_SE
; CHECK-NEXT:    s_endpgm
  %ptr = getelementptr ptr, ptr @table, i32 %idx
  %f = load ptr, ptr %ptr
  %val = call i32 %f()
  store i32 %val, ptr %out
  ret void
}
