; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 4
; RUN: llc -march=amdgcn -mcpu=gfx1250 -verify-machineinstrs < %s | FileCheck %s --check-prefix=GFX1250
; RUN: llc -march=amdgcn -mcpu=gfx1250 -global-isel -verify-machineinstrs < %s | FileCheck %s --check-prefix=GISEL

define amdgpu_ps void @test_swmmac_f32_16x16x64_bf16(<16 x bfloat> %A, <32 x bfloat> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f32_16x16x64_bf16:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f32_16x16x64_bf16 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f32_16x16x64_bf16:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_xcnt 0x0
; GISEL-NEXT:    v_dual_lshrrev_b32 v33, 16, v0 :: v_dual_lshrrev_b32 v36, 16, v1
; GISEL-NEXT:    v_dual_lshrrev_b32 v37, 16, v2 :: v_dual_lshrrev_b32 v38, 16, v3
; GISEL-NEXT:    v_dual_lshrrev_b32 v39, 16, v4 :: v_dual_lshrrev_b32 v40, 16, v5
; GISEL-NEXT:    v_dual_lshrrev_b32 v41, 16, v6 :: v_dual_lshrrev_b32 v42, 16, v7
; GISEL-NEXT:    v_and_b32_e32 v1, 0xffff, v1
; GISEL-NEXT:    v_and_b32_e32 v3, 0xffff, v3
; GISEL-NEXT:    v_and_b32_e32 v5, 0xffff, v5
; GISEL-NEXT:    v_dual_lshrrev_b32 v43, 16, v8 :: v_dual_lshrrev_b32 v44, 16, v9
; GISEL-NEXT:    v_dual_lshrrev_b32 v45, 16, v10 :: v_dual_lshrrev_b32 v46, 16, v11
; GISEL-NEXT:    v_dual_lshlrev_b32 v54, 16, v36 :: v_dual_lshlrev_b32 v55, 16, v37
; GISEL-NEXT:    v_dual_lshlrev_b32 v56, 16, v38 :: v_dual_lshlrev_b32 v57, 16, v39
; GISEL-NEXT:    v_dual_lshlrev_b32 v58, 16, v40 :: v_dual_lshlrev_b32 v59, 16, v41
; GISEL-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; GISEL-NEXT:    v_and_b32_e32 v2, 0xffff, v2
; GISEL-NEXT:    v_and_b32_e32 v4, 0xffff, v4
; GISEL-NEXT:    v_and_b32_e32 v6, 0xffff, v6
; GISEL-NEXT:    v_and_b32_e32 v7, 0xffff, v7
; GISEL-NEXT:    v_dual_lshrrev_b32 v53, 16, v18 :: v_dual_lshlrev_b32 v33, 16, v33
; GISEL-NEXT:    v_and_b32_e32 v8, 0xffff, v8
; GISEL-NEXT:    v_dual_lshlrev_b32 v60, 16, v42 :: v_dual_lshlrev_b32 v61, 16, v43
; GISEL-NEXT:    v_or_b32_e32 v37, v54, v1
; GISEL-NEXT:    v_or_b32_e32 v39, v56, v3
; GISEL-NEXT:    v_or_b32_e32 v41, v58, v5
; GISEL-NEXT:    v_and_b32_e32 v3, 0xffff, v11
; GISEL-NEXT:    v_dual_lshlrev_b32 v5, 16, v46 :: v_dual_lshrrev_b32 v47, 16, v12
; GISEL-NEXT:    v_dual_lshrrev_b32 v48, 16, v13 :: v_dual_lshrrev_b32 v49, 16, v14
; GISEL-NEXT:    v_dual_lshrrev_b32 v50, 16, v15 :: v_dual_lshrrev_b32 v51, 16, v16
; GISEL-NEXT:    v_lshrrev_b32_e32 v52, 16, v17
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v9
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v10
; GISEL-NEXT:    v_dual_lshlrev_b32 v44, 16, v44 :: v_dual_lshlrev_b32 v45, 16, v45
; GISEL-NEXT:    v_or_b32_e32 v36, v33, v0
; GISEL-NEXT:    v_or_b32_e32 v38, v55, v2
; GISEL-NEXT:    v_or_b32_e32 v40, v57, v4
; GISEL-NEXT:    v_or_b32_e32 v42, v59, v6
; GISEL-NEXT:    v_or_b32_e32 v43, v60, v7
; GISEL-NEXT:    v_dual_lshrrev_b32 v4, 16, v19 :: v_dual_bitop2_b32 v0, v61, v8 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v6, 16, v20 :: v_dual_lshrrev_b32 v7, 16, v21
; GISEL-NEXT:    v_dual_lshrrev_b32 v8, 16, v22 :: v_dual_bitop2_b32 v3, v5, v3 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v5, 16, v23 :: v_dual_bitop2_b32 v1, v44, v9 bitop3:0x54
; GISEL-NEXT:    v_lshlrev_b32_e32 v33, 16, v50
; GISEL-NEXT:    v_dual_lshlrev_b32 v44, 16, v51 :: v_dual_bitop2_b32 v2, v45, v10 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v12
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v13
; GISEL-NEXT:    v_and_b32_e32 v11, 0xffff, v14
; GISEL-NEXT:    v_and_b32_e32 v12, 0xffff, v15
; GISEL-NEXT:    v_and_b32_e32 v13, 0xffff, v16
; GISEL-NEXT:    v_and_b32_e32 v14, 0xffff, v17
; GISEL-NEXT:    v_and_b32_e32 v15, 0xffff, v18
; GISEL-NEXT:    v_and_b32_e32 v16, 0xffff, v19
; GISEL-NEXT:    v_and_b32_e32 v17, 0xffff, v20
; GISEL-NEXT:    v_and_b32_e32 v18, 0xffff, v21
; GISEL-NEXT:    v_and_b32_e32 v19, 0xffff, v22
; GISEL-NEXT:    v_and_b32_e32 v20, 0xffff, v23
; GISEL-NEXT:    v_dual_lshlrev_b32 v21, 16, v47 :: v_dual_lshlrev_b32 v22, 16, v48
; GISEL-NEXT:    v_dual_lshlrev_b32 v23, 16, v49 :: v_dual_lshlrev_b32 v45, 16, v52
; GISEL-NEXT:    v_dual_lshlrev_b32 v46, 16, v53 :: v_dual_lshlrev_b32 v47, 16, v4
; GISEL-NEXT:    v_dual_lshlrev_b32 v48, 16, v6 :: v_dual_lshlrev_b32 v49, 16, v7
; GISEL-NEXT:    v_dual_lshlrev_b32 v50, 16, v8 :: v_dual_lshlrev_b32 v51, 16, v5
; GISEL-NEXT:    v_or_b32_e32 v4, v21, v9
; GISEL-NEXT:    v_or_b32_e32 v5, v22, v10
; GISEL-NEXT:    v_or_b32_e32 v6, v23, v11
; GISEL-NEXT:    v_or_b32_e32 v7, v33, v12
; GISEL-NEXT:    v_or_b32_e32 v8, v44, v13
; GISEL-NEXT:    v_or_b32_e32 v9, v45, v14
; GISEL-NEXT:    v_or_b32_e32 v10, v46, v15
; GISEL-NEXT:    v_or_b32_e32 v11, v47, v16
; GISEL-NEXT:    v_or_b32_e32 v12, v48, v17
; GISEL-NEXT:    v_or_b32_e32 v13, v49, v18
; GISEL-NEXT:    v_or_b32_e32 v14, v50, v19
; GISEL-NEXT:    v_or_b32_e32 v15, v51, v20
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GISEL-NEXT:    v_swmmac_f32_16x16x64_bf16 v[24:31], v[36:43], v[0:15], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.f32.16x16x64.bf16.v8f32.v16bf16.v32bf16.i16(i1 0, <16 x bfloat> %A, i1 0, <32 x bfloat> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_bf16_16x16x64_bf16(<16 x bfloat> %A, <32 x bfloat> %B, <8 x bfloat> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_bf16_16x16x64_bf16:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v28, v[28:29], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_bf16_16x16x64_bf16 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GFX1250-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_bf16_16x16x64_bf16:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v28, v[28:29], off
; GISEL-NEXT:    s_wait_xcnt 0x0
; GISEL-NEXT:    v_dual_lshrrev_b32 v29, 16, v0 :: v_dual_lshrrev_b32 v32, 16, v1
; GISEL-NEXT:    v_dual_lshrrev_b32 v33, 16, v2 :: v_dual_lshrrev_b32 v35, 16, v3
; GISEL-NEXT:    v_dual_lshrrev_b32 v36, 16, v4 :: v_dual_lshrrev_b32 v37, 16, v5
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_3) | instskip(SKIP_4) | instid1(VALU_DEP_4)
; GISEL-NEXT:    v_dual_lshlrev_b32 v29, 16, v29 :: v_dual_lshlrev_b32 v34, 16, v32
; GISEL-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; GISEL-NEXT:    v_and_b32_e32 v1, 0xffff, v1
; GISEL-NEXT:    v_dual_lshlrev_b32 v38, 16, v33 :: v_dual_lshrrev_b32 v39, 16, v6
; GISEL-NEXT:    v_and_b32_e32 v2, 0xffff, v2
; GISEL-NEXT:    v_dual_lshrrev_b32 v40, 16, v7 :: v_dual_bitop2_b32 v32, v29, v0 bitop3:0x54
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(NEXT) | instid1(VALU_DEP_3)
; GISEL-NEXT:    v_dual_lshlrev_b32 v0, 16, v35 :: v_dual_bitop2_b32 v33, v34, v1 bitop3:0x54
; GISEL-NEXT:    v_dual_lshlrev_b32 v2, 16, v36 :: v_dual_bitop2_b32 v34, v38, v2 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v1, 0xffff, v3
; GISEL-NEXT:    v_and_b32_e32 v3, 0xffff, v4
; GISEL-NEXT:    v_dual_lshlrev_b32 v4, 16, v37 :: v_dual_lshlrev_b32 v29, 16, v39
; GISEL-NEXT:    v_and_b32_e32 v5, 0xffff, v5
; GISEL-NEXT:    v_and_b32_e32 v6, 0xffff, v6
; GISEL-NEXT:    v_dual_lshlrev_b32 v39, 16, v40 :: v_dual_bitop2_b32 v35, v0, v1 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v7, 0xffff, v7
; GISEL-NEXT:    v_dual_lshrrev_b32 v0, 16, v8 :: v_dual_bitop2_b32 v36, v2, v3 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v2, 16, v10 :: v_dual_bitop2_b32 v37, v4, v5 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v1, 16, v9 :: v_dual_bitop2_b32 v38, v29, v6 bitop3:0x54
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_4)
; GISEL-NEXT:    v_dual_lshrrev_b32 v4, 16, v12 :: v_dual_bitop2_b32 v39, v39, v7 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v3, 16, v11 :: v_dual_lshrrev_b32 v5, 16, v13
; GISEL-NEXT:    v_dual_lshrrev_b32 v41, 16, v18 :: v_dual_lshlrev_b32 v0, 16, v0
; GISEL-NEXT:    v_and_b32_e32 v8, 0xffff, v8
; GISEL-NEXT:    v_dual_lshrrev_b32 v6, 16, v14 :: v_dual_lshrrev_b32 v7, 16, v15
; GISEL-NEXT:    v_dual_lshrrev_b32 v29, 16, v16 :: v_dual_lshrrev_b32 v40, 16, v17
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_3)
; GISEL-NEXT:    v_dual_lshlrev_b32 v1, 16, v1 :: v_dual_bitop2_b32 v0, v0, v8 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v8, 0xffff, v9
; GISEL-NEXT:    v_dual_lshlrev_b32 v2, 16, v2 :: v_dual_lshlrev_b32 v3, 16, v3
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v10
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v11
; GISEL-NEXT:    v_dual_lshlrev_b32 v4, 16, v4 :: v_dual_lshrrev_b32 v46, 16, v23
; GISEL-NEXT:    v_and_b32_e32 v11, 0xffff, v12
; GISEL-NEXT:    v_dual_lshrrev_b32 v42, 16, v19 :: v_dual_lshrrev_b32 v43, 16, v20
; GISEL-NEXT:    v_dual_lshrrev_b32 v44, 16, v21 :: v_dual_lshrrev_b32 v45, 16, v22
; GISEL-NEXT:    v_dual_lshlrev_b32 v5, 16, v5 :: v_dual_bitop2_b32 v1, v1, v8 bitop3:0x54
; GISEL-NEXT:    v_dual_lshlrev_b32 v6, 16, v6 :: v_dual_bitop2_b32 v2, v2, v9 bitop3:0x54
; GISEL-NEXT:    v_dual_lshlrev_b32 v7, 16, v7 :: v_dual_bitop2_b32 v3, v3, v10 bitop3:0x54
; GISEL-NEXT:    v_dual_lshlrev_b32 v11, 16, v29 :: v_dual_bitop2_b32 v4, v4, v11 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v8, 0xffff, v13
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v14
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v15
; GISEL-NEXT:    v_and_b32_e32 v12, 0xffff, v16
; GISEL-NEXT:    v_lshlrev_b32_e32 v13, 16, v40
; GISEL-NEXT:    v_and_b32_e32 v14, 0xffff, v17
; GISEL-NEXT:    v_or_b32_e32 v6, v6, v9
; GISEL-NEXT:    v_or_b32_e32 v5, v5, v8
; GISEL-NEXT:    v_or_b32_e32 v7, v7, v10
; GISEL-NEXT:    v_dual_lshlrev_b32 v12, 16, v42 :: v_dual_bitop2_b32 v8, v11, v12 bitop3:0x54
; GISEL-NEXT:    v_dual_lshlrev_b32 v16, 16, v44 :: v_dual_bitop2_b32 v9, v13, v14 bitop3:0x54
; GISEL-NEXT:    v_lshlrev_b32_e32 v10, 16, v41
; GISEL-NEXT:    v_and_b32_e32 v11, 0xffff, v18
; GISEL-NEXT:    v_and_b32_e32 v13, 0xffff, v19
; GISEL-NEXT:    v_lshlrev_b32_e32 v14, 16, v43
; GISEL-NEXT:    v_and_b32_e32 v15, 0xffff, v20
; GISEL-NEXT:    v_and_b32_e32 v17, 0xffff, v21
; GISEL-NEXT:    v_dual_lshlrev_b32 v18, 16, v45 :: v_dual_bitop2_b32 v10, v10, v11 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v19, 0xffff, v22
; GISEL-NEXT:    v_or_b32_e32 v11, v12, v13
; GISEL-NEXT:    v_or_b32_e32 v12, v14, v15
; GISEL-NEXT:    v_dual_lshrrev_b32 v16, 16, v24 :: v_dual_bitop2_b32 v13, v16, v17 bitop3:0x54
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(SKIP_1) | instid1(VALU_DEP_3)
; GISEL-NEXT:    v_dual_lshrrev_b32 v18, 16, v25 :: v_dual_bitop2_b32 v14, v18, v19 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v19, 16, v26 :: v_dual_lshrrev_b32 v20, 16, v27
; GISEL-NEXT:    v_dual_lshlrev_b32 v16, 16, v16 :: v_dual_lshlrev_b32 v15, 16, v46
; GISEL-NEXT:    v_and_b32_e32 v17, 0xffff, v23
; GISEL-NEXT:    v_and_b32_e32 v21, 0xffff, v24
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_4)
; GISEL-NEXT:    v_dual_lshlrev_b32 v18, 16, v18 :: v_dual_lshlrev_b32 v19, 16, v19
; GISEL-NEXT:    v_and_b32_e32 v22, 0xffff, v25
; GISEL-NEXT:    v_and_b32_e32 v23, 0xffff, v26
; GISEL-NEXT:    v_dual_lshlrev_b32 v20, 16, v20 :: v_dual_bitop2_b32 v15, v15, v17 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v24, 0xffff, v27
; GISEL-NEXT:    v_or_b32_e32 v16, v16, v21
; GISEL-NEXT:    v_or_b32_e32 v17, v18, v22
; GISEL-NEXT:    v_or_b32_e32 v18, v19, v23
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_4) | instskip(SKIP_1) | instid1(VALU_DEP_1)
; GISEL-NEXT:    v_or_b32_e32 v19, v20, v24
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_bf16_16x16x64_bf16 v[16:19], v[32:39], v[0:15], v28 index_key:1
; GISEL-NEXT:    global_store_b128 v[30:31], v[16:19], off
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x bfloat> @llvm.amdgcn.swmmac.bf16.16x16x64.bf16.v8bf16.v16bf16.v32bf16.i16(i1 0, <16 x bfloat> %A, i1 0, <32 x bfloat> %B, <8 x bfloat> %C, i16 %Index)
  store <8 x bfloat> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_bf16f32_16x16x64_bf16(<16 x bfloat> %A, <32 x bfloat> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_bf16f32_16x16x64_bf16:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_bf16f32_16x16x64_bf16 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_bf16f32_16x16x64_bf16:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_xcnt 0x0
; GISEL-NEXT:    v_dual_lshrrev_b32 v33, 16, v0 :: v_dual_lshrrev_b32 v36, 16, v1
; GISEL-NEXT:    v_dual_lshrrev_b32 v37, 16, v2 :: v_dual_lshrrev_b32 v38, 16, v3
; GISEL-NEXT:    v_dual_lshrrev_b32 v39, 16, v4 :: v_dual_lshrrev_b32 v40, 16, v5
; GISEL-NEXT:    v_dual_lshrrev_b32 v41, 16, v6 :: v_dual_lshrrev_b32 v42, 16, v7
; GISEL-NEXT:    v_and_b32_e32 v1, 0xffff, v1
; GISEL-NEXT:    v_and_b32_e32 v3, 0xffff, v3
; GISEL-NEXT:    v_and_b32_e32 v5, 0xffff, v5
; GISEL-NEXT:    v_dual_lshrrev_b32 v43, 16, v8 :: v_dual_lshrrev_b32 v44, 16, v9
; GISEL-NEXT:    v_dual_lshrrev_b32 v45, 16, v10 :: v_dual_lshrrev_b32 v46, 16, v11
; GISEL-NEXT:    v_dual_lshlrev_b32 v54, 16, v36 :: v_dual_lshlrev_b32 v55, 16, v37
; GISEL-NEXT:    v_dual_lshlrev_b32 v56, 16, v38 :: v_dual_lshlrev_b32 v57, 16, v39
; GISEL-NEXT:    v_dual_lshlrev_b32 v58, 16, v40 :: v_dual_lshlrev_b32 v59, 16, v41
; GISEL-NEXT:    v_and_b32_e32 v0, 0xffff, v0
; GISEL-NEXT:    v_and_b32_e32 v2, 0xffff, v2
; GISEL-NEXT:    v_and_b32_e32 v4, 0xffff, v4
; GISEL-NEXT:    v_and_b32_e32 v6, 0xffff, v6
; GISEL-NEXT:    v_and_b32_e32 v7, 0xffff, v7
; GISEL-NEXT:    v_dual_lshrrev_b32 v53, 16, v18 :: v_dual_lshlrev_b32 v33, 16, v33
; GISEL-NEXT:    v_and_b32_e32 v8, 0xffff, v8
; GISEL-NEXT:    v_dual_lshlrev_b32 v60, 16, v42 :: v_dual_lshlrev_b32 v61, 16, v43
; GISEL-NEXT:    v_or_b32_e32 v37, v54, v1
; GISEL-NEXT:    v_or_b32_e32 v39, v56, v3
; GISEL-NEXT:    v_or_b32_e32 v41, v58, v5
; GISEL-NEXT:    v_and_b32_e32 v3, 0xffff, v11
; GISEL-NEXT:    v_dual_lshlrev_b32 v5, 16, v46 :: v_dual_lshrrev_b32 v47, 16, v12
; GISEL-NEXT:    v_dual_lshrrev_b32 v48, 16, v13 :: v_dual_lshrrev_b32 v49, 16, v14
; GISEL-NEXT:    v_dual_lshrrev_b32 v50, 16, v15 :: v_dual_lshrrev_b32 v51, 16, v16
; GISEL-NEXT:    v_lshrrev_b32_e32 v52, 16, v17
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v9
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v10
; GISEL-NEXT:    v_dual_lshlrev_b32 v44, 16, v44 :: v_dual_lshlrev_b32 v45, 16, v45
; GISEL-NEXT:    v_or_b32_e32 v36, v33, v0
; GISEL-NEXT:    v_or_b32_e32 v38, v55, v2
; GISEL-NEXT:    v_or_b32_e32 v40, v57, v4
; GISEL-NEXT:    v_or_b32_e32 v42, v59, v6
; GISEL-NEXT:    v_or_b32_e32 v43, v60, v7
; GISEL-NEXT:    v_dual_lshrrev_b32 v4, 16, v19 :: v_dual_bitop2_b32 v0, v61, v8 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v6, 16, v20 :: v_dual_lshrrev_b32 v7, 16, v21
; GISEL-NEXT:    v_dual_lshrrev_b32 v8, 16, v22 :: v_dual_bitop2_b32 v3, v5, v3 bitop3:0x54
; GISEL-NEXT:    v_dual_lshrrev_b32 v5, 16, v23 :: v_dual_bitop2_b32 v1, v44, v9 bitop3:0x54
; GISEL-NEXT:    v_lshlrev_b32_e32 v33, 16, v50
; GISEL-NEXT:    v_dual_lshlrev_b32 v44, 16, v51 :: v_dual_bitop2_b32 v2, v45, v10 bitop3:0x54
; GISEL-NEXT:    v_and_b32_e32 v9, 0xffff, v12
; GISEL-NEXT:    v_and_b32_e32 v10, 0xffff, v13
; GISEL-NEXT:    v_and_b32_e32 v11, 0xffff, v14
; GISEL-NEXT:    v_and_b32_e32 v12, 0xffff, v15
; GISEL-NEXT:    v_and_b32_e32 v13, 0xffff, v16
; GISEL-NEXT:    v_and_b32_e32 v14, 0xffff, v17
; GISEL-NEXT:    v_and_b32_e32 v15, 0xffff, v18
; GISEL-NEXT:    v_and_b32_e32 v16, 0xffff, v19
; GISEL-NEXT:    v_and_b32_e32 v17, 0xffff, v20
; GISEL-NEXT:    v_and_b32_e32 v18, 0xffff, v21
; GISEL-NEXT:    v_and_b32_e32 v19, 0xffff, v22
; GISEL-NEXT:    v_and_b32_e32 v20, 0xffff, v23
; GISEL-NEXT:    v_dual_lshlrev_b32 v21, 16, v47 :: v_dual_lshlrev_b32 v22, 16, v48
; GISEL-NEXT:    v_dual_lshlrev_b32 v23, 16, v49 :: v_dual_lshlrev_b32 v45, 16, v52
; GISEL-NEXT:    v_dual_lshlrev_b32 v46, 16, v53 :: v_dual_lshlrev_b32 v47, 16, v4
; GISEL-NEXT:    v_dual_lshlrev_b32 v48, 16, v6 :: v_dual_lshlrev_b32 v49, 16, v7
; GISEL-NEXT:    v_dual_lshlrev_b32 v50, 16, v8 :: v_dual_lshlrev_b32 v51, 16, v5
; GISEL-NEXT:    v_or_b32_e32 v4, v21, v9
; GISEL-NEXT:    v_or_b32_e32 v5, v22, v10
; GISEL-NEXT:    v_or_b32_e32 v6, v23, v11
; GISEL-NEXT:    v_or_b32_e32 v7, v33, v12
; GISEL-NEXT:    v_or_b32_e32 v8, v44, v13
; GISEL-NEXT:    v_or_b32_e32 v9, v45, v14
; GISEL-NEXT:    v_or_b32_e32 v10, v46, v15
; GISEL-NEXT:    v_or_b32_e32 v11, v47, v16
; GISEL-NEXT:    v_or_b32_e32 v12, v48, v17
; GISEL-NEXT:    v_or_b32_e32 v13, v49, v18
; GISEL-NEXT:    v_or_b32_e32 v14, v50, v19
; GISEL-NEXT:    v_or_b32_e32 v15, v51, v20
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    s_delay_alu instid0(VALU_DEP_1)
; GISEL-NEXT:    v_swmmac_bf16f32_16x16x64_bf16 v[24:31], v[36:43], v[0:15], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.bf16f32.16x16x64.bf16.v8f32.v16bf16.v32bf16.i16(i1 0, <16 x bfloat> %A, i1 0, <32 x bfloat> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f32_16x16x128_fp8_fp8(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f32_16x16x128_fp8_fp8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f32_16x16x128_fp8_fp8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f32_16x16x128_fp8_fp8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f32_16x16x128_fp8_fp8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.fp8.fp8.v8f32.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f32_16x16x128_fp8_bf8(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f32_16x16x128_fp8_bf8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f32_16x16x128_fp8_bf8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f32_16x16x128_fp8_bf8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f32_16x16x128_fp8_bf8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.fp8.bf8.v8f32.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f32_16x16x128_bf8_fp8(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f32_16x16x128_bf8_fp8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f32_16x16x128_bf8_fp8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f32_16x16x128_bf8_fp8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f32_16x16x128_bf8_fp8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.bf8.fp8.v8f32.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f32_16x16x128_bf8_bf8(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f32_16x16x128_bf8_bf8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f32_16x16x128_bf8_bf8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f32_16x16x128_bf8_bf8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f32_16x16x128_bf8_bf8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.bf8.bf8.v8f32.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x float> %C, i16 %Index)
  store <8 x float> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f16_16x16x128_fp8_fp8(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f16_16x16x128_fp8_fp8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v28, v[28:29], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f16_16x16x128_fp8_fp8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GFX1250-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f16_16x16x128_fp8_fp8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v28, v[28:29], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f16_16x16x128_fp8_fp8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GISEL-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.fp8.v8f16.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, i16 %Index)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f16_16x16x128_fp8_bf8(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f16_16x16x128_fp8_bf8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v28, v[28:29], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f16_16x16x128_fp8_bf8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GFX1250-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f16_16x16x128_fp8_bf8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v28, v[28:29], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f16_16x16x128_fp8_bf8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GISEL-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.bf8.v8f16.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, i16 %Index)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f16_16x16x128_bf8_fp8(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f16_16x16x128_bf8_fp8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v28, v[28:29], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f16_16x16x128_bf8_fp8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GFX1250-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f16_16x16x128_bf8_fp8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v28, v[28:29], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f16_16x16x128_bf8_fp8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GISEL-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.fp8.v8f16.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, i16 %Index)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_f16_16x16x128_bf8_bf8(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_f16_16x16x128_bf8_bf8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v28, v[28:29], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_f16_16x16x128_bf8_bf8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GFX1250-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_f16_16x16x128_bf8_bf8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v28, v[28:29], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_f16_16x16x128_bf8_bf8 v[24:27], v[0:7], v[8:23], v28 index_key:1
; GISEL-NEXT:    global_store_b128 v[30:31], v[24:27], off
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.bf8.v8f16.v8i32.v16i32.i16(<8 x i32> %A, <16 x i32> %B, <8 x half> %C, i16 %Index)
  store <8 x half> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_i32_16x16x128_iu8(<8 x i32> %A, <16 x i32> %B, <8 x i32> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_i32_16x16x128_iu8:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_i32_16x16x128_iu8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_i32_16x16x128_iu8:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_i32_16x16x128_iu8 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x i32> @llvm.amdgcn.swmmac.i32.16x16x128.iu8.v8i32.v8i32.v16i32.i16(i1 0, <8 x i32> %A, i1 0, <16 x i32> %B, <8 x i32> %C, i16 %Index)
  store <8 x i32> %res, ptr addrspace(1) %out
  ret void
}

define amdgpu_ps void @test_swmmac_i32_16x16x256_iu4(<8 x i32> %A, <16 x i32> %B, <8 x i32> %C, ptr addrspace(1) %IndexVecPtr, ptr addrspace(1) %out) {
; GFX1250-LABEL: test_swmmac_i32_16x16x256_iu4:
; GFX1250:       ; %bb.0: ; %bb
; GFX1250-NEXT:    global_load_b32 v32, v[32:33], off
; GFX1250-NEXT:    s_wait_loadcnt 0x0
; GFX1250-NEXT:    v_swmmac_i32_16x16x256_iu4 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GFX1250-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GFX1250-NEXT:    s_endpgm
;
; GISEL-LABEL: test_swmmac_i32_16x16x256_iu4:
; GISEL:       ; %bb.0: ; %bb
; GISEL-NEXT:    global_load_b32 v32, v[32:33], off
; GISEL-NEXT:    s_wait_loadcnt 0x0
; GISEL-NEXT:    v_swmmac_i32_16x16x256_iu4 v[24:31], v[0:7], v[8:23], v32 index_key:1
; GISEL-NEXT:    s_clause 0x1
; GISEL-NEXT:    global_store_b128 v[34:35], v[24:27], off
; GISEL-NEXT:    global_store_b128 v[34:35], v[28:31], off offset:16
; GISEL-NEXT:    s_endpgm
bb:
  %IndexVecPacked = load i32, ptr addrspace(1) %IndexVecPtr, align 4
  %IndexVec = bitcast i32 %IndexVecPacked to <2 x i16>
  %Index = extractelement <2 x i16> %IndexVec, i32 1
  %res = call <8 x i32> @llvm.amdgcn.swmmac.i32.16x16x256.iu4.v8i32.v8i32.v16i32.i16(i1 0, <8 x i32> %A, i1 0, <16 x i32> %B, <8 x i32> %C, i16 %Index)
  store <8 x i32> %res, ptr addrspace(1) %out
  ret void
}

declare <8 x float> @llvm.amdgcn.swmmac.f32.16x16x64.bf16.v8f32.v16bf16.v32bf16.i16(i1, <16 x bfloat>, i1, <32 x bfloat>, <8 x float>, i16)
declare <8 x bfloat> @llvm.amdgcn.swmmac.bf16.16x16x64.bf16.v8bf16.v16bf16.v32bf16.i16(i1, <16 x bfloat>, i1, <32 x bfloat>, <8 x bfloat>, i16)
declare <8 x float> @llvm.amdgcn.swmmac.bf16f32.16x16x64.bf16.v8f32.v16bf16.v32bf16.i16(i1, <16 x bfloat>, i1, <32 x bfloat>, <8 x float>, i16)
declare <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.fp8.fp8.v8f32.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x float>, i16)
declare <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.fp8.bf8.v8f32.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x float>, i16)
declare <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.bf8.fp8.v8f32.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x float>, i16)
declare <8 x float> @llvm.amdgcn.swmmac.f32.16x16x128.bf8.bf8.v8f32.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x float>, i16)
declare <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.fp8.v8f16.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x half>, i16)
declare <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.fp8.bf8.v8f16.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x half>, i16)
declare <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.fp8.v8f16.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x half>, i16)
declare <8 x half> @llvm.amdgcn.swmmac.f16.16x16x128.bf8.bf8.v8f16.v8i32.v16i32.i16(<8 x i32>, <16 x i32>, <8 x half>, i16)
declare <8 x i32> @llvm.amdgcn.swmmac.i32.16x16x128.iu8.v8i32.v8i32.v16i32.i16(i1 immarg, <8 x i32>, i1 immarg, <16 x i32>, <8 x i32>, i16 %Index)
declare <8 x i32> @llvm.amdgcn.swmmac.i32.16x16x256.iu4.v8i32.v8i32.v16i32.i16(i1 immarg, <8 x i32>, i1 immarg, <16 x i32>, <8 x i32>, i16 %Index)
