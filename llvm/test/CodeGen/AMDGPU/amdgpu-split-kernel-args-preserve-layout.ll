; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-attributes --check-globals all --version 5
; Test default behavior: layout preserved (all struct fields kept in original order)
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1250 -amdgpu-kernarg-preload-count=16 -passes=amdgpu-preload-kernel-arguments < %s | FileCheck %s
; RUN: opt -S -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1250 -amdgpu-kernarg-preload-count=16 -passes=amdgpu-preload-kernel-arguments < %s | llc -mtriple=amdgcn-amd-amdhsa -mcpu=gfx1250 | FileCheck %s --check-prefix=ASM

; Simple struct with 4 fields - all should be flattened even if only some are used
%struct.S = type { i32, i8, i64, ptr }

; Test: Only field at offset 8 (i64) is used, but ALL 4 fields should be preserved
define amdgpu_kernel void @test_partial_use(ptr addrspace(4) byref(%struct.S) %s, ptr addrspace(1) %out) {
; CHECK-LABEL: define amdgpu_kernel void @test_partial_use(
; CHECK-SAME: i32 inreg [[TMP0:%.*]], i8 inreg [[TMP1:%.*]], i64 inreg [[S_L:%.*]], ptr inreg [[TMP2:%.*]], ptr addrspace(1) inreg [[OUT:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    store i64 [[S_L]], ptr addrspace(1) [[OUT]], align 8
; CHECK-NEXT:    ret void
;
  %s.l.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 8
  %s.l = load i64, ptr addrspace(4) %s.l.addr, align 8
  store i64 %s.l, ptr addrspace(1) %out, align 8
  ret void
}

; Test: No fields used - all 4 fields should still be preserved as dead args
define amdgpu_kernel void @test_unused_struct(ptr addrspace(4) byref(%struct.S) %unused) {
; CHECK-LABEL: define amdgpu_kernel void @test_unused_struct(
; CHECK-SAME: i32 inreg [[TMP0:%.*]], i8 inreg [[TMP1:%.*]], i64 inreg [[TMP2:%.*]], ptr inreg [[TMP3:%.*]]) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:    ret void
;
  ret void
}

; Test: Multiple fields used - all fields preserved in original order
define amdgpu_kernel void @test_multiple_fields(ptr addrspace(4) byref(%struct.S) %s, ptr addrspace(1) %out) {
; CHECK-LABEL: define amdgpu_kernel void @test_multiple_fields(
; CHECK-SAME: i32 inreg [[S_I:%.*]], i8 inreg [[TMP0:%.*]], i64 inreg [[S_L:%.*]], ptr inreg [[TMP1:%.*]], ptr addrspace(1) inreg [[OUT:%.*]]) #[[ATTR2:[0-9]+]] {
; CHECK-NEXT:    [[S_I_ZEXT:%.*]] = zext i32 [[S_I]] to i64
; CHECK-NEXT:    [[SUM:%.*]] = add i64 [[S_I_ZEXT]], [[S_L]]
; CHECK-NEXT:    store i64 [[SUM]], ptr addrspace(1) [[OUT]], align 8
; CHECK-NEXT:    ret void
;
  %s.i.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 0
  %s.i = load i32, ptr addrspace(4) %s.i.addr, align 4
  %s.l.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 8
  %s.l = load i64, ptr addrspace(4) %s.l.addr, align 8
  %s.i.zext = zext i32 %s.i to i64
  %sum = add i64 %s.i.zext, %s.l
  store i64 %sum, ptr addrspace(1) %out, align 8
  ret void
}

; Nested struct: inner struct should be recursively flattened
%struct.Inner = type { i32, i64 }
%struct.Outer = type { i8, %struct.Inner, i32 }

; Test: Access nested field - all leaf fields preserved
define amdgpu_kernel void @test_nested_struct(ptr addrspace(4) byref(%struct.Outer) %o, ptr addrspace(1) %out) {
; CHECK-LABEL: define amdgpu_kernel void @test_nested_struct(
; CHECK-SAME: i8 inreg [[TMP0:%.*]], i32 inreg [[TMP1:%.*]], i64 inreg [[O_INNER_L:%.*]], i32 inreg [[TMP2:%.*]], ptr addrspace(1) inreg [[OUT:%.*]]) #[[ATTR3:[0-9]+]] {
; CHECK-NEXT:    store i64 [[O_INNER_L]], ptr addrspace(1) [[OUT]], align 8
; CHECK-NEXT:    ret void
;
  ; Access inner.l at offset 16 (outer.c=0, padding to 8, inner.i=8, inner.l=16)
  %o.inner.l.addr = getelementptr inbounds i8, ptr addrspace(4) %o, i64 16
  %o.inner.l = load i64, ptr addrspace(4) %o.inner.l.addr, align 8
  store i64 %o.inner.l, ptr addrspace(1) %out, align 8
  ret void
}

; Struct with vector field - vectors are kept as leaf types, not decomposed
%struct.WithVector = type { i32, <4 x float>, i64 }

; Test: Vector field should be kept as single vector argument
define amdgpu_kernel void @test_vector_field(ptr addrspace(4) byref(%struct.WithVector) %s, ptr addrspace(1) %out) {
  ; Access vector at offset 16
; CHECK-LABEL: define amdgpu_kernel void @test_vector_field(
; CHECK-SAME: i32 inreg [[TMP0:%.*]], <4 x float> inreg [[VEC:%.*]], i64 inreg [[TMP1:%.*]], ptr addrspace(1) inreg [[OUT:%.*]]) #[[ATTR4:[0-9]+]] {
; CHECK-NEXT:    store <4 x float> [[VEC]], ptr addrspace(1) [[OUT]], align 16
; CHECK-NEXT:    ret void
;
  %vec.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 16
  %vec = load <4 x float>, ptr addrspace(4) %vec.addr, align 16
  store <4 x float> %vec, ptr addrspace(1) %out, align 16
  ret void
}

; Struct with array field
%struct.WithArray = type { i32, [4 x i32], i64 }

; Test: Array field with constant index access - should be split, all elements flattened
define amdgpu_kernel void @test_array_const_index(ptr addrspace(4) byref(%struct.WithArray) %s, ptr addrspace(1) %out) {
  ; Access arr[2] at offset 4 + 2*4 = 12
; CHECK-LABEL: define amdgpu_kernel void @test_array_const_index(
; CHECK-SAME: i32 inreg [[TMP0:%.*]], i32 inreg [[TMP1:%.*]], i32 inreg [[TMP2:%.*]], i32 inreg [[ARR2:%.*]], i32 inreg [[TMP3:%.*]], i64 inreg [[TMP4:%.*]], ptr addrspace(1) inreg [[OUT:%.*]]) #[[ATTR5:[0-9]+]] {
; CHECK-NEXT:    store i32 [[ARR2]], ptr addrspace(1) [[OUT]], align 4
; CHECK-NEXT:    ret void
;
  %arr2.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 12
  %arr2 = load i32, ptr addrspace(4) %arr2.addr, align 4
  store i32 %arr2, ptr addrspace(1) %out, align 4
  ret void
}

; Test: Array field with variable index access - should NOT be split
define amdgpu_kernel void @test_array_variable_index(ptr addrspace(4) byref(%struct.WithArray) %s, i32 %idx, ptr addrspace(1) %out) {
  ; Access arr[idx] - non-constant index prevents splitting
; CHECK-LABEL: define amdgpu_kernel void @test_array_variable_index(
; CHECK-SAME: ptr addrspace(4) byref([[STRUCT_WITHARRAY:%.*]]) [[S:%.*]], i32 [[IDX:%.*]], ptr addrspace(1) [[OUT:%.*]]) #[[ATTR6:[0-9]+]] {
; CHECK-NEXT:    [[ARR_ADDR:%.*]] = getelementptr inbounds i8, ptr addrspace(4) [[S]], i64 4
; CHECK-NEXT:    [[ELEM_ADDR:%.*]] = getelementptr inbounds i32, ptr addrspace(4) [[ARR_ADDR]], i32 [[IDX]]
; CHECK-NEXT:    [[ELEM:%.*]] = load i32, ptr addrspace(4) [[ELEM_ADDR]], align 4
; CHECK-NEXT:    store i32 [[ELEM]], ptr addrspace(1) [[OUT]], align 4
; CHECK-NEXT:    ret void
;
  %arr.addr = getelementptr inbounds i8, ptr addrspace(4) %s, i64 4
  %elem.addr = getelementptr inbounds i32, ptr addrspace(4) %arr.addr, i32 %idx
  %elem = load i32, ptr addrspace(4) %elem.addr, align 4
  store i32 %elem, ptr addrspace(1) %out, align 4
  ret void
}

attributes #0 = { nounwind }

; ASM-LABEL: .amdhsa_kernel test_partial_use
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 8
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; ASM-LABEL: .amdhsa_kernel test_unused_struct
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 6
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; ASM-LABEL: .amdhsa_kernel test_multiple_fields
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 8
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; ASM-LABEL: .amdhsa_kernel test_nested_struct
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 8
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; Vector field kept as single argument (not decomposed)
; ASM-LABEL: .amdhsa_kernel test_vector_field
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 12
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; ASM-LABEL: .amdhsa_kernel test_array_const_index
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 10
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; Variable index prevents splitting - no preloading
; ASM-LABEL: .amdhsa_kernel test_array_variable_index
; ASM: .amdhsa_user_sgpr_kernarg_preload_length 0
; ASM: .amdhsa_user_sgpr_kernarg_preload_offset 0

; Verify that backup declarations do NOT generate any code or metadata
; ASM-NOT: __amdgpu_orig_kernel
; ASM-NOT: .amdhsa_kernel __amdgpu_orig_kernel
;.
; CHECK: attributes #[[ATTR0]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_partial_use" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR1]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_unused_struct" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR2]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_multiple_fields" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR3]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_nested_struct" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR4]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_vector_field" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR5]] = { "amdgpu-original-kernel"="__amdgpu_orig_kernel_test_array_const_index" "target-cpu"="gfx1250" }
; CHECK: attributes #[[ATTR6]] = { "target-cpu"="gfx1250" }
;.
