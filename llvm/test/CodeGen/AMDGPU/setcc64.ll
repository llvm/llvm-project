; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn < %s| FileCheck -check-prefixes=GCN,SI %s
; RUN: llc -mtriple=amdgcn -mcpu=tonga -mattr=-flat-for-global < %s | FileCheck -check-prefixes=GCN,VI %s

; XXX: Merge this into setcc, once R600 supports 64-bit operations

;;;==========================================================================;;;
;; Double comparisons
;;;==========================================================================;;;

define amdgpu_kernel void @f64_oeq(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_oeq:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_eq_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_oeq:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_eq_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp oeq double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ogt(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ogt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_gt_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ogt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_gt_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ogt double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_oge(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_oge:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_ge_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_oge:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_ge_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp oge double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_olt(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_olt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_lt_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_olt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_lt_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp olt double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ole(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ole:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_le_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ole:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_le_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ole double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_one(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_one:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_lg_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_one:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_lg_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp one double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ord(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ord:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_o_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ord:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_o_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ord double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ueq(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ueq:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_nlg_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ueq:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_nlg_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ueq double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ugt(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ugt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_nle_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ugt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_nle_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ugt double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_uge(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_uge:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_nlt_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_uge:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_nlt_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp uge double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ult(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ult:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_nge_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ult:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_nge_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ult double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_ule(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_ule:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_ngt_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_ule:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_ngt_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp ule double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_une(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_une:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_neq_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_une:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_neq_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp une double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @f64_uno(ptr addrspace(1) %out, double %a, double %b) #0 {
; SI-LABEL: f64_uno:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_u_f64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: f64_uno:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_u_f64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = fcmp uno double %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

;;;==========================================================================;;;
;; 64-bit integer comparisons
;;;==========================================================================;;;

define amdgpu_kernel void @i64_eq(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_eq:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_eq_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_eq:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    s_cmp_eq_u64 s[2:3], s[8:9]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    s_cselect_b64 s[0:1], -1, 0
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, s[0:1]
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp eq i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_ne(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_ne:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_ne_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_ne:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    s_cmp_lg_u64 s[2:3], s[8:9]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    s_cselect_b64 s[0:1], -1, 0
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, s[0:1]
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp ne i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_ugt(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_ugt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_gt_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_ugt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_gt_u64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp ugt i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_uge(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_uge:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_ge_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_uge:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_ge_u64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp uge i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_ult(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_ult:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_lt_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_ult:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_lt_u64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp ult i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_ule(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_ule:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_le_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_ule:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_le_u64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp ule i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_sgt(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_sgt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_gt_i64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_sgt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_gt_i64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp sgt i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_sge(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_sge:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_ge_i64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_sge:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_ge_i64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp sge i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_slt(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_slt:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_lt_i64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_slt:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_lt_i64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp slt i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i64_sle(ptr addrspace(1) %out, i64 %a, i64 %b) #0 {
; SI-LABEL: i64_sle:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0xd
; SI-NEXT:    s_mov_b32 s7, 0xf000
; SI-NEXT:    s_mov_b32 s6, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s4, s0
; SI-NEXT:    s_mov_b32 s5, s1
; SI-NEXT:    v_mov_b32_e32 v0, s8
; SI-NEXT:    v_mov_b32_e32 v1, s9
; SI-NEXT:    v_cmp_le_i64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i64_sle:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_load_dwordx2 s[8:9], s[4:5], 0x34
; VI-NEXT:    s_mov_b32 s7, 0xf000
; VI-NEXT:    s_mov_b32 s6, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s4, s0
; VI-NEXT:    v_mov_b32_e32 v0, s8
; VI-NEXT:    v_mov_b32_e32 v1, s9
; VI-NEXT:    v_cmp_le_i64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s5, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[4:7], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp sle i64 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i128_sle(ptr addrspace(1) %out, i128 %a, i128 %b) #0 {
; SI-LABEL: i128_sle:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx8 s[0:7], s[4:5], 0x9
; SI-NEXT:    s_mov_b32 s11, 0xf000
; SI-NEXT:    s_mov_b32 s10, -1
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s8, s0
; SI-NEXT:    s_mov_b32 s9, s1
; SI-NEXT:    v_mov_b32_e32 v0, s6
; SI-NEXT:    v_mov_b32_e32 v1, s7
; SI-NEXT:    v_cmp_le_u64_e32 vcc, s[2:3], v[0:1]
; SI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; SI-NEXT:    buffer_store_dword v0, off, s[8:11], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i128_sle:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx8 s[0:7], s[4:5], 0x24
; VI-NEXT:    s_mov_b32 s11, 0xf000
; VI-NEXT:    s_mov_b32 s10, -1
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    v_mov_b32_e32 v0, s6
; VI-NEXT:    v_mov_b32_e32 v1, s7
; VI-NEXT:    v_cmp_le_u64_e32 vcc, s[2:3], v[0:1]
; VI-NEXT:    s_mov_b32 s8, s0
; VI-NEXT:    s_mov_b32 s9, s1
; VI-NEXT:    v_cndmask_b32_e64 v0, 0, -1, vcc
; VI-NEXT:    buffer_store_dword v0, off, s[8:11], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp sle i128 %a, %b
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

define amdgpu_kernel void @i128_eq_const(ptr addrspace(1) %out, i128 %a) #0 {
; SI-LABEL: i128_eq_const:
; SI:       ; %bb.0: ; %entry
; SI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x9
; SI-NEXT:    s_waitcnt lgkmcnt(0)
; SI-NEXT:    s_mov_b32 s3, 0xf000
; SI-NEXT:    s_mov_b32 s2, -1
; SI-NEXT:    v_mov_b32_e32 v0, 0
; SI-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; SI-NEXT:    s_endpgm
;
; VI-LABEL: i128_eq_const:
; VI:       ; %bb.0: ; %entry
; VI-NEXT:    s_load_dwordx4 s[0:3], s[4:5], 0x24
; VI-NEXT:    s_waitcnt lgkmcnt(0)
; VI-NEXT:    s_mov_b32 s3, 0xf000
; VI-NEXT:    s_mov_b32 s2, -1
; VI-NEXT:    v_mov_b32_e32 v0, 0
; VI-NEXT:    buffer_store_dword v0, off, s[0:3], 0
; VI-NEXT:    s_endpgm
entry:
  %tmp0 = icmp eq i128 %a, 85070591730234615865843651857942052992
  %tmp1 = sext i1 %tmp0 to i32
  store i32 %tmp1, ptr addrspace(1) %out
  ret void
}

attributes #0 = { nounwind }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; GCN: {{.*}}
