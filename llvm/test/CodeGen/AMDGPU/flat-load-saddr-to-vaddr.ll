; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=amdgcn -mcpu=gfx1250 < %s | FileCheck -check-prefixes=GCN %s

; The first load produces address in a VGPR which is used in address calculation
; of the second load (one inside the loop). The value is uniform and the inner
; load correctly selected to use SADDR form, however the address is promoted to
; vector registers because it all starts with a VGPR produced by the entry block
; load.
;
; Check that we are changing SADDR form of a load to VADDR and do not have to use
; readfirstlane instructions to move address from VGPRs into SGPRs.

define amdgpu_kernel void @test_move_load_address_to_vgpr(ptr addrspace(1) nocapture %arg1, ptr nocapture %arg2) {
; GCN-LABEL: test_move_load_address_to_vgpr:
; GCN:       ; %bb.0: ; %bb
; GCN-NEXT:    s_load_b128 s[0:3], s[4:5], 0x24
; GCN-NEXT:    v_mov_b32_e32 v3, 0
; GCN-NEXT:    s_wait_kmcnt 0x0
; GCN-NEXT:    global_load_b32 v2, v3, s[0:1] scope:SCOPE_SYS
; GCN-NEXT:    s_wait_loadcnt 0x0
; GCN-NEXT:    v_lshlrev_b64_e32 v[0:1], 2, v[2:3]
; GCN-NEXT:    v_add_nc_u32_e32 v2, 0xffffff00, v2
; GCN-NEXT:    s_delay_alu instid0(VALU_DEP_2)
; GCN-NEXT:    v_add_nc_u64_e32 v[0:1], s[2:3], v[0:1]
; GCN-NEXT:  .LBB0_1: ; %bb3
; GCN-NEXT:    ; =>This Inner Loop Header: Depth=1
; GCN-NEXT:    s_wait_dscnt 0x0
; GCN-NEXT:    flat_load_b32 v3, v[0:1] scope:SCOPE_SYS
; GCN-NEXT:    s_wait_loadcnt 0x0
; GCN-NEXT:    s_wait_xcnt 0x0
; GCN-NEXT:    v_add_nc_u64_e32 v[0:1], 4, v[0:1]
; GCN-NEXT:    v_add_co_u32 v2, s0, v2, 1
; GCN-NEXT:    s_and_b32 vcc_lo, exec_lo, s0
; GCN-NEXT:    s_cbranch_vccz .LBB0_1
; GCN-NEXT:  ; %bb.2: ; %bb2
; GCN-NEXT:    s_endpgm
bb:
  %i2 = load volatile i32, ptr addrspace(1) %arg1, align 4
  br label %bb3

bb2:                                              ; preds = %bb3
  ret void

bb3:                                              ; preds = %bb3, %bb
  %i = phi i32 [ %i2, %bb ], [ %i8, %bb3 ]
  %i4 = zext i32 %i to i64
  %i5 = getelementptr inbounds i32, ptr %arg2, i64 %i4
  %i6 = load volatile i32, ptr %i5, align 4
  %i8 = add nuw nsw i32 %i, 1
  %i9 = icmp eq i32 %i8, 256
  br i1 %i9, label %bb2, label %bb3
}
