; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=amdgcn-amd-amdhsa -O3 -mcpu=gfx1250 < %s | FileCheck --match-full-lines --check-prefixes=GFX1250 %s

; GFX1250 specific test to check we don't force SCOPE_SE on flat stores that are known to never touch
; scratch.
;
; --match-full-lines is important here to ensure we have the right scope.

define amdgpu_kernel void @local_or_global_flat_store(i1 %cond, ptr addrspace(1) %gptr, ptr addrspace(3) %lptr, i32 %val) {
; GFX1250-LABEL: local_or_global_flat_store:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    s_load_b32 s6, s[4:5], 0x0
; GFX1250-NEXT:    s_load_b128 s[0:3], s[4:5], 0x8
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    s_mov_b64 s[4:5], src_shared_base
; GFX1250-NEXT:    v_mov_b32_e32 v0, 0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_and_b32 s4, 1, s6
; GFX1250-NEXT:    v_mov_b32_e32 v1, s3
; GFX1250-NEXT:    s_cmp_lg_u32 s2, -1
; GFX1250-NEXT:    s_cselect_b32 s2, s2, 0
; GFX1250-NEXT:    s_cselect_b32 s3, s5, 0
; GFX1250-NEXT:    s_cmp_eq_u32 s4, 1
; GFX1250-NEXT:    s_cselect_b32 s1, s1, s3
; GFX1250-NEXT:    s_cselect_b32 s0, s0, s2
; GFX1250-NEXT:    flat_store_b32 v0, v1, s[0:1] scope:SCOPE_SE
; GFX1250-NEXT:    s_endpgm
entry:
  %a = addrspacecast ptr addrspace(1) %gptr to ptr
  %b = addrspacecast ptr addrspace(3) %lptr to ptr
  %ptr = select i1 %cond, ptr %a, ptr %b
  store i32 %val, ptr %ptr
  ret void
}

define amdgpu_kernel void @local_or_global_flat_atomic_store(i1 %cond, ptr addrspace(1) %gptr, ptr addrspace(3) %lptr, i32 %val) {
; GFX1250-LABEL: local_or_global_flat_atomic_store:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    s_load_b32 s6, s[4:5], 0x0
; GFX1250-NEXT:    s_load_b128 s[0:3], s[4:5], 0x8
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    s_mov_b64 s[4:5], src_shared_base
; GFX1250-NEXT:    v_mov_b32_e32 v0, 0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_and_b32 s4, 1, s6
; GFX1250-NEXT:    v_mov_b32_e32 v1, s3
; GFX1250-NEXT:    s_cmp_lg_u32 s2, -1
; GFX1250-NEXT:    s_cselect_b32 s2, s2, 0
; GFX1250-NEXT:    s_cselect_b32 s3, s5, 0
; GFX1250-NEXT:    s_cmp_eq_u32 s4, 1
; GFX1250-NEXT:    s_cselect_b32 s1, s1, s3
; GFX1250-NEXT:    s_cselect_b32 s0, s0, s2
; GFX1250-NEXT:    flat_store_b32 v0, v1, s[0:1] scope:SCOPE_SE
; GFX1250-NEXT:    s_endpgm
entry:
  %a = addrspacecast ptr addrspace(1) %gptr to ptr
  %b = addrspacecast ptr addrspace(3) %lptr to ptr
  %ptr = select i1 %cond, ptr %a, ptr %b
  store atomic i32 %val, ptr %ptr syncscope("workgroup") unordered, align 4
  ret void
}

define amdgpu_kernel void @local_or_global_flat_store_no_flat_scratch_init(i1 %cond, ptr addrspace(1) %gptr, ptr addrspace(3) %lptr, i32 %val) #0 {
; GFX1250-LABEL: local_or_global_flat_store_no_flat_scratch_init:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    s_load_b32 s6, s[4:5], 0x0
; GFX1250-NEXT:    s_load_b128 s[0:3], s[4:5], 0x8
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    s_mov_b64 s[4:5], src_shared_base
; GFX1250-NEXT:    v_mov_b32_e32 v0, 0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_and_b32 s4, 1, s6
; GFX1250-NEXT:    v_mov_b32_e32 v1, s3
; GFX1250-NEXT:    s_cmp_lg_u32 s2, -1
; GFX1250-NEXT:    s_cselect_b32 s2, s2, 0
; GFX1250-NEXT:    s_cselect_b32 s3, s5, 0
; GFX1250-NEXT:    s_cmp_eq_u32 s4, 1
; GFX1250-NEXT:    s_cselect_b32 s1, s1, s3
; GFX1250-NEXT:    s_cselect_b32 s0, s0, s2
; GFX1250-NEXT:    flat_store_b32 v0, v1, s[0:1]
; GFX1250-NEXT:    s_endpgm
entry:
  %a = addrspacecast ptr addrspace(1) %gptr to ptr
  %b = addrspacecast ptr addrspace(3) %lptr to ptr
  %ptr = select i1 %cond, ptr %a, ptr %b
  store i32 %val, ptr %ptr
  ret void
}

define amdgpu_kernel void @local_or_global_flat_atomic_store_no_flat_scratch_init(i1 %cond, ptr addrspace(1) %gptr, ptr addrspace(3) %lptr, i32 %val) #0 {
; GFX1250-LABEL: local_or_global_flat_atomic_store_no_flat_scratch_init:
; GFX1250:       ; %bb.0: ; %entry
; GFX1250-NEXT:    s_clause 0x1
; GFX1250-NEXT:    s_load_b32 s6, s[4:5], 0x0
; GFX1250-NEXT:    s_load_b128 s[0:3], s[4:5], 0x8
; GFX1250-NEXT:    s_wait_xcnt 0x0
; GFX1250-NEXT:    s_mov_b64 s[4:5], src_shared_base
; GFX1250-NEXT:    v_mov_b32_e32 v0, 0
; GFX1250-NEXT:    s_wait_kmcnt 0x0
; GFX1250-NEXT:    s_and_b32 s4, 1, s6
; GFX1250-NEXT:    v_mov_b32_e32 v1, s3
; GFX1250-NEXT:    s_cmp_lg_u32 s2, -1
; GFX1250-NEXT:    s_cselect_b32 s2, s2, 0
; GFX1250-NEXT:    s_cselect_b32 s3, s5, 0
; GFX1250-NEXT:    s_cmp_eq_u32 s4, 1
; GFX1250-NEXT:    s_cselect_b32 s1, s1, s3
; GFX1250-NEXT:    s_cselect_b32 s0, s0, s2
; GFX1250-NEXT:    flat_store_b32 v0, v1, s[0:1]
; GFX1250-NEXT:    s_endpgm
entry:
  %a = addrspacecast ptr addrspace(1) %gptr to ptr
  %b = addrspacecast ptr addrspace(3) %lptr to ptr
  %ptr = select i1 %cond, ptr %a, ptr %b
  store atomic i32 %val, ptr %ptr syncscope("workgroup") unordered, align 4
  ret void
}

attributes #0 = { "amdgpu-no-flat-scratch-init"="true" }
