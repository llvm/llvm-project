; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=10 < %s 2>&1 | FileCheck %s
;
;       bb.0.entry
;        /      |
;   bb.3.bb2    |
;      /   |    |
; bb.9.bb5 |    |
;      \   |    |
;    bb.1.Flow1 |
;         \     |
;        bb.8.Flow
;         /  |
;  bb.2.bb1  |
;         \  |
;        bb.6.Flow2
;         /  |
;  bb.7.bb4  |
;         \  |
;        bb.4.Flow3
;         /  |
;  bb.5.bb3  |
;         \  |
;        bb.10.exit
;
define amdgpu_ps i64 @test(i1 %cond, ptr addrspace(3) %p, i64 %val) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.8(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY3]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY2]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_NE_U32_e64_]], %bb.8, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.Flow1:
  ; CHECK-NEXT:   successors: %bb.8(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:sreg_32 = PHI %35, %bb.3, %80, %bb.9
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vreg_64 = PHI undef %33:vreg_64, %bb.3, %20, %bb.9
  ; CHECK-NEXT:   SI_END_CF %6, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 [[PHI]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:sreg_32 = COPY [[S_AND_B32_]]
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   S_BRANCH %bb.8
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb1:
  ; CHECK-NEXT:   successors: %bb.6(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U16_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 %121, 0, 0, implicit $exec :: (load (s16) from %ir.p, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U16_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 %121, 2, 0, implicit $exec :: (load (s16) from %ir.p + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U16_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 %121, 4, 0, implicit $exec :: (load (s16) from %ir.p + 4, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U16_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U16_gfx9 %121, 6, 0, implicit $exec :: (load (s16) from %ir.p + 6, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U16_gfx9_3]], 16, [[DS_READ_U16_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U16_gfx9_1]], 16, [[DS_READ_U16_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_LSHL_OR_B32_e64_1]], %subreg.sub0, [[V_LSHL_OR_B32_e64_]], %subreg.sub1
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vreg_64 = COPY [[REG_SEQUENCE]]
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:sreg_32 = COPY $exec_lo
  ; CHECK-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32 = S_ANDN2_B32 %18, $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_AND_B32_1:%[0-9]+]]:sreg_32 = S_AND_B32 [[V_CMP_NE_U32_e64_]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_]], [[S_AND_B32_1]], implicit-def dead $scc
  ; CHECK-NEXT:   S_BRANCH %bb.6
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.bb2:
  ; CHECK-NEXT:   successors: %bb.9(0x40000000), %bb.1(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_B64_gfx9_:%[0-9]+]]:vreg_64 = DS_READ_B64_gfx9 [[COPY2]], 8, 0, implicit $exec :: (load (s64) from %ir.gep2, addrspace 3)
  ; CHECK-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 -1
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[DS_READ_B64_gfx9_]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF1:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_NE_U32_e64_]], %bb.1, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.9
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.Flow3:
  ; CHECK-NEXT:   successors: %bb.5(0x40000000), %bb.10(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:sreg_32 = PHI %12, %bb.6, %82, %bb.7
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vreg_64 = PHI %16, %bb.6, %15, %bb.7
  ; CHECK-NEXT:   SI_END_CF %14, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_IF2:%[0-9]+]]:sreg_32 = SI_IF [[PHI2]], %bb.10, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.bb3:
  ; CHECK-NEXT:   successors: %bb.10(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ2_B32_gfx9_:%[0-9]+]]:vreg_64 = DS_READ2_B32_gfx9 %121, 6, 7, 0, implicit $exec :: (load (s64) from %ir.gep3, align 4, addrspace 3)
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_1:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[DS_READ2_B32_gfx9_]].sub0, [[COPY1]], 0, implicit $exec
  ; CHECK-NEXT:   %95:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[DS_READ2_B32_gfx9_]].sub1, %119, [[V_ADD_CO_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_]], %subreg.sub0, %95, %subreg.sub1
  ; CHECK-NEXT:   S_BRANCH %bb.10
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.Flow2:
  ; CHECK-NEXT:   successors: %bb.7(0x40000000), %bb.4(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:sreg_32 = PHI %18, %bb.8, [[S_OR_B32_]], %bb.2
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.8, [[COPY6]], %bb.2
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vreg_64 = PHI %17, %bb.8, [[COPY5]], %bb.2
  ; CHECK-NEXT:   SI_END_CF %19, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF3:%[0-9]+]]:sreg_32 = SI_IF [[PHI4]], %bb.4, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.7
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.7.bb4:
  ; CHECK-NEXT:   successors: %bb.4(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_2:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_3:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[PHI6]].sub0, [[COPY1]], 0, implicit $exec
  ; CHECK-NEXT:   %103:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[PHI6]].sub1, [[SI_SPILL_V32_RESTORE]], [[V_ADD_CO_U32_e64_3]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_2]], %subreg.sub0, %103, %subreg.sub1
  ; CHECK-NEXT:   [[S_ANDN2_B32_1:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[PHI5]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:sreg_32 = COPY [[S_ANDN2_B32_1]]
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.8.Flow:
  ; CHECK-NEXT:   successors: %bb.2(0x40000000), %bb.6(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.0, [[COPY4]], %bb.1
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vreg_64 = PHI undef %27:vreg_64, %bb.0, [[PHI1]], %bb.1
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:vreg_64 = PHI undef %27:vreg_64, %bb.0, [[SI_SPILL_V64_RESTORE]], %bb.1
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF]], %bb.6, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.9.bb5:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 32, 0, implicit $exec :: (load (s8) from %ir.gep4, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 33, 0, implicit $exec :: (load (s8) from %ir.gep4 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 34, 0, implicit $exec :: (load (s8) from %ir.gep4 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 35, 0, implicit $exec :: (load (s8) from %ir.gep4 + 3, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_4:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 36, 0, implicit $exec :: (load (s8) from %ir.gep4 + 4, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_5:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 37, 0, implicit $exec :: (load (s8) from %ir.gep4 + 5, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_6:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 38, 0, implicit $exec :: (load (s8) from %ir.gep4 + 6, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_7:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY2]], 39, 0, implicit $exec :: (load (s8) from %ir.gep4 + 7, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_5]], 8, [[DS_READ_U8_gfx9_4]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_7]], 8, [[DS_READ_U8_gfx9_6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_3]], 16, [[V_LSHL_OR_B32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_1]], 8, [[DS_READ_U8_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_3]], 8, [[DS_READ_U8_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_6]], 16, [[V_LSHL_OR_B32_e64_5]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_CO_U32_e64_4:%[0-9]+]]:vgpr_32, [[V_ADD_CO_U32_e64_5:%[0-9]+]]:sreg_32_xm0_xexec = V_ADD_CO_U32_e64 [[V_LSHL_OR_B32_e64_7]], [[COPY1]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   %111:vgpr_32, dead $sgpr_null = V_ADDC_U32_e64 [[V_LSHL_OR_B32_e64_4]], [[SI_SPILL_V32_RESTORE2]], [[V_ADD_CO_U32_e64_5]], 0, implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_ADD_CO_U32_e64_4]], %subreg.sub0, %111, %subreg.sub1
  ; CHECK-NEXT:   [[S_XOR_B32_:%[0-9]+]]:sreg_32 = S_XOR_B32 $exec_lo, -1, implicit-def dead $scc
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.10.exit:
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:vreg_64 = PHI [[PHI3]], %bb.4, [[REG_SEQUENCE1]], %bb.5
  ; CHECK-NEXT:   SI_END_CF [[SI_IF2]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_READFIRSTLANE_B32_:%[0-9]+]]:sreg_32_xm0 = V_READFIRSTLANE_B32 [[PHI10]].sub0, implicit $exec
  ; CHECK-NEXT:   [[V_READFIRSTLANE_B32_1:%[0-9]+]]:sreg_32_xm0 = V_READFIRSTLANE_B32 [[PHI10]].sub1, implicit $exec
  ; CHECK-NEXT:   $sgpr0 = COPY [[V_READFIRSTLANE_B32_]]
  ; CHECK-NEXT:   $sgpr1 = COPY [[V_READFIRSTLANE_B32_1]]
  ; CHECK-NEXT:   SI_RETURN_TO_EPILOG killed $sgpr0, killed $sgpr1
entry:
;      entry
;      /   \
;    bb1   bb2
;    / \   / \
;  bb3  bb4  bb5
;     \  |  /
;      exit
   br i1 %cond, label %bb1, label %bb2

bb1:
   %gep1 = getelementptr inbounds i64, ptr addrspace(3) %p, i64 0
   %ld1 = load i64, ptr addrspace(3) %gep1, align 2
   br i1 %cond, label %bb3, label %bb4

bb2:
   %gep2 = getelementptr inbounds i64, ptr addrspace(3) %p, i64 1
   %ld2 = load i64, ptr addrspace(3) %gep2, align 8
   br i1 %cond, label %bb4, label %bb5

bb3:
   %gep3 = getelementptr inbounds i64, ptr addrspace(3) %p, i64 3
   %ld3 = load i64, ptr addrspace(3) %gep3, align 4
   %add1 = add i64 %ld3, %val
   br label %exit

bb4:
   %phi1 = phi i64 [ %ld1, %bb1 ], [ %ld2, %bb2]
   %add2 = add i64 %phi1, %val
   br label %exit

bb5:
   %gep4 = getelementptr inbounds i64, ptr addrspace(3) %p, i64 4
   %ld4 = load i64, ptr addrspace(3) %gep4, align 1
   %add3 = add i64 %ld4, %val
   br label %exit

exit:
   %phi2 = phi i64 [ %add1, %bb3 ], [ %add2, %bb4 ], [ %add3, %bb5 ]
   ret i64 %phi2
}
