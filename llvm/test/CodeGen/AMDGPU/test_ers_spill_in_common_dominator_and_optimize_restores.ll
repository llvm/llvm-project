; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=20 < %s 2>&1 | FileCheck %s

@array1 = global [5 x i32] zeroinitializer, align 4
@array2 = global [5 x i32] zeroinitializer, align 4
@array3 = global [5 x i32] zeroinitializer, align 4
@array4 = global [5 x i32] zeroinitializer, align 4

;        bb.0.entry
;           /   |
;     bb.1.bb1  |
;           \   |
;         bb.2.bb2
;
define amdgpu_ps void @test(ptr addrspace(1) %p1, ptr addrspace(3) %p2, i1 %cond1, ptr addrspace(1) %p3, ptr addrspace(1) %p4, ptr addrspace(1) %p5, i32 %arg1, i32 %arg2) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr11
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr10
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr9
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr8
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr7
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr6
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr5
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY3]], %subreg.sub0, [[COPY2]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY5]], %subreg.sub0, [[COPY4]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY7]], %subreg.sub0, [[COPY6]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY11]], %subreg.sub0, [[COPY10]], %subreg.sub1
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY8]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_EQ_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_EQ_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE1]], 8, [[GLOBAL_LOAD_UBYTE]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE3:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE3]], 8, [[GLOBAL_LOAD_UBYTE2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_1]], 16, [[V_LSHL_OR_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE4:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 12, 0, implicit $exec :: (load (s8) from %ir.gep1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE5:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 13, 0, implicit $exec :: (load (s8) from %ir.gep1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_3:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE5]], 8, [[GLOBAL_LOAD_UBYTE4]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE6:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 14, 0, implicit $exec :: (load (s8) from %ir.gep1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE7:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE3]], 15, 0, implicit $exec :: (load (s8) from %ir.gep1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_4:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE7]], 8, [[GLOBAL_LOAD_UBYTE6]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_5:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_4]], 16, [[V_LSHL_OR_B32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX4_:%[0-9]+]]:vreg_128 = GLOBAL_LOAD_DWORDX4 [[REG_SEQUENCE2]], 16, 0, implicit $exec :: (load (s128) from %ir.p3 + 16, align 4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORDX4_1:%[0-9]+]]:vreg_128 = GLOBAL_LOAD_DWORDX4 [[REG_SEQUENCE2]], 0, 0, implicit $exec :: (load (s128) from %ir.p3, align 4, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[GLOBAL_LOAD_DWORDX4_1]].sub0, [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE2]], [[V_ADD_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p3, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_]], [[V_LSHL_OR_B32_e64_2]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array1, target-flags(amdgpu-gotprel32-hi) @array1, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY12]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array1, i64 20)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET1:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array3, target-flags(amdgpu-gotprel32-hi) @array3, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM1:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET1]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM1]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY13]], [[FLAT_LOAD_DWORD]], 4, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 4)`)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE8:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 20, 0, implicit $exec :: (load (s8) from %ir.p4 + 20, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE9:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 21, 0, implicit $exec :: (load (s8) from %ir.p4 + 21, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_6:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE9]], 8, [[GLOBAL_LOAD_UBYTE8]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE10:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 22, 0, implicit $exec :: (load (s8) from %ir.p4 + 22, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE11:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 23, 0, implicit $exec :: (load (s8) from %ir.p4 + 23, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_7:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE11]], 8, [[GLOBAL_LOAD_UBYTE10]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_8:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_7]], 16, [[V_LSHL_OR_B32_e64_6]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE12:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 12, 0, implicit $exec :: (load (s8) from %ir.p4 + 12, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE13:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 13, 0, implicit $exec :: (load (s8) from %ir.p4 + 13, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_9:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE13]], 8, [[GLOBAL_LOAD_UBYTE12]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE14:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 14, 0, implicit $exec :: (load (s8) from %ir.p4 + 14, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE15:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE1]], 15, 0, implicit $exec :: (load (s8) from %ir.p4 + 15, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_10:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE15]], 8, [[GLOBAL_LOAD_UBYTE14]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_11:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_10]], 16, [[V_LSHL_OR_B32_e64_9]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE1]], 0, 0, implicit $exec :: (load (s16) from %ir.p4, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_USHORT1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_USHORT [[REG_SEQUENCE1]], 2, 0, implicit $exec :: (load (s16) from %ir.p4 + 2, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_12:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_USHORT1]], 16, [[GLOBAL_LOAD_USHORT]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_LSHL_OR_B32_e64_12]], [[V_ADD_U32_e64_]], 0, implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[REG_SEQUENCE1]], [[V_ADD_U32_e64_2]], 0, 0, implicit $exec :: (store (s32) into %ir.p4, addrspace 1)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.1, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_ADD_U32_e64_]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_8]], %stack.3, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.3, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[V_LSHL_OR_B32_e64_11]], %stack.4, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_EQ_U32_e64_]], %bb.2, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.1
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.bb1:
  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY9]], 0, 0, implicit $exec :: (load (s8) from %ir.p2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_1:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY9]], 1, 0, implicit $exec :: (load (s8) from %ir.p2 + 1, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_2:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY9]], 2, 0, implicit $exec :: (load (s8) from %ir.p2 + 2, addrspace 3)
  ; CHECK-NEXT:   [[DS_READ_U8_gfx9_3:%[0-9]+]]:vgpr_32 = DS_READ_U8_gfx9 [[COPY9]], 3, 0, implicit $exec :: (load (s8) from %ir.p2 + 3, addrspace 3)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_13:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_1]], 8, [[DS_READ_U8_gfx9_]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_14:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[DS_READ_U8_gfx9_3]], 8, [[DS_READ_U8_gfx9_2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_15:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_14]], 16, [[V_LSHL_OR_B32_e64_13]], implicit $exec
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY14]], 28, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array1, i64 28)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET2:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array2, target-flags(amdgpu-gotprel32-hi) @array2, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM2:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET2]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY15:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM2]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY15]], [[FLAT_LOAD_DWORD1]], 68, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array2, i64 68)`)
  ; CHECK-NEXT:   [[SI_PC_ADD_REL_OFFSET3:%[0-9]+]]:sreg_64 = SI_PC_ADD_REL_OFFSET target-flags(amdgpu-gotprel32-lo) @array4, target-flags(amdgpu-gotprel32-hi) @array4, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_LOAD_DWORDX2_IMM3:%[0-9]+]]:sreg_64_xexec = S_LOAD_DWORDX2_IMM [[SI_PC_ADD_REL_OFFSET3]], 0, 0 :: (dereferenceable invariant load (s64) from got, addrspace 4)
  ; CHECK-NEXT:   [[COPY16:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM3]]
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD2:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY16]], 20, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array4, i64 20)`)
  ; CHECK-NEXT:   [[COPY17:%[0-9]+]]:vreg_64 = COPY [[S_LOAD_DWORDX2_IMM1]]
  ; CHECK-NEXT:   FLAT_STORE_DWORD [[COPY17]], [[FLAT_LOAD_DWORD2]], 60, 0, implicit $exec, implicit $flat_scr :: (store (s32) into `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 60)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD3:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY17]], 84, 0, implicit $exec, implicit $flat_scr :: (load (s32) from `ptr getelementptr inbounds nuw (i8, ptr @array3, i64 84)`)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD4:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY17]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array3)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD5:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY14]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array1)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD6:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY15]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array2)
  ; CHECK-NEXT:   [[FLAT_LOAD_DWORD7:%[0-9]+]]:vgpr_32 = FLAT_LOAD_DWORD [[COPY16]], 0, 0, implicit $exec, implicit $flat_scr :: (dereferenceable load (s32) from @array4)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[FLAT_LOAD_DWORD6]], [[FLAT_LOAD_DWORD7]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[V_MUL_LO_U32_e64_]], %subreg.sub0, undef %261:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %222:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[FLAT_LOAD_DWORD4]], [[FLAT_LOAD_DWORD5]], [[REG_SEQUENCE4]], 0, implicit $exec
  ; CHECK-NEXT:   [[COPY18:%[0-9]+]]:vgpr_32 = COPY %222.sub0
  ; CHECK-NEXT:   [[COPY19:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_1]].sub1
  ; CHECK-NEXT:   [[COPY20:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_1]].sub2
  ; CHECK-NEXT:   [[COPY21:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_]].sub2
  ; CHECK-NEXT:   [[COPY22:%[0-9]+]]:vgpr_32 = COPY [[GLOBAL_LOAD_DWORDX4_]].sub3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.bb2:
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_2]], %bb.0, [[FLAT_LOAD_DWORD1]], %bb.1
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD2]], %bb.1
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD3]], %bb.1
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD4]], %bb.1
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD5]], %bb.1
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD6]], %bb.1
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[FLAT_LOAD_DWORD7]], %bb.1
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY18]], %bb.1
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY19]], %bb.1
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY20]], %bb.1
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY21]], %bb.1
  ; CHECK-NEXT:   [[PHI11:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY22]], %bb.1
  ; CHECK-NEXT:   [[PHI12:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_5]], %bb.0, [[V_LSHL_OR_B32_e64_2]], %bb.1
  ; CHECK-NEXT:   [[PHI13:%[0-9]+]]:vgpr_32 = PHI [[FLAT_LOAD_DWORD]], %bb.0, [[COPY1]], %bb.1
  ; CHECK-NEXT:   [[PHI14:%[0-9]+]]:vgpr_32 = PHI [[V_ADD_U32_e64_1]], %bb.0, [[V_LSHL_OR_B32_e64_15]], %bb.1
  ; CHECK-NEXT:   SI_END_CF [[SI_IF]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.4, addrspace 5)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI]], [[SI_SPILL_V32_RESTORE]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.3, addrspace 5)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_3]], [[SI_SPILL_V32_RESTORE1]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_1]], [[FLAT_LOAD_DWORD]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_1:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_]], [[PHI1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_SUB_U32_e64_1]], [[PHI2]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE2:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_2:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_4]], [[SI_SPILL_V32_RESTORE2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_2]], [[PHI3]], implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_LO_U32_e64_2]], [[PHI4]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_3:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_MUL_LO_U32_e64_3]], [[PHI5]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_5:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_SUB_U32_e64_3]], [[PHI6]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_4:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD_U32_e64_5]], [[PHI7]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_MUL_LO_U32_e64_4]], [[PHI8]], [[PHI9]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_4:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD3_U32_e64_]], [[PHI10]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_5:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_4]], [[PHI11]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_5:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_SUB_U32_e64_5]], [[PHI12]], implicit $exec
  ; CHECK-NEXT:   [[REG_SEQUENCE5:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[PHI14]], %subreg.sub0, undef %259:vgpr_32, %subreg.sub1
  ; CHECK-NEXT:   %244:vreg_64, $sgpr_null = V_MAD_U64_U32_e64 [[V_MUL_LO_U32_e64_5]], [[PHI13]], [[REG_SEQUENCE5]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE3:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.1, addrspace 5)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_6:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 %244.sub0, [[SI_SPILL_V32_RESTORE3]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.0, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT_D16_HI [[SI_SPILL_V64_RESTORE]], [[V_SUB_U32_e64_6]], 2, 0, implicit $exec :: (store (s16) into %ir.p5 + 2, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_SHORT [[SI_SPILL_V64_RESTORE]], [[V_SUB_U32_e64_6]], 0, 0, implicit $exec :: (store (s16) into %ir.p5, addrspace 1)
  ; CHECK-NEXT:   S_ENDPGM 0
entry:
   %ld1 = load i32, ptr addrspace(1) %p1, align 1
   %gep1 = getelementptr inbounds i32, ptr addrspace(1) %p1, i64 3
   %ld2 = load i32, ptr addrspace(1) %gep1, align 1
   %load1 = load i32, ptr addrspace(1) %p3, align 4
   %tmp1 = add i32 %load1, %ld1
   %load2 = load <8 x i32>, ptr addrspace(1) %p3, align 1
   store i32 %tmp1, ptr addrspace(1) %p3
   %add1 = add i32 %ld1, %tmp1
   %idx10 = getelementptr inbounds [5 x i32], [5 x i32]* @array1, i64 1, i64 0
   %val0 = load i32, i32* %idx10, align 4
   %idx20 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 0, i64 1
   store i32 %val0, i32 *%idx20
   %load3 = load <8 x i32>, ptr addrspace(1) %p4, align 1
   %load4 = load i32, ptr addrspace(1) %p4, align 2
   %tmp2 = add i32 %load4, %tmp1
   store i32 %tmp2, ptr addrspace(1) %p4
   br i1 %cond1, label %bb1, label %bb2

bb1:
   %ld3 = load i32, ptr addrspace(3) %p2, align 1
   %idx12 = getelementptr inbounds [5 x i32], [5 x i32]* @array1, i64 1, i64 2
   %val2 = load i32, i32* %idx12, align 4
   %idx22 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 3, i64 2
   store i32 %val2, i32 *%idx22
   %idx13 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 1, i64 0
   %val3 = load i32, i32* %idx13, align 4
   %idx23 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 3, i64 0
   store i32 %val3, i32 *%idx23
   %idx14 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 4, i64 1
   %val4 = load i32, i32* %idx14, align 4
   %idx24 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 3, i64 0
   %idx15 = getelementptr inbounds [5 x i32], [5 x i32]* @array3, i64 0, i64 0
   %val5 = load i32, i32* %idx15, align 4
   %idx16 = getelementptr inbounds [5 x i32], [5 x i32]* @array1, i64 0, i64 0
   %val6 = load i32, i32* %idx16, align 4
   %idx17 = getelementptr inbounds [5 x i32], [5 x i32]* @array2, i64 0, i64 0
   %val7 = load i32, i32* %idx17, align 4
   %idx18 = getelementptr inbounds [5 x i32], [5 x i32]* @array4, i64 0, i64 0
   %val8 = load i32, i32* %idx18, align 4
   %mul10 = mul i32 %val5, %val6
   %mul11 = mul i32 %val7, %val8
   %add100 = add i32 %mul10, %mul11
   %extract1 = extractelement < 8 x i32> %load2, i32 1
   %extract2 = extractelement < 8 x i32> %load2, i32 2
   %extract3 = extractelement < 8 x i32> %load2, i32 6
   %extract4 = extractelement < 8 x i32> %load2, i32 7
   br label %bb2

bb2:
   %phi1 = phi i32 [ %ld1, %entry ], [ %val2, %bb1 ]
   %phi2 = phi i32 [ %val0, %entry ], [ %val3, %bb1 ]
   %phi3 = phi i32 [ %val0, %entry ], [ %val4, %bb1 ]
   %phi4 = phi i32 [ %val0, %entry ], [ %val5, %bb1 ]
   %phi5 = phi i32 [ %val0, %entry ], [ %val6, %bb1 ]
   %phi6 = phi i32 [ %val0, %entry ], [ %val7, %bb1 ]
   %phi7 = phi i32 [ %val0, %entry ], [ %val8, %bb1 ]
   %phi8 = phi i32 [ %val0, %entry ], [ %add100, %bb1 ]
   %phi9 = phi i32 [ %val0, %entry ], [ %extract1, %bb1 ]
   %phi10 = phi i32 [ %val0, %entry ], [ %extract2, %bb1 ]
   %phi11 = phi i32 [ %val0, %entry ], [ %extract3, %bb1 ]
   %phi12 = phi i32 [ %val0, %entry ], [ %extract4, %bb1 ]
   %phi13 = phi i32 [ %ld2, %entry ], [ %ld1, %bb1 ]
   %phi14 = phi i32 [ %val0, %entry ], [ %arg1, %bb1 ]
   %phi15 = phi i32 [ %add1, %entry ], [ %ld3, %bb1 ]
   %extract5 = extractelement < 8 x i32> %load3, i32 3
   %extract6 = extractelement < 8 x i32> %load3, i32 5
   %res1 = add i32 %phi1, %extract5
   %res2 = mul i32 %res1, %extract6
   %res3 = sub i32 %res2, %val0
   %res4 = sub i32 %res3, %phi2
   %res5 = add i32 %res4, %phi3
   %res6 = sub i32 %res5, %tmp1
   %res7 = mul i32 %res6, %phi4
   %res8 = mul i32 %res7, %phi5
   %res9 = sub i32 %res8, %phi6
   %res10 = add i32 %res9, %phi7
   %res11 = mul i32 %res10, %phi8
   %res12 = add i32 %res11, %phi9
   %res13 = add i32 %res12, %phi10
   %res14 = sub i32 %res13, %phi11
   %res15 = sub i32 %res14, %phi12
   %res16 = mul i32 %res15, %phi13
   %res17 = mul i32 %res16, %phi14
   %res18 = add i32 %res17, %phi15
   %res19 = sub i32 %res18, %arg2
   store i32 %res19, ptr addrspace(1) %p5, align 2
   ret void
}
