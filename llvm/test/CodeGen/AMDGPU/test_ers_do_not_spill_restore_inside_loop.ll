; NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=amdgcn -mcpu=gfx1200 -stop-after=amdgpu-early-register-spilling -verify-machineinstrs -print-after=amdgpu-early-register-spilling -max-vgprs=13 < %s 2>&1 | FileCheck %s

;
;           bb.0.entry
;               |
;       bb.1.loop1.header<---+
;          /        |        |
;bb.4.loop1.latch2  |        |
;          \        |        |
;           bb.2.Flow        |
;           /       |        |
;bb.3.loop1.latch1  |        |
;           \       |        |
;           bb.5.Flow1-------+
;               |
;            bb.6.bb
;               |
;           bb.7.loop2<------+
;               |            |
;               +------------+
;               |
;           bb.8.exit
;
define amdgpu_ps void @test(ptr addrspace(1) %p1, ptr addrspace(1) %p2, ptr addrspace(1) %p3, ptr addrspace(1) %p4, ptr addrspace(1) %p5, i1 %cond, i32 %TC1, i32 %TC2, i32 %TC3) {
  ; CHECK-LABEL: name: test
  ; CHECK: bb.0.entry:
  ; CHECK-NEXT:   successors: %bb.1(0x80000000)
  ; CHECK-NEXT:   liveins: $vgpr0, $vgpr1, $vgpr2, $vgpr3, $vgpr4, $vgpr5, $vgpr6, $vgpr7, $vgpr8, $vgpr9, $vgpr10, $vgpr11, $vgpr12, $vgpr13
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[COPY:%[0-9]+]]:vgpr_32 = COPY $vgpr13
  ; CHECK-NEXT:   [[COPY1:%[0-9]+]]:vgpr_32 = COPY $vgpr12
  ; CHECK-NEXT:   [[COPY2:%[0-9]+]]:vgpr_32 = COPY $vgpr11
  ; CHECK-NEXT:   [[COPY3:%[0-9]+]]:vgpr_32 = COPY $vgpr10
  ; CHECK-NEXT:   [[COPY4:%[0-9]+]]:vgpr_32 = COPY $vgpr9
  ; CHECK-NEXT:   [[COPY5:%[0-9]+]]:vgpr_32 = COPY $vgpr8
  ; CHECK-NEXT:   [[COPY6:%[0-9]+]]:vgpr_32 = COPY $vgpr7
  ; CHECK-NEXT:   [[COPY7:%[0-9]+]]:vgpr_32 = COPY $vgpr6
  ; CHECK-NEXT:   [[COPY8:%[0-9]+]]:vgpr_32 = COPY $vgpr5
  ; CHECK-NEXT:   [[COPY9:%[0-9]+]]:vgpr_32 = COPY $vgpr4
  ; CHECK-NEXT:   [[COPY10:%[0-9]+]]:vgpr_32 = COPY $vgpr3
  ; CHECK-NEXT:   [[COPY11:%[0-9]+]]:vgpr_32 = COPY $vgpr2
  ; CHECK-NEXT:   [[COPY12:%[0-9]+]]:vgpr_32 = COPY $vgpr1
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY]], %stack.0, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.0, addrspace 5)
  ; CHECK-NEXT:   [[COPY13:%[0-9]+]]:vgpr_32 = COPY $vgpr0
  ; CHECK-NEXT:   SI_SPILL_V32_SAVE [[COPY2]], %stack.2, $sgpr32, 0, implicit $exec :: (store (s32) into %stack.2, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY5]], %subreg.sub0, [[COPY4]], %subreg.sub1
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE]], %stack.1, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.1, align 4, addrspace 5)
  ; CHECK-NEXT:   [[REG_SEQUENCE1:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY7]], %subreg.sub0, [[COPY6]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE2:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY9]], %subreg.sub0, [[COPY8]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE3:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY11]], %subreg.sub0, [[COPY10]], %subreg.sub1
  ; CHECK-NEXT:   [[REG_SEQUENCE4:%[0-9]+]]:vreg_64 = REG_SEQUENCE [[COPY13]], %subreg.sub0, [[COPY12]], %subreg.sub1
  ; CHECK-NEXT:   [[V_AND_B32_e64_:%[0-9]+]]:vgpr_32 = V_AND_B32_e64 1, [[COPY3]], implicit $exec
  ; CHECK-NEXT:   [[V_CMP_NE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_NE_U32_e64 1, [[V_AND_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 0, 0, implicit $exec :: (load (s8) from %ir.p1, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 1, 0, implicit $exec :: (load (s8) from %ir.p1 + 1, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE1]], 8, [[GLOBAL_LOAD_UBYTE]], implicit $exec
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 2, 0, implicit $exec :: (load (s8) from %ir.p1 + 2, addrspace 1)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_UBYTE3:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_UBYTE [[REG_SEQUENCE4]], 3, 0, implicit $exec :: (load (s8) from %ir.p1 + 3, addrspace 1)
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_1:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[GLOBAL_LOAD_UBYTE3]], 8, [[GLOBAL_LOAD_UBYTE2]], implicit $exec
  ; CHECK-NEXT:   [[V_LSHL_OR_B32_e64_2:%[0-9]+]]:vgpr_32 = V_LSHL_OR_B32_e64 [[V_LSHL_OR_B32_e64_1]], 16, [[V_LSHL_OR_B32_e64_]], implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 10, implicit $exec
  ; CHECK-NEXT:   [[S_MOV_B32_:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[V_MOV_B32_e32_1:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE1]], %stack.3, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.3, align 4, addrspace 5)
  ; CHECK-NEXT:   SI_SPILL_V64_SAVE [[REG_SEQUENCE2]], %stack.4, $sgpr32, 0, implicit $exec :: (store (s64) into %stack.4, align 4, addrspace 5)
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.2, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.2, addrspace 5)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.1.loop1.header:
  ; CHECK-NEXT:   successors: %bb.4(0x40000000), %bb.2(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_]], %bb.0, %24, %bb.5
  ; CHECK-NEXT:   [[PHI1:%[0-9]+]]:vgpr_32 = PHI [[V_MOV_B32_e32_1]], %bb.0, %20, %bb.5
  ; CHECK-NEXT:   [[PHI2:%[0-9]+]]:vgpr_32 = PHI [[V_MOV_B32_e32_]], %bb.0, %19, %bb.5
  ; CHECK-NEXT:   [[PHI3:%[0-9]+]]:vgpr_32 = PHI [[V_LSHL_OR_B32_e64_2]], %bb.0, %22, %bb.5
  ; CHECK-NEXT:   [[S_MOV_B32_1:%[0-9]+]]:sreg_32 = S_MOV_B32 -1
  ; CHECK-NEXT:   [[SI_IF:%[0-9]+]]:sreg_32 = SI_IF [[V_CMP_NE_U32_e64_]], %bb.2, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.4
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.2.Flow:
  ; CHECK-NEXT:   successors: %bb.3(0x40000000), %bb.5(0x40000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI4:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_1]], %bb.1, %122, %bb.4
  ; CHECK-NEXT:   [[PHI5:%[0-9]+]]:vgpr_32 = PHI undef %77:vgpr_32, %bb.1, %16, %bb.4
  ; CHECK-NEXT:   [[PHI6:%[0-9]+]]:vgpr_32 = PHI undef %77:vgpr_32, %bb.1, %15, %bb.4
  ; CHECK-NEXT:   [[PHI7:%[0-9]+]]:vgpr_32 = PHI undef %77:vgpr_32, %bb.1, %17, %bb.4
  ; CHECK-NEXT:   [[PHI8:%[0-9]+]]:vgpr_32 = PHI [[PHI2]], %bb.1, undef %129:vgpr_32, %bb.4
  ; CHECK-NEXT:   [[PHI9:%[0-9]+]]:vgpr_32 = PHI [[PHI1]], %bb.1, undef %131:vgpr_32, %bb.4
  ; CHECK-NEXT:   [[V_MOV_B32_e32_2:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[SI_ELSE:%[0-9]+]]:sreg_32 = SI_ELSE [[SI_IF]], %bb.5, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.3
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.3.loop1.latch1:
  ; CHECK-NEXT:   successors: %bb.5(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[V_SUB_U32_e64_:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_LSHL_OR_B32_e64_2]], [[PHI8]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[PHI9]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_]], [[SI_SPILL_V32_RESTORE]], implicit $exec
  ; CHECK-NEXT:   [[V_MOV_B32_e32_3:%[0-9]+]]:vgpr_32 = V_MOV_B32_e32 0, implicit $exec
  ; CHECK-NEXT:   [[S_ANDN2_B32_:%[0-9]+]]:sreg_32 = S_ANDN2_B32 [[PHI4]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_AND_B32_:%[0-9]+]]:sreg_32 = S_AND_B32 [[V_CMP_GE_U32_e64_]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   [[S_OR_B32_:%[0-9]+]]:sreg_32 = S_OR_B32 [[S_ANDN2_B32_]], [[S_AND_B32_]], implicit-def dead $scc
  ; CHECK-NEXT:   S_BRANCH %bb.5
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.4.loop1.latch2:
  ; CHECK-NEXT:   successors: %bb.2(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[REG_SEQUENCE3]], 0, 0, implicit $exec :: (load (s32) from %ir.p2, addrspace 1)
  ; CHECK-NEXT:   [[V_ADD_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 1, [[PHI2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[GLOBAL_LOAD_DWORD]], [[V_ADD_U32_e64_1]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_1:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[V_ADD_U32_e64_1]], [[COPY1]], implicit $exec
  ; CHECK-NEXT:   [[S_ORN2_B32_:%[0-9]+]]:sreg_32 = S_ORN2_B32 [[V_CMP_GE_U32_e64_1]], $exec_lo, implicit-def dead $scc
  ; CHECK-NEXT:   S_BRANCH %bb.2
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.5.Flow2:
  ; CHECK-NEXT:   successors: %bb.6(0x04000000), %bb.1(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI10:%[0-9]+]]:sreg_32 = PHI [[PHI4]], %bb.2, [[S_OR_B32_]], %bb.3
  ; CHECK-NEXT:   [[PHI11:%[0-9]+]]:vgpr_32 = PHI [[PHI5]], %bb.2, [[V_MOV_B32_e32_3]], %bb.3
  ; CHECK-NEXT:   [[PHI12:%[0-9]+]]:vgpr_32 = PHI [[V_MOV_B32_e32_2]], %bb.2, [[V_ADD_U32_e64_]], %bb.3
  ; CHECK-NEXT:   [[PHI13:%[0-9]+]]:vgpr_32 = PHI [[PHI6]], %bb.2, [[V_LSHL_OR_B32_e64_2]], %bb.3
  ; CHECK-NEXT:   [[PHI14:%[0-9]+]]:vgpr_32 = PHI [[PHI7]], %bb.2, [[V_SUB_U32_e64_]], %bb.3
  ; CHECK-NEXT:   SI_END_CF [[SI_ELSE]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[PHI10]], [[PHI]], implicit-def dead $scc
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK]], %bb.1, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.6
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.6.bb1:
  ; CHECK-NEXT:   successors: %bb.7(0x80000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[PHI14]], [[V_LSHL_OR_B32_e64_2]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.4, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.4, align 4, addrspace 5)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE]], [[V_MUL_LO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p3, addrspace 1)
  ; CHECK-NEXT:   [[S_MOV_B32_2:%[0-9]+]]:sreg_32 = S_MOV_B32 0
  ; CHECK-NEXT:   [[SI_SPILL_V32_RESTORE1:%[0-9]+]]:vgpr_32 = SI_SPILL_V32_RESTORE %stack.0, $sgpr32, 0, implicit $exec :: (load (s32) from %stack.0, addrspace 5)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.7.loop2:
  ; CHECK-NEXT:   successors: %bb.8(0x04000000), %bb.7(0x7c000000)
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT:   [[PHI15:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_2]], %bb.6, %34, %bb.7
  ; CHECK-NEXT:   [[PHI16:%[0-9]+]]:sreg_32 = PHI [[S_MOV_B32_2]], %bb.6, %33, %bb.7
  ; CHECK-NEXT:   [[PHI17:%[0-9]+]]:vgpr_32 = PHI [[PHI14]], %bb.6, [[V_LSHL_OR_B32_e64_2]], %bb.7
  ; CHECK-NEXT:   [[S_ADD_I32_:%[0-9]+]]:sreg_32 = S_ADD_I32 [[PHI16]], 2, implicit-def dead $scc
  ; CHECK-NEXT:   [[V_CMP_GE_U32_e64_2:%[0-9]+]]:sreg_32 = V_CMP_GE_U32_e64 [[S_ADD_I32_]], [[SI_SPILL_V32_RESTORE1]], implicit $exec
  ; CHECK-NEXT:   [[SI_IF_BREAK1:%[0-9]+]]:sreg_32 = SI_IF_BREAK [[V_CMP_GE_U32_e64_2]], [[PHI15]], implicit-def dead $scc
  ; CHECK-NEXT:   [[COPY14:%[0-9]+]]:vgpr_32 = COPY [[S_ADD_I32_]], implicit $exec
  ; CHECK-NEXT:   SI_LOOP [[SI_IF_BREAK1]], %bb.7, implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   S_BRANCH %bb.8
  ; CHECK-NEXT: {{  $}}
  ; CHECK-NEXT: bb.8.exit:
  ; CHECK-NEXT:   SI_END_CF [[SI_IF_BREAK1]], implicit-def dead $exec, implicit-def dead $scc, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_3:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[PHI17]], [[COPY14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD_U32_e64_4:%[0-9]+]]:vgpr_32 = V_ADD_U32_e64 [[V_ADD_U32_e64_3]], [[PHI14]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_ADD_U32_e64_4]], [[V_MUL_LO_U32_e64_]], [[V_ADD_U32_e64_3]], implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_1:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_LSHL_OR_B32_e64_2]], [[V_ADD3_U32_e64_]], 100, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_1:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_ADD3_U32_e64_1]], [[PHI17]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_1:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_4]], [[PHI13]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_2:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_LO_U32_e64_1]], [[V_SUB_U32_e64_1]], implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE1:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.3, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.3, align 4, addrspace 5)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD1:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[SI_SPILL_V64_RESTORE1]], 0, 0, implicit $exec :: (load (s32) from %ir.p4, addrspace 1)
  ; CHECK-NEXT:   [[V_MUL_LO_U32_e64_3:%[0-9]+]]:vgpr_32 = V_MUL_LO_U32_e64 [[V_MUL_LO_U32_e64_2]], [[GLOBAL_LOAD_DWORD1]], implicit $exec
  ; CHECK-NEXT:   [[V_SUB_U32_e64_2:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_ADD_U32_e64_4]], [[GLOBAL_LOAD_DWORD1]], 0, implicit $exec
  ; CHECK-NEXT:   [[SI_SPILL_V64_RESTORE2:%[0-9]+]]:vreg_64 = SI_SPILL_V64_RESTORE %stack.1, $sgpr32, 0, implicit $exec :: (load (s64) from %stack.1, align 4, addrspace 5)
  ; CHECK-NEXT:   [[GLOBAL_LOAD_DWORD2:%[0-9]+]]:vgpr_32 = GLOBAL_LOAD_DWORD [[SI_SPILL_V64_RESTORE2]], 0, 0, implicit $exec :: (load (s32) from %ir.p5, addrspace 1)
  ; CHECK-NEXT:   [[V_SUB_U32_e64_3:%[0-9]+]]:vgpr_32 = V_SUB_U32_e64 [[V_SUB_U32_e64_2]], [[GLOBAL_LOAD_DWORD2]], 0, implicit $exec
  ; CHECK-NEXT:   [[V_ADD3_U32_e64_2:%[0-9]+]]:vgpr_32 = V_ADD3_U32_e64 [[V_MUL_LO_U32_e64_3]], [[V_SUB_U32_e64_3]], [[PHI3]], implicit $exec
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE1]], [[V_ADD3_U32_e64_2]], 0, 0, implicit $exec :: (store (s32) into %ir.p4, addrspace 1)
  ; CHECK-NEXT:   GLOBAL_STORE_DWORD [[SI_SPILL_V64_RESTORE2]], [[V_MUL_LO_U32_e64_]], 0, 0, implicit $exec :: (store (s32) into %ir.p5, addrspace 1)
  ; CHECK-NEXT:   S_ENDPGM 0
entry:
;              entry
;                |
;  +-------->loop1.header<--------+
;  |        /           \         |
;  +--loop1.latch1  loop1.latch2--+
;                \  /
;                 bb1
;                 |
;                 +<-----+
;               loop2    |
;                 +------+
;                 |
;                exit
   %ld1 = load i32, ptr addrspace(1) %p1, align 1
   %add1 = add i32 %ld1, 100
   br label %loop1.header

loop1.header:
   %phi.inc1 = phi i32 [ 0, %entry ], [ %inc1, %loop1.latch1 ], [ 0, %loop1.latch2 ]
   %phi.inc2 = phi i32 [ 10, %entry ], [ 0, %loop1.latch1 ], [ %inc2, %loop1.latch2 ]
   %phi1 = phi i32 [ %ld1, %entry ], [ %sub, %loop1.latch1 ], [ %add2, %loop1.latch2 ]
   br i1 %cond, label %loop1.latch1, label %loop1.latch2

loop1.latch1:
   %sub = sub i32 %ld1, %phi.inc2
   %inc1 = add i32 %phi.inc1, 1
   %cond1 = icmp ult i32 %inc1, %TC1
   br i1 %cond1, label %loop1.header, label %bb1

loop1.latch2:
   %ld2 = load i32, ptr addrspace(1) %p2, align 4
   %inc2 = add i32 %phi.inc2, 1
   %add2 = add i32 %ld2, %inc2
   %cond2 = icmp ult i32 %inc2, %TC2
   br i1 %cond2, label %loop1.header, label %bb1

bb1:
   %phi2 = phi i32 [ %sub, %loop1.latch1 ], [ %add2, %loop1.latch2 ]
   %ld3 = phi i32 [ %ld1, %loop1.latch1 ], [ %ld2, %loop1.latch2 ]
   %mul = mul i32 %phi2, %ld1
   store i32 %mul, ptr addrspace(1) %p3
   br label %loop2

loop2:
   %phi.inc3 = phi i32 [ 0, %bb1 ], [ %inc3, %loop2 ]
   %phi3 = phi i32 [ %phi2, %bb1 ], [ %ld1, %loop2 ]
   %inc3 = add i32 %phi.inc3, 2
   %add3 = add i32 %phi3, %inc3
   %cond3 = icmp ult i32 %inc3, %TC3
   br i1 %cond3, label %loop2, label %exit

exit:
   %add4 = add i32 %add3, %phi2
   %add5 = add i32 %add4, %mul
   %add6 = add i32 %add5, %add3
   %add7 = add i32 %add6, %add1
   %mul2 = mul i32 %add7, %phi3
   %sub1 = sub i32 %add4, %ld3
   %mul3 = mul i32 %mul2, %sub1
   %ld4 = load i32, ptr addrspace(1) %p4, align 4
   %mul4 = mul i32 %mul3, %ld4
   %sub2 = sub i32 %add4, %ld4
   %ld5 = load i32, ptr addrspace(1) %p5, align 4
   %sub3 = sub i32 %sub2, %ld5
   %add8 = add i32 %mul4, %sub3
   %add9 = add i32 %add8, %phi1
   store i32 %add9, ptr addrspace(1) %p4, align 4
   store i32 %mul, ptr addrspace(1) %p5, align 4
   ret void
}
