; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=mipsel-unknown-linux-gnu -O3 | FileCheck %s --check-prefix=M32
; RUN: llc < %s -mtriple=mips64el-unknown-linux-gnu -O3 | FileCheck %s --check-prefix=M64

; Test 1: Basic optimizations should still work
define i32 @test_basic_opts(i32 %x) {
; M32-LABEL: test_basic_opts:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    move $2, $4
;
; M64-LABEL: test_basic_opts:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    sll $2, $4, 0
  %a = or i32 %x, 0
  %b = and i32 %a, -1
  %c = xor i32 %b, 0
  ret i32 %c
}

; Test 2: Constant folding should work
define i32 @test_constant_fold() {
; M32-LABEL: test_constant_fold:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    addiu $2, $zero, 0
;
; M64-LABEL: test_constant_fold:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    addiu $2, $zero, 0
  %a = xor i32 -1, -1    ; Should fold to 0
  ret i32 %a
}

; Test 3: Protected pattern should NOT have branches
define i32 @test_protected_no_branch(i1 %cond, i32 %a, i32 %b) {
; M32-LABEL: test_protected_no_branch:
; M32:       # %bb.0:
; M32-NEXT:    andi $1, $4, 1
; M32-NEXT:    andi $1, $1, 1
; M32-NEXT:    negu $1, $1
; M32-NEXT:    and $2, $1, $5
; M32-NEXT:    not $1, $1
; M32-NEXT:    and $1, $1, $6
; M32-NEXT:    jr $ra
; M32-NEXT:    or $2, $2, $1
;
; M64-LABEL: test_protected_no_branch:
; M64:       # %bb.0:
; M64-NEXT:    sll $1, $4, 0
; M64-NEXT:    sll $2, $5, 0
; M64-NEXT:    sll $3, $6, 0
; M64-NEXT:    andi $1, $1, 1
; M64-NEXT:    andi $1, $1, 1
; M64-NEXT:    negu $1, $1
; M64-NEXT:    and $2, $1, $2
; M64-NEXT:    not $1, $1
; M64-NEXT:    and $1, $1, $3
; M64-NEXT:    jr $ra
; M64-NEXT:    or $2, $2, $1
  %result = call i32 @llvm.ct.select.i32(i1 %cond, i32 %a, i32 %b)
  ret i32 %result
}

; Test 4: Explicit branch should still generate branches
define i32 @test_explicit_branch(i1 %cond, i32 %a, i32 %b) {
; M32-LABEL: test_explicit_branch:
; M32:       # %bb.0:
; M32-NEXT:    andi $1, $4, 1
; M32-NEXT:    beqz $1, $BB3_2
; M32-NEXT:    nop
; M32-NEXT:  # %bb.1: # %true
; M32-NEXT:    jr $ra
; M32-NEXT:    move $2, $5
; M32-NEXT:  $BB3_2: # %false
; M32-NEXT:    jr $ra
; M32-NEXT:    move $2, $6
;
; M64-LABEL: test_explicit_branch:
; M64:       # %bb.0:
; M64-NEXT:    sll $1, $4, 0
; M64-NEXT:    andi $1, $1, 1
; M64-NEXT:    beqz $1, .LBB3_2
; M64-NEXT:    nop
; M64-NEXT:  # %bb.1: # %true
; M64-NEXT:    jr $ra
; M64-NEXT:    sll $2, $5, 0
; M64-NEXT:  .LBB3_2: # %false
; M64-NEXT:    jr $ra
; M64-NEXT:    sll $2, $6, 0
  br i1 %cond, label %true, label %false
true:
  ret i32 %a
false:
  ret i32 %b
}

; Test 5: Regular select (not ct.select) - whatever wasm wants to do
define i32 @test_regular_select(i1 %cond, i32 %a, i32 %b) {
; M32-LABEL: test_regular_select:
; M32:       # %bb.0:
; M32-NEXT:    andi $1, $4, 1
; M32-NEXT:    movn $6, $5, $1
; M32-NEXT:    jr $ra
; M32-NEXT:    move $2, $6
;
; M64-LABEL: test_regular_select:
; M64:       # %bb.0:
; M64-NEXT:    sll $3, $4, 0
; M64-NEXT:    sll $2, $6, 0
; M64-NEXT:    sll $1, $5, 0
; M64-NEXT:    andi $3, $3, 1
; M64-NEXT:    jr $ra
; M64-NEXT:    movn $2, $1, $3
  %result = select i1 %cond, i32 %a, i32 %b
  ret i32 %result
}

; Test if XOR with all-ones still gets optimized
define i32 @test_xor_all_ones() {
; M32-LABEL: test_xor_all_ones:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    addiu $2, $zero, 0
;
; M64-LABEL: test_xor_all_ones:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    addiu $2, $zero, 0
  %xor1 = xor i32 -1, -1  ; Should optimize to 0
  ret i32 %xor1
}

define i32 @test_xor_same_value(i32 %x) {
; M32-LABEL: test_xor_same_value:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    addiu $2, $zero, 0
;
; M64-LABEL: test_xor_same_value:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    addiu $2, $zero, 0
  %xor2 = xor i32 %x, %x  ; Should optimize to 0
  ret i32 %xor2
}

define i32 @test_normal_ops(i32 %x) {
; M32-LABEL: test_normal_ops:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    move $2, $4
;
; M64-LABEL: test_normal_ops:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    sll $2, $4, 0
  %or1 = or i32 %x, 0      ; Should optimize to %x
  %and1 = and i32 %or1, -1  ; Should optimize to %x
  %xor1 = xor i32 %and1, 0  ; Should optimize to %x
  ret i32 %xor1
}

; This simulates what the reviewer is worried about
define i32 @test_xor_with_const_operands() {
; M32-LABEL: test_xor_with_const_operands:
; M32:       # %bb.0:
; M32-NEXT:    jr $ra
; M32-NEXT:    addiu $2, $zero, 0
;
; M64-LABEL: test_xor_with_const_operands:
; M64:       # %bb.0:
; M64-NEXT:    jr $ra
; M64-NEXT:    addiu $2, $zero, 0
  %a = xor i32 -1, -1      
  %b = xor i32 0, 0       
  %c = xor i32 42, 42    
  %result = or i32 %a, %b
  %final = or i32 %result, %c
  ret i32 %final  ; Should optimize to 0
}

declare i32 @llvm.ct.select.i32(i1, i32, i32)
