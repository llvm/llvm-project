; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s --mattr=+mve -o - | FileCheck %s

target triple = "thumbv8.1m.main-none-none-eabi"


; Expected to not transform
define arm_aapcs_vfpcc <2 x i64> @complex_add_v2i64(<2 x i64> %a, <2 x i64> %b) {
; CHECK-LABEL: complex_add_v2i64:
; CHECK:       @ %bb.0: @ %entry
; CHECK-NEXT:    .save {r7, lr}
; CHECK-NEXT:    push {r7, lr}
; CHECK-NEXT:    vmov r0, r1, d0
; CHECK-NEXT:    vmov r2, r3, d3
; CHECK-NEXT:    adds.w lr, r2, r0
; CHECK-NEXT:    adc.w r12, r3, r1
; CHECK-NEXT:    vmov r2, r3, d1
; CHECK-NEXT:    vmov r1, r0, d2
; CHECK-NEXT:    subs r1, r1, r2
; CHECK-NEXT:    vmov q0[2], q0[0], r1, lr
; CHECK-NEXT:    sbcs r0, r3
; CHECK-NEXT:    vmov q0[3], q0[1], r0, r12
; CHECK-NEXT:    pop {r7, pc}
entry:
  %a.real = shufflevector <2 x i64> %a, <2 x i64> zeroinitializer, <1 x i32> <i32 0>
  %a.imag = shufflevector <2 x i64> %a, <2 x i64> zeroinitializer, <1 x i32> <i32 1>
  %b.real = shufflevector <2 x i64> %b, <2 x i64> zeroinitializer, <1 x i32> <i32 0>
  %b.imag = shufflevector <2 x i64> %b, <2 x i64> zeroinitializer, <1 x i32> <i32 1>
  %0 = sub <1 x i64> %b.real, %a.imag
  %1 = add <1 x i64> %b.imag, %a.real
  %interleaved.vec = shufflevector <1 x i64> %0, <1 x i64> %1, <2 x i32> <i32 0, i32 1>
  ret <2 x i64> %interleaved.vec
}

; Expected to not transform
define arm_aapcs_vfpcc <4 x i64> @complex_add_v4i64(<4 x i64> %a, <4 x i64> %b) {
; CHECK-LABEL: complex_add_v4i64:
; CHECK:       @ %bb.0: @ %entry
; CHECK-NEXT:    .save {r7, lr}
; CHECK-NEXT:    push {r7, lr}
; CHECK-NEXT:    .vsave {d8, d9}
; CHECK-NEXT:    vpush {d8, d9}
; CHECK-NEXT:    vmov q4, q1
; CHECK-NEXT:    vmov r2, r3, d7
; CHECK-NEXT:    vmov r0, r1, d8
; CHECK-NEXT:    adds.w lr, r2, r0
; CHECK-NEXT:    adc.w r12, r3, r1
; CHECK-NEXT:    vmov r2, r3, d0
; CHECK-NEXT:    vmov r1, r0, d5
; CHECK-NEXT:    adds r1, r1, r2
; CHECK-NEXT:    adcs r0, r3
; CHECK-NEXT:    vmov q1[2], q1[0], r1, lr
; CHECK-NEXT:    vmov q1[3], q1[1], r0, r12
; CHECK-NEXT:    vmov r0, r1, d9
; CHECK-NEXT:    vmov r2, r3, d6
; CHECK-NEXT:    subs.w lr, r2, r0
; CHECK-NEXT:    sbc.w r12, r3, r1
; CHECK-NEXT:    vmov r2, r3, d1
; CHECK-NEXT:    vmov r1, r0, d4
; CHECK-NEXT:    vmov.f32 s2, s4
; CHECK-NEXT:    vmov.f32 s3, s5
; CHECK-NEXT:    subs r1, r1, r2
; CHECK-NEXT:    vmov q2[2], q2[0], r1, lr
; CHECK-NEXT:    sbcs r0, r3
; CHECK-NEXT:    vmov q2[3], q2[1], r0, r12
; CHECK-NEXT:    vmov.f32 s0, s8
; CHECK-NEXT:    vmov.f32 s4, s10
; CHECK-NEXT:    vmov.f32 s1, s9
; CHECK-NEXT:    vmov.f32 s5, s11
; CHECK-NEXT:    vpop {d8, d9}
; CHECK-NEXT:    pop {r7, pc}
entry:
  %a.real = shufflevector <4 x i64> %a, <4 x i64> zeroinitializer, <2 x i32> <i32 0, i32 2>
  %a.imag = shufflevector <4 x i64> %a, <4 x i64> zeroinitializer, <2 x i32> <i32 1, i32 3>
  %b.real = shufflevector <4 x i64> %b, <4 x i64> zeroinitializer, <2 x i32> <i32 0, i32 2>
  %b.imag = shufflevector <4 x i64> %b, <4 x i64> zeroinitializer, <2 x i32> <i32 1, i32 3>
  %0 = sub <2 x i64> %b.real, %a.imag
  %1 = add <2 x i64> %b.imag, %a.real
  %interleaved.vec = shufflevector <2 x i64> %0, <2 x i64> %1, <4 x i32> <i32 0, i32 2, i32 1, i32 3>
  ret <4 x i64> %interleaved.vec
}

; Expected to not transform
define arm_aapcs_vfpcc <8 x i64> @complex_add_v8i64(<8 x i64> %a, <8 x i64> %b) {
; CHECK-LABEL: complex_add_v8i64:
; CHECK:       @ %bb.0: @ %entry
; CHECK-NEXT:    .save {r7, lr}
; CHECK-NEXT:    push {r7, lr}
; CHECK-NEXT:    .vsave {d8, d9, d10, d11, d12, d13}
; CHECK-NEXT:    vpush {d8, d9, d10, d11, d12, d13}
; CHECK-NEXT:    add r2, sp, #72
; CHECK-NEXT:    vmov q4, q1
; CHECK-NEXT:    vldrw.u32 q5, [r2]
; CHECK-NEXT:    vmov r0, r1, d8
; CHECK-NEXT:    vmov r2, r3, d11
; CHECK-NEXT:    adds.w lr, r2, r0
; CHECK-NEXT:    adc.w r12, r3, r1
; CHECK-NEXT:    add r1, sp, #56
; CHECK-NEXT:    vldrw.u32 q6, [r1]
; CHECK-NEXT:    vmov r2, r3, d0
; CHECK-NEXT:    vmov r1, r0, d13
; CHECK-NEXT:    adds r1, r1, r2
; CHECK-NEXT:    adcs r0, r3
; CHECK-NEXT:    vmov q1[2], q1[0], r1, lr
; CHECK-NEXT:    vmov q1[3], q1[1], r0, r12
; CHECK-NEXT:    vmov r0, r1, d9
; CHECK-NEXT:    vmov r2, r3, d10
; CHECK-NEXT:    subs.w lr, r2, r0
; CHECK-NEXT:    sbc.w r12, r3, r1
; CHECK-NEXT:    vmov r2, r3, d1
; CHECK-NEXT:    vmov r1, r0, d12
; CHECK-NEXT:    vmov.f32 s2, s4
; CHECK-NEXT:    vmov.f32 s3, s5
; CHECK-NEXT:    subs r1, r1, r2
; CHECK-NEXT:    add r2, sp, #104
; CHECK-NEXT:    vldrw.u32 q5, [r2]
; CHECK-NEXT:    sbcs r0, r3
; CHECK-NEXT:    vmov q4[2], q4[0], r1, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r0, r12
; CHECK-NEXT:    vmov r0, r1, d6
; CHECK-NEXT:    vmov r2, r3, d11
; CHECK-NEXT:    vmov.f32 s0, s16
; CHECK-NEXT:    vmov.f32 s4, s18
; CHECK-NEXT:    vmov.f32 s1, s17
; CHECK-NEXT:    vmov.f32 s5, s19
; CHECK-NEXT:    adds.w lr, r2, r0
; CHECK-NEXT:    adc.w r12, r3, r1
; CHECK-NEXT:    add r1, sp, #88
; CHECK-NEXT:    vldrw.u32 q6, [r1]
; CHECK-NEXT:    vmov r2, r3, d4
; CHECK-NEXT:    vmov r1, r0, d13
; CHECK-NEXT:    adds r1, r1, r2
; CHECK-NEXT:    adcs r0, r3
; CHECK-NEXT:    vmov q4[2], q4[0], r1, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r0, r12
; CHECK-NEXT:    vmov r0, r1, d7
; CHECK-NEXT:    vmov r2, r3, d10
; CHECK-NEXT:    subs.w lr, r2, r0
; CHECK-NEXT:    sbc.w r12, r3, r1
; CHECK-NEXT:    vmov r2, r3, d5
; CHECK-NEXT:    vmov r1, r0, d12
; CHECK-NEXT:    vmov.f32 s10, s16
; CHECK-NEXT:    vmov.f32 s11, s17
; CHECK-NEXT:    subs r1, r1, r2
; CHECK-NEXT:    vmov q3[2], q3[0], r1, lr
; CHECK-NEXT:    sbcs r0, r3
; CHECK-NEXT:    vmov q3[3], q3[1], r0, r12
; CHECK-NEXT:    vmov.f32 s16, s14
; CHECK-NEXT:    vmov.f32 s8, s12
; CHECK-NEXT:    vmov.f32 s17, s15
; CHECK-NEXT:    vmov.f32 s9, s13
; CHECK-NEXT:    vmov q3, q4
; CHECK-NEXT:    vpop {d8, d9, d10, d11, d12, d13}
; CHECK-NEXT:    pop {r7, pc}
entry:
  %a.real = shufflevector <8 x i64> %a, <8 x i64> zeroinitializer, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %a.imag = shufflevector <8 x i64> %a, <8 x i64> zeroinitializer, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %b.real = shufflevector <8 x i64> %b, <8 x i64> zeroinitializer, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %b.imag = shufflevector <8 x i64> %b, <8 x i64> zeroinitializer, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %0 = sub <4 x i64> %b.real, %a.imag
  %1 = add <4 x i64> %b.imag, %a.real
  %interleaved.vec = shufflevector <4 x i64> %0, <4 x i64> %1, <8 x i32> <i32 0, i32 4, i32 1, i32 5, i32 2, i32 6, i32 3, i32 7>
  ret <8 x i64> %interleaved.vec
}

; Expected to not transform
define arm_aapcs_vfpcc <16 x i64> @complex_add_v16i64(<16 x i64> %a, <16 x i64> %b) {
; CHECK-LABEL: complex_add_v16i64:
; CHECK:       @ %bb.0: @ %entry
; CHECK-NEXT:    .save {r4, lr}
; CHECK-NEXT:    push {r4, lr}
; CHECK-NEXT:    .vsave {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    vpush {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    .pad #16
; CHECK-NEXT:    sub sp, #16
; CHECK-NEXT:    add r1, sp, #136
; CHECK-NEXT:    add r3, sp, #264
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vstrw.32 q0, [sp] @ 16-byte Spill
; CHECK-NEXT:    vmov r1, r12, d10
; CHECK-NEXT:    vmov r3, r2, d13
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #120
; CHECK-NEXT:    add r1, sp, #248
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r1]
; CHECK-NEXT:    adc.w r12, r12, r2
; CHECK-NEXT:    vmov r3, r2, d14
; CHECK-NEXT:    vmov r1, r4, d1
; CHECK-NEXT:    adds r1, r1, r3
; CHECK-NEXT:    vmov q4[2], q4[0], r1, lr
; CHECK-NEXT:    adc.w r1, r4, r2
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #232
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #104
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #112]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #96]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #88
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #216
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #200
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d6
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #80]
; CHECK-NEXT:    vldrw.u32 q5, [r3]
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov r3, r4, d11
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #64]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #184
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d4
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d7
; CHECK-NEXT:    vmov r3, r4, d10
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d5
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    vmov.f32 s10, s18
; CHECK-NEXT:    vmov.f32 s11, s19
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #168
; CHECK-NEXT:    vldrw.u32 q3, [r3]
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d2
; CHECK-NEXT:    vmov r3, r4, d7
; CHECK-NEXT:    vmov.f32 s8, s2
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s9, s3
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q2, [r0, #48]
; CHECK-NEXT:    vstrw.32 q0, [r0, #32]
; CHECK-NEXT:    vldrw.u32 q4, [sp] @ 16-byte Reload
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #152
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d8
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q2[2], q2[0], r2, lr
; CHECK-NEXT:    vmov q2[3], q2[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d3
; CHECK-NEXT:    vmov r3, r4, d6
; CHECK-NEXT:    vmov.f32 s6, s10
; CHECK-NEXT:    vmov.f32 s7, s11
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d9
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov.f32 s4, s2
; CHECK-NEXT:    vmov.f32 s2, s8
; CHECK-NEXT:    vmov.f32 s5, s3
; CHECK-NEXT:    vmov.f32 s3, s9
; CHECK-NEXT:    vstrw.32 q1, [r0, #16]
; CHECK-NEXT:    vstrw.32 q0, [r0]
; CHECK-NEXT:    add sp, #16
; CHECK-NEXT:    vpop {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    pop {r4, pc}
entry:
  %a.real = shufflevector <16 x i64> %a, <16 x i64> zeroinitializer, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %a.imag = shufflevector <16 x i64> %a, <16 x i64> zeroinitializer, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %b.real = shufflevector <16 x i64> %b, <16 x i64> zeroinitializer, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %b.imag = shufflevector <16 x i64> %b, <16 x i64> zeroinitializer, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %0 = sub <8 x i64> %b.real, %a.imag
  %1 = add <8 x i64> %b.imag, %a.real
  %interleaved.vec = shufflevector <8 x i64> %0, <8 x i64> %1, <16 x i32> <i32 0, i32 8, i32 1, i32 9, i32 2, i32 10, i32 3, i32 11, i32 4, i32 12, i32 5, i32 13, i32 6, i32 14, i32 7, i32 15>
  ret <16 x i64> %interleaved.vec
}

; Expected to not transform
define arm_aapcs_vfpcc <32 x i64> @complex_add_v32i64(<32 x i64> %a, <32 x i64> %b) {
; CHECK-LABEL: complex_add_v32i64:
; CHECK:       @ %bb.0: @ %entry
; CHECK-NEXT:    .save {r4, r5, r6, lr}
; CHECK-NEXT:    push {r4, r5, r6, lr}
; CHECK-NEXT:    .vsave {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    vpush {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    .pad #16
; CHECK-NEXT:    sub sp, #16
; CHECK-NEXT:    add r1, sp, #272
; CHECK-NEXT:    add r3, sp, #528
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vstrw.32 q0, [sp] @ 16-byte Spill
; CHECK-NEXT:    vmov r1, r12, d10
; CHECK-NEXT:    vmov r3, r2, d13
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #256
; CHECK-NEXT:    add r1, sp, #512
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r1]
; CHECK-NEXT:    adc.w r12, r12, r2
; CHECK-NEXT:    vmov r3, r2, d14
; CHECK-NEXT:    vmov r1, r4, d1
; CHECK-NEXT:    adds r1, r1, r3
; CHECK-NEXT:    vmov q4[2], q4[0], r1, lr
; CHECK-NEXT:    adc.w r1, r4, r2
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #496
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #240
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #240]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #224]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #224
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #480
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #464
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #208
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #208]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #192]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #192
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #448
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #432
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #176
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #176]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #160]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #160
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #416
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #400
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #144
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #144]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #128]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #128
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #384
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #368
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    add r1, sp, #112
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vldrw.u32 q6, [r3]
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #112]
; CHECK-NEXT:    vldrw.u32 q5, [r1]
; CHECK-NEXT:    vmov r3, r4, d13
; CHECK-NEXT:    vmov r1, r2, d10
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #96]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    add r3, sp, #96
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #352
; CHECK-NEXT:    vldrw.u32 q7, [r3]
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d14
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d11
; CHECK-NEXT:    vmov r3, r4, d12
; CHECK-NEXT:    vmov.f32 s22, s18
; CHECK-NEXT:    vmov.f32 s23, s19
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d15
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #336
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d6
; CHECK-NEXT:    vmov.f32 s20, s2
; CHECK-NEXT:    vmov.f32 s21, s3
; CHECK-NEXT:    vstrw.32 q5, [r0, #80]
; CHECK-NEXT:    vldrw.u32 q5, [r3]
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov r3, r4, d11
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q0, [r0, #64]
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #320
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d4
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q4[2], q4[0], r2, lr
; CHECK-NEXT:    vmov q4[3], q4[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d7
; CHECK-NEXT:    vmov r3, r4, d10
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d5
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    vmov.f32 s10, s18
; CHECK-NEXT:    vmov.f32 s11, s19
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    add r3, sp, #304
; CHECK-NEXT:    vldrw.u32 q3, [r3]
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d2
; CHECK-NEXT:    vmov r3, r4, d7
; CHECK-NEXT:    vmov.f32 s8, s2
; CHECK-NEXT:    vmov.f32 s2, s16
; CHECK-NEXT:    vmov.f32 s9, s3
; CHECK-NEXT:    vmov.f32 s3, s17
; CHECK-NEXT:    vstrw.32 q2, [r0, #48]
; CHECK-NEXT:    vstrw.32 q0, [r0, #32]
; CHECK-NEXT:    vldrw.u32 q4, [sp] @ 16-byte Reload
; CHECK-NEXT:    adds.w lr, r3, r1
; CHECK-NEXT:    adc.w r12, r4, r2
; CHECK-NEXT:    add r2, sp, #288
; CHECK-NEXT:    vldrw.u32 q0, [r2]
; CHECK-NEXT:    vmov r3, r4, d8
; CHECK-NEXT:    vmov r2, r1, d1
; CHECK-NEXT:    adds r2, r2, r3
; CHECK-NEXT:    adcs r1, r4
; CHECK-NEXT:    vmov q2[2], q2[0], r2, lr
; CHECK-NEXT:    vmov q2[3], q2[1], r1, r12
; CHECK-NEXT:    vmov r1, r2, d3
; CHECK-NEXT:    vmov r3, r4, d6
; CHECK-NEXT:    vmov.f32 s6, s10
; CHECK-NEXT:    vmov.f32 s7, s11
; CHECK-NEXT:    subs.w lr, r3, r1
; CHECK-NEXT:    sbc.w r12, r4, r2
; CHECK-NEXT:    vmov r3, r4, d9
; CHECK-NEXT:    vmov r2, r1, d0
; CHECK-NEXT:    subs r2, r2, r3
; CHECK-NEXT:    vmov q0[2], q0[0], r2, lr
; CHECK-NEXT:    sbcs r1, r4
; CHECK-NEXT:    vmov q0[3], q0[1], r1, r12
; CHECK-NEXT:    vmov.f32 s4, s2
; CHECK-NEXT:    vmov.f32 s2, s8
; CHECK-NEXT:    vmov.f32 s5, s3
; CHECK-NEXT:    vmov.f32 s3, s9
; CHECK-NEXT:    vstrw.32 q1, [r0, #16]
; CHECK-NEXT:    vstrw.32 q0, [r0]
; CHECK-NEXT:    add sp, #16
; CHECK-NEXT:    vpop {d8, d9, d10, d11, d12, d13, d14, d15}
; CHECK-NEXT:    pop {r4, r5, r6, pc}
entry:
  %a.real = shufflevector <32 x i64> %a, <32 x i64> zeroinitializer, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %a.imag = shufflevector <32 x i64> %a, <32 x i64> zeroinitializer, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %b.real = shufflevector <32 x i64> %b, <32 x i64> zeroinitializer, <16 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14, i32 16, i32 18, i32 20, i32 22, i32 24, i32 26, i32 28, i32 30>
  %b.imag = shufflevector <32 x i64> %b, <32 x i64> zeroinitializer, <16 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15, i32 17, i32 19, i32 21, i32 23, i32 25, i32 27, i32 29, i32 31>
  %0 = sub <16 x i64> %b.real, %a.imag
  %1 = add <16 x i64> %b.imag, %a.real
  %interleaved.vec = shufflevector <16 x i64> %0, <16 x i64> %1, <32 x i32> <i32 0, i32 16, i32 1, i32 17, i32 2, i32 18, i32 3, i32 19, i32 4, i32 20, i32 5, i32 21, i32 6, i32 22, i32 7, i32 23, i32 8, i32 24, i32 9, i32 25, i32 10, i32 26, i32 11, i32 27, i32 12, i32 28, i32 13, i32 29, i32 14, i32 30, i32 15, i32 31>
  ret <32 x i64> %interleaved.vec
}
