; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+sse2 | FileCheck %s --check-prefix=X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+sse2 | FileCheck %s --check-prefix=X64
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+f16c | FileCheck %s --check-prefix=F16C
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+f16c -O0 | FileCheck %s --check-prefix=F16C-O0

define <1 x half> @ir_fadd_v1f16(<1 x half> %arg0, <1 x half> %arg1) nounwind {
; X86-LABEL: ir_fadd_v1f16:
; X86:       # %bb.0:
; X86-NEXT:    subl $28, %esp
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    movdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; X86-NEXT:    addss {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movss %xmm0, (%esp)
; X86-NEXT:    calll __truncsfhf2
; X86-NEXT:    addl $28, %esp
; X86-NEXT:    retl
;
; X64-LABEL: ir_fadd_v1f16:
; X64:       # %bb.0:
; X64-NEXT:    pushq %rax
; X64-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movaps %xmm1, %xmm0
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    movss %xmm0, (%rsp) # 4-byte Spill
; X64-NEXT:    movss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    addss (%rsp), %xmm0 # 4-byte Folded Reload
; X64-NEXT:    callq __truncsfhf2@PLT
; X64-NEXT:    popq %rax
; X64-NEXT:    retq
;
; F16C-LABEL: ir_fadd_v1f16:
; F16C:       # %bb.0:
; F16C-NEXT:    vpextrw $0, %xmm0, %eax
; F16C-NEXT:    vpextrw $0, %xmm1, %ecx
; F16C-NEXT:    movzwl %cx, %ecx
; F16C-NEXT:    vmovd %ecx, %xmm0
; F16C-NEXT:    vcvtph2ps %xmm0, %xmm0
; F16C-NEXT:    movzwl %ax, %eax
; F16C-NEXT:    vmovd %eax, %xmm1
; F16C-NEXT:    vcvtph2ps %xmm1, %xmm1
; F16C-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; F16C-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; F16C-NEXT:    vmovd %xmm0, %eax
; F16C-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm0
; F16C-NEXT:    retq
;
; F16C-O0-LABEL: ir_fadd_v1f16:
; F16C-O0:       # %bb.0:
; F16C-O0-NEXT:    vpextrw $0, %xmm1, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm1
; F16C-O0-NEXT:    vcvtph2ps %xmm1, %xmm1
; F16C-O0-NEXT:    vpextrw $0, %xmm0, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm0
; F16C-O0-NEXT:    vcvtph2ps %xmm0, %xmm0
; F16C-O0-NEXT:    vaddss %xmm1, %xmm0, %xmm0
; F16C-O0-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; F16C-O0-NEXT:    vmovd %xmm0, %eax
; F16C-O0-NEXT:    movw %ax, %cx
; F16C-O0-NEXT:    # implicit-def: $eax
; F16C-O0-NEXT:    movw %cx, %ax
; F16C-O0-NEXT:    # implicit-def: $xmm0
; F16C-O0-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm0
; F16C-O0-NEXT:    retq
  %retval = fadd <1 x half> %arg0, %arg1
  ret <1 x half> %retval
}

define <2 x half> @ir_fadd_v2f16(<2 x half> %arg0, <2 x half> %arg1) nounwind {
; X86-LABEL: ir_fadd_v2f16:
; X86:       # %bb.0:
; X86-NEXT:    subl $80, %esp
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movdqu %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 16-byte Spill
; X86-NEXT:    pinsrw $0, {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; X86-NEXT:    movdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    fstpt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Spill
; X86-NEXT:    movdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    movdqu {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 16-byte Reload
; X86-NEXT:    pextrw $0, %xmm0, %eax
; X86-NEXT:    movw %ax, (%esp)
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    calll __extendhfsf2
; X86-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; X86-NEXT:    addss {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movss %xmm0, (%esp)
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    fldt {{[-0-9]+}}(%e{{[sb]}}p) # 10-byte Folded Reload
; X86-NEXT:    fstps {{[0-9]+}}(%esp)
; X86-NEXT:    calll __truncsfhf2
; X86-NEXT:    movss %xmm0, {{[-0-9]+}}(%e{{[sb]}}p) # 4-byte Spill
; X86-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; X86-NEXT:    addss {{[0-9]+}}(%esp), %xmm0
; X86-NEXT:    movss %xmm0, (%esp)
; X86-NEXT:    calll __truncsfhf2
; X86-NEXT:    movaps %xmm0, %xmm1
; X86-NEXT:    movss {{[-0-9]+}}(%e{{[sb]}}p), %xmm0 # 4-byte Reload
; X86-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X86-NEXT:    addl $80, %esp
; X86-NEXT:    retl
;
; X64-LABEL: ir_fadd_v2f16:
; X64:       # %bb.0:
; X64-NEXT:    subq $24, %rsp
; X64-NEXT:    movss %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movss %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movaps %xmm2, %xmm0
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    addss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; X64-NEXT:    callq __truncsfhf2@PLT
; X64-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    movss %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; X64-NEXT:    movss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X64-NEXT:    callq __extendhfsf2@PLT
; X64-NEXT:    addss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Folded Reload
; X64-NEXT:    callq __truncsfhf2@PLT
; X64-NEXT:    movaps %xmm0, %xmm1
; X64-NEXT:    movss {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 4-byte Reload
; X64-NEXT:    # xmm0 = mem[0],zero,zero,zero
; X64-NEXT:    addq $24, %rsp
; X64-NEXT:    retq
;
; F16C-LABEL: ir_fadd_v2f16:
; F16C:       # %bb.0:
; F16C-NEXT:    vpextrw $0, %xmm1, %eax
; F16C-NEXT:    vpextrw $0, %xmm3, %ecx
; F16C-NEXT:    vpextrw $0, %xmm0, %edx
; F16C-NEXT:    vpextrw $0, %xmm2, %esi
; F16C-NEXT:    movzwl %si, %esi
; F16C-NEXT:    vmovd %esi, %xmm0
; F16C-NEXT:    vcvtph2ps %xmm0, %xmm0
; F16C-NEXT:    movzwl %dx, %edx
; F16C-NEXT:    vmovd %edx, %xmm1
; F16C-NEXT:    vcvtph2ps %xmm1, %xmm1
; F16C-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; F16C-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; F16C-NEXT:    vmovd %xmm0, %edx
; F16C-NEXT:    vpinsrw $0, %edx, %xmm0, %xmm0
; F16C-NEXT:    movzwl %cx, %ecx
; F16C-NEXT:    vmovd %ecx, %xmm1
; F16C-NEXT:    vcvtph2ps %xmm1, %xmm1
; F16C-NEXT:    movzwl %ax, %eax
; F16C-NEXT:    vmovd %eax, %xmm2
; F16C-NEXT:    vcvtph2ps %xmm2, %xmm2
; F16C-NEXT:    vaddss %xmm1, %xmm2, %xmm1
; F16C-NEXT:    vcvtps2ph $4, %xmm1, %xmm1
; F16C-NEXT:    vmovd %xmm1, %eax
; F16C-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm1
; F16C-NEXT:    retq
;
; F16C-O0-LABEL: ir_fadd_v2f16:
; F16C-O0:       # %bb.0:
; F16C-O0-NEXT:    vpextrw $0, %xmm2, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm2
; F16C-O0-NEXT:    vcvtph2ps %xmm2, %xmm2
; F16C-O0-NEXT:    vpextrw $0, %xmm0, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm0
; F16C-O0-NEXT:    vcvtph2ps %xmm0, %xmm0
; F16C-O0-NEXT:    vaddss %xmm2, %xmm0, %xmm0
; F16C-O0-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; F16C-O0-NEXT:    vmovd %xmm0, %eax
; F16C-O0-NEXT:    movw %ax, %cx
; F16C-O0-NEXT:    # implicit-def: $eax
; F16C-O0-NEXT:    movw %cx, %ax
; F16C-O0-NEXT:    # implicit-def: $xmm0
; F16C-O0-NEXT:    vpinsrw $0, %eax, %xmm0, %xmm0
; F16C-O0-NEXT:    vpextrw $0, %xmm3, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm2
; F16C-O0-NEXT:    vcvtph2ps %xmm2, %xmm2
; F16C-O0-NEXT:    vpextrw $0, %xmm1, %eax
; F16C-O0-NEXT:    # kill: def $ax killed $ax killed $eax
; F16C-O0-NEXT:    movzwl %ax, %eax
; F16C-O0-NEXT:    vmovd %eax, %xmm1
; F16C-O0-NEXT:    vcvtph2ps %xmm1, %xmm1
; F16C-O0-NEXT:    vaddss %xmm2, %xmm1, %xmm1
; F16C-O0-NEXT:    vcvtps2ph $4, %xmm1, %xmm1
; F16C-O0-NEXT:    vmovd %xmm1, %eax
; F16C-O0-NEXT:    movw %ax, %cx
; F16C-O0-NEXT:    # implicit-def: $eax
; F16C-O0-NEXT:    movw %cx, %ax
; F16C-O0-NEXT:    # implicit-def: $xmm1
; F16C-O0-NEXT:    vpinsrw $0, %eax, %xmm1, %xmm1
; F16C-O0-NEXT:    retq
  %retval = fadd <2 x half> %arg0, %arg1
  ret <2 x half> %retval
}
