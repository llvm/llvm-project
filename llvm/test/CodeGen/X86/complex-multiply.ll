; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+fma | FileCheck %s --check-prefixes=ALL,FMA
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512f,+avx512vl | FileCheck %s --check-prefixes=ALL,AVX512VL


; Check the expansion of the complex multiply intrinsic. This only tests
; expansion for 32-bit floats, as the expansion should produce identical IR
; expansions save for ABI of calling __mulsc3, which is tested for each type
; individually in complex-{32,64}bit.ll.

declare <2 x float> @llvm.experimental.complex.fmul.v2f32(<2 x float>, <2 x float>)
declare <4 x float> @llvm.experimental.complex.fmul.v4f32(<4 x float>, <4 x float>)
declare <8 x float> @llvm.experimental.complex.fmul.v8f32(<8 x float>, <8 x float>)
declare <16 x float> @llvm.experimental.complex.fmul.v16f32(<16 x float>, <16 x float>)
declare <2 x double> @llvm.experimental.complex.fmul.v2f64(<2 x double>, <2 x double>)
declare <4 x double> @llvm.experimental.complex.fmul.v4f64(<4 x double>, <4 x double>)
declare <8 x double> @llvm.experimental.complex.fmul.v8f64(<8 x double>, <8 x double>)
declare <6 x float> @llvm.experimental.complex.fmul.v6f32(<6 x float>, <6 x float>)
declare <6 x double> @llvm.experimental.complex.fmul.v6f64(<6 x double>, <6 x double>)
declare <32 x float> @llvm.experimental.complex.fmul.v32f32(<32 x float>, <32 x float>)

; Generate a call to __mulsc3
define <2 x float> @intrinsic_slow_v2f32(<2 x float> %z, <2 x float> %w) {
; ALL-LABEL: intrinsic_slow_v2f32:
; ALL:       # %bb.0:
; ALL-NEXT:    pushq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 16
; ALL-NEXT:    vmovaps %xmm1, %xmm2
; ALL-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; ALL-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; ALL-NEXT:    callq __mulsc3@PLT
; ALL-NEXT:    popq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 8
; ALL-NEXT:    retq
  %mul = call <2 x float> @llvm.experimental.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

; Do an expansion (because of fast-math flags).
define <2 x float> @intrinsic_implied_limited_v2f32(<2 x float> %z, <2 x float> %w)  {
; ALL-LABEL: intrinsic_implied_limited_v2f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm1[1,1,3,3]
; ALL-NEXT:    vshufps {{.*#+}} xmm3 = xmm0[1,0,3,2]
; ALL-NEXT:    vmulps %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovsldup {{.*#+}} xmm1 = xmm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call nnan ninf <2 x float> @llvm.experimental.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

; Do an expansion (because of complex-range=limited).
define <2 x float> @intrinsic_limited_v2f32(<2 x float> %z, <2 x float> %w) {
; ALL-LABEL: intrinsic_limited_v2f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm1[1,1,3,3]
; ALL-NEXT:    vshufps {{.*#+}} xmm3 = xmm0[1,0,3,2]
; ALL-NEXT:    vmulps %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovsldup {{.*#+}} xmm1 = xmm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call <2 x float> @llvm.experimental.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w) #0
  ret <2 x float> %mul
}

; Do an expansion, and use the FMA (because of fast-math flags).
define <2 x float> @intrinsic_fast_v2f32(<2 x float> %z, <2 x float> %w) {
; ALL-LABEL: intrinsic_fast_v2f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm1[1,1,3,3]
; ALL-NEXT:    vshufps {{.*#+}} xmm3 = xmm0[1,0,3,2]
; ALL-NEXT:    vmulps %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovsldup {{.*#+}} xmm1 = xmm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call fast <2 x float> @llvm.experimental.complex.fmul.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %mul
}

define <4 x float> @intrinsic_slow_v4f32(<4 x float> %z, <4 x float> %w) {
; ALL-LABEL: intrinsic_slow_v4f32:
; ALL:       # %bb.0:
; ALL-NEXT:    pushq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 16
; ALL-NEXT:    vmovaps %xmm1, %xmm2
; ALL-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; ALL-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; ALL-NEXT:    callq __mulsc3@PLT
; ALL-NEXT:    popq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 8
; ALL-NEXT:    retq
  %mul = call <4 x float> @llvm.experimental.complex.fmul.v4f32(<4 x float> %z, <4 x float> %w)
  ret <4 x float> %mul
}

define <4 x float> @intrinsic_fast_v4f32(<4 x float> %z, <4 x float> %w) {
; ALL-LABEL: intrinsic_fast_v4f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm1[1,1,3,3]
; ALL-NEXT:    vshufps {{.*#+}} xmm3 = xmm0[1,0,3,2]
; ALL-NEXT:    vmulps %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovsldup {{.*#+}} xmm1 = xmm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call fast <4 x float> @llvm.experimental.complex.fmul.v4f32(<4 x float> %z, <4 x float> %w)
  ret <4 x float> %mul
}

define <4 x float> @intrinsic_limited_v4f32(<4 x float> %z, <4 x float> %w) {
; ALL-LABEL: intrinsic_limited_v4f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm1[1,1,3,3]
; ALL-NEXT:    vshufps {{.*#+}} xmm3 = xmm0[1,0,3,2]
; ALL-NEXT:    vmulps %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovsldup {{.*#+}} xmm1 = xmm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call <4 x float> @llvm.experimental.complex.fmul.v4f32(<4 x float> %z, <4 x float> %w) #0
  ret <4 x float> %mul
}

define <8 x float> @intrinsic_slow_v8f32(<8 x float> %z, <8 x float> %w) {
; ALL-LABEL: intrinsic_slow_v8f32:
; ALL:       # %bb.0:
; ALL-NEXT:    pushq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 16
; ALL-NEXT:    vmovaps %ymm1, %ymm2
; ALL-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; ALL-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; ALL-NEXT:    # kill: def $xmm0 killed $xmm0 killed $ymm0
; ALL-NEXT:    # kill: def $xmm2 killed $xmm2 killed $ymm2
; ALL-NEXT:    callq __mulsc3@PLT
; ALL-NEXT:    popq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 8
; ALL-NEXT:    retq
  %mul = call <8 x float> @llvm.experimental.complex.fmul.v8f32(<8 x float> %z, <8 x float> %w)
  ret <8 x float> %mul
}

define <8 x float> @intrinsic_fast_v8f32(<8 x float> %z, <8 x float> %w) {
; ALL-LABEL: intrinsic_fast_v8f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} ymm2 = ymm1[1,1,3,3,5,5,7,7]
; ALL-NEXT:    vshufps {{.*#+}} ymm3 = ymm0[1,0,3,2,5,4,7,6]
; ALL-NEXT:    vmulps %ymm2, %ymm3, %ymm2
; ALL-NEXT:    vmovsldup {{.*#+}} ymm1 = ymm1[0,0,2,2,4,4,6,6]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm1 * ymm0) +/- ymm2
; ALL-NEXT:    retq
  %mul = call fast <8 x float> @llvm.experimental.complex.fmul.v8f32(<8 x float> %z, <8 x float> %w)
  ret <8 x float> %mul
}

define <8 x float> @intrinsic_limited_v8f32(<8 x float> %z, <8 x float> %w) {
; ALL-LABEL: intrinsic_limited_v8f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} ymm2 = ymm1[1,1,3,3,5,5,7,7]
; ALL-NEXT:    vshufps {{.*#+}} ymm3 = ymm0[1,0,3,2,5,4,7,6]
; ALL-NEXT:    vmulps %ymm2, %ymm3, %ymm2
; ALL-NEXT:    vmovsldup {{.*#+}} ymm1 = ymm1[0,0,2,2,4,4,6,6]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm1 * ymm0) +/- ymm2
; ALL-NEXT:    retq
  %mul = call <8 x float> @llvm.experimental.complex.fmul.v8f32(<8 x float> %z, <8 x float> %w) #0
  ret <8 x float> %mul
}

define <16 x float> @intrinsic_slow_v16f32(<16 x float> %z, <16 x float> %w) {
; FMA-LABEL: intrinsic_slow_v16f32:
; FMA:       # %bb.0:
; FMA-NEXT:    pushq %rax
; FMA-NEXT:    .cfi_def_cfa_offset 16
; FMA-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; FMA-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; FMA-NEXT:    # kill: def $xmm0 killed $xmm0 killed $ymm0
; FMA-NEXT:    # kill: def $xmm2 killed $xmm2 killed $ymm2
; FMA-NEXT:    callq __mulsc3@PLT
; FMA-NEXT:    popq %rax
; FMA-NEXT:    .cfi_def_cfa_offset 8
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_slow_v16f32:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rax
; AVX512VL-NEXT:    .cfi_def_cfa_offset 16
; AVX512VL-NEXT:    vmovaps %zmm1, %zmm2
; AVX512VL-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; AVX512VL-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; AVX512VL-NEXT:    # kill: def $xmm0 killed $xmm0 killed $zmm0
; AVX512VL-NEXT:    # kill: def $xmm2 killed $xmm2 killed $zmm2
; AVX512VL-NEXT:    callq __mulsc3@PLT
; AVX512VL-NEXT:    popq %rax
; AVX512VL-NEXT:    .cfi_def_cfa_offset 8
; AVX512VL-NEXT:    retq
  %mul = call <16 x float> @llvm.experimental.complex.fmul.v16f32(<16 x float> %z, <16 x float> %w)
  ret <16 x float> %mul
}


define <16 x float> @intrinsic_fast_v16f32(<16 x float> %z, <16 x float> %w) {
; FMA-LABEL: intrinsic_fast_v16f32:
; FMA:       # %bb.0:
; FMA-NEXT:    vmovshdup {{.*#+}} ymm4 = ymm2[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm5 = ymm0[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovsldup {{.*#+}} ymm2 = ymm2[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) +/- ymm4
; FMA-NEXT:    vmovshdup {{.*#+}} ymm2 = ymm3[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm4 = ymm1[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm2, %ymm4, %ymm2
; FMA-NEXT:    vmovsldup {{.*#+}} ymm3 = ymm3[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) +/- ymm2
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_fast_v16f32:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vmovshdup {{.*#+}} zmm2 = zmm1[1,1,3,3,5,5,7,7,9,9,11,11,13,13,15,15]
; AVX512VL-NEXT:    vshufps {{.*#+}} zmm3 = zmm0[1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14]
; AVX512VL-NEXT:    vmulps %zmm2, %zmm3, %zmm2
; AVX512VL-NEXT:    vmovsldup {{.*#+}} zmm1 = zmm1[0,0,2,2,4,4,6,6,8,8,10,10,12,12,14,14]
; AVX512VL-NEXT:    vfmaddsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call fast <16 x float> @llvm.experimental.complex.fmul.v16f32(<16 x float> %z, <16 x float> %w)
  ret <16 x float> %mul
}

define <16 x float> @intrinsic_limited_v16f32(<16 x float> %z, <16 x float> %w) {
; FMA-LABEL: intrinsic_limited_v16f32:
; FMA:       # %bb.0:
; FMA-NEXT:    vmovshdup {{.*#+}} ymm4 = ymm2[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm5 = ymm0[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovsldup {{.*#+}} ymm2 = ymm2[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) +/- ymm4
; FMA-NEXT:    vmovshdup {{.*#+}} ymm2 = ymm3[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm4 = ymm1[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm2, %ymm4, %ymm2
; FMA-NEXT:    vmovsldup {{.*#+}} ymm3 = ymm3[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) +/- ymm2
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_limited_v16f32:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vmovshdup {{.*#+}} zmm2 = zmm1[1,1,3,3,5,5,7,7,9,9,11,11,13,13,15,15]
; AVX512VL-NEXT:    vshufps {{.*#+}} zmm3 = zmm0[1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14]
; AVX512VL-NEXT:    vmulps %zmm2, %zmm3, %zmm2
; AVX512VL-NEXT:    vmovsldup {{.*#+}} zmm1 = zmm1[0,0,2,2,4,4,6,6,8,8,10,10,12,12,14,14]
; AVX512VL-NEXT:    vfmaddsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call <16 x float> @llvm.experimental.complex.fmul.v16f32(<16 x float> %z, <16 x float> %w) #0
  ret <16 x float> %mul
}

define <2 x double> @intrinsic_slow_v2f64(<2 x double> %z, <2 x double> %w) {
; ALL-LABEL: intrinsic_slow_v2f64:
; ALL:       # %bb.0:
; ALL-NEXT:    pushq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 16
; ALL-NEXT:    vmovapd %xmm1, %xmm2
; ALL-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; ALL-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; ALL-NEXT:    callq __muldc3@PLT
; ALL-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; ALL-NEXT:    popq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 8
; ALL-NEXT:    retq
  %mul = call <2 x double> @llvm.experimental.complex.fmul.v2f64(<2 x double> %z, <2 x double> %w)
  ret <2 x double> %mul
}

define <2 x double> @intrinsic_fast_v2f64(<2 x double> %z, <2 x double> %w) {
; ALL-LABEL: intrinsic_fast_v2f64:
; ALL:       # %bb.0:
; ALL-NEXT:    vshufpd {{.*#+}} xmm2 = xmm1[1,1]
; ALL-NEXT:    vshufpd {{.*#+}} xmm3 = xmm0[1,0]
; ALL-NEXT:    vmulpd %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovddup {{.*#+}} xmm1 = xmm1[0,0]
; ALL-NEXT:    vfmaddsub213pd {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call fast <2 x double> @llvm.experimental.complex.fmul.v2f64(<2 x double> %z, <2 x double> %w)
  ret <2 x double> %mul
}

define <2 x double> @intrinsic_limited_v2f64(<2 x double> %z, <2 x double> %w) {
; ALL-LABEL: intrinsic_limited_v2f64:
; ALL:       # %bb.0:
; ALL-NEXT:    vshufpd {{.*#+}} xmm2 = xmm1[1,1]
; ALL-NEXT:    vshufpd {{.*#+}} xmm3 = xmm0[1,0]
; ALL-NEXT:    vmulpd %xmm2, %xmm3, %xmm2
; ALL-NEXT:    vmovddup {{.*#+}} xmm1 = xmm1[0,0]
; ALL-NEXT:    vfmaddsub213pd {{.*#+}} xmm0 = (xmm1 * xmm0) +/- xmm2
; ALL-NEXT:    retq
  %mul = call <2 x double> @llvm.experimental.complex.fmul.v2f64(<2 x double> %z, <2 x double> %w) #0
  ret <2 x double> %mul
}

define <4 x double> @intrinsic_slow_v4f64(<4 x double> %z, <4 x double> %w) {
; ALL-LABEL: intrinsic_slow_v4f64:
; ALL:       # %bb.0:
; ALL-NEXT:    pushq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 16
; ALL-NEXT:    vmovapd %ymm1, %ymm2
; ALL-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; ALL-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; ALL-NEXT:    # kill: def $xmm0 killed $xmm0 killed $ymm0
; ALL-NEXT:    # kill: def $xmm2 killed $xmm2 killed $ymm2
; ALL-NEXT:    vzeroupper
; ALL-NEXT:    callq __muldc3@PLT
; ALL-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; ALL-NEXT:    popq %rax
; ALL-NEXT:    .cfi_def_cfa_offset 8
; ALL-NEXT:    retq
  %mul = call <4 x double> @llvm.experimental.complex.fmul.v4f64(<4 x double> %z, <4 x double> %w)
  ret <4 x double> %mul
}

define <4 x double> @intrinsic_fast_v4f64(<4 x double> %z, <4 x double> %w) {
; ALL-LABEL: intrinsic_fast_v4f64:
; ALL:       # %bb.0:
; ALL-NEXT:    vshufpd {{.*#+}} ymm2 = ymm1[1,1,3,3]
; ALL-NEXT:    vshufpd {{.*#+}} ymm3 = ymm0[1,0,3,2]
; ALL-NEXT:    vmulpd %ymm2, %ymm3, %ymm2
; ALL-NEXT:    vmovddup {{.*#+}} ymm1 = ymm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213pd {{.*#+}} ymm0 = (ymm1 * ymm0) +/- ymm2
; ALL-NEXT:    retq
  %mul = call fast <4 x double> @llvm.experimental.complex.fmul.v4f64(<4 x double> %z, <4 x double> %w)
  ret <4 x double> %mul
}

define <4 x double> @intrinsic_limited_v4f64(<4 x double> %z, <4 x double> %w) {
; ALL-LABEL: intrinsic_limited_v4f64:
; ALL:       # %bb.0:
; ALL-NEXT:    vshufpd {{.*#+}} ymm2 = ymm1[1,1,3,3]
; ALL-NEXT:    vshufpd {{.*#+}} ymm3 = ymm0[1,0,3,2]
; ALL-NEXT:    vmulpd %ymm2, %ymm3, %ymm2
; ALL-NEXT:    vmovddup {{.*#+}} ymm1 = ymm1[0,0,2,2]
; ALL-NEXT:    vfmaddsub213pd {{.*#+}} ymm0 = (ymm1 * ymm0) +/- ymm2
; ALL-NEXT:    retq
  %mul = call <4 x double> @llvm.experimental.complex.fmul.v4f64(<4 x double> %z, <4 x double> %w) #0
  ret <4 x double> %mul
}

define <8 x double> @intrinsic_slow_v8f64(<8 x double> %z, <8 x double> %w) {
; FMA-LABEL: intrinsic_slow_v8f64:
; FMA:       # %bb.0:
; FMA-NEXT:    pushq %rax
; FMA-NEXT:    .cfi_def_cfa_offset 16
; FMA-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; FMA-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; FMA-NEXT:    # kill: def $xmm0 killed $xmm0 killed $ymm0
; FMA-NEXT:    # kill: def $xmm2 killed $xmm2 killed $ymm2
; FMA-NEXT:    vzeroupper
; FMA-NEXT:    callq __muldc3@PLT
; FMA-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; FMA-NEXT:    popq %rax
; FMA-NEXT:    .cfi_def_cfa_offset 8
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_slow_v8f64:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    pushq %rax
; AVX512VL-NEXT:    .cfi_def_cfa_offset 16
; AVX512VL-NEXT:    vmovapd %zmm1, %zmm2
; AVX512VL-NEXT:    vshufpd {{.*#+}} xmm1 = xmm0[1,0]
; AVX512VL-NEXT:    vshufpd {{.*#+}} xmm3 = xmm2[1,0]
; AVX512VL-NEXT:    # kill: def $xmm0 killed $xmm0 killed $zmm0
; AVX512VL-NEXT:    # kill: def $xmm2 killed $xmm2 killed $zmm2
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    callq __muldc3@PLT
; AVX512VL-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; AVX512VL-NEXT:    popq %rax
; AVX512VL-NEXT:    .cfi_def_cfa_offset 8
; AVX512VL-NEXT:    retq
  %mul = call <8 x double> @llvm.experimental.complex.fmul.v8f64(<8 x double> %z, <8 x double> %w)
  ret <8 x double> %mul
}

define <8 x double> @intrinsic_fast_v8f64(<8 x double> %z, <8 x double> %w) {
; FMA-LABEL: intrinsic_fast_v8f64:
; FMA:       # %bb.0:
; FMA-NEXT:    vshufpd {{.*#+}} ymm4 = ymm2[1,1,3,3]
; FMA-NEXT:    vshufpd {{.*#+}} ymm5 = ymm0[1,0,3,2]
; FMA-NEXT:    vmulpd %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovddup {{.*#+}} ymm2 = ymm2[0,0,2,2]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} ymm0 = (ymm2 * ymm0) +/- ymm4
; FMA-NEXT:    vshufpd {{.*#+}} ymm2 = ymm3[1,1,3,3]
; FMA-NEXT:    vshufpd {{.*#+}} ymm4 = ymm1[1,0,3,2]
; FMA-NEXT:    vmulpd %ymm2, %ymm4, %ymm2
; FMA-NEXT:    vmovddup {{.*#+}} ymm3 = ymm3[0,0,2,2]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} ymm1 = (ymm3 * ymm1) +/- ymm2
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_fast_v8f64:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm2 = zmm1[1,1,3,3,5,5,7,7]
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm3 = zmm0[1,0,3,2,5,4,7,6]
; AVX512VL-NEXT:    vmulpd %zmm2, %zmm3, %zmm2
; AVX512VL-NEXT:    vmovddup {{.*#+}} zmm1 = zmm1[0,0,2,2,4,4,6,6]
; AVX512VL-NEXT:    vfmaddsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call fast <8 x double> @llvm.experimental.complex.fmul.v8f64(<8 x double> %z, <8 x double> %w)
  ret <8 x double> %mul
}

define <8 x double> @intrinsic_limited_v8f64(<8 x double> %z, <8 x double> %w) {
; FMA-LABEL: intrinsic_limited_v8f64:
; FMA:       # %bb.0:
; FMA-NEXT:    vshufpd {{.*#+}} ymm4 = ymm2[1,1,3,3]
; FMA-NEXT:    vshufpd {{.*#+}} ymm5 = ymm0[1,0,3,2]
; FMA-NEXT:    vmulpd %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovddup {{.*#+}} ymm2 = ymm2[0,0,2,2]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} ymm0 = (ymm2 * ymm0) +/- ymm4
; FMA-NEXT:    vshufpd {{.*#+}} ymm2 = ymm3[1,1,3,3]
; FMA-NEXT:    vshufpd {{.*#+}} ymm4 = ymm1[1,0,3,2]
; FMA-NEXT:    vmulpd %ymm2, %ymm4, %ymm2
; FMA-NEXT:    vmovddup {{.*#+}} ymm3 = ymm3[0,0,2,2]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} ymm1 = (ymm3 * ymm1) +/- ymm2
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_limited_v8f64:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm2 = zmm1[1,1,3,3,5,5,7,7]
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm3 = zmm0[1,0,3,2,5,4,7,6]
; AVX512VL-NEXT:    vmulpd %zmm2, %zmm3, %zmm2
; AVX512VL-NEXT:    vmovddup {{.*#+}} zmm1 = zmm1[0,0,2,2,4,4,6,6]
; AVX512VL-NEXT:    vfmaddsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call <8 x double> @llvm.experimental.complex.fmul.v8f64(<8 x double> %z, <8 x double> %w) #0
  ret <8 x double> %mul
}

define <6 x float> @intrinsic_fast_v6f32(<6 x float> %z, <6 x float> %w) {
; ALL-LABEL: intrinsic_fast_v6f32:
; ALL:       # %bb.0:
; ALL-NEXT:    vmovshdup {{.*#+}} ymm2 = ymm1[1,1,3,3,5,5,7,7]
; ALL-NEXT:    vshufps {{.*#+}} ymm3 = ymm0[1,0,3,2,5,4,7,6]
; ALL-NEXT:    vmulps %ymm2, %ymm3, %ymm2
; ALL-NEXT:    vmovsldup {{.*#+}} ymm1 = ymm1[0,0,2,2,4,4,6,6]
; ALL-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm1 * ymm0) +/- ymm2
; ALL-NEXT:    retq
  %mul = call fast <6 x float> @llvm.experimental.complex.fmul.v6f32(<6 x float> %z, <6 x float> %w)
  ret <6 x float> %mul
}

define <6 x double> @intrinsic_fast_v6f64(<6 x double> %z, <6 x double> %w) {
; FMA-LABEL: intrinsic_fast_v6f64:
; FMA:       # %bb.0:
; FMA-NEXT:    movq %rdi, %rax
; FMA-NEXT:    vmovlhps {{.*#+}} xmm2 = xmm2[0],xmm3[0]
; FMA-NEXT:    vmovlhps {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; FMA-NEXT:    vinsertf128 $1, %xmm2, %ymm0, %ymm0
; FMA-NEXT:    vmovlhps {{.*#+}} xmm1 = xmm6[0],xmm7[0]
; FMA-NEXT:    vinsertf128 $1, {{[0-9]+}}(%rsp), %ymm1, %ymm1
; FMA-NEXT:    vshufpd {{.*#+}} ymm2 = ymm0[1,0,3,2]
; FMA-NEXT:    vshufpd {{.*#+}} ymm3 = ymm1[1,1,3,3]
; FMA-NEXT:    vmulpd %ymm3, %ymm2, %ymm2
; FMA-NEXT:    vmovddup {{.*#+}} ymm1 = ymm1[0,0,2,2]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} ymm1 = (ymm0 * ymm1) +/- ymm2
; FMA-NEXT:    vunpcklpd {{.*#+}} xmm0 = xmm4[0],xmm5[0]
; FMA-NEXT:    vunpcklpd {{.*#+}} xmm2 = xmm5[0],xmm4[0]
; FMA-NEXT:    vmovapd {{[0-9]+}}(%rsp), %xmm3
; FMA-NEXT:    vshufpd {{.*#+}} xmm4 = xmm3[1,1]
; FMA-NEXT:    vmulpd %xmm4, %xmm2, %xmm2
; FMA-NEXT:    vmovddup {{.*#+}} xmm3 = xmm3[0,0]
; FMA-NEXT:    vfmaddsub213pd {{.*#+}} xmm3 = (xmm0 * xmm3) +/- xmm2
; FMA-NEXT:    vmovapd %xmm3, 32(%rdi)
; FMA-NEXT:    vmovapd %ymm1, (%rdi)
; FMA-NEXT:    vzeroupper
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_fast_v6f64:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm2 = zmm1[1,1,3,3,5,5,7,7]
; AVX512VL-NEXT:    vshufpd {{.*#+}} zmm3 = zmm0[1,0,3,2,5,4,7,6]
; AVX512VL-NEXT:    vmulpd %zmm2, %zmm3, %zmm2
; AVX512VL-NEXT:    vmovddup {{.*#+}} zmm1 = zmm1[0,0,2,2,4,4,6,6]
; AVX512VL-NEXT:    vfmaddsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call fast <6 x double> @llvm.experimental.complex.fmul.v6f64(<6 x double> %z, <6 x double> %w)
  ret <6 x double> %mul
}

; Test the vector bigger than 512 bits.
define <32 x float> @intrinsic_fast_v32f32(<32 x float> %z, <32 x float> %w) {
; FMA-LABEL: intrinsic_fast_v32f32:
; FMA:       # %bb.0:
; FMA-NEXT:    vmovshdup {{.*#+}} ymm8 = ymm4[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm9 = ymm0[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm8, %ymm9, %ymm8
; FMA-NEXT:    vmovsldup {{.*#+}} ymm4 = ymm4[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm0 = (ymm4 * ymm0) +/- ymm8
; FMA-NEXT:    vmovshdup {{.*#+}} ymm4 = ymm5[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm8 = ymm1[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm4, %ymm8, %ymm4
; FMA-NEXT:    vmovsldup {{.*#+}} ymm5 = ymm5[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm1 = (ymm5 * ymm1) +/- ymm4
; FMA-NEXT:    vmovshdup {{.*#+}} ymm4 = ymm6[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm5 = ymm2[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovsldup {{.*#+}} ymm5 = ymm6[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm2 = (ymm5 * ymm2) +/- ymm4
; FMA-NEXT:    vmovshdup {{.*#+}} ymm4 = ymm7[1,1,3,3,5,5,7,7]
; FMA-NEXT:    vshufps {{.*#+}} ymm5 = ymm3[1,0,3,2,5,4,7,6]
; FMA-NEXT:    vmulps %ymm4, %ymm5, %ymm4
; FMA-NEXT:    vmovsldup {{.*#+}} ymm5 = ymm7[0,0,2,2,4,4,6,6]
; FMA-NEXT:    vfmaddsub213ps {{.*#+}} ymm3 = (ymm5 * ymm3) +/- ymm4
; FMA-NEXT:    retq
;
; AVX512VL-LABEL: intrinsic_fast_v32f32:
; AVX512VL:       # %bb.0:
; AVX512VL-NEXT:    vmovshdup {{.*#+}} zmm4 = zmm2[1,1,3,3,5,5,7,7,9,9,11,11,13,13,15,15]
; AVX512VL-NEXT:    vshufps {{.*#+}} zmm5 = zmm0[1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14]
; AVX512VL-NEXT:    vmulps %zmm4, %zmm5, %zmm4
; AVX512VL-NEXT:    vmovsldup {{.*#+}} zmm2 = zmm2[0,0,2,2,4,4,6,6,8,8,10,10,12,12,14,14]
; AVX512VL-NEXT:    vfmaddsub213ps {{.*#+}} zmm0 = (zmm2 * zmm0) +/- zmm4
; AVX512VL-NEXT:    vmovshdup {{.*#+}} zmm2 = zmm3[1,1,3,3,5,5,7,7,9,9,11,11,13,13,15,15]
; AVX512VL-NEXT:    vshufps {{.*#+}} zmm4 = zmm1[1,0,3,2,5,4,7,6,9,8,11,10,13,12,15,14]
; AVX512VL-NEXT:    vmulps %zmm2, %zmm4, %zmm2
; AVX512VL-NEXT:    vmovsldup {{.*#+}} zmm3 = zmm3[0,0,2,2,4,4,6,6,8,8,10,10,12,12,14,14]
; AVX512VL-NEXT:    vfmaddsub213ps {{.*#+}} zmm1 = (zmm3 * zmm1) +/- zmm2
; AVX512VL-NEXT:    retq
  %mul = call fast <32 x float> @llvm.experimental.complex.fmul.v32f32(<32 x float> %z, <32 x float> %w)
  ret <32 x float> %mul
}

attributes #0 = { "complex-range"="limited" }
