; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx,+fma | FileCheck %s --check-prefix=FMA --check-prefix=FMA-INFS
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx,+fma4,+fma | FileCheck %s --check-prefix=FMA4 --check-prefix=FMA4-INFS
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx,+fma4 | FileCheck %s --check-prefix=FMA4 --check-prefix=FMA4-INFS
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512dq | FileCheck %s --check-prefix=AVX512 --check-prefix=AVX512-INFS

;
; Pattern: (fadd (fmul x, y), z) -> (fmadd x,y,z)
;

define <16 x float> @test_16f32_fmadd(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) {
; FMA-LABEL: test_16f32_fmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_16f32_fmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_16f32_fmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213ps {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <16 x float> %a0, %a1
  %res = fadd contract <16 x float> %x, %a2
  ret <16 x float> %res
}

define <8 x double> @test_8f64_fmadd(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) {
; FMA-LABEL: test_8f64_fmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_8f64_fmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_8f64_fmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213pd {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <8 x double> %a0, %a1
  %res = fadd contract <8 x double> %x, %a2
  ret <8 x double> %res
}

;
; Pattern: (fsub (fmul x, y), z) -> (fmsub x, y, z)
;

define <16 x float> @test_16f32_fmsub(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) {
; FMA-LABEL: test_16f32_fmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_16f32_fmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_16f32_fmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <16 x float> %a0, %a1
  %res = fsub contract <16 x float> %x, %a2
  ret <16 x float> %res
}

define <8 x double> @test_8f64_fmsub(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) {
; FMA-LABEL: test_8f64_fmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_8f64_fmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_8f64_fmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <8 x double> %a0, %a1
  %res = fsub contract <8 x double> %x, %a2
  ret <8 x double> %res
}

;
; Pattern: (fsub z, (fmul x, y)) -> (fnmadd x, y, z)
;

define <16 x float> @test_16f32_fnmadd(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) {
; FMA-LABEL: test_16f32_fnmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmadd213ps {{.*#+}} ymm0 = -(ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfnmadd213ps {{.*#+}} ymm1 = -(ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_16f32_fnmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmaddps {{.*#+}} ymm0 = -(ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfnmaddps {{.*#+}} ymm1 = -(ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_16f32_fnmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmadd213ps {{.*#+}} zmm0 = -(zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <16 x float> %a0, %a1
  %res = fsub contract <16 x float> %a2, %x
  ret <16 x float> %res
}

define <8 x double> @test_8f64_fnmadd(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) {
; FMA-LABEL: test_8f64_fnmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_8f64_fnmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm0 = -(ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm1 = -(ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_8f64_fnmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmadd213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <8 x double> %a0, %a1
  %res = fsub contract <8 x double> %a2, %x
  ret <8 x double> %res
}

;
; Pattern: (fsub (fneg (fmul x, y)), z) -> (fnmsub x, y, z)
;

define <16 x float> @test_16f32_fnmsub(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) {
; FMA-LABEL: test_16f32_fnmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_16f32_fnmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_16f32_fnmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmsub213ps {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <16 x float> %a0, %a1
  %y = fsub contract <16 x float> <float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00, float -0.000000e+00>, %x
  %res = fsub contract <16 x float> %y, %a2
  ret <16 x float> %res
}

define <8 x double> @test_8f64_fnmsub(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) {
; FMA-LABEL: test_8f64_fnmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_8f64_fnmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_8f64_fnmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmsub213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %x = fmul contract <8 x double> %a0, %a1
  %y = fsub contract <8 x double> <double -0.000000e+00, double -0.000000e+00, double -0.000000e+00, double -0.000000e+00, double -0.000000e+00, double -0.000000e+00, double -0.000000e+00, double -0.000000e+00>, %x
  %res = fsub contract <8 x double> %y, %a2
  ret <8 x double> %res
}

;
; Load Folding Patterns
;

define <16 x float> @test_16f32_fmadd_load(ptr %a0, <16 x float> %a1, <16 x float> %a2) {
; FMA-LABEL: test_16f32_fmadd_load:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd132ps {{.*#+}} ymm0 = (ymm0 * mem) + ymm2
; FMA-NEXT:    vfmadd132ps {{.*#+}} ymm1 = (ymm1 * mem) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_16f32_fmadd_load:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * mem) + ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * mem) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_16f32_fmadd_load:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd132ps {{.*#+}} zmm0 = (zmm0 * mem) + zmm1
; AVX512-NEXT:    retq
  %x = load <16 x float>, ptr %a0
  %y = fmul contract <16 x float> %x, %a1
  %res = fadd contract <16 x float> %y, %a2
  ret <16 x float> %res
}

define <8 x double> @test_8f64_fmsub_load(ptr %a0, <8 x double> %a1, <8 x double> %a2) {
; FMA-LABEL: test_8f64_fmsub_load:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub132pd {{.*#+}} ymm0 = (ymm0 * mem) - ymm2
; FMA-NEXT:    vfmsub132pd {{.*#+}} ymm1 = (ymm1 * mem) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_8f64_fmsub_load:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm0 = (ymm0 * mem) - ymm2
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm1 = (ymm1 * mem) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_8f64_fmsub_load:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub132pd {{.*#+}} zmm0 = (zmm0 * mem) - zmm1
; AVX512-NEXT:    retq
  %x = load <8 x double>, ptr %a0
  %y = fmul contract <8 x double> %x, %a1
  %res = fsub contract <8 x double> %y, %a2
  ret <8 x double> %res
}

;
; Patterns (+ fneg variants): mul(add(1.0,x),y), mul(sub(1.0,x),y), mul(sub(x,1.0),y)
;

define <16 x float> @test_v16f32_mul_add_x_one_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_add_x_one_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_add_x_one_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_add_x_one_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddps {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm0, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %a = fadd contract <16 x float> %x, <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>
  %m = fmul contract <16 x float> %a, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_add_x_one_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_add_x_one_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_add_x_one_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_add_x_one_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213ps {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %a = fadd contract ninf <16 x float> %x, <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>
  %m = fmul contract ninf <16 x float> %a, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_add_x_one(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_add_x_one:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_add_x_one:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_add_x_one:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddpd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %a = fadd contract <8 x double> %x, <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>
  %m = fmul contract <8 x double> %y, %a
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_add_x_one_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_add_x_one_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_add_x_one_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm2
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_add_x_one_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213pd {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %a = fadd contract ninf <8 x double> %x, <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>
  %m = fmul contract ninf <8 x double> %y, %a
  ret <8 x double> %m
}

define <16 x float> @test_v16f32_mul_add_x_negone_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_add_x_negone_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_add_x_negone_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_add_x_negone_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddps {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm0, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %a = fadd contract <16 x float> %x, <float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>
  %m = fmul contract <16 x float> %a, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_add_x_negone_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_add_x_negone_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_add_x_negone_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_add_x_negone_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %a = fadd contract ninf <16 x float> %x, <float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>
  %m = fmul contract ninf <16 x float> %a, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_add_x_negone(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_add_x_negone:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_add_x_negone:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_add_x_negone:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddpd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %a = fadd contract <8 x double> %x, <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>
  %m = fmul contract <8 x double> %y, %a
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_add_x_negone_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_add_x_negone_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_add_x_negone_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_add_x_negone_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %a = fadd contract ninf <8 x double> %x, <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>
  %m = fmul contract ninf <8 x double> %y, %a
  ret <8 x double> %m
}

define <16 x float> @test_v16f32_mul_sub_one_x_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_one_x_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_one_x_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_one_x_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastss {{.*#+}} zmm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; AVX512-NEXT:    vsubps %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <16 x float> <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>, %x
  %m = fmul contract <16 x float> %s, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_sub_one_x_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_one_x_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_one_x_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_one_x_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastss {{.*#+}} zmm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; AVX512-NEXT:    vsubps %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract ninf <16 x float> <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>, %x
  %m = fmul contract ninf <16 x float> %s, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_sub_one_x(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_one_x:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vsubpd %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubpd %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_one_x:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vsubpd %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubpd %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_one_x:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastsd {{.*#+}} zmm2 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; AVX512-NEXT:    vsubpd %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <8 x double> <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>, %x
  %m = fmul contract <8 x double> %y, %s
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_sub_one_x_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_one_x_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) + ymm2
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_one_x_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm0 = -(ymm0 * ymm2) + ymm2
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm1 = -(ymm1 * ymm3) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_one_x_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmadd213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <8 x double> <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>, %x
  %m = fmul contract ninf <8 x double> %y, %s
  ret <8 x double> %m
}

define <16 x float> @test_v16f32_mul_sub_negone_x_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_negone_x_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_negone_x_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_negone_x_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastss {{.*#+}} zmm2 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; AVX512-NEXT:    vsubps %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <16 x float> <float -1.0, float -1.0, float -1.0, float -1.0,float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>, %x
  %m = fmul contract <16 x float> %s, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_sub_negone_x_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_negone_x_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_negone_x_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vsubps %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubps %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_negone_x_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastss {{.*#+}} zmm2 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; AVX512-NEXT:    vsubps %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract ninf <16 x float> <float -1.0, float -1.0, float -1.0, float -1.0,float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>, %x
  %m = fmul contract ninf <16 x float> %s, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_sub_negone_x(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_negone_x:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vsubpd %ymm1, %ymm4, %ymm1
; FMA-NEXT:    vsubpd %ymm0, %ymm4, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_negone_x:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vsubpd %ymm1, %ymm4, %ymm1
; FMA4-NEXT:    vsubpd %ymm0, %ymm4, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_negone_x:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastsd {{.*#+}} zmm2 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; AVX512-NEXT:    vsubpd %zmm0, %zmm2, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <8 x double> <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>, %x
  %m = fmul contract <8 x double> %y, %s
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_sub_negone_x_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_negone_x_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm2
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_negone_x_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm2
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_negone_x_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmsub213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <8 x double> <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>, %x
  %m = fmul contract ninf <8 x double> %y, %s
  ret <8 x double> %m
}

define <16 x float> @test_v16f32_mul_sub_x_one_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_x_one_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_x_one_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_x_one_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddps {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm0, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <16 x float> %x, <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>
  %m = fmul contract <16 x float> %s, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_sub_x_one_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_x_one_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_x_one_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_x_one_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <16 x float> %x, <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>
  %m = fmul contract ninf <16 x float> %s, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_sub_x_one(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_x_one:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_x_one:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [-1.0E+0,-1.0E+0,-1.0E+0,-1.0E+0]
; FMA4-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_x_one:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddpd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <8 x double> %x, <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>
  %m = fmul contract <8 x double> %y, %s
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_sub_x_one_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_x_one_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_x_one_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_x_one_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213pd {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <8 x double> %x, <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>
  %m = fmul contract ninf <8 x double> %y, %s
  ret <8 x double> %m
}

define <16 x float> @test_v16f32_mul_sub_x_negone_y(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_x_negone_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_x_negone_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vaddps %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddps %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vmulps %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_x_negone_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddps {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm0, %zmm0
; AVX512-NEXT:    vmulps %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <16 x float> %x, <float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>
  %m = fmul contract <16 x float> %s, %y
  ret <16 x float> %m
}

define <16 x float> @test_v16f32_mul_sub_x_negone_y_ninf(<16 x float> %x, <16 x float> %y) {
; FMA-LABEL: test_v16f32_mul_sub_x_negone_y_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_mul_sub_x_negone_y_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_mul_sub_x_negone_y_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213ps {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <16 x float> %x, <float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0, float -1.0>
  %m = fmul contract ninf <16 x float> %s, %y
  ret <16 x float> %m
}

define <8 x double> @test_v8f64_mul_y_sub_x_negone(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_x_negone:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_x_negone:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm4 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vaddpd %ymm4, %ymm1, %ymm1
; FMA4-NEXT:    vaddpd %ymm4, %ymm0, %ymm0
; FMA4-NEXT:    vmulpd %ymm0, %ymm2, %ymm0
; FMA4-NEXT:    vmulpd %ymm1, %ymm3, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_x_negone:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vaddpd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    vmulpd %zmm0, %zmm1, %zmm0
; AVX512-NEXT:    retq
  %s = fsub contract <8 x double> %x, <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>
  %m = fmul contract <8 x double> %y, %s
  ret <8 x double> %m
}

define <8 x double> @test_v8f64_mul_y_sub_x_negone_ninf(<8 x double> %x, <8 x double> %y) {
; FMA-LABEL: test_v8f64_mul_y_sub_x_negone_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_mul_y_sub_x_negone_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm2
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_mul_y_sub_x_negone_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213pd {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %s = fsub contract ninf <8 x double> %x, <double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0, double -1.0>
  %m = fmul contract ninf <8 x double> %y, %s
  ret <8 x double> %m
}


define <16 x float> @test_v16f32_interp(<16 x float> %x, <16 x float> %y, <16 x float> %t) {
; FMA-LABEL: test_v16f32_interp:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastss {{.*#+}} ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vsubps %ymm4, %ymm6, %ymm7
; FMA-NEXT:    vsubps %ymm5, %ymm6, %ymm6
; FMA-NEXT:    vmulps %ymm6, %ymm3, %ymm3
; FMA-NEXT:    vmulps %ymm7, %ymm2, %ymm2
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm0 = (ymm4 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213ps {{.*#+}} ymm1 = (ymm5 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_interp:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastss {{.*#+}} ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vsubps %ymm4, %ymm6, %ymm7
; FMA4-NEXT:    vsubps %ymm5, %ymm6, %ymm6
; FMA4-NEXT:    vmulps %ymm6, %ymm3, %ymm3
; FMA4-NEXT:    vmulps %ymm7, %ymm2, %ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * ymm4) + ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * ymm5) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_interp:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastss {{.*#+}} zmm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; AVX512-NEXT:    vsubps %zmm2, %zmm3, %zmm3
; AVX512-NEXT:    vmulps %zmm3, %zmm1, %zmm1
; AVX512-NEXT:    vfmadd213ps {{.*#+}} zmm0 = (zmm2 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %t1 = fsub contract nsz <16 x float> <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>, %t
  %tx = fmul contract nsz <16 x float> %x, %t
  %ty = fmul contract nsz <16 x float> %y, %t1
  %r = fadd contract nsz <16 x float> %tx, %ty
  ret <16 x float> %r
}

define <16 x float> @test_v16f32_interp_ninf(<16 x float> %x, <16 x float> %y, <16 x float> %t) {
; FMA-LABEL: test_v16f32_interp_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm3 = (ymm5 * ymm3) - ymm3
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm2 = (ymm4 * ymm2) - ymm2
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm0 = (ymm4 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm1 = (ymm5 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_interp_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm3 = (ymm5 * ymm3) - ymm3
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm2 = (ymm4 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm0 = (ymm0 * ymm4) - ymm2
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm1 = (ymm1 * ymm5) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_interp_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm1 = (zmm2 * zmm1) - zmm1
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm0 = (zmm2 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %t1 = fsub contract ninf nsz <16 x float> <float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0, float 1.0>, %t
  %tx = fmul contract ninf nsz <16 x float> %x, %t
  %ty = fmul contract ninf nsz <16 x float> %y, %t1
  %r = fadd contract ninf nsz <16 x float> %tx, %ty
  ret <16 x float> %r
}

define <8 x double> @test_v8f64_interp(<8 x double> %x, <8 x double> %y, <8 x double> %t) {
; FMA-LABEL: test_v8f64_interp:
; FMA:       # %bb.0:
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA-NEXT:    vsubpd %ymm4, %ymm6, %ymm7
; FMA-NEXT:    vsubpd %ymm5, %ymm6, %ymm6
; FMA-NEXT:    vmulpd %ymm6, %ymm3, %ymm3
; FMA-NEXT:    vmulpd %ymm7, %ymm2, %ymm2
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm0 = (ymm4 * ymm0) + ymm2
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm5 * ymm1) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_interp:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm6 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; FMA4-NEXT:    vsubpd %ymm4, %ymm6, %ymm7
; FMA4-NEXT:    vsubpd %ymm5, %ymm6, %ymm6
; FMA4-NEXT:    vmulpd %ymm6, %ymm3, %ymm3
; FMA4-NEXT:    vmulpd %ymm7, %ymm2, %ymm2
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm0 = (ymm0 * ymm4) + ymm2
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm1 = (ymm1 * ymm5) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_interp:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vbroadcastsd {{.*#+}} zmm3 = [1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0,1.0E+0]
; AVX512-NEXT:    vsubpd %zmm2, %zmm3, %zmm3
; AVX512-NEXT:    vmulpd %zmm3, %zmm1, %zmm1
; AVX512-NEXT:    vfmadd213pd {{.*#+}} zmm0 = (zmm2 * zmm0) + zmm1
; AVX512-NEXT:    retq
  %t1 = fsub contract nsz <8 x double> <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>, %t
  %tx = fmul contract nsz <8 x double> %x, %t
  %ty = fmul contract nsz <8 x double> %y, %t1
  %r = fadd contract nsz <8 x double> %tx, %ty
  ret <8 x double> %r
}

define <8 x double> @test_v8f64_interp_ninf(<8 x double> %x, <8 x double> %y, <8 x double> %t) {
; FMA-LABEL: test_v8f64_interp_ninf:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm3 = (ymm5 * ymm3) - ymm3
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm2 = (ymm4 * ymm2) - ymm2
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm0 = (ymm4 * ymm0) - ymm2
; FMA-NEXT:    vfmsub213pd {{.*#+}} ymm1 = (ymm5 * ymm1) - ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_interp_ninf:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm3 = (ymm5 * ymm3) - ymm3
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm2 = (ymm4 * ymm2) - ymm2
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm0 = (ymm0 * ymm4) - ymm2
; FMA4-NEXT:    vfmsubpd {{.*#+}} ymm1 = (ymm1 * ymm5) - ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_interp_ninf:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213pd {{.*#+}} zmm1 = (zmm2 * zmm1) - zmm1
; AVX512-NEXT:    vfmsub213pd {{.*#+}} zmm0 = (zmm2 * zmm0) - zmm1
; AVX512-NEXT:    retq
  %t1 = fsub contract ninf nsz <8 x double> <double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0, double 1.0>, %t
  %tx = fmul contract ninf nsz <8 x double> %x, %t
  %ty = fmul contract ninf nsz <8 x double> %y, %t1
  %r = fadd contract ninf nsz <8 x double> %tx, %ty
  ret <8 x double> %r
}

;
; Pattern: (fneg (fma x, y, z)) -> (fma x, -y, -z)
;

define <16 x float> @test_v16f32_fneg_fmadd(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) #0 {
; FMA-LABEL: test_v16f32_fneg_fmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_fneg_fmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_fneg_fmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmsub213ps {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %mul = fmul contract nsz <16 x float> %a0, %a1
  %add = fadd contract nsz <16 x float> %mul, %a2
  %neg = fsub contract nsz <16 x float> <float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0>, %add
  ret <16 x float> %neg
}

define <8 x double> @test_v8f64_fneg_fmsub(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) #0 {
; FMA-LABEL: test_v8f64_fneg_fmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfnmadd213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_fneg_fmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm0 = -(ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfnmaddpd {{.*#+}} ymm1 = -(ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_fneg_fmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfnmadd213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %mul = fmul contract nsz <8 x double> %a0, %a1
  %sub = fsub contract nsz <8 x double> %mul, %a2
  %neg = fsub contract nsz <8 x double> <double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0>, %sub
  ret <8 x double> %neg
}

define <16 x float> @test_v16f32_fneg_fnmadd(<16 x float> %a0, <16 x float> %a1, <16 x float> %a2) #0 {
; FMA-LABEL: test_v16f32_fneg_fnmadd:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm0 = (ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfmsub213ps {{.*#+}} ymm1 = (ymm3 * ymm1) - ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_fneg_fnmadd:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm0 = (ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfmsubps {{.*#+}} ymm1 = (ymm1 * ymm3) - ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_fneg_fnmadd:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmsub213ps {{.*#+}} zmm0 = (zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %mul = fmul contract nsz <16 x float> %a0, %a1
  %neg0 = fsub contract nsz <16 x float> <float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0>, %mul
  %add = fadd contract nsz <16 x float> %neg0, %a2
  %neg1 = fsub contract nsz <16 x float> <float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0>, %add
  ret <16 x float> %neg1
}

define <8 x double> @test_v8f64_fneg_fnmsub(<8 x double> %a0, <8 x double> %a1, <8 x double> %a2) #0 {
; FMA-LABEL: test_v8f64_fneg_fnmsub:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm0 = (ymm2 * ymm0) + ymm4
; FMA-NEXT:    vfmadd213pd {{.*#+}} ymm1 = (ymm3 * ymm1) + ymm5
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_fneg_fnmsub:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm0 = (ymm0 * ymm2) + ymm4
; FMA4-NEXT:    vfmaddpd {{.*#+}} ymm1 = (ymm1 * ymm3) + ymm5
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_fneg_fnmsub:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd213pd {{.*#+}} zmm0 = (zmm1 * zmm0) + zmm2
; AVX512-NEXT:    retq
  %mul = fmul contract nsz <8 x double> %a0, %a1
  %neg0 = fsub contract nsz <8 x double> <double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0>, %mul
  %sub = fsub contract nsz <8 x double> %neg0, %a2
  %neg1 = fsub contract nsz <8 x double> <double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0>, %sub
  ret <8 x double> %neg1
}

;
; Pattern: (fma x, c1, (fmul x, c2)) -> (fmul x, c1+c2)
;

define <16 x float> @test_v16f32_fma_x_c1_fmul_x_c2(<16 x float> %x) #0 {
; FMA-LABEL: test_v16f32_fma_x_c1_fmul_x_c2:
; FMA:       # %bb.0:
; FMA-NEXT:    vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; FMA-NEXT:    vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_fma_x_c1_fmul_x_c2:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; FMA4-NEXT:    vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_fma_x_c1_fmul_x_c2:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmulps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %zmm0, %zmm0
; AVX512-NEXT:    retq
  %m0 = fmul contract reassoc <16 x float> %x, <float 17.0, float 16.0, float 15.0, float 14.0, float 13.0, float 12.0, float 11.0, float 10.0, float 9.0, float 8.0, float 7.0, float 6.0, float 5.0, float 4.0, float 3.0, float 2.0>
  %m1 = fmul contract reassoc <16 x float> %x, <float 16.0, float 15.0, float 14.0, float 13.0, float 12.0, float 11.0, float 10.0, float 9.0, float 8.0, float 7.0, float 6.0, float 5.0, float 4.0, float 3.0, float 2.0, float 1.0>
  %a  = fadd contract reassoc <16 x float> %m0, %m1
  ret <16 x float> %a
}

;
; Pattern: (fma (fmul x, c1), c2, y) -> (fma x, c1*c2, y)
;

define <16 x float> @test_v16f32_fma_fmul_x_c1_c2_y(<16 x float> %x, <16 x float> %y) #0 {
; FMA-LABEL: test_v16f32_fma_fmul_x_c1_c2_y:
; FMA:       # %bb.0:
; FMA-NEXT:    vfmadd132ps {{.*#+}} ymm0 = (ymm0 * mem) + ymm2
; FMA-NEXT:    vfmadd132ps {{.*#+}} ymm1 = (ymm1 * mem) + ymm3
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_fma_fmul_x_c1_c2_y:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm0 = (ymm0 * mem) + ymm2
; FMA4-NEXT:    vfmaddps {{.*#+}} ymm1 = (ymm1 * mem) + ymm3
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_fma_fmul_x_c1_c2_y:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vfmadd132ps {{.*#+}} zmm0 = (zmm0 * mem) + zmm1
; AVX512-NEXT:    retq
  %m0 = fmul contract reassoc <16 x float> %x,  <float 1.0, float 2.0, float 3.0, float 4.0, float 5.0, float 6.0, float 7.0, float 8.0, float 9.0, float 10.0, float 11.0, float 12.0, float 13.0, float 14.0, float 15.0, float 16.0>
  %m1 = fmul contract reassoc <16 x float> %m0, <float 16.0, float 15.0, float 14.0, float 13.0, float 12.0, float 11.0, float 10.0, float 9.0, float 8.0, float 7.0, float 6.0, float 5.0, float 4.0, float 3.0, float 2.0, float 1.0>
  %a  = fadd contract reassoc <16 x float> %m1, %y
  ret <16 x float> %a
}

; Pattern: (fneg (fmul x, y)) -> (fnmsub x, y, 0)

define <16 x float> @test_v16f32_fneg_fmul(<16 x float> %x, <16 x float> %y) #0 {
; FMA-LABEL: test_v16f32_fneg_fmul:
; FMA:       # %bb.0:
; FMA-NEXT:    vxorps %xmm4, %xmm4, %xmm4
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfnmsub213ps {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm4
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v16f32_fneg_fmul:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vxorps %xmm4, %xmm4, %xmm4
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfnmsubps {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm4
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v16f32_fneg_fmul:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vxorps %xmm2, %xmm2, %xmm2
; AVX512-NEXT:    vfnmsub213ps {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %m = fmul contract nsz <16 x float> %x, %y
  %n = fsub contract <16 x float> <float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0, float -0.0>, %m
  ret <16 x float> %n
}

define <8 x double> @test_v8f64_fneg_fmul(<8 x double> %x, <8 x double> %y) #0 {
; FMA-LABEL: test_v8f64_fneg_fmul:
; FMA:       # %bb.0:
; FMA-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm0 = -(ymm2 * ymm0) - ymm4
; FMA-NEXT:    vfnmsub213pd {{.*#+}} ymm1 = -(ymm3 * ymm1) - ymm4
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_fneg_fmul:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vxorpd %xmm4, %xmm4, %xmm4
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm0 = -(ymm0 * ymm2) - ymm4
; FMA4-NEXT:    vfnmsubpd {{.*#+}} ymm1 = -(ymm1 * ymm3) - ymm4
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_fneg_fmul:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vxorpd %xmm2, %xmm2, %xmm2
; AVX512-NEXT:    vfnmsub213pd {{.*#+}} zmm0 = -(zmm1 * zmm0) - zmm2
; AVX512-NEXT:    retq
  %m = fmul contract nsz <8 x double> %x, %y
  %n = fsub contract <8 x double> <double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0>, %m
  ret <8 x double> %n
}

define <8 x double> @test_v8f64_fneg_fmul_no_nsz(<8 x double> %x, <8 x double> %y) #0 {
; FMA-LABEL: test_v8f64_fneg_fmul_no_nsz:
; FMA:       # %bb.0:
; FMA-NEXT:    vmulpd %ymm3, %ymm1, %ymm1
; FMA-NEXT:    vmulpd %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vbroadcastsd {{.*#+}} ymm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
; FMA-NEXT:    vxorpd %ymm2, %ymm0, %ymm0
; FMA-NEXT:    vxorpd %ymm2, %ymm1, %ymm1
; FMA-NEXT:    retq
;
; FMA4-LABEL: test_v8f64_fneg_fmul_no_nsz:
; FMA4:       # %bb.0:
; FMA4-NEXT:    vmulpd %ymm3, %ymm1, %ymm1
; FMA4-NEXT:    vmulpd %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vbroadcastsd {{.*#+}} ymm2 = [-0.0E+0,-0.0E+0,-0.0E+0,-0.0E+0]
; FMA4-NEXT:    vxorpd %ymm2, %ymm0, %ymm0
; FMA4-NEXT:    vxorpd %ymm2, %ymm1, %ymm1
; FMA4-NEXT:    retq
;
; AVX512-LABEL: test_v8f64_fneg_fmul_no_nsz:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmulpd %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vxorpd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %m = fmul contract <8 x double> %x, %y
  %n = fsub contract <8 x double> <double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0, double -0.0>, %m
  ret <8 x double> %n
}

attributes #0 = { "unsafe-fp-math"="true" }
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; AVX512-INFS: {{.*}}
; FMA-INFS: {{.*}}
; FMA4-INFS: {{.*}}
