; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown | FileCheck %s

; Check the expansion of the complex divide intrinsic. This only tests
; expansion for 32-bit floats, as the expansion should produce identical IR
; expansions save for the ABI of calling __divsc3, which is tested (indirectly)
; for each type individually in complex-{32,64}bit.ll.

declare <2 x float> @llvm.experimental.complex.fdiv.v2f32(<2 x float>, <2 x float>)

; Generate a call to __divsc3
define <2 x float> @intrinsic_slow_f32(<2 x float> %z, <2 x float> %w) {
; CHECK-LABEL: intrinsic_slow_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    movaps %xmm1, %xmm2
; CHECK-NEXT:    movaps %xmm0, %xmm1
; CHECK-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1],xmm0[1,1]
; CHECK-NEXT:    movaps %xmm2, %xmm3
; CHECK-NEXT:    shufps {{.*#+}} xmm3 = xmm3[1,1],xmm2[1,1]
; CHECK-NEXT:    callq __divsc3@PLT
; CHECK-NEXT:    popq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    retq
  %div = call <2 x float> @llvm.experimental.complex.fdiv.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %div
}

; Do not do an expansion (because fast is not sufficient to imply full
; complex-range=limited.
define <2 x float> @intrinsic_implied_not_limited_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_implied_not_limited_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 16
; CHECK-NEXT:    vmovaps %xmm1, %xmm2
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm1 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm2[1,1,3,3]
; CHECK-NEXT:    callq __divsc3@PLT
; CHECK-NEXT:    popq %rax
; CHECK-NEXT:    .cfi_def_cfa_offset 8
; CHECK-NEXT:    retq
  %div = call fast <2 x float> @llvm.experimental.complex.fdiv.v2f32(<2 x float> %z, <2 x float> %w)
  ret <2 x float> %div
}

; Do an expansion (because of complex-range=limited)
define <2 x float> @intrinsic_limited_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_limited_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm1[1,1,3,3]
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vaddss {{.*}} %xmm4
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vaddss {{.*}} %xmm5
; CHECK-NEXT:    vdivss %xmm4, %xmm5, %xmm5
; CHECK-COUNT-2: vmulss
; CHECK-NEXT:    vsubss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vdivss %xmm4, %xmm0, %xmm0
; CHECK-NEXT:    vinsertps {{.*#+}} xmm0 = xmm5[0],xmm0[0],xmm5[2,3]
; CHECK-NEXT:    retq
  %div = call <2 x float> @llvm.experimental.complex.fdiv.v2f32(<2 x float> %z, <2 x float> %w) #0
  ret <2 x float> %div
}

; Do an expansion, and use the FMA (because of fast-math flags).
define <2 x float> @intrinsic_fast_f32(<2 x float> %z, <2 x float> %w) #1 {
; CHECK-LABEL: intrinsic_fast_f32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm2 = xmm0[1,1,3,3]
; CHECK-NEXT:    vmovshdup {{.*#+}} xmm3 = xmm1[1,1,3,3]
; CHECK-NEXT:    vmulss %xmm3, %xmm3, %xmm4
; CHECK-NEXT:    vfmadd231ss {{.*#+}} xmm4 = (xmm1 * xmm1) + xmm4
; CHECK-NEXT:    vmulss %xmm3, %xmm2, %xmm5
; CHECK-NEXT:    vfmadd231ss {{.*#+}} xmm5 = (xmm0 * xmm1) + xmm5
; CHECK-NEXT:    vmovss {{.*#+}} xmm6 = mem[0],zero,zero,zero
; CHECK-NEXT:    vdivss %xmm4, %xmm6, %xmm4
; CHECK-NEXT:    vmulss %xmm4, %xmm5, %xmm5
; CHECK-NEXT:    vmulss %xmm3, %xmm0, %xmm0
; CHECK-NEXT:    vfmsub231ss {{.*#+}} xmm0 = (xmm2 * xmm1) - xmm0
; CHECK-NEXT:    vmulss %xmm4, %xmm0, %xmm0
; CHECK-NEXT:    vinsertps {{.*#+}} xmm0 = xmm5[0],xmm0[0],xmm5[2,3]
; CHECK-NEXT:    retq
  %div = call fast <2 x float> @llvm.experimental.complex.fdiv.v2f32(<2 x float> %z, <2 x float> %w) #0
  ret <2 x float> %div
}

attributes #0 = { "complex-range"="limited" }
attributes #1 = { "target-features"="+fma" }
attributes #2 = { "complex-range"="no-nan" }
