; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=x86_64 < %s | FileCheck %s

; Quick test to ensure that atomics which are not naturally-aligned
; emit unsized libcalls, and aren't emitted as native instructions or
; sized libcalls.
define void @test_i32(ptr %a) nounwind {
; CHECK-LABEL: test_i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorl %ecx, %ecx
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:    lock cmpxchgl %ecx, (%rdi)
; CHECK-NEXT:    movl $1, %ecx
; CHECK-NEXT:    movl $1, %eax
; CHECK-NEXT:    xchgl %eax, (%rdi)
; CHECK-NEXT:    movl $1, %eax
; CHECK-NEXT:    xchgl %eax, (%rdi)
; CHECK-NEXT:    lock addl $2, (%rdi)
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:    lock cmpxchgl %ecx, (%rdi)
; CHECK-NEXT:    retq
  %t0 = load atomic i32, ptr %a seq_cst, align 2
  store atomic i32 1, ptr %a seq_cst, align 2
  %t1 = atomicrmw xchg ptr %a, i32 1 seq_cst, align 2
  %t3 = atomicrmw add ptr %a, i32 2 seq_cst, align 2
  %t2 = cmpxchg ptr %a, i32 0, i32 1 seq_cst seq_cst, align 2
  ret void
}

define void @test_i128(ptr %a) nounwind {
; CHECK-LABEL: test_i128:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushq %r15
; CHECK-NEXT:    pushq %r14
; CHECK-NEXT:    pushq %rbx
; CHECK-NEXT:    subq $32, %rsp
; CHECK-NEXT:    movq %rdi, %rbx
; CHECK-NEXT:    movq %rsp, %r14
; CHECK-NEXT:    movl $16, %edi
; CHECK-NEXT:    movq %rbx, %rsi
; CHECK-NEXT:    movq %r14, %rdx
; CHECK-NEXT:    movl $5, %ecx
; CHECK-NEXT:    callq __atomic_load@PLT
; CHECK-NEXT:    movq $0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $1, (%rsp)
; CHECK-NEXT:    movq %rsp, %rdx
; CHECK-NEXT:    movl $16, %edi
; CHECK-NEXT:    movq %rbx, %rsi
; CHECK-NEXT:    movl $5, %ecx
; CHECK-NEXT:    callq __atomic_store@PLT
; CHECK-NEXT:    movq $0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $1, (%rsp)
; CHECK-NEXT:    movq %rsp, %rdx
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %r15
; CHECK-NEXT:    movl $16, %edi
; CHECK-NEXT:    movq %rbx, %rsi
; CHECK-NEXT:    movq %r15, %rcx
; CHECK-NEXT:    movl $5, %r8d
; CHECK-NEXT:    callq __atomic_exchange@PLT
; CHECK-NEXT:    movq (%rbx), %rdx
; CHECK-NEXT:    movq 8(%rbx), %rcx
; CHECK-NEXT:    .p2align 4
; CHECK-NEXT:  .LBB1_1: # %atomicrmw.start
; CHECK-NEXT:    # =>This Inner Loop Header: Depth=1
; CHECK-NEXT:    movq %rdx, %rax
; CHECK-NEXT:    addq $2, %rax
; CHECK-NEXT:    movq %rdx, (%rsp)
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    adcq $0, %rcx
; CHECK-NEXT:    movq %rcx, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rax, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movl $16, %edi
; CHECK-NEXT:    movq %rbx, %rsi
; CHECK-NEXT:    movq %r14, %rdx
; CHECK-NEXT:    movq %r15, %rcx
; CHECK-NEXT:    movl $5, %r8d
; CHECK-NEXT:    movl $5, %r9d
; CHECK-NEXT:    callq __atomic_compare_exchange@PLT
; CHECK-NEXT:    movq (%rsp), %rdx
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    testb %al, %al
; CHECK-NEXT:    je .LBB1_1
; CHECK-NEXT:  # %bb.2: # %atomicrmw.end
; CHECK-NEXT:    xorps %xmm0, %xmm0
; CHECK-NEXT:    movaps %xmm0, (%rsp)
; CHECK-NEXT:    movq $0, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq $1, {{[0-9]+}}(%rsp)
; CHECK-NEXT:    movq %rsp, %rdx
; CHECK-NEXT:    leaq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    movl $16, %edi
; CHECK-NEXT:    movq %rbx, %rsi
; CHECK-NEXT:    movl $5, %r8d
; CHECK-NEXT:    movl $5, %r9d
; CHECK-NEXT:    callq __atomic_compare_exchange@PLT
; CHECK-NEXT:    addq $32, %rsp
; CHECK-NEXT:    popq %rbx
; CHECK-NEXT:    popq %r14
; CHECK-NEXT:    popq %r15
; CHECK-NEXT:    retq
  %t0 = load atomic i128, ptr %a seq_cst, align 8
  store atomic i128 1, ptr %a seq_cst, align 8
  %t1 = atomicrmw xchg ptr %a, i128 1 seq_cst, align 8
  %t2 = atomicrmw add ptr %a, i128 2 seq_cst, align 8
  %t3 = cmpxchg ptr %a, i128 0, i128 1 seq_cst seq_cst, align 8
  ret void
}
