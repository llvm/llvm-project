; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc -mtriple=x86_64-unknown-unknown -O3 -mattr=+avx2 -mcpu=skylake < %s | FileCheck %s --check-prefix=AVX2

define <4 x i32> @gather_avx_dd_128(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_dd_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovaps %xmm0, %xmm1
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %xmm0
; AVX2-NEXT:    vmovdqa {{.*#+}} xmm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    movl $4, %esi
; AVX2-NEXT:    jmp llvm.x86.avx2.gather.d.d.128@PLT # TAILCALL
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %m32 = sext <4 x i1> %m to <4 x i32>
  %res = tail call <4 x i32> @llvm.x86.avx2.gather.d.d.128(<4 x i32> zeroinitializer, ptr %data, <4 x i32> %indices, <4 x i32> %m32, i8 4)
  ret <4 x i32> %res
}

define <4 x i32> @gather_portable_dd_128(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_dd_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $1, %eax, %xmm1, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb $2, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $2, %eax, %xmm1, %xmm1
; AVX2-NEXT:    andb $8, %dil
; AVX2-NEXT:    shrb $3, %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $3, %eax, %xmm1, %xmm1
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm2 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqd %xmm1, (%rsi,%ymm2,4), %xmm0
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i32, ptr %data, <4 x i64> %idx64
  %res = tail call <4 x i32> @llvm.masked.gather.v4i32.v4p0(<4 x ptr> %ptrs, i32 4, <4 x i1> %m, <4 x i32> zeroinitializer)
  ret <4 x i32> %res
}

define <8 x i32> @gather_avx_dd_256(<8 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_dd_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovd %edi, %xmm1
; AVX2-NEXT:    vpbroadcastb %xmm1, %ymm1
; AVX2-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVX2-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm2
; AVX2-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX2-NEXT:    vpgatherdd %ymm2, (%rsi,%ymm0,4), %ymm1
; AVX2-NEXT:    vmovdqa %ymm1, %ymm0
; AVX2-NEXT:    retq
  %m  = bitcast i8 %maskbits to <8 x i1>
  %m32 = sext <8 x i1> %m to <8 x i32>
  %res = tail call <8 x i32> @llvm.x86.avx2.gather.d.d.256(<8 x i32> zeroinitializer, ptr %data, <8 x i32> %indices, <8 x i32> %m32, i8 4)
  ret <8 x i32> %res
}

define <8 x i32> @gather_portable_dd_256(<8 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_dd_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb $3, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    movl %edi, %ecx
; AVX2-NEXT:    shrb $2, %cl
; AVX2-NEXT:    movzbl %cl, %ecx
; AVX2-NEXT:    andl $1, %ecx
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    movl %edi, %edx
; AVX2-NEXT:    shrb %dl
; AVX2-NEXT:    movzbl %dl, %edx
; AVX2-NEXT:    andl $1, %edx
; AVX2-NEXT:    negl %edx
; AVX2-NEXT:    movl %edi, %r8d
; AVX2-NEXT:    andl $1, %r8d
; AVX2-NEXT:    negl %r8d
; AVX2-NEXT:    movl %edi, %r9d
; AVX2-NEXT:    shrb $7, %r9b
; AVX2-NEXT:    movzbl %r9b, %r9d
; AVX2-NEXT:    negl %r9d
; AVX2-NEXT:    movl %edi, %r10d
; AVX2-NEXT:    shrb $6, %r10b
; AVX2-NEXT:    movzbl %r10b, %r10d
; AVX2-NEXT:    andl $1, %r10d
; AVX2-NEXT:    negl %r10d
; AVX2-NEXT:    movl %edi, %r11d
; AVX2-NEXT:    shrb $5, %r11b
; AVX2-NEXT:    movzbl %r11b, %r11d
; AVX2-NEXT:    andl $1, %r11d
; AVX2-NEXT:    negl %r11d
; AVX2-NEXT:    shrb $4, %dil
; AVX2-NEXT:    movzbl %dil, %edi
; AVX2-NEXT:    andl $1, %edi
; AVX2-NEXT:    negl %edi
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm1 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vmovd %edi, %xmm2
; AVX2-NEXT:    vpinsrd $1, %r11d, %xmm2, %xmm2
; AVX2-NEXT:    vpinsrd $2, %r10d, %xmm2, %xmm2
; AVX2-NEXT:    vpinsrd $3, %r9d, %xmm2, %xmm2
; AVX2-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX2-NEXT:    vpxor %xmm4, %xmm4, %xmm4
; AVX2-NEXT:    vpgatherqd %xmm2, (%rsi,%ymm0,4), %xmm4
; AVX2-NEXT:    vmovd %r8d, %xmm0
; AVX2-NEXT:    vpinsrd $1, %edx, %xmm0, %xmm0
; AVX2-NEXT:    vpinsrd $2, %ecx, %xmm0, %xmm0
; AVX2-NEXT:    vpinsrd $3, %eax, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqd %xmm0, (%rsi,%ymm1,4), %xmm3
; AVX2-NEXT:    vinserti128 $1, %xmm4, %ymm3, %ymm0
; AVX2-NEXT:    retq
  %m  = bitcast i8 %maskbits to <8 x i1>
  %idx64 = zext <8 x i32> %indices to <8 x i64>
  %ptrs = getelementptr i32, ptr %data, <8 x i64> %idx64
  %res = tail call <8 x i32> @llvm.masked.gather.v8i32.v8p0(<8 x ptr> %ptrs, i32 4, <8 x i1> %m, <8 x i32> zeroinitializer)
  ret <8 x i32> %res
}

define <2 x i32> @gather_avx_qd_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_qd_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm1 = xmm0[0],zero,xmm0[1],zero
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %xmm0
; AVX2-NEXT:    vpbroadcastq {{.*#+}} xmm2 = [1,2,1,2]
; AVX2-NEXT:    vpand %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    movl $4, %esi
; AVX2-NEXT:    jmp llvm.x86.avx2.gather.q.d.128@PLT # TAILCALL
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %m32 = sext <2 x i1> %m to <2 x i32>
  %res = tail call <2 x i32> @llvm.x86.avx2.gather.q.d.128(<2 x i32> zeroinitializer, ptr %data, <2 x i64> %idx64, <2 x i32> %m32, i8 4)
  ret <2 x i32> %res
}

define <2 x i32> @gather_portable_qd_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_qd_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andb $2, %al
; AVX2-NEXT:    shrb %al
; AVX2-NEXT:    andb $1, %dil
; AVX2-NEXT:    vmovd %edi, %xmm1
; AVX2-NEXT:    vpinsrb $8, %eax, %xmm1, %xmm1
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm2 = xmm0[0],zero,xmm0[1],zero
; AVX2-NEXT:    vpshufd {{.*#+}} xmm0 = xmm1[0,2,2,3]
; AVX2-NEXT:    vpslld $31, %xmm0, %xmm1
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqd %xmm1, (%rsi,%xmm2,4), %xmm0
; AVX2-NEXT:    retq
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %ptrs = getelementptr i32, ptr %data, <2 x i64> %idx64
  %res = tail call <2 x i32> @llvm.masked.gather.v2i32.v2p0(<2 x ptr> %ptrs, i32 4, <2 x i1> %m, <2 x i32> zeroinitializer)
  ret <2 x i32> %res
}

define <4 x i32> @gather_avx_qd_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_qd_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm1 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %xmm0
; AVX2-NEXT:    vmovdqa {{.*#+}} xmm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqd %xmm2, (%rsi,%ymm1,4), %xmm0
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %m32 = sext <4 x i1> %m to <4 x i32>
  %res = tail call <4 x i32> @llvm.x86.avx2.gather.q.d.256(<4 x i32> zeroinitializer, ptr %data, <4 x i64> %idx64, <4 x i32> %m32, i8 4)
  ret <4 x i32> %res
}

define <4 x i32> @gather_portable_qd_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_qd_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $1, %eax, %xmm1, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb $2, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $2, %eax, %xmm1, %xmm1
; AVX2-NEXT:    andb $8, %dil
; AVX2-NEXT:    shrb $3, %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vpinsrd $3, %eax, %xmm1, %xmm1
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm2 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqd %xmm1, (%rsi,%ymm2,4), %xmm0
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i32, ptr %data, <4 x i64> %idx64
  %res = tail call <4 x i32> @llvm.masked.gather.v4i32.v4p0(<4 x ptr> %ptrs, i32 4, <4 x i1> %m, <4 x i32> zeroinitializer)
  ret <4 x i32> %res
}

define <2 x i64> @gather_avx_dq_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_dq_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovaps %xmm0, %xmm1
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %xmm0
; AVX2-NEXT:    vmovdqa {{.*#+}} xmm2 = [1,2]
; AVX2-NEXT:    vpand %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpcmpeqq %xmm2, %xmm0, %xmm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    movl $8, %esi
; AVX2-NEXT:    jmp llvm.x86.avx2.gather.d.q.128@PLT # TAILCALL
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %m64 = sext <2 x i1> %m to <2 x i64>
  %res = tail call <2 x i64> @llvm.x86.avx2.gather.d.q.128(<2 x i64> zeroinitializer, ptr %data, <2 x i32> %indices, <2 x i64> %m64, i8 8)
  ret <2 x i64> %res
}

define <2 x i64> @gather_portable_dq_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_dq_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm1
; AVX2-NEXT:    andb $2, %dil
; AVX2-NEXT:    shrb %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm2[0]
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm2 = xmm0[0],zero,xmm0[1],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqq %xmm1, (%rsi,%xmm2,8), %xmm0
; AVX2-NEXT:    retq
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %ptrs = getelementptr i64, ptr %data, <2 x i64> %idx64
  %res = tail call <2 x i64> @llvm.masked.gather.v2i64.v2p0(<2 x ptr> %ptrs, i32 8, <2 x i1> %m, <2 x i64> zeroinitializer)
  ret <2 x i64> %res
}

define <4 x i64> @gather_avx_dq_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_dq_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovd %edi, %xmm1
; AVX2-NEXT:    vpbroadcastd %xmm1, %ymm1
; AVX2-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpcmpeqq %ymm2, %ymm1, %ymm2
; AVX2-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX2-NEXT:    vpgatherdq %ymm2, (%rsi,%xmm0,8), %ymm1
; AVX2-NEXT:    vmovdqa %ymm1, %ymm0
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %m64 = sext <4 x i1> %m to <4 x i64>
  %res = tail call <4 x i64> @llvm.x86.avx2.gather.d.q.256(<4 x i64> zeroinitializer, ptr %data, <4 x i32> %indices, <4 x i64> %m64, i8 8)
  ret <4 x i64> %res
}

define <4 x i64> @gather_portable_dq_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_dq_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andb $8, %al
; AVX2-NEXT:    shrb $3, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb $2, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    shrb %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm3
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm3[0]
; AVX2-NEXT:    vinserti128 $1, %xmm1, %ymm2, %ymm1
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm2 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqq %ymm1, (%rsi,%ymm2,8), %ymm0
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i64, ptr %data, <4 x i64> %idx64
  %res = tail call <4 x i64> @llvm.masked.gather.v4i64.v4p0(<4 x ptr> %ptrs, i32 8, <4 x i1> %m, <4 x i64> zeroinitializer)
  ret <4 x i64> %res
}

define <2 x i64> @gather_avx_qq_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_qq_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm1 = xmm0[0],zero,xmm0[1],zero
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %xmm0
; AVX2-NEXT:    vmovdqa {{.*#+}} xmm2 = [1,2]
; AVX2-NEXT:    vpand %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpcmpeqq %xmm2, %xmm0, %xmm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    movq %rsi, %rdi
; AVX2-NEXT:    movl $8, %esi
; AVX2-NEXT:    jmp llvm.x86.avx2.gather.q.q.128@PLT # TAILCALL
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %m64 = sext <2 x i1> %m to <2 x i64>
  %res = tail call <2 x i64> @llvm.x86.avx2.gather.q.q.128(<2 x i64> zeroinitializer, ptr %data, <2 x i64> %idx64, <2 x i64> %m64, i8 8)
  ret <2 x i64> %res
}

define <2 x i64> @gather_portable_qq_128(<2 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_qq_128:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm1
; AVX2-NEXT:    andb $2, %dil
; AVX2-NEXT:    shrb %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm2[0]
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm2 = xmm0[0],zero,xmm0[1],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqq %xmm1, (%rsi,%xmm2,8), %xmm0
; AVX2-NEXT:    retq
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %ptrs = getelementptr i64, ptr %data, <2 x i64> %idx64
  %res = tail call <2 x i64> @llvm.masked.gather.v2i64.v2p0(<2 x ptr> %ptrs, i32 8, <2 x i1> %m, <2 x i64> zeroinitializer)
  ret <2 x i64> %res
}

define <4 x i64> @gather_avx_qq_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_avx_qq_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm1 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vmovd %edi, %xmm0
; AVX2-NEXT:    vpbroadcastd %xmm0, %ymm0
; AVX2-NEXT:    vmovdqa {{.*#+}} ymm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpcmpeqq %ymm2, %ymm0, %ymm2
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqq %ymm2, (%rsi,%ymm1,8), %ymm0
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %m64 = sext <4 x i1> %m to <4 x i64>
  %res = tail call <4 x i64> @llvm.x86.avx2.gather.q.q.256(<4 x i64> zeroinitializer, ptr %data, <4 x i64> %idx64, <4 x i64> %m64, i8 8)
  ret <4 x i64> %res
}

define <4 x i64> @gather_portable_qq_256(<4 x i32> %indices, i8 %maskbits, ptr noundef readonly %data) nounwind {
; AVX2-LABEL: gather_portable_qq_256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andb $8, %al
; AVX2-NEXT:    shrb $3, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm1
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    shrb $2, %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX2-NEXT:    movl %edi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm2
; AVX2-NEXT:    shrb %dil
; AVX2-NEXT:    movzbl %dil, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    negq %rax
; AVX2-NEXT:    vmovq %rax, %xmm3
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm3[0]
; AVX2-NEXT:    vinserti128 $1, %xmm1, %ymm2, %ymm1
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm2 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX2-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vpgatherqq %ymm1, (%rsi,%ymm2,8), %ymm0
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i64, ptr %data, <4 x i64> %idx64
  %res = tail call <4 x i64> @llvm.masked.gather.v4i64.v4p0(<4 x ptr> %ptrs, i32 8, <4 x i1> %m, <4 x i64> zeroinitializer)
  ret <4 x i64> %res
}

declare <4 x i32> @llvm.x86.avx2.gather.d.d.128(<4 x i32>, ptr, <4 x i32>, <4 x i32>, i8)
declare <8 x i32> @llvm.x86.avx2.gather.d.d.256(<8 x i32>, ptr, <8 x i32>, <8 x i32>, i8)

declare <2 x i32> @llvm.x86.avx2.gather.q.d.128(<2 x i32>, ptr, <2 x i64>, <2 x i32>, i8)
declare <4 x i32> @llvm.x86.avx2.gather.q.d.256(<4 x i32>, ptr, <4 x i64>, <4 x i32>, i8)

declare <2 x i64> @llvm.x86.avx2.gather.d.q.128(<2 x i64>, ptr, <2 x i32>, <2 x i64>, i8)
declare <4 x i64> @llvm.x86.avx2.gather.d.q.256(<4 x i64>, ptr, <4 x i32>, <4 x i64>, i8)

declare <2 x i64> @llvm.x86.avx2.gather.q.q.128(<2 x i64>, ptr, <2 x i64>, <2 x i64>, i8)
declare <4 x i64> @llvm.x86.avx2.gather.q.q.256(<4 x i64>, ptr, <4 x i64>, <4 x i64>, i8)

declare <2 x i32> @llvm.masked.gather.v2i32.v2p0(<2 x ptr>, i32 immarg, <2 x i1>, <2 x i32>)
declare <4 x i32> @llvm.masked.gather.v4i32.v4p0(<4 x ptr>, i32 immarg, <4 x i1>, <4 x i32>)
declare <8 x i32> @llvm.masked.gather.v8i32.v8p0(<8 x ptr>, i32 immarg, <8 x i1>, <8 x i32>)

declare <2 x i64> @llvm.masked.gather.v2i64.v2p0(<2 x ptr>, i32 immarg, <2 x i1>, <2 x i64>)
declare <4 x i64> @llvm.masked.gather.v4i64.v4p0(<4 x ptr>, i32 immarg, <4 x i1>, <4 x i64>)

define void @scatter_portable_i32_4(<4 x i32> %values, <4 x i32> %indices, i8 %maskbits, ptr noundef %data) nounwind {
; AVX2-LABEL: scatter_portable_i32_4:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX2-NEXT:    vpsllq $2, %ymm1, %ymm1
; AVX2-NEXT:    vmovq %rsi, %xmm2
; AVX2-NEXT:    vpbroadcastq %xmm2, %ymm2
; AVX2-NEXT:    vpaddq %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    jne .LBB16_1
; AVX2-NEXT:  # %bb.2: # %else
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    jne .LBB16_3
; AVX2-NEXT:  .LBB16_4: # %else2
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    jne .LBB16_5
; AVX2-NEXT:  .LBB16_6: # %else4
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    jne .LBB16_7
; AVX2-NEXT:  .LBB16_8: # %else6
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
; AVX2-NEXT:  .LBB16_1: # %cond.store
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vmovss %xmm0, (%rax)
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    je .LBB16_4
; AVX2-NEXT:  .LBB16_3: # %cond.store1
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextractps $1, %xmm0, (%rax)
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    je .LBB16_6
; AVX2-NEXT:  .LBB16_5: # %cond.store3
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vextractps $2, %xmm0, (%rax)
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    je .LBB16_8
; AVX2-NEXT:  .LBB16_7: # %cond.store5
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextractps $3, %xmm0, (%rax)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i32, ptr %data, <4 x i64> %idx64
  tail call void @llvm.masked.scatter.v4i32.v4p0(<4 x i32> %values, <4 x ptr> %ptrs, i32 4, <4 x i1> %m)
  ret void
}

define void @scatter_portable_i32_8(<8 x i32> %values, <8 x i32> %indices, i8 %maskbits, ptr noundef %data) nounwind {
; AVX2-LABEL: scatter_portable_i32_8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm3 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX2-NEXT:    vmovq %rsi, %xmm2
; AVX2-NEXT:    vpbroadcastq %xmm2, %ymm2
; AVX2-NEXT:    vpsllq $2, %ymm3, %ymm3
; AVX2-NEXT:    vpaddq %ymm3, %ymm2, %ymm3
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    jne .LBB17_1
; AVX2-NEXT:  # %bb.2: # %else
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    jne .LBB17_3
; AVX2-NEXT:  .LBB17_4: # %else2
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm4 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX2-NEXT:    vextracti128 $1, %ymm3, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    jne .LBB17_5
; AVX2-NEXT:  .LBB17_6: # %else4
; AVX2-NEXT:    vpsllq $2, %ymm4, %ymm3
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    jne .LBB17_7
; AVX2-NEXT:  .LBB17_8: # %else6
; AVX2-NEXT:    vpaddq %ymm3, %ymm2, %ymm1
; AVX2-NEXT:    vextractf128 $1, %ymm0, %xmm0
; AVX2-NEXT:    testb $16, %dil
; AVX2-NEXT:    jne .LBB17_9
; AVX2-NEXT:  .LBB17_10: # %else8
; AVX2-NEXT:    testb $32, %dil
; AVX2-NEXT:    jne .LBB17_11
; AVX2-NEXT:  .LBB17_12: # %else10
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $64, %dil
; AVX2-NEXT:    jne .LBB17_13
; AVX2-NEXT:  .LBB17_14: # %else12
; AVX2-NEXT:    testb $-128, %dil
; AVX2-NEXT:    jne .LBB17_15
; AVX2-NEXT:  .LBB17_16: # %else14
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
; AVX2-NEXT:  .LBB17_1: # %cond.store
; AVX2-NEXT:    vmovq %xmm3, %rax
; AVX2-NEXT:    vmovss %xmm0, (%rax)
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    je .LBB17_4
; AVX2-NEXT:  .LBB17_3: # %cond.store1
; AVX2-NEXT:    vpextrq $1, %xmm3, %rax
; AVX2-NEXT:    vextractps $1, %xmm0, (%rax)
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm4 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX2-NEXT:    vextracti128 $1, %ymm3, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    je .LBB17_6
; AVX2-NEXT:  .LBB17_5: # %cond.store3
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vextractps $2, %xmm0, (%rax)
; AVX2-NEXT:    vpsllq $2, %ymm4, %ymm3
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    je .LBB17_8
; AVX2-NEXT:  .LBB17_7: # %cond.store5
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextractps $3, %xmm0, (%rax)
; AVX2-NEXT:    vpaddq %ymm3, %ymm2, %ymm1
; AVX2-NEXT:    vextractf128 $1, %ymm0, %xmm0
; AVX2-NEXT:    testb $16, %dil
; AVX2-NEXT:    je .LBB17_10
; AVX2-NEXT:  .LBB17_9: # %cond.store7
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vmovss %xmm0, (%rax)
; AVX2-NEXT:    testb $32, %dil
; AVX2-NEXT:    je .LBB17_12
; AVX2-NEXT:  .LBB17_11: # %cond.store9
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextractps $1, %xmm0, (%rax)
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $64, %dil
; AVX2-NEXT:    je .LBB17_14
; AVX2-NEXT:  .LBB17_13: # %cond.store11
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vextractps $2, %xmm0, (%rax)
; AVX2-NEXT:    testb $-128, %dil
; AVX2-NEXT:    je .LBB17_16
; AVX2-NEXT:  .LBB17_15: # %cond.store13
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vextractps $3, %xmm0, (%rax)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m  = bitcast i8 %maskbits to <8 x i1>
  %idx64 = zext <8 x i32> %indices to <8 x i64>
  %ptrs = getelementptr i32, ptr %data, <8 x i64> %idx64
  tail call void @llvm.masked.scatter.v8i32.v8p0(<8 x i32> %values, <8 x ptr> %ptrs, i32 4, <8 x i1> %m)
  ret void
}

define void @scatter_portable_i64_2(<2 x i64> %values, <2 x i32> %indices, i8 %maskbits, ptr noundef %data) nounwind {
; AVX2-LABEL: scatter_portable_i64_2:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} xmm1 = xmm1[0],zero,xmm1[1],zero
; AVX2-NEXT:    vpsllq $3, %xmm1, %xmm1
; AVX2-NEXT:    vmovq %rsi, %xmm2
; AVX2-NEXT:    vpbroadcastq %xmm2, %xmm2
; AVX2-NEXT:    vpaddq %xmm1, %xmm2, %xmm1
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    jne .LBB18_1
; AVX2-NEXT:  # %bb.2: # %else
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    jne .LBB18_3
; AVX2-NEXT:  .LBB18_4: # %else2
; AVX2-NEXT:    retq
; AVX2-NEXT:  .LBB18_1: # %cond.store
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vmovq %xmm0, (%rax)
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    je .LBB18_4
; AVX2-NEXT:  .LBB18_3: # %cond.store1
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vpextrq $1, %xmm0, (%rax)
; AVX2-NEXT:    retq
  %m2 = trunc i8 %maskbits to i2
  %m  = bitcast i2 %m2 to <2 x i1>
  %idx64 = zext <2 x i32> %indices to <2 x i64>
  %ptrs = getelementptr i64, ptr %data, <2 x i64> %idx64
  tail call void @llvm.masked.scatter.v2i64.v2p0(<2 x i64> %values, <2 x ptr> %ptrs, i32 8, <2 x i1> %m)
  ret void
}

define void @scatter_portable_i64_4(<4 x i64> %values, <4 x i32> %indices, i8 %maskbits, ptr noundef %data) nounwind {
; AVX2-LABEL: scatter_portable_i64_4:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxdq {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX2-NEXT:    vpsllq $3, %ymm1, %ymm1
; AVX2-NEXT:    vmovq %rsi, %xmm2
; AVX2-NEXT:    vpbroadcastq %xmm2, %ymm2
; AVX2-NEXT:    vpaddq %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    jne .LBB19_1
; AVX2-NEXT:  # %bb.2: # %else
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    jne .LBB19_3
; AVX2-NEXT:  .LBB19_4: # %else2
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    jne .LBB19_5
; AVX2-NEXT:  .LBB19_6: # %else4
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    jne .LBB19_7
; AVX2-NEXT:  .LBB19_8: # %else6
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
; AVX2-NEXT:  .LBB19_1: # %cond.store
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vmovq %xmm0, (%rax)
; AVX2-NEXT:    testb $2, %dil
; AVX2-NEXT:    je .LBB19_4
; AVX2-NEXT:  .LBB19_3: # %cond.store1
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vpextrq $1, %xmm0, (%rax)
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm0
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm1
; AVX2-NEXT:    testb $4, %dil
; AVX2-NEXT:    je .LBB19_6
; AVX2-NEXT:  .LBB19_5: # %cond.store3
; AVX2-NEXT:    vmovq %xmm1, %rax
; AVX2-NEXT:    vmovq %xmm0, (%rax)
; AVX2-NEXT:    testb $8, %dil
; AVX2-NEXT:    je .LBB19_8
; AVX2-NEXT:  .LBB19_7: # %cond.store5
; AVX2-NEXT:    vpextrq $1, %xmm1, %rax
; AVX2-NEXT:    vpextrq $1, %xmm0, (%rax)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
  %m4 = trunc i8 %maskbits to i4
  %m  = bitcast i4 %m4 to <4 x i1>
  %idx64 = zext <4 x i32> %indices to <4 x i64>
  %ptrs = getelementptr i64, ptr %data, <4 x i64> %idx64
  tail call void @llvm.masked.scatter.v4i64.v4p0(<4 x i64> %values, <4 x ptr> %ptrs, i32 8, <4 x i1> %m)
  ret void
}


declare void @llvm.masked.scatter.v4i32.v4p0(<4 x i32>, <4 x ptr>, i32 immarg, <4 x i1>)
declare void @llvm.masked.scatter.v8i32.v8p0(<8 x i32>, <8 x ptr>, i32 immarg, <8 x i1>)
declare void @llvm.masked.scatter.v2i64.v2p0(<2 x i64>, <2 x ptr>, i32 immarg, <2 x i1>)
declare void @llvm.masked.scatter.v4i64.v4p0(<4 x i64>, <4 x ptr>, i32 immarg, <4 x i1>)
