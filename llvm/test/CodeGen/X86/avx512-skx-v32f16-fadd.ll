; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=avx512f| FileCheck %s --check-prefixes=CHECK

define <32 x half> @foo(<32 x half> %a, <32 x half> %b) nounwind {
; CHECK-LABEL: foo:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $168, %rsp
; CHECK-NEXT:    movq %rdi, %rax
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edi
; CHECK-NEXT:    vmovd %edi, %xmm8
; CHECK-NEXT:    movzwl %si, %esi
; CHECK-NEXT:    vmovd %esi, %xmm0
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %esi
; CHECK-NEXT:    vmovd %esi, %xmm1
; CHECK-NEXT:    movzwl %dx, %edx
; CHECK-NEXT:    vmovd %edx, %xmm2
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm3
; CHECK-NEXT:    movzwl %cx, %ecx
; CHECK-NEXT:    vmovd %ecx, %xmm4
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    vmovd %ecx, %xmm5
; CHECK-NEXT:    movzwl %r8w, %ecx
; CHECK-NEXT:    vmovd %ecx, %xmm6
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    vmovd %ecx, %xmm9
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl %r9w, %edx
; CHECK-NEXT:    vmovd %edx, %xmm10
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm11
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm12
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm13
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm14
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm15
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm8, %xmm8
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vaddss %xmm0, %xmm8, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm8
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm2, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm3, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm4, %xmm2
; CHECK-NEXT:    vaddss %xmm1, %xmm2, %xmm1
; CHECK-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm1
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm5, %xmm2
; CHECK-NEXT:    vcvtph2ps %xmm6, %xmm3
; CHECK-NEXT:    vaddss %xmm2, %xmm3, %xmm2
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm2
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm9, %xmm3
; CHECK-NEXT:    vcvtph2ps %xmm10, %xmm4
; CHECK-NEXT:    vaddss %xmm3, %xmm4, %xmm3
; CHECK-NEXT:    vmovaps %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm3
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm11, %xmm4
; CHECK-NEXT:    vcvtph2ps %xmm12, %xmm5
; CHECK-NEXT:    vaddss %xmm4, %xmm5, %xmm4
; CHECK-NEXT:    vmovaps %xmm4, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm4
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm13, %xmm5
; CHECK-NEXT:    vcvtph2ps %xmm14, %xmm6
; CHECK-NEXT:    vaddss %xmm5, %xmm6, %xmm5
; CHECK-NEXT:    vmovaps %xmm5, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm5
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm15, %xmm6
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vaddss %xmm6, %xmm7, %xmm6
; CHECK-NEXT:    vmovaps %xmm6, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm6
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm8, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vaddss %xmm7, %xmm0, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm2, %xmm2
; CHECK-NEXT:    vaddss %xmm1, %xmm2, %xmm1
; CHECK-NEXT:    vmovaps %xmm1, (%rsp) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm1
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm3, %xmm2
; CHECK-NEXT:    vcvtph2ps %xmm4, %xmm3
; CHECK-NEXT:    vaddss %xmm2, %xmm3, %xmm2
; CHECK-NEXT:    vmovaps %xmm2, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm2
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm5, %xmm3
; CHECK-NEXT:    vcvtph2ps %xmm6, %xmm4
; CHECK-NEXT:    vaddss %xmm3, %xmm4, %xmm3
; CHECK-NEXT:    vmovaps %xmm3, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm3
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm2, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm3, %xmm2
; CHECK-NEXT:    vaddss %xmm1, %xmm2, %xmm1
; CHECK-NEXT:    vmovaps %xmm1, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm1
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm0
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm14
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm13
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm12
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm11
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm10
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm9
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm8
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm1
; CHECK-NEXT:    vcvtph2ps %xmm1, %xmm1
; CHECK-NEXT:    vaddss %xmm0, %xmm1, %xmm1
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm0
; CHECK-NEXT:    vcvtph2ps %xmm0, %xmm0
; CHECK-NEXT:    vmovd %ecx, %xmm15
; CHECK-NEXT:    vcvtph2ps %xmm15, %xmm7
; CHECK-NEXT:    vaddss %xmm0, %xmm7, %xmm0
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vmovd %ecx, %xmm6
; CHECK-NEXT:    vcvtph2ps %xmm6, %xmm6
; CHECK-NEXT:    vaddss %xmm7, %xmm6, %xmm6
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vmovd %ecx, %xmm5
; CHECK-NEXT:    vcvtph2ps %xmm5, %xmm5
; CHECK-NEXT:    vaddss %xmm7, %xmm5, %xmm5
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vmovd %ecx, %xmm4
; CHECK-NEXT:    vcvtph2ps %xmm4, %xmm4
; CHECK-NEXT:    vaddss %xmm7, %xmm4, %xmm4
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vmovd %ecx, %xmm3
; CHECK-NEXT:    vcvtph2ps %xmm3, %xmm3
; CHECK-NEXT:    vaddss %xmm7, %xmm3, %xmm3
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %ecx
; CHECK-NEXT:    movzwl {{[0-9]+}}(%rsp), %edx
; CHECK-NEXT:    vmovd %edx, %xmm7
; CHECK-NEXT:    vcvtph2ps %xmm7, %xmm7
; CHECK-NEXT:    vmovd %ecx, %xmm2
; CHECK-NEXT:    vcvtph2ps %xmm2, %xmm2
; CHECK-NEXT:    vaddss %xmm7, %xmm2, %xmm2
; CHECK-NEXT:    vcvtps2ph $4, %xmm2, %xmm2
; CHECK-NEXT:    vpextrw $0, %xmm2, 62(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm3, %xmm2
; CHECK-NEXT:    vpextrw $0, %xmm2, 60(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm4, %xmm2
; CHECK-NEXT:    vpextrw $0, %xmm2, 58(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm5, %xmm2
; CHECK-NEXT:    vpextrw $0, %xmm2, 56(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm6, %xmm2
; CHECK-NEXT:    vpextrw $0, %xmm2, 54(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 52(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm1, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 50(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm8, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 48(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm9, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 46(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm10, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 44(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm11, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 42(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm12, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 40(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm13, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 38(%rax)
; CHECK-NEXT:    vcvtps2ph $4, %xmm14, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 36(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 34(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 32(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 30(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 28(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 26(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 24(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 22(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 20(%rax)
; CHECK-NEXT:    vmovaps (%rsp), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 18(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 16(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 14(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 12(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 10(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 8(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 6(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 4(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, 2(%rax)
; CHECK-NEXT:    vmovaps {{[-0-9]+}}(%r{{[sb]}}p), %xmm0 # 16-byte Reload
; CHECK-NEXT:    vcvtps2ph $4, %xmm0, %xmm0
; CHECK-NEXT:    vpextrw $0, %xmm0, (%rax)
; CHECK-NEXT:    addq $168, %rsp
; CHECK-NEXT:    retq
  %1 = fadd <32 x half> %a, %b
  ret <32 x half> %1
}
