; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse2    | FileCheck %s --check-prefixes=CHECK,SCALAR
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse2,+pclmul | FileCheck %s --check-prefixes=CHECK,SSE-PCLMUL,SSE2-PCLMUL
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse4.2,+pclmul | FileCheck %s --check-prefixes=CHECK,SSE-PCLMUL,SSE42-PCLMUL
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx2,+pclmul | FileCheck %s --check-prefixes=CHECK,AVX
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx2,+vpclmulqdq  | FileCheck %s --check-prefixes=CHECK,AVX
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512vl,+vpclmulqdq | FileCheck %s --check-prefixes=CHECK,AVX

define i8 @clmul_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    # kill: def $edi killed $edi def $rdi
; SCALAR-NEXT:    xorl %ecx, %ecx
; SCALAR-NEXT:    testb $1, %sil
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    cmovel %ecx, %eax
; SCALAR-NEXT:    leal (%rdi,%rdi), %edx
; SCALAR-NEXT:    movzbl %dl, %edx
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovael %ecx, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    leal (,%rdi,4), %eax
; SCALAR-NEXT:    movzbl %al, %r8d
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovael %ecx, %r8d
; SCALAR-NEXT:    leal (,%rdi,8), %eax
; SCALAR-NEXT:    movzbl %al, %eax
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovael %ecx, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shlb $4, %dl
; SCALAR-NEXT:    movzbl %dl, %edx
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovael %ecx, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shlb $5, %r8b
; SCALAR-NEXT:    movzbl %r8b, %r8d
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovael %ecx, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shlb $6, %dl
; SCALAR-NEXT:    movzbl %dl, %edx
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovael %ecx, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    shlb $7, %dil
; SCALAR-NEXT:    movzbl %dil, %eax
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovael %ecx, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    # kill: def $al killed $al killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %res = call i8 @llvm.clmul.i8(i8 %a, i8 %b)
  ret i8 %res
}

define i16 @clmul_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmul_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    # kill: def $edi killed $edi def $rdi
; SCALAR-NEXT:    leal (%rdi,%rdi), %ecx
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    cmovnel %edi, %edx
; SCALAR-NEXT:    leal (,%rdi,4), %r8d
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    leal (,%rdi,8), %ecx
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $4, %r9d
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %ecx, %r9d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $5, %edx
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r9d, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $6, %ecx
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $7, %r8d
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $9, %ecx
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r9d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $10, %edx
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $11, %r8d
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $12, %edx
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $13, %r8d
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $14, %edx
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    shll $15, %edi
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %res = call i16 @llvm.clmul.i16(i16 %a, i16 %b)
  ret i16 %res
}

define i32 @clmul_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmul_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    # kill: def $edi killed $edi def $rdi
; SCALAR-NEXT:    leal (%rdi,%rdi), %ecx
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    cmovnel %edi, %edx
; SCALAR-NEXT:    leal (,%rdi,4), %r8d
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    leal (,%rdi,8), %ecx
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $4, %r9d
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %ecx, %r9d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $5, %edx
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r9d, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $6, %ecx
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $7, %r8d
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $9, %ecx
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r9d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $10, %edx
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $11, %r8d
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $12, %edx
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $13, %r8d
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $14, %edx
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $15, %ecx
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $16, %r8d
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $17, %ecx
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $18, %r8d
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $19, %r9d
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $20, %ecx
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r9d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $21, %edx
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $22, %r8d
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $23, %edx
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $24, %r8d
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $25, %edx
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $26, %r8d
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $27, %edx
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $28, %ecx
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $29, %r8d
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shll $30, %ecx
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    shll $31, %edi
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %res = call i32 @llvm.clmul.i32(i32 %a, i32 %b)
  ret i32 %res
}

define i64 @clmul_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmul_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    leaq (%rdi,%rdi), %rcx
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    cmovneq %rdi, %rdx
; SCALAR-NEXT:    leaq (,%rdi,4), %r8
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    leaq (,%rdi,8), %rcx
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $4, %rdx
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $5, %rcx
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $6, %rdx
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $7, %r8
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    shlq $8, %r9
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $9, %rdx
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r9, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $10, %rcx
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $11, %r8
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $12, %rcx
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $13, %r8
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $14, %rcx
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $15, %rdx
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $16, %r8
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $17, %rdx
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $18, %r8
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    shlq $19, %r9
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $20, %rdx
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r9, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $21, %rcx
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $22, %r8
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $23, %rcx
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $24, %r8
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $25, %rcx
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $26, %r8
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $27, %rcx
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $28, %rdx
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $29, %r8
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $30, %rdx
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $31, %r8
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $32, %rdx
; SCALAR-NEXT:    btq $32, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $33, %r8
; SCALAR-NEXT:    btq $33, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    shlq $34, %r9
; SCALAR-NEXT:    btq $34, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $35, %rdx
; SCALAR-NEXT:    btq $35, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r9, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $36, %rcx
; SCALAR-NEXT:    btq $36, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $37, %r8
; SCALAR-NEXT:    btq $37, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $38, %rcx
; SCALAR-NEXT:    btq $38, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $39, %r8
; SCALAR-NEXT:    btq $39, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $40, %rcx
; SCALAR-NEXT:    btq $40, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $41, %r8
; SCALAR-NEXT:    btq $41, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $42, %rcx
; SCALAR-NEXT:    btq $42, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $43, %r8
; SCALAR-NEXT:    btq $43, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $44, %rcx
; SCALAR-NEXT:    btq $44, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $45, %rdx
; SCALAR-NEXT:    btq $45, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $46, %r8
; SCALAR-NEXT:    btq $46, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $47, %rdx
; SCALAR-NEXT:    btq $47, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $48, %r8
; SCALAR-NEXT:    btq $48, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $49, %rdx
; SCALAR-NEXT:    btq $49, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $50, %r8
; SCALAR-NEXT:    btq $50, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $51, %rdx
; SCALAR-NEXT:    btq $51, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $52, %r8
; SCALAR-NEXT:    btq $52, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    shlq $53, %r9
; SCALAR-NEXT:    btq $53, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    shlq $54, %rdx
; SCALAR-NEXT:    btq $54, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rdx
; SCALAR-NEXT:    xorq %r9, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $55, %rcx
; SCALAR-NEXT:    btq $55, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $56, %r8
; SCALAR-NEXT:    btq $56, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $57, %rcx
; SCALAR-NEXT:    btq $57, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $58, %r8
; SCALAR-NEXT:    btq $58, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $59, %rcx
; SCALAR-NEXT:    btq $59, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $60, %r8
; SCALAR-NEXT:    btq $60, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    shlq $61, %rcx
; SCALAR-NEXT:    btq $61, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %rcx
; SCALAR-NEXT:    xorq %r8, %rcx
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    shlq $62, %r8
; SCALAR-NEXT:    btq $62, %rsi
; SCALAR-NEXT:    cmovaeq %rax, %r8
; SCALAR-NEXT:    xorq %rcx, %r8
; SCALAR-NEXT:    shlq $63, %rdi
; SCALAR-NEXT:    btq $63, %rsi
; SCALAR-NEXT:    cmovbq %rdi, %rax
; SCALAR-NEXT:    xorq %r8, %rax
; SCALAR-NEXT:    xorq %rdx, %rax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i64:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    retq
  %res = call i64 @llvm.clmul.i64(i64 %a, i64 %b)
  ret i64 %res
}

define i8 @clmulr_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmulr_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movzbl %dil, %ecx
; SCALAR-NEXT:    movzbl %sil, %edx
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    cmovnel %ecx, %esi
; SCALAR-NEXT:    leal (%rcx,%rcx), %r8d
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    leal (,%rcx,4), %esi
; SCALAR-NEXT:    btl $2, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    leal (,%rcx,8), %r9d
; SCALAR-NEXT:    btl $3, %edx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $4, %esi
; SCALAR-NEXT:    btl $4, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %ecx, %r8d
; SCALAR-NEXT:    shll $5, %r8d
; SCALAR-NEXT:    btl $5, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $6, %esi
; SCALAR-NEXT:    btl $6, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    movl %ecx, %r8d
; SCALAR-NEXT:    shll $7, %r8d
; SCALAR-NEXT:    btl $7, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    movl %ecx, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %edx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %ecx, %r10d
; SCALAR-NEXT:    shll $9, %r10d
; SCALAR-NEXT:    btl $9, %edx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %ecx, %r8d
; SCALAR-NEXT:    shll $10, %r8d
; SCALAR-NEXT:    btl $10, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r10d, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $11, %esi
; SCALAR-NEXT:    btl $11, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    shll $12, %ecx
; SCALAR-NEXT:    btl $12, %edx
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %esi, %ecx
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $13, %esi
; SCALAR-NEXT:    btl $13, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %ecx, %esi
; SCALAR-NEXT:    shll $14, %edi
; SCALAR-NEXT:    btl $14, %edx
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %esi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    shrl $7, %eax
; SCALAR-NEXT:    # kill: def $al killed $al killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $7, %eax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $7, %eax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i8 %a to i16
  %b.ext = zext i8 %b to i16
  %clmul = call i16 @llvm.clmul.i16(i16 %a.ext, i16 %b.ext)
  %res.ext = lshr i16 %clmul, 7
  %res = trunc i16 %res.ext to i8
  ret i8 %res
}

define i16 @clmulr_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmulr_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movzwl %di, %edx
; SCALAR-NEXT:    movzwl %si, %ecx
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    cmovnel %edx, %esi
; SCALAR-NEXT:    leal (%rdx,%rdx), %r8d
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    leal (,%rdx,4), %esi
; SCALAR-NEXT:    btl $2, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    leal (,%rdx,8), %r9d
; SCALAR-NEXT:    btl $3, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $4, %esi
; SCALAR-NEXT:    btl $4, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edx, %r10d
; SCALAR-NEXT:    shll $5, %r10d
; SCALAR-NEXT:    btl $5, %ecx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %esi, %r10d
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $6, %r8d
; SCALAR-NEXT:    btl $6, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r10d, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $7, %esi
; SCALAR-NEXT:    btl $7, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    movl %edx, %r10d
; SCALAR-NEXT:    shll $9, %r10d
; SCALAR-NEXT:    btl $9, %ecx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $10, %esi
; SCALAR-NEXT:    btl $10, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r10d, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $11, %r8d
; SCALAR-NEXT:    btl $11, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $12, %r9d
; SCALAR-NEXT:    btl $12, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $13, %r8d
; SCALAR-NEXT:    btl $13, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $14, %r9d
; SCALAR-NEXT:    btl $14, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    shll $15, %edx
; SCALAR-NEXT:    btl $15, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r9d, %edx
; SCALAR-NEXT:    xorl %esi, %edx
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $16, %esi
; SCALAR-NEXT:    btl $16, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $17, %r8d
; SCALAR-NEXT:    btl $17, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $18, %esi
; SCALAR-NEXT:    btl $18, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $19, %r8d
; SCALAR-NEXT:    btl $19, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $20, %r9d
; SCALAR-NEXT:    btl $20, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $21, %esi
; SCALAR-NEXT:    btl $21, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    xorl %edx, %esi
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $22, %edx
; SCALAR-NEXT:    btl $22, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $23, %r8d
; SCALAR-NEXT:    btl $23, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $24, %edx
; SCALAR-NEXT:    btl $24, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $25, %r8d
; SCALAR-NEXT:    btl $25, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $26, %edx
; SCALAR-NEXT:    btl $26, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $27, %r8d
; SCALAR-NEXT:    btl $27, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $28, %edx
; SCALAR-NEXT:    btl $28, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %esi, %edx
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $29, %esi
; SCALAR-NEXT:    btl $29, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    shll $30, %edi
; SCALAR-NEXT:    btl $30, %ecx
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %esi, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    shrl $15, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzwl %si, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzwl %di, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $15, %eax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    movzwl %si, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzwl %di, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $15, %eax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i16 %a to i32
  %b.ext = zext i16 %b to i32
  %clmul = call i32 @llvm.clmul.i32(i32 %a.ext, i32 %b.ext)
  %res.ext = lshr i32 %clmul, 15
  %res = trunc i32 %res.ext to i16
  ret i16 %res
}

define i32 @clmulr_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmulr_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    leaq (%rax,%rax), %rdi
; SCALAR-NEXT:    xorl %ecx, %ecx
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    andl $1, %r8d
; SCALAR-NEXT:    cmovneq %rax, %r8
; SCALAR-NEXT:    leaq (,%rax,4), %r9
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    leaq (,%rax,8), %rdi
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $4, %r10
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %rdi, %r10
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $5, %r8
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r10, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $6, %rdi
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $7, %r9
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $8, %r10
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $9, %rdi
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r10, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $10, %r8
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $11, %r9
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $12, %r8
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $13, %r9
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $14, %r8
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $15, %rdi
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $16, %r9
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $17, %rdi
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $18, %r9
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $19, %r10
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $20, %rdi
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r10, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $21, %r8
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $22, %r9
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $23, %r8
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $24, %r9
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $25, %r8
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $26, %r9
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $27, %r8
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $28, %rdi
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $29, %r9
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $30, %rdi
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $31, %r9
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $32, %rsi
; SCALAR-NEXT:    btq $32, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r9, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $33, %rdi
; SCALAR-NEXT:    btq $33, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $34, %rsi
; SCALAR-NEXT:    btq $34, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $35, %rdi
; SCALAR-NEXT:    btq $35, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $36, %rsi
; SCALAR-NEXT:    btq $36, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $37, %r8
; SCALAR-NEXT:    btq $37, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $38, %rsi
; SCALAR-NEXT:    btq $38, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $39, %r8
; SCALAR-NEXT:    btq $39, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $40, %rsi
; SCALAR-NEXT:    btq $40, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $41, %r8
; SCALAR-NEXT:    btq $41, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $42, %rsi
; SCALAR-NEXT:    btq $42, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $43, %r8
; SCALAR-NEXT:    btq $43, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $44, %rsi
; SCALAR-NEXT:    btq $44, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $45, %rdi
; SCALAR-NEXT:    btq $45, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $46, %r8
; SCALAR-NEXT:    btq $46, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $47, %rdi
; SCALAR-NEXT:    btq $47, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $48, %r8
; SCALAR-NEXT:    btq $48, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $49, %rdi
; SCALAR-NEXT:    btq $49, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $50, %r8
; SCALAR-NEXT:    btq $50, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $51, %rdi
; SCALAR-NEXT:    btq $51, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $52, %r8
; SCALAR-NEXT:    btq $52, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $53, %r9
; SCALAR-NEXT:    btq $53, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $54, %rdi
; SCALAR-NEXT:    btq $54, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $55, %rsi
; SCALAR-NEXT:    btq $55, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $56, %r8
; SCALAR-NEXT:    btq $56, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $57, %rsi
; SCALAR-NEXT:    btq $57, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $58, %r8
; SCALAR-NEXT:    btq $58, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $59, %rsi
; SCALAR-NEXT:    btq $59, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $60, %r8
; SCALAR-NEXT:    btq $60, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $61, %rsi
; SCALAR-NEXT:    btq $61, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    shlq $62, %rax
; SCALAR-NEXT:    btq $62, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rax
; SCALAR-NEXT:    xorq %rsi, %rax
; SCALAR-NEXT:    xorq %rdi, %rax
; SCALAR-NEXT:    shrq $31, %rax
; SCALAR-NEXT:    # kill: def $eax killed $eax killed $rax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrq $31, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrq $31, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i32 %a to i64
  %b.ext = zext i32 %b to i64
  %clmul = call i64 @llvm.clmul.i64(i64 %a.ext, i64 %b.ext)
  %res.ext = lshr i64 %clmul, 31
  %res = trunc i64 %res.ext to i32
  ret i32 %res
}

define i64 @clmulr_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmulr_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %r14
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    movabsq $1085102592571150095, %rcx # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %rcx, %rax
; SCALAR-NEXT:    andq %rcx, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rax, %rdi
; SCALAR-NEXT:    movabsq $3689348814741910323, %rax # imm = 0x3333333333333333
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    andq %rax, %rdx
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %rax, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rdx,4), %rdi
; SCALAR-NEXT:    movabsq $6148914691236517205, %rdx # imm = 0x5555555555555555
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    andq %rdx, %r8
; SCALAR-NEXT:    shrq %rdi
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    andq %rdx, %r9
; SCALAR-NEXT:    leaq (%r9,%r8,2), %r8
; SCALAR-NEXT:    bswapq %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    shrq $4, %r9
; SCALAR-NEXT:    andq %rcx, %r9
; SCALAR-NEXT:    andq %rcx, %rsi
; SCALAR-NEXT:    shlq $4, %rsi
; SCALAR-NEXT:    orq %r9, %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    andq %rax, %r9
; SCALAR-NEXT:    shrq $2, %rsi
; SCALAR-NEXT:    andq %rax, %rsi
; SCALAR-NEXT:    leaq (%rsi,%r9,4), %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    andq %rdx, %r9
; SCALAR-NEXT:    shrq %rsi
; SCALAR-NEXT:    andq %rdx, %rsi
; SCALAR-NEXT:    leaq (%rsi,%r9,2), %rsi
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $1, %r10d
; SCALAR-NEXT:    cmovneq %r8, %r10
; SCALAR-NEXT:    leaq (%r8,%r8), %r11
; SCALAR-NEXT:    xorl %r9d, %r9d
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    leaq (,%r8,4), %rbx
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    leaq (,%r8,8), %r10
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $4, %r11
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $5, %r10
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $6, %r11
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $7, %rbx
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $8, %r14
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $9, %r11
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $10, %r10
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $11, %rbx
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $12, %r10
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $13, %rbx
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $14, %r10
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $15, %r11
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $16, %rbx
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $17, %r11
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $18, %rbx
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $19, %r14
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $20, %r11
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $21, %r10
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $22, %rbx
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $23, %r10
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $24, %rbx
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $25, %r10
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $26, %rbx
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $27, %r10
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $28, %r11
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $29, %rbx
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $30, %r11
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $31, %rbx
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $32, %r11
; SCALAR-NEXT:    btq $32, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $33, %rbx
; SCALAR-NEXT:    btq $33, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $34, %r14
; SCALAR-NEXT:    btq $34, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $35, %r11
; SCALAR-NEXT:    btq $35, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $36, %r10
; SCALAR-NEXT:    btq $36, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $37, %rbx
; SCALAR-NEXT:    btq $37, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $38, %r10
; SCALAR-NEXT:    btq $38, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $39, %rbx
; SCALAR-NEXT:    btq $39, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $40, %r10
; SCALAR-NEXT:    btq $40, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $41, %rbx
; SCALAR-NEXT:    btq $41, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $42, %r10
; SCALAR-NEXT:    btq $42, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $43, %rbx
; SCALAR-NEXT:    btq $43, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $44, %r10
; SCALAR-NEXT:    btq $44, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $45, %r11
; SCALAR-NEXT:    btq $45, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $46, %rbx
; SCALAR-NEXT:    btq $46, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $47, %r11
; SCALAR-NEXT:    btq $47, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $48, %rbx
; SCALAR-NEXT:    btq $48, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $49, %r11
; SCALAR-NEXT:    btq $49, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $50, %rbx
; SCALAR-NEXT:    btq $50, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $51, %r11
; SCALAR-NEXT:    btq $51, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $52, %rbx
; SCALAR-NEXT:    btq $52, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $53, %r14
; SCALAR-NEXT:    btq $53, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $54, %r11
; SCALAR-NEXT:    btq $54, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $55, %r10
; SCALAR-NEXT:    btq $55, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $56, %rbx
; SCALAR-NEXT:    btq $56, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $57, %r10
; SCALAR-NEXT:    btq $57, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $58, %rbx
; SCALAR-NEXT:    btq $58, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $59, %r10
; SCALAR-NEXT:    btq $59, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $60, %rbx
; SCALAR-NEXT:    btq $60, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $61, %r10
; SCALAR-NEXT:    btq $61, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    shlq $62, %r8
; SCALAR-NEXT:    btq $62, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r8
; SCALAR-NEXT:    xorq %r10, %r8
; SCALAR-NEXT:    shlq $63, %rdi
; SCALAR-NEXT:    btq $63, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    xorq %r11, %rdi
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rsi
; SCALAR-NEXT:    shrq $4, %rsi
; SCALAR-NEXT:    andq %rcx, %rsi
; SCALAR-NEXT:    andq %rcx, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rsi, %rdi
; SCALAR-NEXT:    movq %rdi, %rcx
; SCALAR-NEXT:    andq %rax, %rcx
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %rax, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rcx,4), %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rax
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %r14
; SCALAR-NEXT:    retq
;
; SSE2-PCLMUL-LABEL: clmulr_i64:
; SSE2-PCLMUL:       # %bb.0:
; SSE2-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE2-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE2-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE2-PCLMUL-NEXT:    movq %xmm1, %rcx
; SSE2-PCLMUL-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-PCLMUL-NEXT:    movq %xmm0, %rax
; SSE2-PCLMUL-NEXT:    shldq $1, %rcx, %rax
; SSE2-PCLMUL-NEXT:    retq
;
; SSE42-PCLMUL-LABEL: clmulr_i64:
; SSE42-PCLMUL:       # %bb.0:
; SSE42-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE42-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE42-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE42-PCLMUL-NEXT:    movq %xmm1, %rcx
; SSE42-PCLMUL-NEXT:    pextrq $1, %xmm1, %rax
; SSE42-PCLMUL-NEXT:    shldq $1, %rcx, %rax
; SSE42-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rcx
; AVX-NEXT:    vpextrq $1, %xmm0, %rax
; AVX-NEXT:    shldq $1, %rcx, %rax
; AVX-NEXT:    retq
  %a.ext = zext i64 %a to i128
  %b.ext = zext i64 %b to i128
  %clmul = call i128 @llvm.clmul.i128(i128 %a.ext, i128 %b.ext)
  %res.ext = lshr i128 %clmul, 63
  %res = trunc i128 %res.ext to i64
  ret i64 %res
}

define i8 @clmulh_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmulh_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movzbl %dil, %ecx
; SCALAR-NEXT:    movzbl %sil, %edx
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    cmovnel %ecx, %esi
; SCALAR-NEXT:    leal (%rcx,%rcx), %r8d
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    leal (,%rcx,4), %esi
; SCALAR-NEXT:    btl $2, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    leal (,%rcx,8), %r9d
; SCALAR-NEXT:    btl $3, %edx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $4, %esi
; SCALAR-NEXT:    btl $4, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %ecx, %r10d
; SCALAR-NEXT:    shll $5, %r10d
; SCALAR-NEXT:    btl $5, %edx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %esi, %r10d
; SCALAR-NEXT:    movl %ecx, %r8d
; SCALAR-NEXT:    shll $6, %r8d
; SCALAR-NEXT:    btl $6, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r10d, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $7, %esi
; SCALAR-NEXT:    btl $7, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %ecx, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %edx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    movl %ecx, %r10d
; SCALAR-NEXT:    shll $9, %r10d
; SCALAR-NEXT:    btl $9, %edx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %ecx, %esi
; SCALAR-NEXT:    shll $10, %esi
; SCALAR-NEXT:    btl $10, %edx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r10d, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %ecx, %r8d
; SCALAR-NEXT:    shll $11, %r8d
; SCALAR-NEXT:    btl $11, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    movl %ecx, %r9d
; SCALAR-NEXT:    shll $12, %r9d
; SCALAR-NEXT:    btl $12, %edx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    shll $13, %ecx
; SCALAR-NEXT:    btl $13, %edx
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r9d, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $14, %r8d
; SCALAR-NEXT:    btl $14, %edx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    shll $15, %edi
; SCALAR-NEXT:    btl $15, %edx
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    xorl %esi, %eax
; SCALAR-NEXT:    shrl $8, %eax
; SCALAR-NEXT:    # kill: def $al killed $al killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $8, %eax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $8, %eax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i8 %a to i16
  %b.ext = zext i8 %b to i16
  %clmul = call i16 @llvm.clmul.i16(i16 %a.ext, i16 %b.ext)
  %res.ext = lshr i16 %clmul, 8
  %res = trunc i16 %res.ext to i8
  ret i8 %res
}

define i16 @clmulh_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmulh_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movzwl %di, %edx
; SCALAR-NEXT:    movzwl %si, %ecx
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    cmovnel %edx, %esi
; SCALAR-NEXT:    leal (%rdx,%rdx), %r8d
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    btl $1, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    leal (,%rdx,4), %esi
; SCALAR-NEXT:    btl $2, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    leal (,%rdx,8), %r9d
; SCALAR-NEXT:    btl $3, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $4, %esi
; SCALAR-NEXT:    btl $4, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edx, %r10d
; SCALAR-NEXT:    shll $5, %r10d
; SCALAR-NEXT:    btl $5, %ecx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %esi, %r10d
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $6, %r8d
; SCALAR-NEXT:    btl $6, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r10d, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $7, %esi
; SCALAR-NEXT:    btl $7, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $8, %r9d
; SCALAR-NEXT:    btl $8, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    movl %edx, %r10d
; SCALAR-NEXT:    shll $9, %r10d
; SCALAR-NEXT:    btl $9, %ecx
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %edx, %esi
; SCALAR-NEXT:    shll $10, %esi
; SCALAR-NEXT:    btl $10, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r10d, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $11, %r8d
; SCALAR-NEXT:    btl $11, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $12, %r9d
; SCALAR-NEXT:    btl $12, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    shll $13, %r8d
; SCALAR-NEXT:    btl $13, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %edx, %r9d
; SCALAR-NEXT:    shll $14, %r9d
; SCALAR-NEXT:    btl $14, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    shll $15, %edx
; SCALAR-NEXT:    btl $15, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r9d, %edx
; SCALAR-NEXT:    xorl %esi, %edx
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $16, %esi
; SCALAR-NEXT:    btl $16, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $17, %r8d
; SCALAR-NEXT:    btl $17, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $18, %esi
; SCALAR-NEXT:    btl $18, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $19, %r8d
; SCALAR-NEXT:    btl $19, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shll $20, %r9d
; SCALAR-NEXT:    btl $20, %ecx
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $21, %esi
; SCALAR-NEXT:    btl $21, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    xorl %edx, %esi
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $22, %edx
; SCALAR-NEXT:    btl $22, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $23, %r8d
; SCALAR-NEXT:    btl $23, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $24, %edx
; SCALAR-NEXT:    btl $24, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $25, %r8d
; SCALAR-NEXT:    btl $25, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $26, %edx
; SCALAR-NEXT:    btl $26, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $27, %r8d
; SCALAR-NEXT:    btl $27, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %edi, %edx
; SCALAR-NEXT:    shll $28, %edx
; SCALAR-NEXT:    btl $28, %ecx
; SCALAR-NEXT:    cmovael %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %esi, %edx
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    shll $29, %esi
; SCALAR-NEXT:    btl $29, %ecx
; SCALAR-NEXT:    cmovael %eax, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shll $30, %r8d
; SCALAR-NEXT:    btl $30, %ecx
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    shll $31, %edi
; SCALAR-NEXT:    btl $31, %ecx
; SCALAR-NEXT:    cmovbl %edi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    shrl $16, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzwl %si, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzwl %di, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $16, %eax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    movzwl %si, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzwl %di, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $16, %eax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i16 %a to i32
  %b.ext = zext i16 %b to i32
  %clmul = call i32 @llvm.clmul.i32(i32 %a.ext, i32 %b.ext)
  %res.ext = lshr i32 %clmul, 16
  %res = trunc i32 %res.ext to i16
  ret i16 %res
}

define i32 @clmulh_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmulh_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    leaq (%rax,%rax), %rdi
; SCALAR-NEXT:    xorl %ecx, %ecx
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    movl %edx, %r8d
; SCALAR-NEXT:    andl $1, %r8d
; SCALAR-NEXT:    cmovneq %rax, %r8
; SCALAR-NEXT:    leaq (,%rax,4), %r9
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    leaq (,%rax,8), %rdi
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $4, %r10
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %rdi, %r10
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $5, %r8
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r10, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $6, %rdi
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $7, %r9
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $8, %r10
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $9, %rdi
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r10, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $10, %r8
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $11, %r9
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $12, %r8
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $13, %r9
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $14, %r8
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $15, %rdi
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $16, %r9
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $17, %rdi
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $18, %r9
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %r10
; SCALAR-NEXT:    shlq $19, %r10
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $20, %rdi
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r10, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $21, %r8
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $22, %r9
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $23, %r8
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $24, %r9
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $25, %r8
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $26, %r9
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $27, %r8
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $28, %rdi
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $29, %r9
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $30, %rdi
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $31, %r9
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %rdi, %r9
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $32, %rsi
; SCALAR-NEXT:    btq $32, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r9, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $33, %rdi
; SCALAR-NEXT:    btq $33, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $34, %rsi
; SCALAR-NEXT:    btq $34, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $35, %rdi
; SCALAR-NEXT:    btq $35, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $36, %rsi
; SCALAR-NEXT:    btq $36, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $37, %r8
; SCALAR-NEXT:    btq $37, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $38, %rsi
; SCALAR-NEXT:    btq $38, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $39, %r8
; SCALAR-NEXT:    btq $39, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $40, %rsi
; SCALAR-NEXT:    btq $40, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $41, %r8
; SCALAR-NEXT:    btq $41, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $42, %rsi
; SCALAR-NEXT:    btq $42, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $43, %r8
; SCALAR-NEXT:    btq $43, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $44, %rsi
; SCALAR-NEXT:    btq $44, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $45, %rdi
; SCALAR-NEXT:    btq $45, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $46, %r8
; SCALAR-NEXT:    btq $46, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $47, %rdi
; SCALAR-NEXT:    btq $47, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $48, %r8
; SCALAR-NEXT:    btq $48, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $49, %rdi
; SCALAR-NEXT:    btq $49, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $50, %r8
; SCALAR-NEXT:    btq $50, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $51, %rdi
; SCALAR-NEXT:    btq $51, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $52, %r8
; SCALAR-NEXT:    btq $52, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movq %rax, %r9
; SCALAR-NEXT:    shlq $53, %r9
; SCALAR-NEXT:    btq $53, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r9
; SCALAR-NEXT:    xorq %r8, %r9
; SCALAR-NEXT:    movq %rax, %rdi
; SCALAR-NEXT:    shlq $54, %rdi
; SCALAR-NEXT:    btq $54, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rdi
; SCALAR-NEXT:    xorq %r9, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $55, %rsi
; SCALAR-NEXT:    btq $55, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $56, %r8
; SCALAR-NEXT:    btq $56, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $57, %rsi
; SCALAR-NEXT:    btq $57, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $58, %r8
; SCALAR-NEXT:    btq $58, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $59, %rsi
; SCALAR-NEXT:    btq $59, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $60, %r8
; SCALAR-NEXT:    btq $60, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    movq %rax, %rsi
; SCALAR-NEXT:    shlq $61, %rsi
; SCALAR-NEXT:    btq $61, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    movq %rax, %r8
; SCALAR-NEXT:    shlq $62, %r8
; SCALAR-NEXT:    btq $62, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %r8
; SCALAR-NEXT:    xorq %rsi, %r8
; SCALAR-NEXT:    shlq $63, %rax
; SCALAR-NEXT:    btq $63, %rdx
; SCALAR-NEXT:    cmovaeq %rcx, %rax
; SCALAR-NEXT:    xorq %r8, %rax
; SCALAR-NEXT:    xorq %rdi, %rax
; SCALAR-NEXT:    shrq $32, %rax
; SCALAR-NEXT:    # kill: def $eax killed $eax killed $rax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrq $32, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrq $32, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i32 %a to i64
  %b.ext = zext i32 %b to i64
  %clmul = call i64 @llvm.clmul.i64(i64 %a.ext, i64 %b.ext)
  %res.ext = lshr i64 %clmul, 32
  %res = trunc i64 %res.ext to i32
  ret i32 %res
}

define i64 @clmulh_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmulh_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %r14
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    movabsq $1085102592571150095, %rdx # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    andq %rdx, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rax, %rdi
; SCALAR-NEXT:    movabsq $3689348814741910323, %rcx # imm = 0x3333333333333333
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    andq %rcx, %rax
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %rcx, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rax,4), %rdi
; SCALAR-NEXT:    movabsq $6148914691236517205, %rax # imm = 0x5555555555555555
; SCALAR-NEXT:    movq %rdi, %r8
; SCALAR-NEXT:    andq %rax, %r8
; SCALAR-NEXT:    shrq %rdi
; SCALAR-NEXT:    movq %rdi, %r9
; SCALAR-NEXT:    andq %rax, %r9
; SCALAR-NEXT:    leaq (%r9,%r8,2), %r8
; SCALAR-NEXT:    bswapq %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    shrq $4, %r9
; SCALAR-NEXT:    andq %rdx, %r9
; SCALAR-NEXT:    andq %rdx, %rsi
; SCALAR-NEXT:    shlq $4, %rsi
; SCALAR-NEXT:    orq %r9, %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    andq %rcx, %r9
; SCALAR-NEXT:    shrq $2, %rsi
; SCALAR-NEXT:    andq %rcx, %rsi
; SCALAR-NEXT:    leaq (%rsi,%r9,4), %rsi
; SCALAR-NEXT:    movq %rsi, %r9
; SCALAR-NEXT:    andq %rax, %r9
; SCALAR-NEXT:    shrq %rsi
; SCALAR-NEXT:    andq %rax, %rsi
; SCALAR-NEXT:    leaq (%rsi,%r9,2), %rsi
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $1, %r10d
; SCALAR-NEXT:    cmovneq %r8, %r10
; SCALAR-NEXT:    leaq (%r8,%r8), %r11
; SCALAR-NEXT:    xorl %r9d, %r9d
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    leaq (,%r8,4), %rbx
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    leaq (,%r8,8), %r10
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $4, %r11
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $5, %r10
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $6, %r11
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $7, %rbx
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $8, %r14
; SCALAR-NEXT:    btl $8, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $9, %r11
; SCALAR-NEXT:    btl $9, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $10, %r10
; SCALAR-NEXT:    btl $10, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $11, %rbx
; SCALAR-NEXT:    btl $11, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $12, %r10
; SCALAR-NEXT:    btl $12, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $13, %rbx
; SCALAR-NEXT:    btl $13, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $14, %r10
; SCALAR-NEXT:    btl $14, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $15, %r11
; SCALAR-NEXT:    btl $15, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $16, %rbx
; SCALAR-NEXT:    btl $16, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $17, %r11
; SCALAR-NEXT:    btl $17, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $18, %rbx
; SCALAR-NEXT:    btl $18, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $19, %r14
; SCALAR-NEXT:    btl $19, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $20, %r11
; SCALAR-NEXT:    btl $20, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $21, %r10
; SCALAR-NEXT:    btl $21, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $22, %rbx
; SCALAR-NEXT:    btl $22, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $23, %r10
; SCALAR-NEXT:    btl $23, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $24, %rbx
; SCALAR-NEXT:    btl $24, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $25, %r10
; SCALAR-NEXT:    btl $25, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $26, %rbx
; SCALAR-NEXT:    btl $26, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $27, %r10
; SCALAR-NEXT:    btl $27, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $28, %r11
; SCALAR-NEXT:    btl $28, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $29, %rbx
; SCALAR-NEXT:    btl $29, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $30, %r11
; SCALAR-NEXT:    btl $30, %esi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $31, %rbx
; SCALAR-NEXT:    btl $31, %esi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $32, %r11
; SCALAR-NEXT:    btq $32, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $33, %rbx
; SCALAR-NEXT:    btq $33, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $34, %r14
; SCALAR-NEXT:    btq $34, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $35, %r11
; SCALAR-NEXT:    btq $35, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $36, %r10
; SCALAR-NEXT:    btq $36, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $37, %rbx
; SCALAR-NEXT:    btq $37, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $38, %r10
; SCALAR-NEXT:    btq $38, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $39, %rbx
; SCALAR-NEXT:    btq $39, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $40, %r10
; SCALAR-NEXT:    btq $40, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $41, %rbx
; SCALAR-NEXT:    btq $41, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $42, %r10
; SCALAR-NEXT:    btq $42, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $43, %rbx
; SCALAR-NEXT:    btq $43, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $44, %r10
; SCALAR-NEXT:    btq $44, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $45, %r11
; SCALAR-NEXT:    btq $45, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $46, %rbx
; SCALAR-NEXT:    btq $46, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $47, %r11
; SCALAR-NEXT:    btq $47, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $48, %rbx
; SCALAR-NEXT:    btq $48, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $49, %r11
; SCALAR-NEXT:    btq $49, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $50, %rbx
; SCALAR-NEXT:    btq $50, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $51, %r11
; SCALAR-NEXT:    btq $51, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $52, %rbx
; SCALAR-NEXT:    btq $52, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movq %r8, %r14
; SCALAR-NEXT:    shlq $53, %r14
; SCALAR-NEXT:    btq $53, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movq %r8, %r11
; SCALAR-NEXT:    shlq $54, %r11
; SCALAR-NEXT:    btq $54, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $55, %r10
; SCALAR-NEXT:    btq $55, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $56, %rbx
; SCALAR-NEXT:    btq $56, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $57, %r10
; SCALAR-NEXT:    btq $57, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $58, %rbx
; SCALAR-NEXT:    btq $58, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $59, %r10
; SCALAR-NEXT:    btq $59, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    movq %r8, %rbx
; SCALAR-NEXT:    shlq $60, %rbx
; SCALAR-NEXT:    btq $60, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rbx
; SCALAR-NEXT:    xorq %r10, %rbx
; SCALAR-NEXT:    movq %r8, %r10
; SCALAR-NEXT:    shlq $61, %r10
; SCALAR-NEXT:    btq $61, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    shlq $62, %r8
; SCALAR-NEXT:    btq $62, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %r8
; SCALAR-NEXT:    xorq %r10, %r8
; SCALAR-NEXT:    shlq $63, %rdi
; SCALAR-NEXT:    btq $63, %rsi
; SCALAR-NEXT:    cmovaeq %r9, %rdi
; SCALAR-NEXT:    xorq %r8, %rdi
; SCALAR-NEXT:    xorq %r11, %rdi
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rsi
; SCALAR-NEXT:    shrq $4, %rsi
; SCALAR-NEXT:    andq %rdx, %rsi
; SCALAR-NEXT:    andq %rdx, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rsi, %rdi
; SCALAR-NEXT:    movq %rdi, %rdx
; SCALAR-NEXT:    andq %rcx, %rdx
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %rcx, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rdx,4), %rcx
; SCALAR-NEXT:    andq %rcx, %rax
; SCALAR-NEXT:    shrq %rcx
; SCALAR-NEXT:    movabsq $6148914691236517204, %rdx # imm = 0x5555555555555554
; SCALAR-NEXT:    andq %rcx, %rdx
; SCALAR-NEXT:    leaq (%rdx,%rax,2), %rax
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %r14
; SCALAR-NEXT:    retq
;
; SSE2-PCLMUL-LABEL: clmulh_i64:
; SSE2-PCLMUL:       # %bb.0:
; SSE2-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE2-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE2-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE2-PCLMUL-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-PCLMUL-NEXT:    movq %xmm0, %rax
; SSE2-PCLMUL-NEXT:    retq
;
; SSE42-PCLMUL-LABEL: clmulh_i64:
; SSE42-PCLMUL:       # %bb.0:
; SSE42-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE42-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE42-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE42-PCLMUL-NEXT:    pextrq $1, %xmm1, %rax
; SSE42-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vpextrq $1, %xmm0, %rax
; AVX-NEXT:    retq
  %a.ext = zext i64 %a to i128
  %b.ext = zext i64 %b to i128
  %clmul = call i128 @llvm.clmul.i128(i128 %a.ext, i128 %b.ext)
  %res.ext = lshr i128 %clmul, 64
  %res = trunc i128 %res.ext to i64
  ret i64 %res
}

define i8 @clmul_i8_noimplicitfloat(i8 %a, i8 %b) nounwind noimplicitfloat {
; CHECK-LABEL: clmul_i8_noimplicitfloat:
; CHECK:       # %bb.0:
; CHECK-NEXT:    # kill: def $edi killed $edi def $rdi
; CHECK-NEXT:    xorl %ecx, %ecx
; CHECK-NEXT:    testb $1, %sil
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    cmovel %ecx, %eax
; CHECK-NEXT:    leal (%rdi,%rdi), %edx
; CHECK-NEXT:    movzbl %dl, %edx
; CHECK-NEXT:    btl $1, %esi
; CHECK-NEXT:    cmovael %ecx, %edx
; CHECK-NEXT:    xorl %eax, %edx
; CHECK-NEXT:    leal (,%rdi,4), %eax
; CHECK-NEXT:    movzbl %al, %r8d
; CHECK-NEXT:    btl $2, %esi
; CHECK-NEXT:    cmovael %ecx, %r8d
; CHECK-NEXT:    leal (,%rdi,8), %eax
; CHECK-NEXT:    movzbl %al, %eax
; CHECK-NEXT:    btl $3, %esi
; CHECK-NEXT:    cmovael %ecx, %eax
; CHECK-NEXT:    xorl %r8d, %eax
; CHECK-NEXT:    xorl %edx, %eax
; CHECK-NEXT:    movl %edi, %edx
; CHECK-NEXT:    shlb $4, %dl
; CHECK-NEXT:    movzbl %dl, %edx
; CHECK-NEXT:    btl $4, %esi
; CHECK-NEXT:    cmovael %ecx, %edx
; CHECK-NEXT:    movl %edi, %r8d
; CHECK-NEXT:    shlb $5, %r8b
; CHECK-NEXT:    movzbl %r8b, %r8d
; CHECK-NEXT:    btl $5, %esi
; CHECK-NEXT:    cmovael %ecx, %r8d
; CHECK-NEXT:    xorl %edx, %r8d
; CHECK-NEXT:    movl %edi, %edx
; CHECK-NEXT:    shlb $6, %dl
; CHECK-NEXT:    movzbl %dl, %edx
; CHECK-NEXT:    btl $6, %esi
; CHECK-NEXT:    cmovael %ecx, %edx
; CHECK-NEXT:    xorl %r8d, %edx
; CHECK-NEXT:    xorl %eax, %edx
; CHECK-NEXT:    shlb $7, %dil
; CHECK-NEXT:    movzbl %dil, %eax
; CHECK-NEXT:    btl $7, %esi
; CHECK-NEXT:    cmovael %ecx, %eax
; CHECK-NEXT:    xorl %edx, %eax
; CHECK-NEXT:    # kill: def $al killed $al killed $eax
; CHECK-NEXT:    retq
  %res = call i8 @llvm.clmul.i8(i8 %a, i8 %b)
  ret i8 %res
}

declare void @use(i8)

define void @commutative_clmul_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    # kill: def $edi killed $edi def $rdi
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    testb $1, %sil
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    cmovel %eax, %r8d
; SCALAR-NEXT:    leal (%rdi,%rdi), %r9d
; SCALAR-NEXT:    movzbl %r9b, %r9d
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    leal (,%rdi,4), %r8d
; SCALAR-NEXT:    movzbl %r8b, %r10d
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    leal (,%rdi,8), %r8d
; SCALAR-NEXT:    movzbl %r8b, %r8d
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %r10d, %r8d
; SCALAR-NEXT:    xorl %r9d, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shlb $4, %r9b
; SCALAR-NEXT:    movzbl %r9b, %r9d
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    movl %edi, %r10d
; SCALAR-NEXT:    shlb $5, %r10b
; SCALAR-NEXT:    movzbl %r10b, %r10d
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovael %eax, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    shlb $6, %r9b
; SCALAR-NEXT:    movzbl %r9b, %r9d
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovael %eax, %r9d
; SCALAR-NEXT:    xorl %r10d, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    shlb $7, %dil
; SCALAR-NEXT:    movzbl %dil, %edi
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovael %eax, %edi
; SCALAR-NEXT:    xorl %r9d, %edi
; SCALAR-NEXT:    movb %dil, (%rdx)
; SCALAR-NEXT:    movb %dil, (%rcx)
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %xy = call i8 @llvm.clmul.i8(i8 %x, i8 %y)
  %yx = call i8 @llvm.clmul.i8(i8 %y, i8 %x)
  store i8 %xy, ptr %p0
  store i8 %yx, ptr %p1
  ret void
}

define void @commutative_clmulh_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmulh_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    movzbl %sil, %eax
; SCALAR-NEXT:    movzbl %dil, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    andl $1, %r9d
; SCALAR-NEXT:    cmovnel %eax, %r9d
; SCALAR-NEXT:    leal (%rax,%rax), %r10d
; SCALAR-NEXT:    xorl %edi, %edi
; SCALAR-NEXT:    btl $1, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    leal (,%rax,4), %r9d
; SCALAR-NEXT:    btl $2, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    leal (,%rax,8), %r11d
; SCALAR-NEXT:    btl $3, %r8d
; SCALAR-NEXT:    cmovael %edi, %r11d
; SCALAR-NEXT:    xorl %r9d, %r11d
; SCALAR-NEXT:    xorl %r10d, %r11d
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $4, %r9d
; SCALAR-NEXT:    btl $4, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    movl %eax, %ebx
; SCALAR-NEXT:    shll $5, %ebx
; SCALAR-NEXT:    btl $5, %r8d
; SCALAR-NEXT:    cmovael %edi, %ebx
; SCALAR-NEXT:    xorl %r9d, %ebx
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    shll $6, %r10d
; SCALAR-NEXT:    btl $6, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %ebx, %r10d
; SCALAR-NEXT:    xorl %r11d, %r10d
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $7, %r9d
; SCALAR-NEXT:    btl $7, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    movl %eax, %r11d
; SCALAR-NEXT:    shll $8, %r11d
; SCALAR-NEXT:    btl $8, %r8d
; SCALAR-NEXT:    cmovael %edi, %r11d
; SCALAR-NEXT:    xorl %r9d, %r11d
; SCALAR-NEXT:    movl %eax, %ebx
; SCALAR-NEXT:    shll $9, %ebx
; SCALAR-NEXT:    btl $9, %r8d
; SCALAR-NEXT:    cmovael %edi, %ebx
; SCALAR-NEXT:    xorl %r11d, %ebx
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $10, %r9d
; SCALAR-NEXT:    btl $10, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    xorl %ebx, %r9d
; SCALAR-NEXT:    xorl %r10d, %r9d
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    shll $11, %r10d
; SCALAR-NEXT:    btl $11, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    movl %eax, %r11d
; SCALAR-NEXT:    shll $12, %r11d
; SCALAR-NEXT:    btl $12, %r8d
; SCALAR-NEXT:    cmovael %edi, %r11d
; SCALAR-NEXT:    xorl %r10d, %r11d
; SCALAR-NEXT:    shll $13, %eax
; SCALAR-NEXT:    btl $13, %r8d
; SCALAR-NEXT:    cmovael %edi, %eax
; SCALAR-NEXT:    xorl %r11d, %eax
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    shll $14, %r10d
; SCALAR-NEXT:    btl $14, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %eax, %r10d
; SCALAR-NEXT:    shll $15, %esi
; SCALAR-NEXT:    btl $15, %r8d
; SCALAR-NEXT:    cmovael %edi, %esi
; SCALAR-NEXT:    xorl %r10d, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    shrl $8, %esi
; SCALAR-NEXT:    movb %sil, (%rdx)
; SCALAR-NEXT:    movb %sil, (%rcx)
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmulh_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $8, %eax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmulh_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $8, %eax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %x.ext = zext i8 %x to i16
  %y.ext = zext i8 %y to i16
  %clmul_xy = call i16 @llvm.clmul.i16(i16 %x.ext, i16 %y.ext)
  %clmul_yx = call i16 @llvm.clmul.i16(i16 %y.ext, i16 %x.ext)
  %clmul_xy_lshr = lshr i16 %clmul_xy, 8
  %clmul_yx_lshr = lshr i16 %clmul_yx, 8
  %clmulh_xy = trunc i16 %clmul_xy_lshr to i8
  %clmulh_yx = trunc i16 %clmul_yx_lshr to i8
  store i8 %clmulh_xy, ptr %p0
  store i8 %clmulh_yx, ptr %p1
  ret void
}

define void @commutative_clmulr_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmulr_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    movzbl %sil, %eax
; SCALAR-NEXT:    movzbl %dil, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    andl $1, %r9d
; SCALAR-NEXT:    cmovnel %eax, %r9d
; SCALAR-NEXT:    leal (%rax,%rax), %r10d
; SCALAR-NEXT:    xorl %edi, %edi
; SCALAR-NEXT:    btl $1, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    leal (,%rax,4), %r9d
; SCALAR-NEXT:    btl $2, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    leal (,%rax,8), %r11d
; SCALAR-NEXT:    btl $3, %r8d
; SCALAR-NEXT:    cmovael %edi, %r11d
; SCALAR-NEXT:    xorl %r9d, %r11d
; SCALAR-NEXT:    xorl %r10d, %r11d
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $4, %r9d
; SCALAR-NEXT:    btl $4, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    shll $5, %r10d
; SCALAR-NEXT:    btl $5, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $6, %r9d
; SCALAR-NEXT:    btl $6, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    xorl %r10d, %r9d
; SCALAR-NEXT:    xorl %r11d, %r9d
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    shll $7, %r10d
; SCALAR-NEXT:    btl $7, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    movl %eax, %r11d
; SCALAR-NEXT:    shll $8, %r11d
; SCALAR-NEXT:    btl $8, %r8d
; SCALAR-NEXT:    cmovael %edi, %r11d
; SCALAR-NEXT:    xorl %r10d, %r11d
; SCALAR-NEXT:    movl %eax, %ebx
; SCALAR-NEXT:    shll $9, %ebx
; SCALAR-NEXT:    btl $9, %r8d
; SCALAR-NEXT:    cmovael %edi, %ebx
; SCALAR-NEXT:    xorl %r11d, %ebx
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    shll $10, %r10d
; SCALAR-NEXT:    btl $10, %r8d
; SCALAR-NEXT:    cmovael %edi, %r10d
; SCALAR-NEXT:    xorl %ebx, %r10d
; SCALAR-NEXT:    xorl %r9d, %r10d
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    shll $11, %r9d
; SCALAR-NEXT:    btl $11, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    shll $12, %eax
; SCALAR-NEXT:    btl $12, %r8d
; SCALAR-NEXT:    cmovael %edi, %eax
; SCALAR-NEXT:    xorl %r9d, %eax
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    shll $13, %r9d
; SCALAR-NEXT:    btl $13, %r8d
; SCALAR-NEXT:    cmovael %edi, %r9d
; SCALAR-NEXT:    xorl %eax, %r9d
; SCALAR-NEXT:    shll $14, %esi
; SCALAR-NEXT:    btl $14, %r8d
; SCALAR-NEXT:    cmovael %edi, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    xorl %r10d, %esi
; SCALAR-NEXT:    shrl $7, %esi
; SCALAR-NEXT:    movb %sil, (%rdx)
; SCALAR-NEXT:    movb %sil, (%rcx)
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmulr_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $7, %eax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmulr_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $7, %eax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %x.ext = zext i8 %x to i16
  %y.ext = zext i8 %y to i16
  %clmul_xy = call i16 @llvm.clmul.i16(i16 %x.ext, i16 %y.ext)
  %clmul_yx = call i16 @llvm.clmul.i16(i16 %y.ext, i16 %x.ext)
  %clmul_xy_lshr = lshr i16 %clmul_xy, 7
  %clmul_yx_lshr = lshr i16 %clmul_yx, 7
  %clmulh_xy = trunc i16 %clmul_xy_lshr to i8
  %clmulh_yx = trunc i16 %clmul_yx_lshr to i8
  store i8 %clmulh_xy, ptr %p0
  store i8 %clmulh_yx, ptr %p1
  ret void
}

define void @mul_use_commutative_clmul_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: mul_use_commutative_clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    pushq %rax
; SCALAR-NEXT:    movq %rcx, %rbx
; SCALAR-NEXT:    # kill: def $edi killed $edi def $rdi
; SCALAR-NEXT:    xorl %eax, %eax
; SCALAR-NEXT:    testb $1, %sil
; SCALAR-NEXT:    movl %edi, %ebp
; SCALAR-NEXT:    cmovel %eax, %ebp
; SCALAR-NEXT:    leal (%rdi,%rdi), %ecx
; SCALAR-NEXT:    movzbl %cl, %ecx
; SCALAR-NEXT:    btl $1, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %ecx, %ebp
; SCALAR-NEXT:    leal (,%rdi,4), %ecx
; SCALAR-NEXT:    movzbl %cl, %ecx
; SCALAR-NEXT:    btl $2, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    leal (,%rdi,8), %r8d
; SCALAR-NEXT:    movzbl %r8b, %r8d
; SCALAR-NEXT:    btl $3, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    xorl %r8d, %ebp
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shlb $4, %cl
; SCALAR-NEXT:    movzbl %cl, %ecx
; SCALAR-NEXT:    btl $4, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    shlb $5, %r8b
; SCALAR-NEXT:    movzbl %r8b, %r8d
; SCALAR-NEXT:    btl $5, %esi
; SCALAR-NEXT:    cmovael %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    shlb $6, %cl
; SCALAR-NEXT:    movzbl %cl, %ecx
; SCALAR-NEXT:    btl $6, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    xorl %ecx, %ebp
; SCALAR-NEXT:    shlb $7, %dil
; SCALAR-NEXT:    movzbl %dil, %ecx
; SCALAR-NEXT:    btl $7, %esi
; SCALAR-NEXT:    cmovael %eax, %ecx
; SCALAR-NEXT:    xorl %ecx, %ebp
; SCALAR-NEXT:    movb %bpl, (%rdx)
; SCALAR-NEXT:    movl %ebp, %edi
; SCALAR-NEXT:    callq use@PLT
; SCALAR-NEXT:    movb %bpl, (%rbx)
; SCALAR-NEXT:    addq $8, %rsp
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: mul_use_commutative_clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    pushq %r14
; SSE-PCLMUL-NEXT:    pushq %rbx
; SSE-PCLMUL-NEXT:    pushq %rax
; SSE-PCLMUL-NEXT:    movq %rcx, %rbx
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %r14
; SSE-PCLMUL-NEXT:    movb %r14b, (%rdx)
; SSE-PCLMUL-NEXT:    movl %r14d, %edi
; SSE-PCLMUL-NEXT:    callq use@PLT
; SSE-PCLMUL-NEXT:    movb %r14b, (%rbx)
; SSE-PCLMUL-NEXT:    addq $8, %rsp
; SSE-PCLMUL-NEXT:    popq %rbx
; SSE-PCLMUL-NEXT:    popq %r14
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: mul_use_commutative_clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    pushq %r14
; AVX-NEXT:    pushq %rbx
; AVX-NEXT:    pushq %rax
; AVX-NEXT:    movq %rcx, %rbx
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %r14
; AVX-NEXT:    movb %r14b, (%rdx)
; AVX-NEXT:    movl %r14d, %edi
; AVX-NEXT:    callq use@PLT
; AVX-NEXT:    movb %r14b, (%rbx)
; AVX-NEXT:    addq $8, %rsp
; AVX-NEXT:    popq %rbx
; AVX-NEXT:    popq %r14
; AVX-NEXT:    retq
  %xy = call i8 @llvm.clmul.i8(i8 %x, i8 %y)
  %yx = call i8 @llvm.clmul.i8(i8 %y, i8 %x)
  store i8 %xy, ptr %p0
  call void @use(i8 %xy)
  store i8 %yx, ptr %p1
  ret void
}

; Test with constant 0 - should optimize to just returning 0
define i32 @clmul_i32_zero(i32 %a) {
; CHECK-LABEL: clmul_i32_zero:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:    retq
  %res = call i32 @llvm.clmul.i32(i32 %a, i32 0)
  ret i32 %res
}
; Expected SCALAR output: xorl %eax, %eax / retq

; Test with constant 1 - should optimize to just returning %a
define i32 @clmul_i32_one(i32 %a) {
; SCALAR-LABEL: clmul_i32_one:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i32_one:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movl $1, %eax
; SSE-PCLMUL-NEXT:    movq %rax, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i32_one:
; AVX:       # %bb.0:
; AVX-NEXT:    movl $1, %eax
; AVX-NEXT:    vmovq %rax, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %res = call i32 @llvm.clmul.i32(i32 %a, i32 1)
  ret i32 %res
}
; Expected SCALAR output: movl %edi, %eax / retq

; Test with power of 2 - should become a shift
define i32 @clmul_i32_pow2(i32 %a) {
; SCALAR-LABEL: clmul_i32_pow2:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    shll $4, %eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i32_pow2:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movl $16, %eax
; SSE-PCLMUL-NEXT:    movq %rax, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i32_pow2:
; AVX:       # %bb.0:
; AVX-NEXT:    movl $16, %eax
; AVX-NEXT:    vmovq %rax, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %res = call i32 @llvm.clmul.i32(i32 %a, i32 16)  ; 0x10 = 1 << 4
  ret i32 %res
}
; Expected SCALAR output: shll $4, %edi / movl %edi, %eax / retq
