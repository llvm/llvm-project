; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse2    | FileCheck %s --check-prefixes=CHECK,SCALAR
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse2,+pclmul | FileCheck %s --check-prefixes=CHECK,SSE-PCLMUL,SSE2-PCLMUL
; RUN: llc < %s -mtriple=x86_64-- -mattr=+sse4.2,+pclmul | FileCheck %s --check-prefixes=CHECK,SSE-PCLMUL,SSE42-PCLMUL
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx2,+pclmul | FileCheck %s --check-prefixes=CHECK,AVX
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx2,+vpclmulqdq  | FileCheck %s --check-prefixes=CHECK,AVX
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512vl,+vpclmulqdq | FileCheck %s --check-prefixes=CHECK,AVX

define i8 @clmul_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andb $1, %dl
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andb $2, %cl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %cl
; SCALAR-NEXT:    movl %eax, %ecx
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %dl
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    xorb %cl, %dl
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andb $4, %cl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %cl
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andb $8, %cl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %cl
; SCALAR-NEXT:    movl %eax, %ecx
; SCALAR-NEXT:    xorb %r8b, %cl
; SCALAR-NEXT:    xorb %dl, %cl
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andb $16, %dl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %dl
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andb $32, %r8b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r8b
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andb $64, %r9b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r9b
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    xorb %dl, %r8b
; SCALAR-NEXT:    xorb %r8b, %r9b
; SCALAR-NEXT:    xorb %cl, %r9b
; SCALAR-NEXT:    andb $-128, %sil
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %sil
; SCALAR-NEXT:    xorb %r9b, %al
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %res = call i8 @llvm.clmul.i8(i8 %a, i8 %b)
  ret i8 %res
}

define i16 @clmul_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmul_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $2, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $1, %ecx
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $4, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $8, %edx
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $16, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $32, %ecx
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $64, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $128, %ecx
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $256, %edx # imm = 0x100
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imull %edi, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $1024, %ecx # imm = 0x400
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $2048, %eax # imm = 0x800
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $4096, %edx # imm = 0x1000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $8192, %r8d # imm = 0x2000
; SCALAR-NEXT:    imull %edi, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $16384, %eax # imm = 0x4000
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    andl $-32768, %esi # imm = 0x8000
; SCALAR-NEXT:    imull %edi, %esi
; SCALAR-NEXT:    xorl %esi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %res = call i16 @llvm.clmul.i16(i16 %a, i16 %b)
  ret i16 %res
}

define i32 @clmul_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmul_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $2, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $1, %ecx
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $4, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $8, %edx
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $16, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imull %edi, %r8d
; SCALAR-NEXT:    xorl %eax, %r8d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $64, %ecx
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $128, %eax
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $256, %edx # imm = 0x100
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imull %edi, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $1024, %eax # imm = 0x400
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $2048, %ecx # imm = 0x800
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $4096, %edx # imm = 0x1000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $8192, %ecx # imm = 0x2000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $16384, %edx # imm = 0x4000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $32768, %ecx # imm = 0x8000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $65536, %eax # imm = 0x10000
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $131072, %edx # imm = 0x20000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $262144, %eax # imm = 0x40000
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $524288, %edx # imm = 0x80000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %eax, %edx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $1048576, %r8d # imm = 0x100000
; SCALAR-NEXT:    imull %edi, %r8d
; SCALAR-NEXT:    xorl %edx, %r8d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $2097152, %eax # imm = 0x200000
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %r8d, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4194304, %ecx # imm = 0x400000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $8388608, %edx # imm = 0x800000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $16777216, %ecx # imm = 0x1000000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $33554432, %edx # imm = 0x2000000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $67108864, %ecx # imm = 0x4000000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $134217728, %edx # imm = 0x8000000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $268435456, %ecx # imm = 0x10000000
; SCALAR-NEXT:    imull %edi, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    xorl %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $536870912, %edx # imm = 0x20000000
; SCALAR-NEXT:    imull %edi, %edx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $1073741824, %eax # imm = 0x40000000
; SCALAR-NEXT:    imull %edi, %eax
; SCALAR-NEXT:    xorl %edx, %eax
; SCALAR-NEXT:    andl $-2147483648, %esi # imm = 0x80000000
; SCALAR-NEXT:    imull %edi, %esi
; SCALAR-NEXT:    xorl %esi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %res = call i32 @llvm.clmul.i32(i32 %a, i32 %b)
  ret i32 %res
}

define i64 @clmul_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmul_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %r15
; SCALAR-NEXT:    pushq %r14
; SCALAR-NEXT:    pushq %r13
; SCALAR-NEXT:    pushq %r12
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    subq $40, %rsp
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    movl %esi, %r12d
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $2, %r14d
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    andl $1, %r10d
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r14, %r10
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $4, %r15d
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    andl $8, %eax
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r15, %rax
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    xorq %r10, %rax
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $16, %r13d
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    andl $32, %r12d
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %r13, %r12
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    movq %r13, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $64, %r8d
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r12, %r8
; SCALAR-NEXT:    xorq %rax, %r8
; SCALAR-NEXT:    movl %esi, %r12d
; SCALAR-NEXT:    andl $128, %ebp
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    andl $256, %r9d # imm = 0x100
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rbp, %r9
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    andl $512, %ecx # imm = 0x200
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %r9, %rcx
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andl $1024, %edx # imm = 0x400
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %rcx, %rdx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $2048, %r11d # imm = 0x800
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    andl $4096, %ebx # imm = 0x1000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    andl $8192, %r14d # imm = 0x2000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %rbx, %r14
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    andl $16384, %r15d # imm = 0x4000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %r14, %r15
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $32768, %r10d # imm = 0x8000
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r15, %r10
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    xorq %rdx, %r10
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rcx # 8-byte Reload
; SCALAR-NEXT:    andl $65536, %ecx # imm = 0x10000
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    andl $131072, %r13d # imm = 0x20000
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rcx, %r13
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $262144, %r12d # imm = 0x40000
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %r13, %r12
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    andl $524288, %ebp # imm = 0x80000
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r12, %rbp
; SCALAR-NEXT:    movl %esi, %r12d
; SCALAR-NEXT:    andl $1048576, %r9d # imm = 0x100000
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rbp, %r9
; SCALAR-NEXT:    movabsq $4294967296, %rbp # imm = 0x100000000
; SCALAR-NEXT:    andq %rsi, %rbp
; SCALAR-NEXT:    andl $2097152, %eax # imm = 0x200000
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r9, %rax
; SCALAR-NEXT:    movabsq $8589934592, %r9 # imm = 0x200000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    xorq %r10, %rax
; SCALAR-NEXT:    movabsq $17179869184, %r9 # imm = 0x400000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r9 # 8-byte Reload
; SCALAR-NEXT:    andl $4194304, %r9d # imm = 0x400000
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    andl $8388608, %r11d # imm = 0x800000
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r9, %r11
; SCALAR-NEXT:    movabsq $34359738368, %r9 # imm = 0x800000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $16777216, %ebx # imm = 0x1000000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %r11, %rbx
; SCALAR-NEXT:    movabsq $68719476736, %r9 # imm = 0x1000000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $33554432, %r8d # imm = 0x2000000
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %rbx, %r8
; SCALAR-NEXT:    movabsq $137438953472, %r9 # imm = 0x2000000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movq %r9, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $67108864, %r14d # imm = 0x4000000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r8, %r14
; SCALAR-NEXT:    movabsq $274877906944, %r8 # imm = 0x4000000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movq %r8, (%rsp) # 8-byte Spill
; SCALAR-NEXT:    andl $134217728, %edx # imm = 0x8000000
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r14, %rdx
; SCALAR-NEXT:    movabsq $549755813888, %r8 # imm = 0x8000000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movq %r8, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movq %rcx, %r13
; SCALAR-NEXT:    andl $268435456, %r13d # imm = 0x10000000
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rdx, %r13
; SCALAR-NEXT:    movabsq $1099511627776, %rcx # imm = 0x10000000000
; SCALAR-NEXT:    andq %rsi, %rcx
; SCALAR-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    xorq %rax, %r13
; SCALAR-NEXT:    movabsq $2199023255552, %rax # imm = 0x20000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $536870912, %r15d # imm = 0x20000000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    andl $1073741824, %r12d # imm = 0x40000000
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %r15, %r12
; SCALAR-NEXT:    movq %r12, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4398046511104, %rax # imm = 0x40000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $8796093022208, %rax # imm = 0x80000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $17592186044416, %rax # imm = 0x100000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $35184372088832, %rax # imm = 0x200000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $70368744177664, %rax # imm = 0x400000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $140737488355328, %rax # imm = 0x800000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $281474976710656, %rax # imm = 0x1000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $562949953421312, %rax # imm = 0x2000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $1125899906842624, %rax # imm = 0x4000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $2251799813685248, %rax # imm = 0x8000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4503599627370496, %r12 # imm = 0x10000000000000
; SCALAR-NEXT:    andq %rsi, %r12
; SCALAR-NEXT:    movabsq $9007199254740992, %r14 # imm = 0x20000000000000
; SCALAR-NEXT:    andq %rsi, %r14
; SCALAR-NEXT:    movabsq $18014398509481984, %rax # imm = 0x40000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $36028797018963968, %r15 # imm = 0x80000000000000
; SCALAR-NEXT:    andq %rsi, %r15
; SCALAR-NEXT:    movabsq $72057594037927936, %rbx # imm = 0x100000000000000
; SCALAR-NEXT:    andq %rsi, %rbx
; SCALAR-NEXT:    movabsq $144115188075855872, %r11 # imm = 0x200000000000000
; SCALAR-NEXT:    andq %rsi, %r11
; SCALAR-NEXT:    movabsq $288230376151711744, %r10 # imm = 0x400000000000000
; SCALAR-NEXT:    andq %rsi, %r10
; SCALAR-NEXT:    movabsq $576460752303423488, %r9 # imm = 0x800000000000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movabsq $1152921504606846976, %r8 # imm = 0x1000000000000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movabsq $2305843009213693952, %rdx # imm = 0x2000000000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movabsq $4611686018427387904, %rcx # imm = 0x4000000000000000
; SCALAR-NEXT:    andq %rsi, %rcx
; SCALAR-NEXT:    movabsq $-9223372036854775808, %rax # imm = 0x8000000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    # kill: def $esi killed $esi killed $rsi def $rsi
; SCALAR-NEXT:    andl $-2147483648, %esi # imm = 0x80000000
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Folded Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    xorq %r13, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    movq (%rsp), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    xorq %rsi, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %rsi, %r12
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r12, %r14
; SCALAR-NEXT:    xorq %r13, %r14
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %rsi, %r15
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %r15, %rbx
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %rbx, %r11
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r11, %r10
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %r10, %r9
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    xorq %r14, %rax
; SCALAR-NEXT:    addq $40, %rsp
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %r12
; SCALAR-NEXT:    popq %r13
; SCALAR-NEXT:    popq %r14
; SCALAR-NEXT:    popq %r15
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmul_i64:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmul_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    retq
  %res = call i64 @llvm.clmul.i64(i64 %a, i64 %b)
  ret i64 %res
}

define i8 @clmulr_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmulr_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $2, %ecx
; SCALAR-NEXT:    movzbl %dil, %eax
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $16, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $32, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $64, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    xorl %edi, %ecx
; SCALAR-NEXT:    andl $128, %esi
; SCALAR-NEXT:    imull %esi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    shrl $7, %eax
; SCALAR-NEXT:    # kill: def $al killed $al killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $7, %eax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $7, %eax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i8 %a to i16
  %b.ext = zext i8 %b to i16
  %clmul = call i16 @llvm.clmul.i16(i16 %a.ext, i16 %b.ext)
  %res.ext = lshr i16 %clmul, 7
  %res = trunc i16 %res.ext to i8
  ret i8 %res
}

define i16 @clmulr_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmulr_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $2, %ecx
; SCALAR-NEXT:    movzwl %di, %eax
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $16, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $64, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %edi, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $128, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $256, %edi # imm = 0x100
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %edi, %r8d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $1024, %ecx # imm = 0x400
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $2048, %edx # imm = 0x800
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $4096, %edi # imm = 0x1000
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $8192, %edx # imm = 0x2000
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %edi, %edx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $16384, %edi # imm = 0x4000
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    andl $32768, %esi # imm = 0x8000
; SCALAR-NEXT:    imull %esi, %eax
; SCALAR-NEXT:    xorl %edi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    shrl $15, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzwl %si, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzwl %di, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $15, %eax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    movzwl %si, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzwl %di, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $15, %eax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i16 %a to i32
  %b.ext = zext i16 %b to i32
  %clmul = call i32 @llvm.clmul.i32(i32 %a.ext, i32 %b.ext)
  %res.ext = lshr i32 %clmul, 15
  %res = trunc i32 %res.ext to i16
  ret i16 %res
}

define i32 @clmulr_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmulr_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $2, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $4, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $16, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $64, %esi
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $128, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $256, %edi # imm = 0x100
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $1024, %edx # imm = 0x400
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    xorq %rsi, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $2048, %esi # imm = 0x800
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $4096, %edi # imm = 0x1000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $8192, %esi # imm = 0x2000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $16384, %edi # imm = 0x4000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $32768, %esi # imm = 0x8000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $65536, %edx # imm = 0x10000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $131072, %edi # imm = 0x20000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $262144, %edx # imm = 0x40000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %rdi, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $524288, %edi # imm = 0x80000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $1048576, %r8d # imm = 0x100000
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $2097152, %edx # imm = 0x200000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    xorq %rsi, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $4194304, %esi # imm = 0x400000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $8388608, %edi # imm = 0x800000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $16777216, %esi # imm = 0x1000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $33554432, %edi # imm = 0x2000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $67108864, %esi # imm = 0x4000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $134217728, %edi # imm = 0x8000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $268435456, %esi # imm = 0x10000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $536870912, %edx # imm = 0x20000000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $1073741824, %edi # imm = 0x40000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    andl $-2147483648, %eax # imm = 0x80000000
; SCALAR-NEXT:    imulq %rcx, %rax
; SCALAR-NEXT:    xorq %rdi, %rax
; SCALAR-NEXT:    xorq %rsi, %rax
; SCALAR-NEXT:    shrq $31, %rax
; SCALAR-NEXT:    # kill: def $eax killed $eax killed $rax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulr_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrq $31, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrq $31, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i32 %a to i64
  %b.ext = zext i32 %b to i64
  %clmul = call i64 @llvm.clmul.i64(i64 %a.ext, i64 %b.ext)
  %res.ext = lshr i64 %clmul, 31
  %res = trunc i64 %res.ext to i32
  ret i32 %res
}

define i64 @clmulr_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmulr_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %r15
; SCALAR-NEXT:    pushq %r14
; SCALAR-NEXT:    pushq %r13
; SCALAR-NEXT:    pushq %r12
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    subq $40, %rsp
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    movabsq $1085102592571150095, %r8 # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %r8, %rax
; SCALAR-NEXT:    andq %r8, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rax, %rdi
; SCALAR-NEXT:    movabsq $3689348814741910323, %rdx # imm = 0x3333333333333333
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %rdx, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rax,4), %rax
; SCALAR-NEXT:    movabsq $6148914691236517205, %r9 # imm = 0x5555555555555555
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    andq %r9, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %r9, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rdi
; SCALAR-NEXT:    bswapq %rsi
; SCALAR-NEXT:    movq %rsi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    andq %r8, %rax
; SCALAR-NEXT:    andq %r8, %rsi
; SCALAR-NEXT:    shlq $4, %rsi
; SCALAR-NEXT:    orq %rax, %rsi
; SCALAR-NEXT:    movq %rsi, %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    shrq $2, %rsi
; SCALAR-NEXT:    andq %rdx, %rsi
; SCALAR-NEXT:    leaq (%rsi,%rax,4), %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    andq %r9, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %r9, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rsi
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $2, %edx
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    andl $1, %r10d
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %rdx, %r10
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $4, %r15d
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    andl $8, %ecx
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %r15, %rcx
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    xorq %r10, %rcx
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $16, %r14d
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    andl $32, %r11d
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $64, %eax
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r11, %rax
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $128, %ebx
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    andl $256, %r9d # imm = 0x100
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rbx, %r9
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    andl $1024, %edx # imm = 0x400
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movl %esi, %r12d
; SCALAR-NEXT:    xorq %rax, %rdx
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    andl $2048, %r13d # imm = 0x800
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    andl $4096, %r10d # imm = 0x1000
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r13, %r10
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    andl $8192, %r14d # imm = 0x2000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r10, %r14
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $16384, %r11d # imm = 0x4000
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $32768, %ecx # imm = 0x8000
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %r11, %rcx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $65536, %ebp # imm = 0x10000
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    andl $131072, %r15d # imm = 0x20000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %rbp, %r15
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andl $262144, %r12d # imm = 0x40000
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %r15, %r12
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $524288, %ebx # imm = 0x80000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %r12, %rbx
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    andl $1048576, %r13d # imm = 0x100000
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbx, %r13
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    andl $2097152, %eax # imm = 0x200000
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r13, %rax
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4194304, %r14d # imm = 0x400000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    andl $8388608, %r8d # imm = 0x800000
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r14, %r8
; SCALAR-NEXT:    movabsq $4294967296, %rbp # imm = 0x100000000
; SCALAR-NEXT:    andq %rsi, %rbp
; SCALAR-NEXT:    andl $16777216, %edx # imm = 0x1000000
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movabsq $8589934592, %r8 # imm = 0x200000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movq %r8, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $33554432, %r9d # imm = 0x2000000
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rdx, %r9
; SCALAR-NEXT:    movabsq $17179869184, %rdx # imm = 0x400000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $67108864, %r10d # imm = 0x4000000
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movabsq $34359738368, %rdx # imm = 0x800000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $134217728, %r11d # imm = 0x8000000
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movabsq $68719476736, %rdx # imm = 0x1000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $268435456, %r15d # imm = 0x10000000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %r11, %r15
; SCALAR-NEXT:    movabsq $137438953472, %rdx # imm = 0x2000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, (%rsp) # 8-byte Spill
; SCALAR-NEXT:    xorq %rax, %r15
; SCALAR-NEXT:    movq %r15, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $274877906944, %r13 # imm = 0x4000000000
; SCALAR-NEXT:    andq %rsi, %r13
; SCALAR-NEXT:    andl $536870912, %ebx # imm = 0x20000000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    andl $1073741824, %ecx # imm = 0x40000000
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %rbx, %rcx
; SCALAR-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $549755813888, %rax # imm = 0x8000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $1099511627776, %rax # imm = 0x10000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $2199023255552, %rax # imm = 0x20000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4398046511104, %rax # imm = 0x40000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $8796093022208, %rax # imm = 0x80000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $17592186044416, %rax # imm = 0x100000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $35184372088832, %rax # imm = 0x200000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $70368744177664, %rax # imm = 0x400000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $140737488355328, %rax # imm = 0x800000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $281474976710656, %rax # imm = 0x1000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $562949953421312, %rax # imm = 0x2000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $1125899906842624, %rax # imm = 0x4000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $2251799813685248, %rax # imm = 0x8000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4503599627370496, %r12 # imm = 0x10000000000000
; SCALAR-NEXT:    andq %rsi, %r12
; SCALAR-NEXT:    movabsq $9007199254740992, %r15 # imm = 0x20000000000000
; SCALAR-NEXT:    andq %rsi, %r15
; SCALAR-NEXT:    movabsq $18014398509481984, %r14 # imm = 0x40000000000000
; SCALAR-NEXT:    andq %rsi, %r14
; SCALAR-NEXT:    movabsq $36028797018963968, %r11 # imm = 0x80000000000000
; SCALAR-NEXT:    andq %rsi, %r11
; SCALAR-NEXT:    movabsq $72057594037927936, %rax # imm = 0x100000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $144115188075855872, %rbx # imm = 0x200000000000000
; SCALAR-NEXT:    andq %rsi, %rbx
; SCALAR-NEXT:    movabsq $288230376151711744, %r10 # imm = 0x400000000000000
; SCALAR-NEXT:    andq %rsi, %r10
; SCALAR-NEXT:    movabsq $576460752303423488, %r9 # imm = 0x800000000000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movabsq $1152921504606846976, %r8 # imm = 0x1000000000000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movabsq $2305843009213693952, %rdx # imm = 0x2000000000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movabsq $4611686018427387904, %rcx # imm = 0x4000000000000000
; SCALAR-NEXT:    andq %rsi, %rcx
; SCALAR-NEXT:    movabsq $-9223372036854775808, %rax # imm = 0x8000000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    # kill: def $esi killed $esi killed $rsi def $rsi
; SCALAR-NEXT:    andl $-2147483648, %esi # imm = 0x80000000
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Folded Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    xorq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Folded Reload
; SCALAR-NEXT:    movq (%rsp), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    xorq %rsi, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %rbp, %r12
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %r12, %r15
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r15, %r14
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r13, %r11
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %rsi, %rbx
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %r10, %r9
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    xorq %r11, %rax
; SCALAR-NEXT:    bswapq %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    shrq $4, %rcx
; SCALAR-NEXT:    movabsq $1085102592571150095, %rdx # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    shlq $4, %rax
; SCALAR-NEXT:    orq %rcx, %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    movabsq $3689348814741910323, %rdx # imm = 0x3333333333333333
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq $2, %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,4), %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    movabsq $6148914691236517205, %rdx # imm = 0x5555555555555555
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rax
; SCALAR-NEXT:    addq $40, %rsp
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %r12
; SCALAR-NEXT:    popq %r13
; SCALAR-NEXT:    popq %r14
; SCALAR-NEXT:    popq %r15
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE2-PCLMUL-LABEL: clmulr_i64:
; SSE2-PCLMUL:       # %bb.0:
; SSE2-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE2-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE2-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE2-PCLMUL-NEXT:    movq %xmm1, %rcx
; SSE2-PCLMUL-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-PCLMUL-NEXT:    movq %xmm0, %rax
; SSE2-PCLMUL-NEXT:    shldq $1, %rcx, %rax
; SSE2-PCLMUL-NEXT:    retq
;
; SSE42-PCLMUL-LABEL: clmulr_i64:
; SSE42-PCLMUL:       # %bb.0:
; SSE42-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE42-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE42-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE42-PCLMUL-NEXT:    movq %xmm1, %rcx
; SSE42-PCLMUL-NEXT:    pextrq $1, %xmm1, %rax
; SSE42-PCLMUL-NEXT:    shldq $1, %rcx, %rax
; SSE42-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulr_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rcx
; AVX-NEXT:    vpextrq $1, %xmm0, %rax
; AVX-NEXT:    shldq $1, %rcx, %rax
; AVX-NEXT:    retq
  %a.ext = zext i64 %a to i128
  %b.ext = zext i64 %b to i128
  %clmul = call i128 @llvm.clmul.i128(i128 %a.ext, i128 %b.ext)
  %res.ext = lshr i128 %clmul, 63
  %res = trunc i128 %res.ext to i64
  ret i64 %res
}

define i8 @clmulh_i8(i8 %a, i8 %b) nounwind {
; SCALAR-LABEL: clmulh_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $2, %ecx
; SCALAR-NEXT:    movzbl %dil, %eax
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $16, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $32, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $64, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    xorl %edi, %ecx
; SCALAR-NEXT:    andl $128, %esi
; SCALAR-NEXT:    imull %esi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    shrl $8, %eax
; SCALAR-NEXT:    # kill: def $al killed $al killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $8, %eax
; SSE-PCLMUL-NEXT:    # kill: def $al killed $al killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $8, %eax
; AVX-NEXT:    # kill: def $al killed $al killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i8 %a to i16
  %b.ext = zext i8 %b to i16
  %clmul = call i16 @llvm.clmul.i16(i16 %a.ext, i16 %b.ext)
  %res.ext = lshr i16 %clmul, 8
  %res = trunc i16 %res.ext to i8
  ret i8 %res
}

define i16 @clmulh_i16(i16 %a, i16 %b) nounwind {
; SCALAR-LABEL: clmulh_i16:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $2, %ecx
; SCALAR-NEXT:    movzwl %di, %eax
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $1, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %ecx, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $16, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %ecx, %r8d
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $64, %edx
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %r8d, %edx
; SCALAR-NEXT:    xorl %edi, %edx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $128, %ecx
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $256, %edi # imm = 0x100
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %ecx, %edi
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %edi, %r8d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $1024, %ecx # imm = 0x400
; SCALAR-NEXT:    imull %eax, %ecx
; SCALAR-NEXT:    xorl %r8d, %ecx
; SCALAR-NEXT:    xorl %edx, %ecx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $2048, %edx # imm = 0x800
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $4096, %edi # imm = 0x1000
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $8192, %edx # imm = 0x2000
; SCALAR-NEXT:    imull %eax, %edx
; SCALAR-NEXT:    xorl %edi, %edx
; SCALAR-NEXT:    movl %esi, %edi
; SCALAR-NEXT:    andl $16384, %edi # imm = 0x4000
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %edx, %edi
; SCALAR-NEXT:    andl $32768, %esi # imm = 0x8000
; SCALAR-NEXT:    imull %esi, %eax
; SCALAR-NEXT:    xorl %edi, %eax
; SCALAR-NEXT:    xorl %ecx, %eax
; SCALAR-NEXT:    shrl $16, %eax
; SCALAR-NEXT:    # kill: def $ax killed $ax killed $eax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i16:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzwl %si, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzwl %di, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $16, %eax
; SSE-PCLMUL-NEXT:    # kill: def $ax killed $ax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i16:
; AVX:       # %bb.0:
; AVX-NEXT:    movzwl %si, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzwl %di, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $16, %eax
; AVX-NEXT:    # kill: def $ax killed $ax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i16 %a to i32
  %b.ext = zext i16 %b to i32
  %clmul = call i32 @llvm.clmul.i32(i32 %a.ext, i32 %b.ext)
  %res.ext = lshr i32 %clmul, 16
  %res = trunc i32 %res.ext to i16
  ret i16 %res
}

define i32 @clmulh_i32(i32 %a, i32 %b) nounwind {
; SCALAR-LABEL: clmulh_i32:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %ecx
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $2, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $4, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $8, %edi
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $16, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdx, %r8
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $64, %esi
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %r8, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $128, %edx
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $256, %edi # imm = 0x100
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $1024, %edx # imm = 0x400
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    xorq %rsi, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $2048, %esi # imm = 0x800
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $4096, %edi # imm = 0x1000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $8192, %esi # imm = 0x2000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $16384, %edi # imm = 0x4000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $32768, %esi # imm = 0x8000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $65536, %edx # imm = 0x10000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $131072, %edi # imm = 0x20000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $262144, %edx # imm = 0x40000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %rdi, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $524288, %edi # imm = 0x80000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    andl $1048576, %r8d # imm = 0x100000
; SCALAR-NEXT:    imulq %rcx, %r8
; SCALAR-NEXT:    xorq %rdi, %r8
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $2097152, %edx # imm = 0x200000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    xorq %rsi, %rdx
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $4194304, %esi # imm = 0x400000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $8388608, %edi # imm = 0x800000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $16777216, %esi # imm = 0x1000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $33554432, %edi # imm = 0x2000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $67108864, %esi # imm = 0x4000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $134217728, %edi # imm = 0x8000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rsi, %rdi
; SCALAR-NEXT:    movl %eax, %esi
; SCALAR-NEXT:    andl $268435456, %esi # imm = 0x10000000
; SCALAR-NEXT:    imulq %rcx, %rsi
; SCALAR-NEXT:    xorq %rdi, %rsi
; SCALAR-NEXT:    xorq %rdx, %rsi
; SCALAR-NEXT:    movl %eax, %edx
; SCALAR-NEXT:    andl $536870912, %edx # imm = 0x20000000
; SCALAR-NEXT:    imulq %rcx, %rdx
; SCALAR-NEXT:    movl %eax, %edi
; SCALAR-NEXT:    andl $1073741824, %edi # imm = 0x40000000
; SCALAR-NEXT:    imulq %rcx, %rdi
; SCALAR-NEXT:    xorq %rdx, %rdi
; SCALAR-NEXT:    andl $-2147483648, %eax # imm = 0x80000000
; SCALAR-NEXT:    imulq %rcx, %rax
; SCALAR-NEXT:    xorq %rdi, %rax
; SCALAR-NEXT:    xorq %rsi, %rax
; SCALAR-NEXT:    shrq $32, %rax
; SCALAR-NEXT:    # kill: def $eax killed $eax killed $rax
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: clmulh_i32:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrq $32, %rax
; SSE-PCLMUL-NEXT:    # kill: def $eax killed $eax killed $rax
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrq $32, %rax
; AVX-NEXT:    # kill: def $eax killed $eax killed $rax
; AVX-NEXT:    retq
  %a.ext = zext i32 %a to i64
  %b.ext = zext i32 %b to i64
  %clmul = call i64 @llvm.clmul.i64(i64 %a.ext, i64 %b.ext)
  %res.ext = lshr i64 %clmul, 32
  %res = trunc i64 %res.ext to i32
  ret i32 %res
}

define i64 @clmulh_i64(i64 %a, i64 %b) nounwind {
; SCALAR-LABEL: clmulh_i64:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %r15
; SCALAR-NEXT:    pushq %r14
; SCALAR-NEXT:    pushq %r13
; SCALAR-NEXT:    pushq %r12
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    subq $40, %rsp
; SCALAR-NEXT:    bswapq %rdi
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    movabsq $1085102592571150095, %r9 # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %r9, %rax
; SCALAR-NEXT:    andq %r9, %rdi
; SCALAR-NEXT:    shlq $4, %rdi
; SCALAR-NEXT:    orq %rax, %rdi
; SCALAR-NEXT:    movabsq $3689348814741910323, %r8 # imm = 0x3333333333333333
; SCALAR-NEXT:    movq %rdi, %rax
; SCALAR-NEXT:    andq %r8, %rax
; SCALAR-NEXT:    shrq $2, %rdi
; SCALAR-NEXT:    andq %r8, %rdi
; SCALAR-NEXT:    leaq (%rdi,%rax,4), %rax
; SCALAR-NEXT:    movabsq $6148914691236517205, %rdx # imm = 0x5555555555555555
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rdi
; SCALAR-NEXT:    bswapq %rsi
; SCALAR-NEXT:    movq %rsi, %rax
; SCALAR-NEXT:    shrq $4, %rax
; SCALAR-NEXT:    andq %r9, %rax
; SCALAR-NEXT:    andq %r9, %rsi
; SCALAR-NEXT:    shlq $4, %rsi
; SCALAR-NEXT:    orq %rax, %rsi
; SCALAR-NEXT:    movq %rsi, %rax
; SCALAR-NEXT:    andq %r8, %rax
; SCALAR-NEXT:    shrq $2, %rsi
; SCALAR-NEXT:    andq %r8, %rsi
; SCALAR-NEXT:    leaq (%rsi,%rax,4), %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,2), %rsi
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $2, %edx
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    andl $1, %r10d
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %rdx, %r10
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $4, %r15d
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    andl $8, %ecx
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %r15, %rcx
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    xorq %r10, %rcx
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $16, %r14d
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    andl $32, %r11d
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $64, %eax
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r11, %rax
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $128, %ebx
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    andl $256, %r9d # imm = 0x100
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rbx, %r9
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    andl $512, %r8d # imm = 0x200
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    andl $1024, %edx # imm = 0x400
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movl %esi, %r12d
; SCALAR-NEXT:    xorq %rax, %rdx
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    andl $2048, %r13d # imm = 0x800
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    andl $4096, %r10d # imm = 0x1000
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r13, %r10
; SCALAR-NEXT:    movl %esi, %r13d
; SCALAR-NEXT:    andl $8192, %r14d # imm = 0x2000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r10, %r14
; SCALAR-NEXT:    movl %esi, %eax
; SCALAR-NEXT:    andl $16384, %r11d # imm = 0x4000
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    movl %esi, %r14d
; SCALAR-NEXT:    andl $32768, %ecx # imm = 0x8000
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %r11, %rcx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    movl %esi, %edx
; SCALAR-NEXT:    andl $65536, %ebp # imm = 0x10000
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    andl $131072, %r15d # imm = 0x20000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %rbp, %r15
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andl $262144, %r12d # imm = 0x40000
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %r15, %r12
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andl $524288, %ebx # imm = 0x80000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %r12, %rbx
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    andl $1048576, %r13d # imm = 0x100000
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbx, %r13
; SCALAR-NEXT:    movl %esi, %r15d
; SCALAR-NEXT:    andl $2097152, %eax # imm = 0x200000
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %r13, %rax
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andl $4194304, %r14d # imm = 0x400000
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    andl $8388608, %r8d # imm = 0x800000
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r14, %r8
; SCALAR-NEXT:    movabsq $4294967296, %rbp # imm = 0x100000000
; SCALAR-NEXT:    andq %rsi, %rbp
; SCALAR-NEXT:    andl $16777216, %edx # imm = 0x1000000
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    movabsq $8589934592, %r8 # imm = 0x200000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movq %r8, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $33554432, %r9d # imm = 0x2000000
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %rdx, %r9
; SCALAR-NEXT:    movabsq $17179869184, %rdx # imm = 0x400000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $67108864, %r10d # imm = 0x4000000
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %r9, %r10
; SCALAR-NEXT:    movabsq $34359738368, %rdx # imm = 0x800000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $134217728, %r11d # imm = 0x8000000
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r10, %r11
; SCALAR-NEXT:    movabsq $68719476736, %rdx # imm = 0x1000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    andl $268435456, %r15d # imm = 0x10000000
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %r11, %r15
; SCALAR-NEXT:    movabsq $137438953472, %rdx # imm = 0x2000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movq %rdx, (%rsp) # 8-byte Spill
; SCALAR-NEXT:    xorq %rax, %r15
; SCALAR-NEXT:    movq %r15, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $274877906944, %r13 # imm = 0x4000000000
; SCALAR-NEXT:    andq %rsi, %r13
; SCALAR-NEXT:    andl $536870912, %ebx # imm = 0x20000000
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    andl $1073741824, %ecx # imm = 0x40000000
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %rbx, %rcx
; SCALAR-NEXT:    movq %rcx, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $549755813888, %rax # imm = 0x8000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $1099511627776, %rax # imm = 0x10000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $2199023255552, %rax # imm = 0x20000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4398046511104, %rax # imm = 0x40000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $8796093022208, %rax # imm = 0x80000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $17592186044416, %rax # imm = 0x100000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $35184372088832, %rax # imm = 0x200000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $70368744177664, %rax # imm = 0x400000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $140737488355328, %rax # imm = 0x800000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $281474976710656, %rax # imm = 0x1000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $562949953421312, %rax # imm = 0x2000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $1125899906842624, %rax # imm = 0x4000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $2251799813685248, %rax # imm = 0x8000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $4503599627370496, %r12 # imm = 0x10000000000000
; SCALAR-NEXT:    andq %rsi, %r12
; SCALAR-NEXT:    movabsq $9007199254740992, %r15 # imm = 0x20000000000000
; SCALAR-NEXT:    andq %rsi, %r15
; SCALAR-NEXT:    movabsq $18014398509481984, %r14 # imm = 0x40000000000000
; SCALAR-NEXT:    andq %rsi, %r14
; SCALAR-NEXT:    movabsq $36028797018963968, %r11 # imm = 0x80000000000000
; SCALAR-NEXT:    andq %rsi, %r11
; SCALAR-NEXT:    movabsq $72057594037927936, %rax # imm = 0x100000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    movq %rax, {{[-0-9]+}}(%r{{[sb]}}p) # 8-byte Spill
; SCALAR-NEXT:    movabsq $144115188075855872, %rbx # imm = 0x200000000000000
; SCALAR-NEXT:    andq %rsi, %rbx
; SCALAR-NEXT:    movabsq $288230376151711744, %r10 # imm = 0x400000000000000
; SCALAR-NEXT:    andq %rsi, %r10
; SCALAR-NEXT:    movabsq $576460752303423488, %r9 # imm = 0x800000000000000
; SCALAR-NEXT:    andq %rsi, %r9
; SCALAR-NEXT:    movabsq $1152921504606846976, %r8 # imm = 0x1000000000000000
; SCALAR-NEXT:    andq %rsi, %r8
; SCALAR-NEXT:    movabsq $2305843009213693952, %rdx # imm = 0x2000000000000000
; SCALAR-NEXT:    andq %rsi, %rdx
; SCALAR-NEXT:    movabsq $4611686018427387904, %rcx # imm = 0x4000000000000000
; SCALAR-NEXT:    andq %rsi, %rcx
; SCALAR-NEXT:    movabsq $-9223372036854775808, %rax # imm = 0x8000000000000000
; SCALAR-NEXT:    andq %rsi, %rax
; SCALAR-NEXT:    # kill: def $esi killed $esi killed $rsi def $rsi
; SCALAR-NEXT:    andl $-2147483648, %esi # imm = 0x80000000
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Folded Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    xorq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Folded Reload
; SCALAR-NEXT:    movq (%rsp), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    movq %r13, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %r13 # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %r13
; SCALAR-NEXT:    xorq %rbp, %r13
; SCALAR-NEXT:    xorq %rsi, %r13
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    xorq %rbp, %rsi
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rbp # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rbp
; SCALAR-NEXT:    xorq %rsi, %rbp
; SCALAR-NEXT:    imulq %rdi, %r12
; SCALAR-NEXT:    xorq %rbp, %r12
; SCALAR-NEXT:    imulq %rdi, %r15
; SCALAR-NEXT:    xorq %r12, %r15
; SCALAR-NEXT:    imulq %rdi, %r14
; SCALAR-NEXT:    xorq %r15, %r14
; SCALAR-NEXT:    imulq %rdi, %r11
; SCALAR-NEXT:    xorq %r14, %r11
; SCALAR-NEXT:    xorq %r13, %r11
; SCALAR-NEXT:    movq {{[-0-9]+}}(%r{{[sb]}}p), %rsi # 8-byte Reload
; SCALAR-NEXT:    imulq %rdi, %rsi
; SCALAR-NEXT:    imulq %rdi, %rbx
; SCALAR-NEXT:    xorq %rsi, %rbx
; SCALAR-NEXT:    imulq %rdi, %r10
; SCALAR-NEXT:    xorq %rbx, %r10
; SCALAR-NEXT:    imulq %rdi, %r9
; SCALAR-NEXT:    xorq %r10, %r9
; SCALAR-NEXT:    imulq %rdi, %r8
; SCALAR-NEXT:    xorq %r9, %r8
; SCALAR-NEXT:    imulq %rdi, %rdx
; SCALAR-NEXT:    xorq %r8, %rdx
; SCALAR-NEXT:    imulq %rdi, %rcx
; SCALAR-NEXT:    xorq %rdx, %rcx
; SCALAR-NEXT:    imulq %rdi, %rax
; SCALAR-NEXT:    xorq %rcx, %rax
; SCALAR-NEXT:    xorq %r11, %rax
; SCALAR-NEXT:    bswapq %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    shrq $4, %rcx
; SCALAR-NEXT:    movabsq $1085102592571150095, %rdx # imm = 0xF0F0F0F0F0F0F0F
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    shlq $4, %rax
; SCALAR-NEXT:    orq %rcx, %rax
; SCALAR-NEXT:    movq %rax, %rcx
; SCALAR-NEXT:    movabsq $3689348814741910323, %rdx # imm = 0x3333333333333333
; SCALAR-NEXT:    andq %rdx, %rcx
; SCALAR-NEXT:    shrq $2, %rax
; SCALAR-NEXT:    andq %rdx, %rax
; SCALAR-NEXT:    leaq (%rax,%rcx,4), %rax
; SCALAR-NEXT:    movabsq $6148914691236517205, %rdx # imm = 0x5555555555555555
; SCALAR-NEXT:    andq %rax, %rdx
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    movabsq $6148914691236517204, %rcx # imm = 0x5555555555555554
; SCALAR-NEXT:    andq %rax, %rcx
; SCALAR-NEXT:    leaq (%rcx,%rdx,2), %rax
; SCALAR-NEXT:    shrq %rax
; SCALAR-NEXT:    addq $40, %rsp
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %r12
; SCALAR-NEXT:    popq %r13
; SCALAR-NEXT:    popq %r14
; SCALAR-NEXT:    popq %r15
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE2-PCLMUL-LABEL: clmulh_i64:
; SSE2-PCLMUL:       # %bb.0:
; SSE2-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE2-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE2-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE2-PCLMUL-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-PCLMUL-NEXT:    movq %xmm0, %rax
; SSE2-PCLMUL-NEXT:    retq
;
; SSE42-PCLMUL-LABEL: clmulh_i64:
; SSE42-PCLMUL:       # %bb.0:
; SSE42-PCLMUL-NEXT:    movq %rsi, %xmm0
; SSE42-PCLMUL-NEXT:    movq %rdi, %xmm1
; SSE42-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE42-PCLMUL-NEXT:    pextrq $1, %xmm1, %rax
; SSE42-PCLMUL-NEXT:    retq
;
; AVX-LABEL: clmulh_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovq %rsi, %xmm0
; AVX-NEXT:    vmovq %rdi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vpextrq $1, %xmm0, %rax
; AVX-NEXT:    retq
  %a.ext = zext i64 %a to i128
  %b.ext = zext i64 %b to i128
  %clmul = call i128 @llvm.clmul.i128(i128 %a.ext, i128 %b.ext)
  %res.ext = lshr i128 %clmul, 64
  %res = trunc i128 %res.ext to i64
  ret i64 %res
}

define i8 @clmul_i8_noimplicitfloat(i8 %a, i8 %b) nounwind noimplicitfloat {
; CHECK-LABEL: clmul_i8_noimplicitfloat:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl %esi, %edx
; CHECK-NEXT:    andb $1, %dl
; CHECK-NEXT:    movl %esi, %ecx
; CHECK-NEXT:    andb $2, %cl
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %cl
; CHECK-NEXT:    movl %eax, %ecx
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %dl
; CHECK-NEXT:    movl %eax, %edx
; CHECK-NEXT:    xorb %cl, %dl
; CHECK-NEXT:    movl %esi, %ecx
; CHECK-NEXT:    andb $4, %cl
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %cl
; CHECK-NEXT:    movl %eax, %r8d
; CHECK-NEXT:    movl %esi, %ecx
; CHECK-NEXT:    andb $8, %cl
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %cl
; CHECK-NEXT:    movl %eax, %ecx
; CHECK-NEXT:    xorb %r8b, %cl
; CHECK-NEXT:    xorb %dl, %cl
; CHECK-NEXT:    movl %esi, %edx
; CHECK-NEXT:    andb $16, %dl
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %dl
; CHECK-NEXT:    movl %eax, %edx
; CHECK-NEXT:    movl %esi, %r8d
; CHECK-NEXT:    andb $32, %r8b
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %r8b
; CHECK-NEXT:    movl %eax, %r8d
; CHECK-NEXT:    movl %esi, %r9d
; CHECK-NEXT:    andb $64, %r9b
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %r9b
; CHECK-NEXT:    movl %eax, %r9d
; CHECK-NEXT:    xorb %dl, %r8b
; CHECK-NEXT:    xorb %r8b, %r9b
; CHECK-NEXT:    xorb %cl, %r9b
; CHECK-NEXT:    andb $-128, %sil
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    mulb %sil
; CHECK-NEXT:    xorb %r9b, %al
; CHECK-NEXT:    retq
  %res = call i8 @llvm.clmul.i8(i8 %a, i8 %b)
  ret i8 %res
}

declare void @use(i8)

define void @commutative_clmul_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andb $1, %r8b
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andb $2, %r9b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r9b
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r8b
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    xorb %r9b, %r8b
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andb $4, %r9b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r9b
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andb $8, %r10b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r10b
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    andb $16, %r11b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r11b
; SCALAR-NEXT:    movl %eax, %r11d
; SCALAR-NEXT:    movl %esi, %ebx
; SCALAR-NEXT:    andb $32, %bl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %bl
; SCALAR-NEXT:    movl %eax, %ebx
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    andb $64, %bpl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %bpl
; SCALAR-NEXT:    movl %eax, %ebp
; SCALAR-NEXT:    xorb %r9b, %r10b
; SCALAR-NEXT:    xorb %r8b, %r10b
; SCALAR-NEXT:    xorb %r11b, %bl
; SCALAR-NEXT:    xorb %bl, %bpl
; SCALAR-NEXT:    xorb %r10b, %bpl
; SCALAR-NEXT:    andb $-128, %sil
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %sil
; SCALAR-NEXT:    xorb %bpl, %al
; SCALAR-NEXT:    movb %al, (%rdx)
; SCALAR-NEXT:    movb %al, (%rcx)
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %xy = call i8 @llvm.clmul.i8(i8 %x, i8 %y)
  %yx = call i8 @llvm.clmul.i8(i8 %y, i8 %x)
  store i8 %xy, ptr %p0
  store i8 %yx, ptr %p1
  ret void
}

define void @commutative_clmulh_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmulh_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $2, %r8d
; SCALAR-NEXT:    movzbl %sil, %eax
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $4, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    andl $8, %r9d
; SCALAR-NEXT:    imull %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $16, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $64, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    andl $128, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %esi, %edi
; SCALAR-NEXT:    shrl $8, %edi
; SCALAR-NEXT:    movb %dil, (%rdx)
; SCALAR-NEXT:    movb %dil, (%rcx)
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmulh_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $8, %eax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmulh_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $8, %eax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %x.ext = zext i8 %x to i16
  %y.ext = zext i8 %y to i16
  %clmul_xy = call i16 @llvm.clmul.i16(i16 %x.ext, i16 %y.ext)
  %clmul_yx = call i16 @llvm.clmul.i16(i16 %y.ext, i16 %x.ext)
  %clmul_xy_lshr = lshr i16 %clmul_xy, 8
  %clmul_yx_lshr = lshr i16 %clmul_yx, 8
  %clmulh_xy = trunc i16 %clmul_xy_lshr to i8
  %clmulh_yx = trunc i16 %clmul_yx_lshr to i8
  store i8 %clmulh_xy, ptr %p0
  store i8 %clmulh_yx, ptr %p1
  ret void
}

define void @commutative_clmulr_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: commutative_clmulr_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $2, %r8d
; SCALAR-NEXT:    movzbl %sil, %eax
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $1, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $4, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    movl %edi, %r9d
; SCALAR-NEXT:    andl $8, %r9d
; SCALAR-NEXT:    imull %eax, %r9d
; SCALAR-NEXT:    xorl %r8d, %r9d
; SCALAR-NEXT:    xorl %esi, %r9d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $16, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    movl %edi, %r8d
; SCALAR-NEXT:    andl $32, %r8d
; SCALAR-NEXT:    imull %eax, %r8d
; SCALAR-NEXT:    xorl %esi, %r8d
; SCALAR-NEXT:    movl %edi, %esi
; SCALAR-NEXT:    andl $64, %esi
; SCALAR-NEXT:    imull %eax, %esi
; SCALAR-NEXT:    xorl %r8d, %esi
; SCALAR-NEXT:    xorl %r9d, %esi
; SCALAR-NEXT:    andl $128, %edi
; SCALAR-NEXT:    imull %eax, %edi
; SCALAR-NEXT:    xorl %esi, %edi
; SCALAR-NEXT:    shrl $7, %edi
; SCALAR-NEXT:    movb %dil, (%rdx)
; SCALAR-NEXT:    movb %dil, (%rcx)
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: commutative_clmulr_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    movzbl %dil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm0
; SSE-PCLMUL-NEXT:    movzbl %sil, %eax
; SSE-PCLMUL-NEXT:    movd %eax, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %rax
; SSE-PCLMUL-NEXT:    shrl $7, %eax
; SSE-PCLMUL-NEXT:    movb %al, (%rdx)
; SSE-PCLMUL-NEXT:    movb %al, (%rcx)
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: commutative_clmulr_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    movzbl %dil, %eax
; AVX-NEXT:    vmovd %eax, %xmm0
; AVX-NEXT:    movzbl %sil, %eax
; AVX-NEXT:    vmovd %eax, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %rax
; AVX-NEXT:    shrl $7, %eax
; AVX-NEXT:    movb %al, (%rdx)
; AVX-NEXT:    movb %al, (%rcx)
; AVX-NEXT:    retq
  %x.ext = zext i8 %x to i16
  %y.ext = zext i8 %y to i16
  %clmul_xy = call i16 @llvm.clmul.i16(i16 %x.ext, i16 %y.ext)
  %clmul_yx = call i16 @llvm.clmul.i16(i16 %y.ext, i16 %x.ext)
  %clmul_xy_lshr = lshr i16 %clmul_xy, 7
  %clmul_yx_lshr = lshr i16 %clmul_yx, 7
  %clmulh_xy = trunc i16 %clmul_xy_lshr to i8
  %clmulh_yx = trunc i16 %clmul_yx_lshr to i8
  store i8 %clmulh_xy, ptr %p0
  store i8 %clmulh_yx, ptr %p1
  ret void
}

define void @mul_use_commutative_clmul_i8(i8 %x, i8 %y, ptr %p0, ptr %p1) nounwind {
; SCALAR-LABEL: mul_use_commutative_clmul_i8:
; SCALAR:       # %bb.0:
; SCALAR-NEXT:    pushq %rbp
; SCALAR-NEXT:    pushq %rbx
; SCALAR-NEXT:    pushq %rax
; SCALAR-NEXT:    movq %rcx, %rbx
; SCALAR-NEXT:    movl %esi, %ecx
; SCALAR-NEXT:    andb $1, %cl
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andb $2, %r8b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r8b
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %cl
; SCALAR-NEXT:    movl %eax, %ecx
; SCALAR-NEXT:    xorb %r8b, %cl
; SCALAR-NEXT:    movl %esi, %r8d
; SCALAR-NEXT:    andb $4, %r8b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r8b
; SCALAR-NEXT:    movl %eax, %r8d
; SCALAR-NEXT:    movl %esi, %r9d
; SCALAR-NEXT:    andb $8, %r9b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r9b
; SCALAR-NEXT:    movl %eax, %r9d
; SCALAR-NEXT:    movl %esi, %r10d
; SCALAR-NEXT:    andb $16, %r10b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r10b
; SCALAR-NEXT:    movl %eax, %r10d
; SCALAR-NEXT:    movl %esi, %r11d
; SCALAR-NEXT:    andb $32, %r11b
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %r11b
; SCALAR-NEXT:    movl %eax, %r11d
; SCALAR-NEXT:    movl %esi, %ebp
; SCALAR-NEXT:    andb $64, %bpl
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %bpl
; SCALAR-NEXT:    movl %eax, %ebp
; SCALAR-NEXT:    xorb %r8b, %r9b
; SCALAR-NEXT:    xorb %cl, %r9b
; SCALAR-NEXT:    xorb %r10b, %r11b
; SCALAR-NEXT:    xorb %r11b, %bpl
; SCALAR-NEXT:    xorb %r9b, %bpl
; SCALAR-NEXT:    andb $-128, %sil
; SCALAR-NEXT:    movl %edi, %eax
; SCALAR-NEXT:    mulb %sil
; SCALAR-NEXT:    xorb %bpl, %al
; SCALAR-NEXT:    movb %al, (%rdx)
; SCALAR-NEXT:    movzbl %al, %ebp
; SCALAR-NEXT:    movl %ebp, %edi
; SCALAR-NEXT:    callq use@PLT
; SCALAR-NEXT:    movb %bpl, (%rbx)
; SCALAR-NEXT:    addq $8, %rsp
; SCALAR-NEXT:    popq %rbx
; SCALAR-NEXT:    popq %rbp
; SCALAR-NEXT:    retq
;
; SSE-PCLMUL-LABEL: mul_use_commutative_clmul_i8:
; SSE-PCLMUL:       # %bb.0:
; SSE-PCLMUL-NEXT:    pushq %r14
; SSE-PCLMUL-NEXT:    pushq %rbx
; SSE-PCLMUL-NEXT:    pushq %rax
; SSE-PCLMUL-NEXT:    movq %rcx, %rbx
; SSE-PCLMUL-NEXT:    movd %esi, %xmm0
; SSE-PCLMUL-NEXT:    movd %edi, %xmm1
; SSE-PCLMUL-NEXT:    pclmulqdq $0, %xmm0, %xmm1
; SSE-PCLMUL-NEXT:    movq %xmm1, %r14
; SSE-PCLMUL-NEXT:    movb %r14b, (%rdx)
; SSE-PCLMUL-NEXT:    movl %r14d, %edi
; SSE-PCLMUL-NEXT:    callq use@PLT
; SSE-PCLMUL-NEXT:    movb %r14b, (%rbx)
; SSE-PCLMUL-NEXT:    addq $8, %rsp
; SSE-PCLMUL-NEXT:    popq %rbx
; SSE-PCLMUL-NEXT:    popq %r14
; SSE-PCLMUL-NEXT:    retq
;
; AVX-LABEL: mul_use_commutative_clmul_i8:
; AVX:       # %bb.0:
; AVX-NEXT:    pushq %r14
; AVX-NEXT:    pushq %rbx
; AVX-NEXT:    pushq %rax
; AVX-NEXT:    movq %rcx, %rbx
; AVX-NEXT:    vmovd %esi, %xmm0
; AVX-NEXT:    vmovd %edi, %xmm1
; AVX-NEXT:    vpclmulqdq $0, %xmm0, %xmm1, %xmm0
; AVX-NEXT:    vmovq %xmm0, %r14
; AVX-NEXT:    movb %r14b, (%rdx)
; AVX-NEXT:    movl %r14d, %edi
; AVX-NEXT:    callq use@PLT
; AVX-NEXT:    movb %r14b, (%rbx)
; AVX-NEXT:    addq $8, %rsp
; AVX-NEXT:    popq %rbx
; AVX-NEXT:    popq %r14
; AVX-NEXT:    retq
  %xy = call i8 @llvm.clmul.i8(i8 %x, i8 %y)
  %yx = call i8 @llvm.clmul.i8(i8 %y, i8 %x)
  store i8 %xy, ptr %p0
  call void @use(i8 %xy)
  store i8 %yx, ptr %p1
  ret void
}
