; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+sse2 | FileCheck %s --check-prefix=SSE2
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx | FileCheck %s --check-prefix=AVX
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx2 | FileCheck %s --check-prefix=AVX2

; Test ct.select functionality for vector types

; 128-bit vectors
define <4 x i32> @test_ctselect_v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB0_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB0_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

define <4 x float> @test_ctselect_v4f32(i1 %cond, <4 x float> %a, <4 x float> %b) {
; SSE2-LABEL: test_ctselect_v4f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4f32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB1_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB1_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x float> @llvm.ct.select.v4f32(i1 %cond, <4 x float> %a, <4 x float> %b)
  ret <4 x float> %result
}

define <2 x i64> @test_ctselect_v2i64(i1 %cond, <2 x i64> %a, <2 x i64> %b) {
; SSE2-LABEL: test_ctselect_v2i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v2i64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v2i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v2i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB2_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB2_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <2 x i64> @llvm.ct.select.v2i64(i1 %cond, <2 x i64> %a, <2 x i64> %b)
  ret <2 x i64> %result
}

define <2 x double> @test_ctselect_v2f64(i1 %cond, <2 x double> %a, <2 x double> %b) {
; SSE2-LABEL: test_ctselect_v2f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v2f64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v2f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v2f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB3_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %xmm0, %xmm1
; AVX512-NEXT:  .LBB3_2:
; AVX512-NEXT:    vmovapd %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <2 x double> @llvm.ct.select.v2f64(i1 %cond, <2 x double> %a, <2 x double> %b)
  ret <2 x double> %result
}

; 256-bit vectors
define <8 x i32> @test_ctselect_v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b) {
; SSE2-LABEL: test_ctselect_v8i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm5, %xmm5
; SSE2-NEXT:    movd %eax, %xmm5
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = xmm5[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm5, %xmm4
; SSE2-NEXT:    pand %xmm0, %xmm5
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm2
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm3, %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm3, %ymm3
; AVX-NEXT:    vmovd %eax, %ymm3
; AVX-NEXT:    vshufps {{.*#+}} ymm3 = ymm3[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm3, %ymm2
; AVX-NEXT:    andps %ymm0, %ymm3
; AVX-NEXT:    andnps %ymm1, %ymm2
; AVX-NEXT:    orps %ymm3, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm3, %ymm3
; AVX2-NEXT:    vmovd %eax, %ymm3
; AVX2-NEXT:    vpshufd {{.*#+}} ymm3 = ymm3[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm3, %ymm2
; AVX2-NEXT:    pand %ymm0, %ymm3
; AVX2-NEXT:    pandn %ymm1, %ymm2
; AVX2-NEXT:    por %ymm3, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v8i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB4_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB4_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <8 x i32> @llvm.ct.select.v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %result
}

define <8 x float> @test_ctselect_v8f32(i1 %cond, <8 x float> %a, <8 x float> %b) {
; SSE2-LABEL: test_ctselect_v8f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm5, %xmm5
; SSE2-NEXT:    movd %eax, %xmm5
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = xmm5[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm5, %xmm4
; SSE2-NEXT:    pand %xmm0, %xmm5
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm2
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm3, %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8f32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm3, %ymm3
; AVX-NEXT:    vmovd %eax, %ymm3
; AVX-NEXT:    vshufps {{.*#+}} ymm3 = ymm3[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm3, %ymm2
; AVX-NEXT:    andps %ymm0, %ymm3
; AVX-NEXT:    andnps %ymm1, %ymm2
; AVX-NEXT:    orps %ymm3, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm3, %ymm3
; AVX2-NEXT:    vmovd %eax, %ymm3
; AVX2-NEXT:    vpshufd {{.*#+}} ymm3 = ymm3[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm3, %ymm2
; AVX2-NEXT:    pand %ymm0, %ymm3
; AVX2-NEXT:    pandn %ymm1, %ymm2
; AVX2-NEXT:    por %ymm3, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v8f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB5_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB5_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <8 x float> @llvm.ct.select.v8f32(i1 %cond, <8 x float> %a, <8 x float> %b)
  ret <8 x float> %result
}

define <4 x i64> @test_ctselect_v4i64(i1 %cond, <4 x i64> %a, <4 x i64> %b) {
; SSE2-LABEL: test_ctselect_v4i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm5, %xmm5
; SSE2-NEXT:    movd %eax, %xmm5
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = xmm5[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm5, %xmm4
; SSE2-NEXT:    pand %xmm0, %xmm5
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm2
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm3, %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm3, %ymm3
; AVX-NEXT:    vmovd %eax, %ymm3
; AVX-NEXT:    vshufpd {{.*#+}} ymm3 = ymm3[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm3, %ymm2
; AVX-NEXT:    andpd %ymm0, %ymm3
; AVX-NEXT:    andnpd %ymm1, %ymm2
; AVX-NEXT:    orpd %ymm3, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm3, %ymm3
; AVX2-NEXT:    vmovd %eax, %ymm3
; AVX2-NEXT:    vshufpd {{.*#+}} ymm3 = ymm3[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm3, %ymm2
; AVX2-NEXT:    andpd %ymm0, %ymm3
; AVX2-NEXT:    andnpd %ymm1, %ymm2
; AVX2-NEXT:    orpd %ymm3, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB6_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB6_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <4 x i64> @llvm.ct.select.v4i64(i1 %cond, <4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %result
}

define <4 x double> @test_ctselect_v4f64(i1 %cond, <4 x double> %a, <4 x double> %b) {
; SSE2-LABEL: test_ctselect_v4f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm5, %xmm5
; SSE2-NEXT:    movd %eax, %xmm5
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = xmm5[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm5, %xmm4
; SSE2-NEXT:    pand %xmm0, %xmm5
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm2
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm3, %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4f64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm3, %ymm3
; AVX-NEXT:    vmovd %eax, %ymm3
; AVX-NEXT:    vshufpd {{.*#+}} ymm3 = ymm3[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm3, %ymm2
; AVX-NEXT:    andpd %ymm0, %ymm3
; AVX-NEXT:    andnpd %ymm1, %ymm2
; AVX-NEXT:    orpd %ymm3, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm3, %ymm3
; AVX2-NEXT:    vmovd %eax, %ymm3
; AVX2-NEXT:    vshufpd {{.*#+}} ymm3 = ymm3[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm3, %ymm2
; AVX2-NEXT:    andpd %ymm0, %ymm3
; AVX2-NEXT:    andnpd %ymm1, %ymm2
; AVX2-NEXT:    orpd %ymm3, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB7_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %ymm0, %ymm1
; AVX512-NEXT:  .LBB7_2:
; AVX512-NEXT:    vmovapd %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <4 x double> @llvm.ct.select.v4f64(i1 %cond, <4 x double> %a, <4 x double> %b)
  ret <4 x double> %result
}

; 512-bit vectors (AVX512 only)
define <16 x i32> @test_ctselect_v16i32(i1 %cond, <16 x i32> %a, <16 x i32> %b) {
; SSE2-LABEL: test_ctselect_v16i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm9, %xmm9
; SSE2-NEXT:    movd %eax, %xmm9
; SSE2-NEXT:    pshufd {{.*#+}} xmm9 = xmm9[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm9, %xmm8
; SSE2-NEXT:    pand %xmm0, %xmm9
; SSE2-NEXT:    pandn %xmm4, %xmm8
; SSE2-NEXT:    por %xmm9, %xmm8
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm4
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm5, %xmm4
; SSE2-NEXT:    por %xmm0, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm5
; SSE2-NEXT:    pand %xmm2, %xmm0
; SSE2-NEXT:    pandn %xmm6, %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm6
; SSE2-NEXT:    pand %xmm3, %xmm0
; SSE2-NEXT:    pandn %xmm7, %xmm6
; SSE2-NEXT:    por %xmm0, %xmm6
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    movdqa %xmm5, %xmm2
; SSE2-NEXT:    movdqa %xmm6, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v16i32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm5, %ymm5
; AVX-NEXT:    vmovd %eax, %ymm5
; AVX-NEXT:    vshufps {{.*#+}} ymm5 = ymm5[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm5, %ymm4
; AVX-NEXT:    andps %ymm0, %ymm5
; AVX-NEXT:    andnps %ymm2, %ymm4
; AVX-NEXT:    orps %ymm5, %ymm4
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm0, %ymm0
; AVX-NEXT:    vmovd %eax, %ymm0
; AVX-NEXT:    vshufps {{.*#+}} ymm0 = ymm0[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm0, %ymm2
; AVX-NEXT:    andps %ymm1, %ymm0
; AVX-NEXT:    andnps %ymm3, %ymm2
; AVX-NEXT:    orps %ymm0, %ymm2
; AVX-NEXT:    vmovaps %ymm4, %ymm0
; AVX-NEXT:    vmovaps %ymm2, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v16i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm5, %ymm5
; AVX2-NEXT:    vmovd %eax, %ymm5
; AVX2-NEXT:    vpshufd {{.*#+}} ymm5 = ymm5[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm5, %ymm4
; AVX2-NEXT:    pand %ymm0, %ymm5
; AVX2-NEXT:    pandn %ymm2, %ymm4
; AVX2-NEXT:    por %ymm5, %ymm4
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    pxor %ymm0, %ymm0
; AVX2-NEXT:    vmovd %eax, %ymm0
; AVX2-NEXT:    vpshufd {{.*#+}} ymm0 = ymm0[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm0, %ymm2
; AVX2-NEXT:    pand %ymm1, %ymm0
; AVX2-NEXT:    pandn %ymm3, %ymm2
; AVX2-NEXT:    por %ymm0, %ymm2
; AVX2-NEXT:    vmovdqa %ymm4, %ymm0
; AVX2-NEXT:    vmovdqa %ymm2, %ymm1
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v16i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB8_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB8_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <16 x i32> @llvm.ct.select.v16i32(i1 %cond, <16 x i32> %a, <16 x i32> %b)
  ret <16 x i32> %result
}

define <16 x float> @test_ctselect_v16f32(i1 %cond, <16 x float> %a, <16 x float> %b) {
; SSE2-LABEL: test_ctselect_v16f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm9, %xmm9
; SSE2-NEXT:    movd %eax, %xmm9
; SSE2-NEXT:    pshufd {{.*#+}} xmm9 = xmm9[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm9, %xmm8
; SSE2-NEXT:    pand %xmm0, %xmm9
; SSE2-NEXT:    pandn %xmm4, %xmm8
; SSE2-NEXT:    por %xmm9, %xmm8
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm4
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm5, %xmm4
; SSE2-NEXT:    por %xmm0, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm5
; SSE2-NEXT:    pand %xmm2, %xmm0
; SSE2-NEXT:    pandn %xmm6, %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm6
; SSE2-NEXT:    pand %xmm3, %xmm0
; SSE2-NEXT:    pandn %xmm7, %xmm6
; SSE2-NEXT:    por %xmm0, %xmm6
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    movdqa %xmm5, %xmm2
; SSE2-NEXT:    movdqa %xmm6, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v16f32:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm5, %ymm5
; AVX-NEXT:    vmovd %eax, %ymm5
; AVX-NEXT:    vshufps {{.*#+}} ymm5 = ymm5[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm5, %ymm4
; AVX-NEXT:    andps %ymm0, %ymm5
; AVX-NEXT:    andnps %ymm2, %ymm4
; AVX-NEXT:    orps %ymm5, %ymm4
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm0, %ymm0
; AVX-NEXT:    vmovd %eax, %ymm0
; AVX-NEXT:    vshufps {{.*#+}} ymm0 = ymm0[0,0,0,0,4,4,4,4]
; AVX-NEXT:    vmovaps %ymm0, %ymm2
; AVX-NEXT:    andps %ymm1, %ymm0
; AVX-NEXT:    andnps %ymm3, %ymm2
; AVX-NEXT:    orps %ymm0, %ymm2
; AVX-NEXT:    vmovaps %ymm4, %ymm0
; AVX-NEXT:    vmovaps %ymm2, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v16f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm5, %ymm5
; AVX2-NEXT:    vmovd %eax, %ymm5
; AVX2-NEXT:    vpshufd {{.*#+}} ymm5 = ymm5[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm5, %ymm4
; AVX2-NEXT:    pand %ymm0, %ymm5
; AVX2-NEXT:    pandn %ymm2, %ymm4
; AVX2-NEXT:    por %ymm5, %ymm4
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    pxor %ymm0, %ymm0
; AVX2-NEXT:    vmovd %eax, %ymm0
; AVX2-NEXT:    vpshufd {{.*#+}} ymm0 = ymm0[0,0,0,0,4,4,4,4]
; AVX2-NEXT:    vmovdqa %ymm0, %ymm2
; AVX2-NEXT:    pand %ymm1, %ymm0
; AVX2-NEXT:    pandn %ymm3, %ymm2
; AVX2-NEXT:    por %ymm0, %ymm2
; AVX2-NEXT:    vmovdqa %ymm4, %ymm0
; AVX2-NEXT:    vmovdqa %ymm2, %ymm1
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v16f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB9_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB9_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <16 x float> @llvm.ct.select.v16f32(i1 %cond, <16 x float> %a, <16 x float> %b)
  ret <16 x float> %result
}

define <8 x i64> @test_ctselect_v8i64(i1 %cond, <8 x i64> %a, <8 x i64> %b) {
; SSE2-LABEL: test_ctselect_v8i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm9, %xmm9
; SSE2-NEXT:    movd %eax, %xmm9
; SSE2-NEXT:    pshufd {{.*#+}} xmm9 = xmm9[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm9, %xmm8
; SSE2-NEXT:    pand %xmm0, %xmm9
; SSE2-NEXT:    pandn %xmm4, %xmm8
; SSE2-NEXT:    por %xmm9, %xmm8
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm4
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm5, %xmm4
; SSE2-NEXT:    por %xmm0, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm5
; SSE2-NEXT:    pand %xmm2, %xmm0
; SSE2-NEXT:    pandn %xmm6, %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm6
; SSE2-NEXT:    pand %xmm3, %xmm0
; SSE2-NEXT:    pandn %xmm7, %xmm6
; SSE2-NEXT:    por %xmm0, %xmm6
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    movdqa %xmm5, %xmm2
; SSE2-NEXT:    movdqa %xmm6, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8i64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm5, %ymm5
; AVX-NEXT:    vmovd %eax, %ymm5
; AVX-NEXT:    vshufpd {{.*#+}} ymm5 = ymm5[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm5, %ymm4
; AVX-NEXT:    andpd %ymm0, %ymm5
; AVX-NEXT:    andnpd %ymm2, %ymm4
; AVX-NEXT:    orpd %ymm5, %ymm4
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorpd %ymm0, %ymm0
; AVX-NEXT:    vmovd %eax, %ymm0
; AVX-NEXT:    vshufpd {{.*#+}} ymm0 = ymm0[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm0, %ymm2
; AVX-NEXT:    andpd %ymm1, %ymm0
; AVX-NEXT:    andnpd %ymm3, %ymm2
; AVX-NEXT:    orpd %ymm0, %ymm2
; AVX-NEXT:    vmovapd %ymm4, %ymm0
; AVX-NEXT:    vmovapd %ymm2, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm5, %ymm5
; AVX2-NEXT:    vmovd %eax, %ymm5
; AVX2-NEXT:    vshufpd {{.*#+}} ymm5 = ymm5[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm5, %ymm4
; AVX2-NEXT:    andpd %ymm0, %ymm5
; AVX2-NEXT:    andnpd %ymm2, %ymm4
; AVX2-NEXT:    orpd %ymm5, %ymm4
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorpd %ymm0, %ymm0
; AVX2-NEXT:    vmovd %eax, %ymm0
; AVX2-NEXT:    vshufpd {{.*#+}} ymm0 = ymm0[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm0, %ymm2
; AVX2-NEXT:    andpd %ymm1, %ymm0
; AVX2-NEXT:    andnpd %ymm3, %ymm2
; AVX2-NEXT:    orpd %ymm0, %ymm2
; AVX2-NEXT:    vmovapd %ymm4, %ymm0
; AVX2-NEXT:    vmovapd %ymm2, %ymm1
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB10_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB10_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <8 x i64> @llvm.ct.select.v8i64(i1 %cond, <8 x i64> %a, <8 x i64> %b)
  ret <8 x i64> %result
}

define <8 x double> @test_ctselect_v8f64(i1 %cond, <8 x double> %a, <8 x double> %b) {
; SSE2-LABEL: test_ctselect_v8f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm9, %xmm9
; SSE2-NEXT:    movd %eax, %xmm9
; SSE2-NEXT:    pshufd {{.*#+}} xmm9 = xmm9[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm9, %xmm8
; SSE2-NEXT:    pand %xmm0, %xmm9
; SSE2-NEXT:    pandn %xmm4, %xmm8
; SSE2-NEXT:    por %xmm9, %xmm8
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm4
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pandn %xmm5, %xmm4
; SSE2-NEXT:    por %xmm0, %xmm4
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm5
; SSE2-NEXT:    pand %xmm2, %xmm0
; SSE2-NEXT:    pandn %xmm6, %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm0, %xmm6
; SSE2-NEXT:    pand %xmm3, %xmm0
; SSE2-NEXT:    pandn %xmm7, %xmm6
; SSE2-NEXT:    por %xmm0, %xmm6
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    movdqa %xmm5, %xmm2
; SSE2-NEXT:    movdqa %xmm6, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8f64:
; AVX:       # %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %ymm5, %ymm5
; AVX-NEXT:    vmovd %eax, %ymm5
; AVX-NEXT:    vshufpd {{.*#+}} ymm5 = ymm5[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm5, %ymm4
; AVX-NEXT:    andpd %ymm0, %ymm5
; AVX-NEXT:    andnpd %ymm2, %ymm4
; AVX-NEXT:    orpd %ymm5, %ymm4
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorpd %ymm0, %ymm0
; AVX-NEXT:    vmovd %eax, %ymm0
; AVX-NEXT:    vshufpd {{.*#+}} ymm0 = ymm0[0,0,2,2]
; AVX-NEXT:    vmovapd %ymm0, %ymm2
; AVX-NEXT:    andpd %ymm1, %ymm0
; AVX-NEXT:    andnpd %ymm3, %ymm2
; AVX-NEXT:    orpd %ymm0, %ymm2
; AVX-NEXT:    vmovapd %ymm4, %ymm0
; AVX-NEXT:    vmovapd %ymm2, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %ymm5, %ymm5
; AVX2-NEXT:    vmovd %eax, %ymm5
; AVX2-NEXT:    vshufpd {{.*#+}} ymm5 = ymm5[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm5, %ymm4
; AVX2-NEXT:    andpd %ymm0, %ymm5
; AVX2-NEXT:    andnpd %ymm2, %ymm4
; AVX2-NEXT:    orpd %ymm5, %ymm4
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorpd %ymm0, %ymm0
; AVX2-NEXT:    vmovd %eax, %ymm0
; AVX2-NEXT:    vshufpd {{.*#+}} ymm0 = ymm0[0,0,2,2]
; AVX2-NEXT:    vmovapd %ymm0, %ymm2
; AVX2-NEXT:    andpd %ymm1, %ymm0
; AVX2-NEXT:    andnpd %ymm3, %ymm2
; AVX2-NEXT:    orpd %ymm0, %ymm2
; AVX2-NEXT:    vmovapd %ymm4, %ymm0
; AVX2-NEXT:    vmovapd %ymm2, %ymm1
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v8f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB11_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %zmm0, %zmm1
; AVX512-NEXT:  .LBB11_2:
; AVX512-NEXT:    vmovapd %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <8 x double> @llvm.ct.select.v8f64(i1 %cond, <8 x double> %a, <8 x double> %b)
  ret <8 x double> %result
}

; Test with constant conditions for vector types
define <4 x i32> @test_ctselect_v4i32_const_true(<4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_const_true:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movb $1, %al
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_const_true:
; AVX:       # %bb.0:
; AVX-NEXT:    movb $1, %al
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_const_true:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movb $1, %al
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4i32_const_true:
; AVX512:       # %bb.0:
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 true, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

define <4 x i32> @test_ctselect_v4i32_const_false(<4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_const_false:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_const_false:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_const_false:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4i32_const_false:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 false, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

; Test with comparison conditions for vector types
define <4 x i32> @test_ctselect_v4i32_icmp(i32 %x, i32 %y, <4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_icmp:
; SSE2:       # %bb.0:
; SSE2-NEXT:    cmpl %esi, %edi
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    movl $0, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    movzbl %al, %eax
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    xorps %xmm3, %xmm3
; SSE2-NEXT:    movd %eax, %xmm3
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; SSE2-NEXT:    movdqa %xmm3, %xmm2
; SSE2-NEXT:    pand %xmm0, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_icmp:
; AVX:       # %bb.0:
; AVX-NEXT:    cmpl %esi, %edi
; AVX-NEXT:    sete %al
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    movl $0, %eax
; AVX-NEXT:    setne %al
; AVX-NEXT:    movzbl %al, %eax
; AVX-NEXT:    negl %eax
; AVX-NEXT:    xorps %xmm3, %xmm3
; AVX-NEXT:    movd %eax, %xmm3
; AVX-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX-NEXT:    movdqa %xmm3, %xmm2
; AVX-NEXT:    pand %xmm0, %xmm3
; AVX-NEXT:    pandn %xmm1, %xmm2
; AVX-NEXT:    por %xmm3, %xmm2
; AVX-NEXT:    vmovdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_icmp:
; AVX2:       # %bb.0:
; AVX2-NEXT:    cmpl %esi, %edi
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    movl $0, %eax
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    movzbl %al, %eax
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    xorps %xmm3, %xmm3
; AVX2-NEXT:    movd %eax, %xmm3
; AVX2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[0,0,0,0]
; AVX2-NEXT:    movdqa %xmm3, %xmm2
; AVX2-NEXT:    pand %xmm0, %xmm3
; AVX2-NEXT:    pandn %xmm1, %xmm2
; AVX2-NEXT:    por %xmm3, %xmm2
; AVX2-NEXT:    vmovdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
; AVX512-LABEL: test_ctselect_v4i32_icmp:
; AVX512:       # %bb.0:
; AVX512-NEXT:    cmpl %esi, %edi
; AVX512-NEXT:    je .LBB14_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:  .LBB14_2:
; AVX512-NEXT:    retq
  %cond = icmp eq i32 %x, %y
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

; Declare the intrinsics
declare <4 x i32> @llvm.ct.select.v4i32(i1, <4 x i32>, <4 x i32>)
declare <4 x float> @llvm.ct.select.v4f32(i1, <4 x float>, <4 x float>)
declare <2 x i64> @llvm.ct.select.v2i64(i1, <2 x i64>, <2 x i64>)
declare <2 x double> @llvm.ct.select.v2f64(i1, <2 x double>, <2 x double>)
declare <8 x i32> @llvm.ct.select.v8i32(i1, <8 x i32>, <8 x i32>)
declare <8 x float> @llvm.ct.select.v8f32(i1, <8 x float>, <8 x float>)
declare <4 x i64> @llvm.ct.select.v4i64(i1, <4 x i64>, <4 x i64>)
declare <4 x double> @llvm.ct.select.v4f64(i1, <4 x double>, <4 x double>)
declare <16 x i32> @llvm.ct.select.v16i32(i1, <16 x i32>, <16 x i32>)
declare <16 x float> @llvm.ct.select.v16f32(i1, <16 x float>, <16 x float>)
declare <8 x i64> @llvm.ct.select.v8i64(i1, <8 x i64>, <8 x i64>)
declare <8 x double> @llvm.ct.select.v8f64(i1, <8 x double>, <8 x double>)
