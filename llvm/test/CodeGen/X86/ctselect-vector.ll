; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+sse2 | FileCheck %s --check-prefix=SSE2
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx | FileCheck %s --check-prefix=AVX
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mattr=+avx2 | FileCheck %s --check-prefix=AVX2

; Test ct.select functionality for vector types

; 128-bit vectors
define <4 x i32> @test_ctselect_v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    movd %eax, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    movd %eax, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB0_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB0_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

define <4 x float> @test_ctselect_v4f32(i1 %cond, <4 x float> %a, <4 x float> %b) {
; SSE2-LABEL: test_ctselect_v4f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4f32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    movd %eax, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    movd %eax, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB1_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB1_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x float> @llvm.ct.select.v4f32(i1 %cond, <4 x float> %a, <4 x float> %b)
  ret <4 x float> %result
}

define <2 x i64> @test_ctselect_v2i64(i1 %cond, <2 x i64> %a, <2 x i64> %b) {
; SSE2-LABEL: test_ctselect_v2i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v2i64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    movd %eax, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v2i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    movd %eax, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v2i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB2_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm0, %xmm1
; AVX512-NEXT:  .LBB2_2:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <2 x i64> @llvm.ct.select.v2i64(i1 %cond, <2 x i64> %a, <2 x i64> %b)
  ret <2 x i64> %result
}

define <2 x double> @test_ctselect_v2f64(i1 %cond, <2 x double> %a, <2 x double> %b) {
; SSE2-LABEL: test_ctselect_v2f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v2f64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    movd %eax, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v2f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    movd %eax, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v2f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB3_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %xmm0, %xmm1
; AVX512-NEXT:  .LBB3_2:
; AVX512-NEXT:    vmovapd %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <2 x double> @llvm.ct.select.v2f64(i1 %cond, <2 x double> %a, <2 x double> %b)
  ret <2 x double> %result
}

; 256-bit vectors
define <8 x i32> @test_ctselect_v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b) {
; SSE2-LABEL: test_ctselect_v8i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm2
; SSE2-NEXT:    pandn %xmm0, %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm2
; AVX-NEXT:    vshufps $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm2
; AVX-NEXT:    vandps %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vandnps %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vorps %ymm0, %ymm1, %ymm0
; AVX-NEXT:    vmovaps %ymm0, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm2
; AVX2-NEXT:    vpshufd $0, %ymm2, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm2
; AVX2-NEXT:    vpand %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vpandn %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vpor %ymm0, %ymm1, %ymm0
; AVX2-NEXT:    vmovdqa %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v8i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB4_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB4_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <8 x i32> @llvm.ct.select.v8i32(i1 %cond, <8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %result
}

define <8 x float> @test_ctselect_v8f32(i1 %cond, <8 x float> %a, <8 x float> %b) {
; SSE2-LABEL: test_ctselect_v8f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm2
; SSE2-NEXT:    pandn %xmm0, %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8f32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm2
; AVX-NEXT:    vshufps $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm2
; AVX-NEXT:    vandps %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vandnps %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vorps %ymm0, %ymm1, %ymm0
; AVX-NEXT:    vmovaps %ymm0, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm2
; AVX2-NEXT:    vpshufd $0, %ymm2, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm2
; AVX2-NEXT:    vpand %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vpandn %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vpor %ymm0, %ymm1, %ymm0
; AVX2-NEXT:    vmovdqa %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v8f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB5_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB5_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <8 x float> @llvm.ct.select.v8f32(i1 %cond, <8 x float> %a, <8 x float> %b)
  ret <8 x float> %result
}

define <4 x i64> @test_ctselect_v4i64(i1 %cond, <4 x i64> %a, <4 x i64> %b) {
; SSE2-LABEL: test_ctselect_v4i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm2
; SSE2-NEXT:    pandn %xmm0, %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm2
; AVX-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm2
; AVX-NEXT:    vandpd %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vandnpd %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vorpd %ymm0, %ymm1, %ymm0
; AVX-NEXT:    vmovapd %ymm0, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm2
; AVX2-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm2
; AVX2-NEXT:    vandpd %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vandnpd %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vorpd %ymm0, %ymm1, %ymm0
; AVX2-NEXT:    vmovapd %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB6_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %ymm0, %ymm1
; AVX512-NEXT:  .LBB6_2:
; AVX512-NEXT:    vmovaps %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <4 x i64> @llvm.ct.select.v4i64(i1 %cond, <4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %result
}

define <4 x double> @test_ctselect_v4f64(i1 %cond, <4 x double> %a, <4 x double> %b) {
; SSE2-LABEL: test_ctselect_v4f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm2
; SSE2-NEXT:    pandn %xmm0, %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm3
; SSE2-NEXT:    pandn %xmm1, %xmm2
; SSE2-NEXT:    por %xmm3, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm1
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4f64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm2
; AVX-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm2
; AVX-NEXT:    vandpd %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vandnpd %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vorpd %ymm0, %ymm1, %ymm0
; AVX-NEXT:    vmovapd %ymm0, %ymm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm2
; AVX2-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm2
; AVX2-NEXT:    vandpd %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vandnpd %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vorpd %ymm0, %ymm1, %ymm0
; AVX2-NEXT:    vmovapd %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB7_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %ymm0, %ymm1
; AVX512-NEXT:  .LBB7_2:
; AVX512-NEXT:    vmovapd %ymm1, %ymm0
; AVX512-NEXT:    retq
  %result = call <4 x double> @llvm.ct.select.v4f64(i1 %cond, <4 x double> %a, <4 x double> %b)
  ret <4 x double> %result
}

; 512-bit vectors (AVX512 only)
define <16 x i32> @test_ctselect_v16i32(i1 %cond, <16 x i32> %a, <16 x i32> %b) {
; SSE2-LABEL: test_ctselect_v16i32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm8
; SSE2-NEXT:    pshufd $0, %xmm8, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm8
; SSE2-NEXT:    pand %xmm8, %xmm4
; SSE2-NEXT:    pandn %xmm0, %xmm8
; SSE2-NEXT:    por %xmm4, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm5
; SSE2-NEXT:    pandn %xmm1, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm6
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm2
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm7
; SSE2-NEXT:    pandn %xmm3, %xmm4
; SSE2-NEXT:    por %xmm7, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v16i32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm4
; AVX-NEXT:    vshufps $0, %ymm4, %ymm4, %ymm4
; AVX-NEXT:    vmovaps %ymm4, %ymm4
; AVX-NEXT:    vandps %ymm2, %ymm4, %ymm2
; AVX-NEXT:    vandnps %ymm0, %ymm4, %ymm0
; AVX-NEXT:    vorps %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovaps %ymm0, %ymm0
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    vmovd %ecx, %ymm2
; AVX-NEXT:    vshufps $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm2
; AVX-NEXT:    vandps %ymm3, %ymm2, %ymm3
; AVX-NEXT:    vandnps %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vorps %ymm1, %ymm3, %ymm1
; AVX-NEXT:    vmovaps %ymm1, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v16i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm4
; AVX2-NEXT:    vpshufd $0, %ymm4, %ymm4
; AVX2-NEXT:    vmovdqa %ymm4, %ymm4
; AVX2-NEXT:    vpand %ymm2, %ymm4, %ymm2
; AVX2-NEXT:    vpandn %ymm0, %ymm4, %ymm0
; AVX2-NEXT:    vpor %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovdqa %ymm0, %ymm0
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    vmovd %ecx, %ymm2
; AVX2-NEXT:    vpshufd $0, %ymm2, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm2
; AVX2-NEXT:    vpand %ymm3, %ymm2, %ymm3
; AVX2-NEXT:    vpandn %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vpor %ymm1, %ymm3, %ymm1
; AVX2-NEXT:    vmovdqa %ymm1, %ymm1
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v16i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB8_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB8_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <16 x i32> @llvm.ct.select.v16i32(i1 %cond, <16 x i32> %a, <16 x i32> %b)
  ret <16 x i32> %result
}

define <16 x float> @test_ctselect_v16f32(i1 %cond, <16 x float> %a, <16 x float> %b) {
; SSE2-LABEL: test_ctselect_v16f32:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm8
; SSE2-NEXT:    pshufd $0, %xmm8, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm8
; SSE2-NEXT:    pand %xmm8, %xmm4
; SSE2-NEXT:    pandn %xmm0, %xmm8
; SSE2-NEXT:    por %xmm4, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm5
; SSE2-NEXT:    pandn %xmm1, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm6
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm2
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm7
; SSE2-NEXT:    pandn %xmm3, %xmm4
; SSE2-NEXT:    por %xmm7, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v16f32:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm4
; AVX-NEXT:    vshufps $0, %ymm4, %ymm4, %ymm4
; AVX-NEXT:    vmovaps %ymm4, %ymm4
; AVX-NEXT:    vandps %ymm2, %ymm4, %ymm2
; AVX-NEXT:    vandnps %ymm0, %ymm4, %ymm0
; AVX-NEXT:    vorps %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovaps %ymm0, %ymm0
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    vmovd %ecx, %ymm2
; AVX-NEXT:    vshufps $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovaps %ymm2, %ymm2
; AVX-NEXT:    vandps %ymm3, %ymm2, %ymm3
; AVX-NEXT:    vandnps %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vorps %ymm1, %ymm3, %ymm1
; AVX-NEXT:    vmovaps %ymm1, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v16f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm4
; AVX2-NEXT:    vpshufd $0, %ymm4, %ymm4
; AVX2-NEXT:    vmovdqa %ymm4, %ymm4
; AVX2-NEXT:    vpand %ymm2, %ymm4, %ymm2
; AVX2-NEXT:    vpandn %ymm0, %ymm4, %ymm0
; AVX2-NEXT:    vpor %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovdqa %ymm0, %ymm0
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    vmovd %ecx, %ymm2
; AVX2-NEXT:    vpshufd $0, %ymm2, %ymm2
; AVX2-NEXT:    vmovdqa %ymm2, %ymm2
; AVX2-NEXT:    vpand %ymm3, %ymm2, %ymm3
; AVX2-NEXT:    vpandn %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vpor %ymm1, %ymm3, %ymm1
; AVX2-NEXT:    vmovdqa %ymm1, %ymm1
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v16f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB9_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB9_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <16 x float> @llvm.ct.select.v16f32(i1 %cond, <16 x float> %a, <16 x float> %b)
  ret <16 x float> %result
}

define <8 x i64> @test_ctselect_v8i64(i1 %cond, <8 x i64> %a, <8 x i64> %b) {
; SSE2-LABEL: test_ctselect_v8i64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm8
; SSE2-NEXT:    pshufd $0, %xmm8, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm8
; SSE2-NEXT:    pand %xmm8, %xmm4
; SSE2-NEXT:    pandn %xmm0, %xmm8
; SSE2-NEXT:    por %xmm4, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm5
; SSE2-NEXT:    pandn %xmm1, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm6
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm2
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm7
; SSE2-NEXT:    pandn %xmm3, %xmm4
; SSE2-NEXT:    por %xmm7, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8i64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm4
; AVX-NEXT:    vshufpd $0, %ymm4, %ymm4, %ymm4
; AVX-NEXT:    vmovapd %ymm4, %ymm4
; AVX-NEXT:    vandpd %ymm2, %ymm4, %ymm2
; AVX-NEXT:    vandnpd %ymm0, %ymm4, %ymm0
; AVX-NEXT:    vorpd %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovapd %ymm0, %ymm0
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    vmovd %ecx, %ymm2
; AVX-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm2
; AVX-NEXT:    vandpd %ymm3, %ymm2, %ymm3
; AVX-NEXT:    vandnpd %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vorpd %ymm1, %ymm3, %ymm1
; AVX-NEXT:    vmovapd %ymm1, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm4
; AVX2-NEXT:    vshufpd $0, %ymm4, %ymm4, %ymm4
; AVX2-NEXT:    vmovapd %ymm4, %ymm4
; AVX2-NEXT:    vandpd %ymm2, %ymm4, %ymm2
; AVX2-NEXT:    vandnpd %ymm0, %ymm4, %ymm0
; AVX2-NEXT:    vorpd %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovapd %ymm0, %ymm0
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    vmovd %ecx, %ymm2
; AVX2-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm2
; AVX2-NEXT:    vandpd %ymm3, %ymm2, %ymm3
; AVX2-NEXT:    vandnpd %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vorpd %ymm1, %ymm3, %ymm1
; AVX2-NEXT:    vmovapd %ymm1, %ymm1
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB10_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %zmm0, %zmm1
; AVX512-NEXT:  .LBB10_2:
; AVX512-NEXT:    vmovaps %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <8 x i64> @llvm.ct.select.v8i64(i1 %cond, <8 x i64> %a, <8 x i64> %b)
  ret <8 x i64> %result
}

define <8 x double> @test_ctselect_v8f64(i1 %cond, <8 x double> %a, <8 x double> %b) {
; SSE2-LABEL: test_ctselect_v8f64:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm8
; SSE2-NEXT:    pshufd $0, %xmm8, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm8
; SSE2-NEXT:    pand %xmm8, %xmm4
; SSE2-NEXT:    pandn %xmm0, %xmm8
; SSE2-NEXT:    por %xmm4, %xmm8
; SSE2-NEXT:    movdqa %xmm8, %xmm0
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm5
; SSE2-NEXT:    pandn %xmm1, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm1
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    negl %eax
; SSE2-NEXT:    movd %eax, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm6
; SSE2-NEXT:    pandn %xmm2, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm2
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm4
; SSE2-NEXT:    pshufd $0, %xmm4, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm4
; SSE2-NEXT:    pand %xmm4, %xmm7
; SSE2-NEXT:    pandn %xmm3, %xmm4
; SSE2-NEXT:    por %xmm7, %xmm4
; SSE2-NEXT:    movdqa %xmm4, %xmm3
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v8f64:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    sete %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    negl %eax
; AVX-NEXT:    vmovd %eax, %ymm4
; AVX-NEXT:    vshufpd $0, %ymm4, %ymm4, %ymm4
; AVX-NEXT:    vmovapd %ymm4, %ymm4
; AVX-NEXT:    vandpd %ymm2, %ymm4, %ymm2
; AVX-NEXT:    vandnpd %ymm0, %ymm4, %ymm0
; AVX-NEXT:    vorpd %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovapd %ymm0, %ymm0
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    vmovd %ecx, %ymm2
; AVX-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX-NEXT:    vmovapd %ymm2, %ymm2
; AVX-NEXT:    vandpd %ymm3, %ymm2, %ymm3
; AVX-NEXT:    vandnpd %ymm1, %ymm2, %ymm1
; AVX-NEXT:    vorpd %ymm1, %ymm3, %ymm1
; AVX-NEXT:    vmovapd %ymm1, %ymm1
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v8f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    testb $1, %dil
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    negl %eax
; AVX2-NEXT:    vmovd %eax, %ymm4
; AVX2-NEXT:    vshufpd $0, %ymm4, %ymm4, %ymm4
; AVX2-NEXT:    vmovapd %ymm4, %ymm4
; AVX2-NEXT:    vandpd %ymm2, %ymm4, %ymm2
; AVX2-NEXT:    vandnpd %ymm0, %ymm4, %ymm0
; AVX2-NEXT:    vorpd %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovapd %ymm0, %ymm0
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    vmovd %ecx, %ymm2
; AVX2-NEXT:    vshufpd $0, %ymm2, %ymm2, %ymm2
; AVX2-NEXT:    vmovapd %ymm2, %ymm2
; AVX2-NEXT:    vandpd %ymm3, %ymm2, %ymm3
; AVX2-NEXT:    vandnpd %ymm1, %ymm2, %ymm1
; AVX2-NEXT:    vorpd %ymm1, %ymm3, %ymm1
; AVX2-NEXT:    vmovapd %ymm1, %ymm1
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v8f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    testb %dil, %dil
; AVX512-NEXT:    je .LBB11_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovapd %zmm0, %zmm1
; AVX512-NEXT:  .LBB11_2:
; AVX512-NEXT:    vmovapd %zmm1, %zmm0
; AVX512-NEXT:    retq
  %result = call <8 x double> @llvm.ct.select.v8f64(i1 %cond, <8 x double> %a, <8 x double> %b)
  ret <8 x double> %result
}

; Test with constant conditions for vector types
define <4 x i32> @test_ctselect_v4i32_const_true(<4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_const_true:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movb $1, %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_const_true:
; AVX:       # %bb.0:
; AVX-NEXT:    movb $1, %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    movd %ecx, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_const_true:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movb $1, %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    movd %ecx, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4i32_const_true:
; AVX512:       # %bb.0:
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 true, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

define <4 x i32> @test_ctselect_v4i32_const_false(<4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_const_false:
; SSE2:       # %bb.0:
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_const_false:
; AVX:       # %bb.0:
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    movd %ecx, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_const_false:
; AVX2:       # %bb.0:
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    movd %ecx, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4i32_const_false:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:    retq
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 false, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

; Test with comparison conditions for vector types
define <4 x i32> @test_ctselect_v4i32_icmp(i32 %x, i32 %y, <4 x i32> %a, <4 x i32> %b) {
; SSE2-LABEL: test_ctselect_v4i32_icmp:
; SSE2:       # %bb.0:
; SSE2-NEXT:    cmpl %esi, %edi
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    xorl %ecx, %ecx
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    sete %cl
; SSE2-NEXT:    negl %ecx
; SSE2-NEXT:    movd %ecx, %xmm2
; SSE2-NEXT:    pshufd $0, %xmm2, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm2
; SSE2-NEXT:    pand %xmm2, %xmm1
; SSE2-NEXT:    pandn %xmm0, %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movdqa %xmm2, %xmm0
; SSE2-NEXT:    retq
;
; AVX-LABEL: test_ctselect_v4i32_icmp:
; AVX:       # %bb.0:
; AVX-NEXT:    cmpl %esi, %edi
; AVX-NEXT:    sete %al
; AVX-NEXT:    xorl %ecx, %ecx
; AVX-NEXT:    testb %al, %al
; AVX-NEXT:    sete %cl
; AVX-NEXT:    negl %ecx
; AVX-NEXT:    movd %ecx, %xmm2
; AVX-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm2
; AVX-NEXT:    pand %xmm2, %xmm1
; AVX-NEXT:    pandn %xmm0, %xmm2
; AVX-NEXT:    por %xmm1, %xmm2
; AVX-NEXT:    movdqa %xmm2, %xmm0
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_ctselect_v4i32_icmp:
; AVX2:       # %bb.0:
; AVX2-NEXT:    cmpl %esi, %edi
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    xorl %ecx, %ecx
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    sete %cl
; AVX2-NEXT:    negl %ecx
; AVX2-NEXT:    movd %ecx, %xmm2
; AVX2-NEXT:    pshufd $0, %xmm2, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm2
; AVX2-NEXT:    pand %xmm2, %xmm1
; AVX2-NEXT:    pandn %xmm0, %xmm2
; AVX2-NEXT:    por %xmm1, %xmm2
; AVX2-NEXT:    movdqa %xmm2, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_ctselect_v4i32_icmp:
; AVX512:       # %bb.0:
; AVX512-NEXT:    cmpl %esi, %edi
; AVX512-NEXT:    je .LBB14_2
; AVX512-NEXT:  # %bb.1:
; AVX512-NEXT:    vmovaps %xmm1, %xmm0
; AVX512-NEXT:  .LBB14_2:
; AVX512-NEXT:    retq
  %cond = icmp eq i32 %x, %y
  %result = call <4 x i32> @llvm.ct.select.v4i32(i1 %cond, <4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %result
}

; Declare the intrinsics
declare <4 x i32> @llvm.ct.select.v4i32(i1, <4 x i32>, <4 x i32>)
declare <4 x float> @llvm.ct.select.v4f32(i1, <4 x float>, <4 x float>)
declare <2 x i64> @llvm.ct.select.v2i64(i1, <2 x i64>, <2 x i64>)
declare <2 x double> @llvm.ct.select.v2f64(i1, <2 x double>, <2 x double>)
declare <8 x i32> @llvm.ct.select.v8i32(i1, <8 x i32>, <8 x i32>)
declare <8 x float> @llvm.ct.select.v8f32(i1, <8 x float>, <8 x float>)
declare <4 x i64> @llvm.ct.select.v4i64(i1, <4 x i64>, <4 x i64>)
declare <4 x double> @llvm.ct.select.v4f64(i1, <4 x double>, <4 x double>)
declare <16 x i32> @llvm.ct.select.v16i32(i1, <16 x i32>, <16 x i32>)
declare <16 x float> @llvm.ct.select.v16f32(i1, <16 x float>, <16 x float>)
declare <8 x i64> @llvm.ct.select.v8i64(i1, <8 x i64>, <8 x i64>)
declare <8 x double> @llvm.ct.select.v8f64(i1, <8 x double>, <8 x double>)
