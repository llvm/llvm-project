; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=sse2     | FileCheck %s --check-prefixes=CHECK,SSE,SSE2
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=sse4.1   | FileCheck %s --check-prefixes=CHECK,SSE,SSE41
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx      | FileCheck %s --check-prefixes=CHECK,AVX,AVX1
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx2     | FileCheck %s --check-prefixes=CHECK,AVX,AVX2
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512f  | FileCheck %s --check-prefixes=CHECK,AVX,AVX512,AVX512F
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=avx512bw | FileCheck %s --check-prefixes=CHECK,AVX,AVX512,AVX512BW

; Equality checks of 128/256-bit values can use PMOVMSK or PTEST to avoid scalarization.

define i32 @ne_i128(<2 x i64> %x, <2 x i64> %y) {
; SSE2-LABEL: ne_i128:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pcmpeqb %xmm1, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    cmpl $65535, %ecx # imm = 0xFFFF
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i128:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: ne_i128:
; AVX:       # %bb.0:
; AVX-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    vptest %xmm0, %xmm0
; AVX-NEXT:    setne %al
; AVX-NEXT:    retq
  %bcx = bitcast <2 x i64> %x to i128
  %bcy = bitcast <2 x i64> %y to i128
  %cmp = icmp ne i128 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i32 @eq_i128(<2 x i64> %x, <2 x i64> %y) {
; SSE2-LABEL: eq_i128:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pcmpeqb %xmm1, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    cmpl $65535, %ecx # imm = 0xFFFF
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i128:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: eq_i128:
; AVX:       # %bb.0:
; AVX-NEXT:    vpxor %xmm1, %xmm0, %xmm0
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    vptest %xmm0, %xmm0
; AVX-NEXT:    sete %al
; AVX-NEXT:    retq
  %bcx = bitcast <2 x i64> %x to i128
  %bcy = bitcast <2 x i64> %y to i128
  %cmp = icmp eq i128 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i32 @ne_i256(<4 x i64> %x, <4 x i64> %y) {
; SSE2-LABEL: ne_i256:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pxor %xmm3, %xmm1
; SSE2-NEXT:    pxor %xmm2, %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i256:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm3, %xmm1
; SSE41-NEXT:    pxor %xmm2, %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: ne_i256:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vxorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    setne %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: ne_i256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpxor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: ne_i256:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpxor %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    vptest %ymm0, %ymm0
; AVX512-NEXT:    setne %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %bcx = bitcast <4 x i64> %x to i256
  %bcy = bitcast <4 x i64> %y to i256
  %cmp = icmp ne i256 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i32 @eq_i256(<4 x i64> %x, <4 x i64> %y) {
; SSE2-LABEL: eq_i256:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pxor %xmm3, %xmm1
; SSE2-NEXT:    pxor %xmm2, %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i256:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm3, %xmm1
; SSE41-NEXT:    pxor %xmm2, %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: eq_i256:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vxorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    sete %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: eq_i256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpxor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: eq_i256:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpxor %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    vptest %ymm0, %ymm0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %bcx = bitcast <4 x i64> %x to i256
  %bcy = bitcast <4 x i64> %y to i256
  %cmp = icmp eq i256 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i32 @ne_i512(<8 x i64> %x, <8 x i64> %y) {
; SSE2-LABEL: ne_i512:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pxor %xmm7, %xmm3
; SSE2-NEXT:    pxor %xmm5, %xmm1
; SSE2-NEXT:    por %xmm3, %xmm1
; SSE2-NEXT:    pxor %xmm6, %xmm2
; SSE2-NEXT:    pxor %xmm4, %xmm0
; SSE2-NEXT:    por %xmm2, %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i512:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm7, %xmm3
; SSE41-NEXT:    pxor %xmm5, %xmm1
; SSE41-NEXT:    por %xmm3, %xmm1
; SSE41-NEXT:    pxor %xmm6, %xmm2
; SSE41-NEXT:    pxor %xmm4, %xmm0
; SSE41-NEXT:    por %xmm2, %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: ne_i512:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vxorps %ymm3, %ymm1, %ymm1
; AVX1-NEXT:    vxorps %ymm2, %ymm0, %ymm0
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    setne %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: ne_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpxor %ymm3, %ymm1, %ymm1
; AVX2-NEXT:    vpxor %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: ne_i512:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpcmpneqd %zmm1, %zmm0, %k0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    kortestw %k0, %k0
; AVX512-NEXT:    setne %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %bcx = bitcast <8 x i64> %x to i512
  %bcy = bitcast <8 x i64> %y to i512
  %cmp = icmp ne i512 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i32 @eq_i512(<8 x i64> %x, <8 x i64> %y) {
; SSE2-LABEL: eq_i512:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pxor %xmm7, %xmm3
; SSE2-NEXT:    pxor %xmm5, %xmm1
; SSE2-NEXT:    por %xmm3, %xmm1
; SSE2-NEXT:    pxor %xmm6, %xmm2
; SSE2-NEXT:    pxor %xmm4, %xmm0
; SSE2-NEXT:    por %xmm2, %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i512:
; SSE41:       # %bb.0:
; SSE41-NEXT:    pxor %xmm7, %xmm3
; SSE41-NEXT:    pxor %xmm5, %xmm1
; SSE41-NEXT:    por %xmm3, %xmm1
; SSE41-NEXT:    pxor %xmm6, %xmm2
; SSE41-NEXT:    pxor %xmm4, %xmm0
; SSE41-NEXT:    por %xmm2, %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: eq_i512:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vxorps %ymm3, %ymm1, %ymm1
; AVX1-NEXT:    vxorps %ymm2, %ymm0, %ymm0
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    sete %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: eq_i512:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpxor %ymm3, %ymm1, %ymm1
; AVX2-NEXT:    vpxor %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: eq_i512:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpcmpneqd %zmm1, %zmm0, %k0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    kortestw %k0, %k0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %bcx = bitcast <8 x i64> %x to i512
  %bcy = bitcast <8 x i64> %y to i512
  %cmp = icmp eq i512 %bcx, %bcy
  %zext = zext i1 %cmp to i32
  ret i32 %zext
}

define i1 @ne_v4i256(<4 x i256> %a0) {
; SSE2-LABEL: ne_v4i256:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movq %r9, %xmm0
; SSE2-NEXT:    movq %r8, %xmm1
; SSE2-NEXT:    punpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm0[0]
; SSE2-NEXT:    por {{[0-9]+}}(%rsp), %xmm1
; SSE2-NEXT:    movq %rsi, %xmm0
; SSE2-NEXT:    movq %rdi, %xmm2
; SSE2-NEXT:    punpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm0[0]
; SSE2-NEXT:    por {{[0-9]+}}(%rsp), %xmm2
; SSE2-NEXT:    por %xmm1, %xmm2
; SSE2-NEXT:    movq %rcx, %xmm0
; SSE2-NEXT:    movq %rdx, %xmm1
; SSE2-NEXT:    punpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm0[0]
; SSE2-NEXT:    por {{[0-9]+}}(%rsp), %xmm1
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm0
; SSE2-NEXT:    por {{[0-9]+}}(%rsp), %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    por %xmm2, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    xorl $15, %eax
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_v4i256:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movq %r9, %xmm0
; SSE41-NEXT:    movq %r8, %xmm1
; SSE41-NEXT:    punpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm0[0]
; SSE41-NEXT:    por {{[0-9]+}}(%rsp), %xmm1
; SSE41-NEXT:    movq %rsi, %xmm0
; SSE41-NEXT:    movq %rdi, %xmm2
; SSE41-NEXT:    punpcklqdq {{.*#+}} xmm2 = xmm2[0],xmm0[0]
; SSE41-NEXT:    por {{[0-9]+}}(%rsp), %xmm2
; SSE41-NEXT:    por %xmm1, %xmm2
; SSE41-NEXT:    movq %rcx, %xmm0
; SSE41-NEXT:    movq %rdx, %xmm1
; SSE41-NEXT:    punpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm0[0]
; SSE41-NEXT:    por {{[0-9]+}}(%rsp), %xmm1
; SSE41-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm0
; SSE41-NEXT:    por {{[0-9]+}}(%rsp), %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    por %xmm2, %xmm0
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: ne_v4i256:
; AVX1:       # %bb.0:
; AVX1-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; AVX1-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %r10
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %rcx
; AVX1-NEXT:    orq %r10, %rcx
; AVX1-NEXT:    vmovq %rcx, %xmm0
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %rax
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %rdx
; AVX1-NEXT:    orq %rax, %rdx
; AVX1-NEXT:    vmovq %rdx, %xmm1
; AVX1-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %r9
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %rsi
; AVX1-NEXT:    orq %r9, %rsi
; AVX1-NEXT:    vmovq %rsi, %xmm1
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %r8
; AVX1-NEXT:    orq {{[0-9]+}}(%rsp), %rdi
; AVX1-NEXT:    orq %r8, %rdi
; AVX1-NEXT:    vmovq %rdi, %xmm2
; AVX1-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX1-NEXT:    vinsertf128 $1, %xmm0, %ymm1, %ymm0
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    sete %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: ne_v4i256:
; AVX2:       # %bb.0:
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; AVX2-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %r10
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %rcx
; AVX2-NEXT:    orq %r10, %rcx
; AVX2-NEXT:    vmovq %rcx, %xmm0
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %rax
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %rdx
; AVX2-NEXT:    orq %rax, %rdx
; AVX2-NEXT:    vmovq %rdx, %xmm1
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %r9
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %rsi
; AVX2-NEXT:    orq %r9, %rsi
; AVX2-NEXT:    vmovq %rsi, %xmm1
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %r8
; AVX2-NEXT:    orq {{[0-9]+}}(%rsp), %rdi
; AVX2-NEXT:    orq %r8, %rdi
; AVX2-NEXT:    vmovq %rdi, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX2-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: ne_v4i256:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovq %rcx, %xmm0
; AVX512-NEXT:    vmovq %rdx, %xmm1
; AVX512-NEXT:    vpunpcklqdq {{.*#+}} xmm0 = xmm1[0],xmm0[0]
; AVX512-NEXT:    vmovq %rsi, %xmm1
; AVX512-NEXT:    vmovq %rdi, %xmm2
; AVX512-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX512-NEXT:    vinserti128 $1, %xmm0, %ymm1, %ymm0
; AVX512-NEXT:    vmovq %r9, %xmm1
; AVX512-NEXT:    vmovq %r8, %xmm2
; AVX512-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm2[0],xmm1[0]
; AVX512-NEXT:    vinserti128 $1, {{[0-9]+}}(%rsp), %ymm1, %ymm1
; AVX512-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512-NEXT:    vporq {{[0-9]+}}(%rsp), %zmm0, %zmm0
; AVX512-NEXT:    vptestmd %zmm0, %zmm0, %k0
; AVX512-NEXT:    kortestw %k0, %k0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %c = icmp ne <4 x i256> %a0, zeroinitializer
  %b = bitcast <4 x i1> %c to i4
  %r = icmp eq i4 %b, 0
  ret i1 %r
}

; This test models the expansion of 'memcmp(a, b, 32) != 0'
; if we allowed 2 pairs of 16-byte loads per block.

define i32 @ne_i128_pair(ptr %a, ptr %b) {
; SSE2-LABEL: ne_i128_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    pcmpeqb 16(%rsi), %xmm1
; SSE2-NEXT:    pcmpeqb (%rsi), %xmm0
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    cmpl $65535, %ecx # imm = 0xFFFF
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i128_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: ne_i128_pair:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovdqa (%rdi), %xmm0
; AVX-NEXT:    vmovdqa 16(%rdi), %xmm1
; AVX-NEXT:    vpxor 16(%rsi), %xmm1, %xmm1
; AVX-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; AVX-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    vptest %xmm0, %xmm0
; AVX-NEXT:    setne %al
; AVX-NEXT:    retq
  %a0 = load i128, ptr %a
  %b0 = load i128, ptr %b
  %xor1 = xor i128 %a0, %b0
  %ap1 = getelementptr i128, ptr %a, i128 1
  %bp1 = getelementptr i128, ptr %b, i128 1
  %a1 = load i128, ptr %ap1
  %b1 = load i128, ptr %bp1
  %xor2 = xor i128 %a1, %b1
  %or = or i128 %xor1, %xor2
  %cmp = icmp ne i128 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; This test models the expansion of 'memcmp(a, b, 32) == 0'
; if we allowed 2 pairs of 16-byte loads per block.

define i32 @eq_i128_pair(ptr %a, ptr %b) {
; SSE2-LABEL: eq_i128_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    pcmpeqb 16(%rsi), %xmm1
; SSE2-NEXT:    pcmpeqb (%rsi), %xmm0
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    cmpl $65535, %ecx # imm = 0xFFFF
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i128_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: eq_i128_pair:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovdqa (%rdi), %xmm0
; AVX-NEXT:    vmovdqa 16(%rdi), %xmm1
; AVX-NEXT:    vpxor 16(%rsi), %xmm1, %xmm1
; AVX-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; AVX-NEXT:    vpor %xmm1, %xmm0, %xmm0
; AVX-NEXT:    xorl %eax, %eax
; AVX-NEXT:    vptest %xmm0, %xmm0
; AVX-NEXT:    sete %al
; AVX-NEXT:    retq
  %a0 = load i128, ptr %a
  %b0 = load i128, ptr %b
  %xor1 = xor i128 %a0, %b0
  %ap1 = getelementptr i128, ptr %a, i128 1
  %bp1 = getelementptr i128, ptr %b, i128 1
  %a1 = load i128, ptr %ap1
  %b1 = load i128, ptr %bp1
  %xor2 = xor i128 %a1, %b1
  %or = or i128 %xor1, %xor2
  %cmp = icmp eq i128 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; This test models the expansion of 'memcmp(a, b, 64) != 0'
; if we allowed 2 pairs of 32-byte loads per block.

define i32 @ne_i256_pair(ptr %a, ptr %b) {
; SSE2-LABEL: ne_i256_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    movdqa 32(%rdi), %xmm2
; SSE2-NEXT:    movdqa 48(%rdi), %xmm3
; SSE2-NEXT:    pxor (%rsi), %xmm0
; SSE2-NEXT:    pxor 16(%rsi), %xmm1
; SSE2-NEXT:    pxor 32(%rsi), %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    pxor 48(%rsi), %xmm3
; SSE2-NEXT:    por %xmm1, %xmm3
; SSE2-NEXT:    por %xmm2, %xmm3
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    pcmpeqd %xmm3, %xmm0
; SSE2-NEXT:    movmskps %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i256_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    movdqa 32(%rdi), %xmm2
; SSE41-NEXT:    movdqa 48(%rdi), %xmm3
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor 32(%rsi), %xmm2
; SSE41-NEXT:    por %xmm0, %xmm2
; SSE41-NEXT:    pxor 48(%rsi), %xmm3
; SSE41-NEXT:    por %xmm1, %xmm3
; SSE41-NEXT:    por %xmm2, %xmm3
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm3, %xmm3
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: ne_i256_pair:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vmovups 32(%rdi), %ymm1
; AVX1-NEXT:    vxorps 32(%rsi), %ymm1, %ymm1
; AVX1-NEXT:    vxorps (%rsi), %ymm0, %ymm0
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    setne %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: ne_i256_pair:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX2-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX2-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: ne_i256_pair:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovdqu (%rdi), %ymm0
; AVX512-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX512-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX512-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX512-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    vptest %ymm0, %ymm0
; AVX512-NEXT:    setne %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %a0 = load i256, ptr %a
  %b0 = load i256, ptr %b
  %xor1 = xor i256 %a0, %b0
  %ap1 = getelementptr i256, ptr %a, i256 1
  %bp1 = getelementptr i256, ptr %b, i256 1
  %a1 = load i256, ptr %ap1
  %b1 = load i256, ptr %bp1
  %xor2 = xor i256 %a1, %b1
  %or = or i256 %xor1, %xor2
  %cmp = icmp ne i256 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; This test models the expansion of 'memcmp(a, b, 64) == 0'
; if we allowed 2 pairs of 32-byte loads per block.

define i32 @eq_i256_pair(ptr %a, ptr %b) {
; SSE2-LABEL: eq_i256_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    movdqa 32(%rdi), %xmm2
; SSE2-NEXT:    movdqa 48(%rdi), %xmm3
; SSE2-NEXT:    pxor (%rsi), %xmm0
; SSE2-NEXT:    pxor 16(%rsi), %xmm1
; SSE2-NEXT:    pxor 32(%rsi), %xmm2
; SSE2-NEXT:    por %xmm0, %xmm2
; SSE2-NEXT:    pxor 48(%rsi), %xmm3
; SSE2-NEXT:    por %xmm1, %xmm3
; SSE2-NEXT:    por %xmm2, %xmm3
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    pcmpeqd %xmm3, %xmm0
; SSE2-NEXT:    movmskps %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i256_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    movdqa 32(%rdi), %xmm2
; SSE41-NEXT:    movdqa 48(%rdi), %xmm3
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor 32(%rsi), %xmm2
; SSE41-NEXT:    por %xmm0, %xmm2
; SSE41-NEXT:    pxor 48(%rsi), %xmm3
; SSE41-NEXT:    por %xmm1, %xmm3
; SSE41-NEXT:    por %xmm2, %xmm3
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm3, %xmm3
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: eq_i256_pair:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vmovups 32(%rdi), %ymm1
; AVX1-NEXT:    vxorps 32(%rsi), %ymm1, %ymm1
; AVX1-NEXT:    vxorps (%rsi), %ymm0, %ymm0
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    sete %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: eq_i256_pair:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX2-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX2-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: eq_i256_pair:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovdqu (%rdi), %ymm0
; AVX512-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX512-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX512-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX512-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    vptest %ymm0, %ymm0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %a0 = load i256, ptr %a
  %b0 = load i256, ptr %b
  %xor1 = xor i256 %a0, %b0
  %ap1 = getelementptr i256, ptr %a, i256 1
  %bp1 = getelementptr i256, ptr %b, i256 1
  %a1 = load i256, ptr %ap1
  %b1 = load i256, ptr %bp1
  %xor2 = xor i256 %a1, %b1
  %or = or i256 %xor1, %xor2
  %cmp = icmp eq i256 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; This test models the expansion of 'memcmp(a, b, 64) != 0'
; if we allowed 2 pairs of 64-byte loads per block.

define i32 @ne_i512_pair(ptr %a, ptr %b) {
; SSE2-LABEL: ne_i512_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    movdqa 32(%rdi), %xmm2
; SSE2-NEXT:    movdqa 48(%rdi), %xmm3
; SSE2-NEXT:    pxor 16(%rsi), %xmm1
; SSE2-NEXT:    pxor 48(%rsi), %xmm3
; SSE2-NEXT:    pxor (%rsi), %xmm0
; SSE2-NEXT:    pxor 32(%rsi), %xmm2
; SSE2-NEXT:    movdqa 96(%rdi), %xmm4
; SSE2-NEXT:    movdqa 64(%rdi), %xmm5
; SSE2-NEXT:    movdqa 112(%rdi), %xmm6
; SSE2-NEXT:    movdqa 80(%rdi), %xmm7
; SSE2-NEXT:    pxor 80(%rsi), %xmm7
; SSE2-NEXT:    por %xmm1, %xmm7
; SSE2-NEXT:    pxor 112(%rsi), %xmm6
; SSE2-NEXT:    por %xmm3, %xmm6
; SSE2-NEXT:    por %xmm7, %xmm6
; SSE2-NEXT:    pxor 64(%rsi), %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    pxor 96(%rsi), %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    pcmpeqd %xmm4, %xmm0
; SSE2-NEXT:    movmskps %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: ne_i512_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    movdqa 32(%rdi), %xmm2
; SSE41-NEXT:    movdqa 48(%rdi), %xmm3
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor 48(%rsi), %xmm3
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    pxor 32(%rsi), %xmm2
; SSE41-NEXT:    movdqa 96(%rdi), %xmm4
; SSE41-NEXT:    movdqa 64(%rdi), %xmm5
; SSE41-NEXT:    movdqa 112(%rdi), %xmm6
; SSE41-NEXT:    movdqa 80(%rdi), %xmm7
; SSE41-NEXT:    pxor 80(%rsi), %xmm7
; SSE41-NEXT:    por %xmm1, %xmm7
; SSE41-NEXT:    pxor 112(%rsi), %xmm6
; SSE41-NEXT:    por %xmm3, %xmm6
; SSE41-NEXT:    por %xmm7, %xmm6
; SSE41-NEXT:    pxor 64(%rsi), %xmm5
; SSE41-NEXT:    por %xmm0, %xmm5
; SSE41-NEXT:    pxor 96(%rsi), %xmm4
; SSE41-NEXT:    por %xmm2, %xmm4
; SSE41-NEXT:    por %xmm5, %xmm4
; SSE41-NEXT:    por %xmm6, %xmm4
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm4, %xmm4
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: ne_i512_pair:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vmovups 32(%rdi), %ymm1
; AVX1-NEXT:    vmovups 64(%rdi), %ymm2
; AVX1-NEXT:    vmovups 96(%rdi), %ymm3
; AVX1-NEXT:    vxorps (%rsi), %ymm0, %ymm0
; AVX1-NEXT:    vxorps 32(%rsi), %ymm1, %ymm1
; AVX1-NEXT:    vxorps 64(%rsi), %ymm2, %ymm2
; AVX1-NEXT:    vorps %ymm2, %ymm0, %ymm0
; AVX1-NEXT:    vxorps 96(%rsi), %ymm3, %ymm2
; AVX1-NEXT:    vorps %ymm2, %ymm1, %ymm1
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    setne %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: ne_i512_pair:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX2-NEXT:    vmovdqu 64(%rdi), %ymm2
; AVX2-NEXT:    vmovdqu 96(%rdi), %ymm3
; AVX2-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX2-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX2-NEXT:    vpxor 64(%rsi), %ymm2, %ymm2
; AVX2-NEXT:    vpor %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpxor 96(%rsi), %ymm3, %ymm2
; AVX2-NEXT:    vpor %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: ne_i512_pair:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqu64 (%rdi), %zmm0
; AVX512F-NEXT:    vmovdqu64 64(%rdi), %zmm1
; AVX512F-NEXT:    vpcmpneqd 64(%rsi), %zmm1, %k0
; AVX512F-NEXT:    vpcmpneqd (%rsi), %zmm0, %k1
; AVX512F-NEXT:    xorl %eax, %eax
; AVX512F-NEXT:    kortestw %k0, %k1
; AVX512F-NEXT:    setne %al
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512BW-LABEL: ne_i512_pair:
; AVX512BW:       # %bb.0:
; AVX512BW-NEXT:    vmovdqu64 (%rdi), %zmm0
; AVX512BW-NEXT:    vmovdqu64 64(%rdi), %zmm1
; AVX512BW-NEXT:    vpcmpneqb 64(%rsi), %zmm1, %k0
; AVX512BW-NEXT:    vpcmpneqb (%rsi), %zmm0, %k1
; AVX512BW-NEXT:    xorl %eax, %eax
; AVX512BW-NEXT:    kortestq %k0, %k1
; AVX512BW-NEXT:    setne %al
; AVX512BW-NEXT:    vzeroupper
; AVX512BW-NEXT:    retq
  %a0 = load i512, ptr %a
  %b0 = load i512, ptr %b
  %xor1 = xor i512 %a0, %b0
  %ap1 = getelementptr i512, ptr %a, i512 1
  %bp1 = getelementptr i512, ptr %b, i512 1
  %a1 = load i512, ptr %ap1
  %b1 = load i512, ptr %bp1
  %xor2 = xor i512 %a1, %b1
  %or = or i512 %xor1, %xor2
  %cmp = icmp ne i512 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; This test models the expansion of 'memcmp(a, b, 64) == 0'
; if we allowed 2 pairs of 64-byte loads per block.

define i32 @eq_i512_pair(ptr %a, ptr %b) {
; SSE2-LABEL: eq_i512_pair:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    movdqa 32(%rdi), %xmm2
; SSE2-NEXT:    movdqa 48(%rdi), %xmm3
; SSE2-NEXT:    pxor 16(%rsi), %xmm1
; SSE2-NEXT:    pxor 48(%rsi), %xmm3
; SSE2-NEXT:    pxor (%rsi), %xmm0
; SSE2-NEXT:    pxor 32(%rsi), %xmm2
; SSE2-NEXT:    movdqa 96(%rdi), %xmm4
; SSE2-NEXT:    movdqa 64(%rdi), %xmm5
; SSE2-NEXT:    movdqa 112(%rdi), %xmm6
; SSE2-NEXT:    movdqa 80(%rdi), %xmm7
; SSE2-NEXT:    pxor 80(%rsi), %xmm7
; SSE2-NEXT:    por %xmm1, %xmm7
; SSE2-NEXT:    pxor 112(%rsi), %xmm6
; SSE2-NEXT:    por %xmm3, %xmm6
; SSE2-NEXT:    por %xmm7, %xmm6
; SSE2-NEXT:    pxor 64(%rsi), %xmm5
; SSE2-NEXT:    por %xmm0, %xmm5
; SSE2-NEXT:    pxor 96(%rsi), %xmm4
; SSE2-NEXT:    por %xmm2, %xmm4
; SSE2-NEXT:    por %xmm5, %xmm4
; SSE2-NEXT:    por %xmm6, %xmm4
; SSE2-NEXT:    pxor %xmm0, %xmm0
; SSE2-NEXT:    pcmpeqd %xmm4, %xmm0
; SSE2-NEXT:    movmskps %xmm0, %ecx
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    xorl $15, %ecx
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: eq_i512_pair:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    movdqa 32(%rdi), %xmm2
; SSE41-NEXT:    movdqa 48(%rdi), %xmm3
; SSE41-NEXT:    pxor 16(%rsi), %xmm1
; SSE41-NEXT:    pxor 48(%rsi), %xmm3
; SSE41-NEXT:    pxor (%rsi), %xmm0
; SSE41-NEXT:    pxor 32(%rsi), %xmm2
; SSE41-NEXT:    movdqa 96(%rdi), %xmm4
; SSE41-NEXT:    movdqa 64(%rdi), %xmm5
; SSE41-NEXT:    movdqa 112(%rdi), %xmm6
; SSE41-NEXT:    movdqa 80(%rdi), %xmm7
; SSE41-NEXT:    pxor 80(%rsi), %xmm7
; SSE41-NEXT:    por %xmm1, %xmm7
; SSE41-NEXT:    pxor 112(%rsi), %xmm6
; SSE41-NEXT:    por %xmm3, %xmm6
; SSE41-NEXT:    por %xmm7, %xmm6
; SSE41-NEXT:    pxor 64(%rsi), %xmm5
; SSE41-NEXT:    por %xmm0, %xmm5
; SSE41-NEXT:    pxor 96(%rsi), %xmm4
; SSE41-NEXT:    por %xmm2, %xmm4
; SSE41-NEXT:    por %xmm5, %xmm4
; SSE41-NEXT:    por %xmm6, %xmm4
; SSE41-NEXT:    xorl %eax, %eax
; SSE41-NEXT:    ptest %xmm4, %xmm4
; SSE41-NEXT:    sete %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: eq_i512_pair:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vmovups 32(%rdi), %ymm1
; AVX1-NEXT:    vmovups 64(%rdi), %ymm2
; AVX1-NEXT:    vmovups 96(%rdi), %ymm3
; AVX1-NEXT:    vxorps (%rsi), %ymm0, %ymm0
; AVX1-NEXT:    vxorps 32(%rsi), %ymm1, %ymm1
; AVX1-NEXT:    vxorps 64(%rsi), %ymm2, %ymm2
; AVX1-NEXT:    vorps %ymm2, %ymm0, %ymm0
; AVX1-NEXT:    vxorps 96(%rsi), %ymm3, %ymm2
; AVX1-NEXT:    vorps %ymm2, %ymm1, %ymm1
; AVX1-NEXT:    vorps %ymm1, %ymm0, %ymm0
; AVX1-NEXT:    xorl %eax, %eax
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    sete %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: eq_i512_pair:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vmovdqu 32(%rdi), %ymm1
; AVX2-NEXT:    vmovdqu 64(%rdi), %ymm2
; AVX2-NEXT:    vmovdqu 96(%rdi), %ymm3
; AVX2-NEXT:    vpxor (%rsi), %ymm0, %ymm0
; AVX2-NEXT:    vpxor 32(%rsi), %ymm1, %ymm1
; AVX2-NEXT:    vpxor 64(%rsi), %ymm2, %ymm2
; AVX2-NEXT:    vpor %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpxor 96(%rsi), %ymm3, %ymm2
; AVX2-NEXT:    vpor %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpor %ymm1, %ymm0, %ymm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: eq_i512_pair:
; AVX512F:       # %bb.0:
; AVX512F-NEXT:    vmovdqu64 (%rdi), %zmm0
; AVX512F-NEXT:    vmovdqu64 64(%rdi), %zmm1
; AVX512F-NEXT:    vpcmpneqd 64(%rsi), %zmm1, %k0
; AVX512F-NEXT:    vpcmpneqd (%rsi), %zmm0, %k1
; AVX512F-NEXT:    xorl %eax, %eax
; AVX512F-NEXT:    kortestw %k0, %k1
; AVX512F-NEXT:    sete %al
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512BW-LABEL: eq_i512_pair:
; AVX512BW:       # %bb.0:
; AVX512BW-NEXT:    vmovdqu64 (%rdi), %zmm0
; AVX512BW-NEXT:    vmovdqu64 64(%rdi), %zmm1
; AVX512BW-NEXT:    vpcmpneqb 64(%rsi), %zmm1, %k0
; AVX512BW-NEXT:    vpcmpneqb (%rsi), %zmm0, %k1
; AVX512BW-NEXT:    xorl %eax, %eax
; AVX512BW-NEXT:    kortestq %k0, %k1
; AVX512BW-NEXT:    sete %al
; AVX512BW-NEXT:    vzeroupper
; AVX512BW-NEXT:    retq
  %a0 = load i512, ptr %a
  %b0 = load i512, ptr %b
  %xor1 = xor i512 %a0, %b0
  %ap1 = getelementptr i512, ptr %a, i512 1
  %bp1 = getelementptr i512, ptr %b, i512 1
  %a1 = load i512, ptr %ap1
  %b1 = load i512, ptr %bp1
  %xor2 = xor i512 %a1, %b1
  %or = or i512 %xor1, %xor2
  %cmp = icmp eq i512 %or, 0
  %z = zext i1 %cmp to i32
  ret i32 %z
}

; PR41971: Comparison using vector types is not favorable here.
define i1 @eq_i128_args(i128 %a, i128 %b) {
; CHECK-LABEL: eq_i128_args:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorq %rcx, %rsi
; CHECK-NEXT:    xorq %rdx, %rdi
; CHECK-NEXT:    orq %rsi, %rdi
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %r = icmp eq i128 %a, %b
  ret i1 %r
}

define i1 @eq_i256_args(i256 %a, i256 %b) {
; CHECK-LABEL: eq_i256_args:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorq %r9, %rsi
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    orq %rsi, %rcx
; CHECK-NEXT:    xorq %r8, %rdi
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    orq %rdi, %rdx
; CHECK-NEXT:    orq %rcx, %rdx
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %r = icmp eq i256 %a, %b
  ret i1 %r
}

define i1 @eq_i512_args(i512 %a, i512 %b) {
; CHECK-LABEL: eq_i512_args:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    orq %r10, %rcx
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %r9
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rsi
; CHECK-NEXT:    orq %r9, %rsi
; CHECK-NEXT:    orq %rcx, %rsi
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    orq %rax, %rdx
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %r8
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rdi
; CHECK-NEXT:    orq %r8, %rdi
; CHECK-NEXT:    orq %rdx, %rdi
; CHECK-NEXT:    orq %rsi, %rdi
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %r = icmp eq i512 %a, %b
  ret i1 %r
}

define i1 @eq_i128_op(i128 %a, i128 %b) {
; CHECK-LABEL: eq_i128_op:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addq $1, %rdi
; CHECK-NEXT:    adcq $0, %rsi
; CHECK-NEXT:    xorq %rdx, %rdi
; CHECK-NEXT:    xorq %rcx, %rsi
; CHECK-NEXT:    orq %rdi, %rsi
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %a2 = add i128 %a, 1
  %r = icmp eq i128 %a2, %b
  ret i1 %r
}

define i1 @eq_i256_op(i256 %a, i256 %b) {
; CHECK-LABEL: eq_i256_op:
; CHECK:       # %bb.0:
; CHECK-NEXT:    addq $1, %rdi
; CHECK-NEXT:    adcq $0, %rsi
; CHECK-NEXT:    adcq $0, %rdx
; CHECK-NEXT:    adcq $0, %rcx
; CHECK-NEXT:    xorq %r8, %rdi
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rdx
; CHECK-NEXT:    orq %rdi, %rdx
; CHECK-NEXT:    xorq %r9, %rsi
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rcx
; CHECK-NEXT:    orq %rsi, %rcx
; CHECK-NEXT:    orq %rdx, %rcx
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %a2 = add i256 %a, 1
  %r = icmp eq i256 %a2, %b
  ret i1 %r
}

define i1 @eq_i512_op(i512 %a, i512 %b) {
; SSE-LABEL: eq_i512_op:
; SSE:       # %bb.0:
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; SSE-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; SSE-NEXT:    addq $1, %rdi
; SSE-NEXT:    adcq $0, %rsi
; SSE-NEXT:    adcq $0, %rdx
; SSE-NEXT:    adcq $0, %rcx
; SSE-NEXT:    adcq $0, %r8
; SSE-NEXT:    adcq $0, %r9
; SSE-NEXT:    adcq $0, %r10
; SSE-NEXT:    adcq $0, %rax
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %rsi
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %r9
; SSE-NEXT:    orq %rsi, %r9
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %rcx
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %rax
; SSE-NEXT:    orq %rcx, %rax
; SSE-NEXT:    orq %r9, %rax
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %rdx
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %r10
; SSE-NEXT:    orq %rdx, %r10
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %r8
; SSE-NEXT:    xorq {{[0-9]+}}(%rsp), %rdi
; SSE-NEXT:    orq %r8, %rdi
; SSE-NEXT:    orq %r10, %rdi
; SSE-NEXT:    orq %rax, %rdi
; SSE-NEXT:    sete %al
; SSE-NEXT:    retq
;
; AVX-LABEL: eq_i512_op:
; AVX:       # %bb.0:
; AVX-NEXT:    movq {{[0-9]+}}(%rsp), %r10
; AVX-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; AVX-NEXT:    addq $1, %rdi
; AVX-NEXT:    adcq $0, %rsi
; AVX-NEXT:    adcq $0, %rdx
; AVX-NEXT:    adcq $0, %rcx
; AVX-NEXT:    adcq $0, %r8
; AVX-NEXT:    adcq $0, %r9
; AVX-NEXT:    adcq $0, %r10
; AVX-NEXT:    adcq $0, %rax
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %rsi
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %r9
; AVX-NEXT:    orq %rsi, %r9
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %rcx
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %rax
; AVX-NEXT:    orq %rcx, %rax
; AVX-NEXT:    orq %r9, %rax
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %rdx
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %r10
; AVX-NEXT:    orq %rdx, %r10
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %r8
; AVX-NEXT:    xorq {{[0-9]+}}(%rsp), %rdi
; AVX-NEXT:    orq %r8, %rdi
; AVX-NEXT:    orq %r10, %rdi
; AVX-NEXT:    orq %rax, %rdi
; AVX-NEXT:    sete %al
; AVX-NEXT:    retq
  %a2 = add i512 %a, 1
  %r = icmp eq i512 %a2, %b
  ret i1 %r
}

define i1 @eq_i128_load_arg(ptr%p, i128 %b) {
; CHECK-LABEL: eq_i128_load_arg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorq 8(%rdi), %rdx
; CHECK-NEXT:    xorq (%rdi), %rsi
; CHECK-NEXT:    orq %rdx, %rsi
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %a = load i128, ptr %p
  %r = icmp eq i128 %a, %b
  ret i1 %r
}

define i1 @eq_i256_load_arg(ptr%p, i256 %b) {
; CHECK-LABEL: eq_i256_load_arg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xorq 24(%rdi), %r8
; CHECK-NEXT:    xorq 8(%rdi), %rdx
; CHECK-NEXT:    orq %r8, %rdx
; CHECK-NEXT:    xorq 16(%rdi), %rcx
; CHECK-NEXT:    xorq (%rdi), %rsi
; CHECK-NEXT:    orq %rcx, %rsi
; CHECK-NEXT:    orq %rdx, %rsi
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %a = load i256, ptr %p
  %r = icmp eq i256 %a, %b
  ret i1 %r
}

define i1 @eq_i512_load_arg(ptr%p, i512 %b) {
; CHECK-LABEL: eq_i512_load_arg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movq 40(%rdi), %rax
; CHECK-NEXT:    movq 48(%rdi), %r10
; CHECK-NEXT:    movq 56(%rdi), %r11
; CHECK-NEXT:    xorq 24(%rdi), %r8
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %r11
; CHECK-NEXT:    orq %r8, %r11
; CHECK-NEXT:    xorq 8(%rdi), %rdx
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %rax
; CHECK-NEXT:    orq %rdx, %rax
; CHECK-NEXT:    orq %r11, %rax
; CHECK-NEXT:    xorq 32(%rdi), %r9
; CHECK-NEXT:    xorq (%rdi), %rsi
; CHECK-NEXT:    orq %r9, %rsi
; CHECK-NEXT:    xorq 16(%rdi), %rcx
; CHECK-NEXT:    xorq {{[0-9]+}}(%rsp), %r10
; CHECK-NEXT:    orq %rcx, %r10
; CHECK-NEXT:    orq %rsi, %r10
; CHECK-NEXT:    orq %rax, %r10
; CHECK-NEXT:    sete %al
; CHECK-NEXT:    retq
  %a = load i512, ptr %p
  %r = icmp eq i512 %a, %b
  ret i1 %r
}

; Tests for any/allbits from memory.

define i1 @anybits_i128_load_arg(ptr %w) {
; CHECK-LABEL: anybits_i128_load_arg:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movq (%rdi), %rax
; CHECK-NEXT:    orq 8(%rdi), %rax
; CHECK-NEXT:    setne %al
; CHECK-NEXT:    retq
  %ld = load i128, ptr %w
  %cmp = icmp ne i128 %ld, 0
  ret i1 %cmp
}

define i1 @allbits_i128_load_arg(ptr %w) {
; SSE2-LABEL: allbits_i128_load_arg:
; SSE2:       # %bb.0:
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm0
; SSE2-NEXT:    pcmpeqb (%rdi), %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %eax
; SSE2-NEXT:    cmpl $65535, %eax # imm = 0xFFFF
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: allbits_i128_load_arg:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    pcmpeqd %xmm1, %xmm1
; SSE41-NEXT:    ptest %xmm1, %xmm0
; SSE41-NEXT:    setb %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: allbits_i128_load_arg:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovdqa (%rdi), %xmm0
; AVX-NEXT:    vpcmpeqd %xmm1, %xmm1, %xmm1
; AVX-NEXT:    vptest %xmm1, %xmm0
; AVX-NEXT:    setb %al
; AVX-NEXT:    retq
  %ld = load i128, ptr %w
  %cmp = icmp eq i128 %ld, -1
  ret i1 %cmp
}

define i1 @anybits_i256_load_arg(ptr %w) {
; SSE2-LABEL: anybits_i256_load_arg:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    por 16(%rdi), %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    xorl $15, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: anybits_i256_load_arg:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    por 16(%rdi), %xmm0
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX-LABEL: anybits_i256_load_arg:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovdqu (%rdi), %ymm0
; AVX-NEXT:    vptest %ymm0, %ymm0
; AVX-NEXT:    setne %al
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
  %ld = load i256, ptr %w
  %cmp = icmp ne i256 %ld, 0
  ret i1 %cmp
}

define i1 @allbits_i256_load_arg(ptr %w) {
; SSE2-LABEL: allbits_i256_load_arg:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    pand 16(%rdi), %xmm0
; SSE2-NEXT:    pcmpeqd %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    xorl $15, %eax
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: allbits_i256_load_arg:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    pand 16(%rdi), %xmm0
; SSE41-NEXT:    pcmpeqd %xmm1, %xmm1
; SSE41-NEXT:    ptest %xmm1, %xmm0
; SSE41-NEXT:    setb %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: allbits_i256_load_arg:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovdqu (%rdi), %ymm0
; AVX1-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1-NEXT:    vcmptrueps %ymm1, %ymm1, %ymm1
; AVX1-NEXT:    vptest %ymm1, %ymm0
; AVX1-NEXT:    setb %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: allbits_i256_load_arg:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vpcmpeqd %ymm1, %ymm1, %ymm1
; AVX2-NEXT:    vptest %ymm1, %ymm0
; AVX2-NEXT:    setb %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: allbits_i256_load_arg:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovdqu (%rdi), %ymm0
; AVX512-NEXT:    vpcmpeqd %ymm1, %ymm1, %ymm1
; AVX512-NEXT:    vptest %ymm1, %ymm0
; AVX512-NEXT:    setb %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ld = load i256, ptr %w
  %cmp = icmp eq i256 %ld, -1
  ret i1 %cmp
}

define i1 @anybits_i512_load_arg(ptr %w) {
; SSE2-LABEL: anybits_i512_load_arg:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    por 48(%rdi), %xmm1
; SSE2-NEXT:    por 32(%rdi), %xmm0
; SSE2-NEXT:    por %xmm1, %xmm0
; SSE2-NEXT:    pxor %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    xorl $15, %eax
; SSE2-NEXT:    setne %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: anybits_i512_load_arg:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    por 48(%rdi), %xmm1
; SSE41-NEXT:    por 32(%rdi), %xmm0
; SSE41-NEXT:    por %xmm1, %xmm0
; SSE41-NEXT:    ptest %xmm0, %xmm0
; SSE41-NEXT:    setne %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: anybits_i512_load_arg:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vorps 32(%rdi), %ymm0, %ymm0
; AVX1-NEXT:    vptest %ymm0, %ymm0
; AVX1-NEXT:    setne %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: anybits_i512_load_arg:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vpor 32(%rdi), %ymm0, %ymm0
; AVX2-NEXT:    vptest %ymm0, %ymm0
; AVX2-NEXT:    setne %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: anybits_i512_load_arg:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vmovdqu64 (%rdi), %zmm0
; AVX512-NEXT:    vptestmd %zmm0, %zmm0, %k0
; AVX512-NEXT:    kortestw %k0, %k0
; AVX512-NEXT:    setne %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ld = load i512, ptr %w
  %cmp = icmp ne i512 %ld, 0
  ret i1 %cmp
}

define i1 @allbits_i512_load_arg(ptr %w) {
; SSE2-LABEL: allbits_i512_load_arg:
; SSE2:       # %bb.0:
; SSE2-NEXT:    movdqa (%rdi), %xmm0
; SSE2-NEXT:    movdqa 16(%rdi), %xmm1
; SSE2-NEXT:    pand 48(%rdi), %xmm1
; SSE2-NEXT:    pand 32(%rdi), %xmm0
; SSE2-NEXT:    pand %xmm1, %xmm0
; SSE2-NEXT:    pcmpeqd %xmm1, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm1
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    xorl $15, %eax
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    retq
;
; SSE41-LABEL: allbits_i512_load_arg:
; SSE41:       # %bb.0:
; SSE41-NEXT:    movdqa (%rdi), %xmm0
; SSE41-NEXT:    movdqa 16(%rdi), %xmm1
; SSE41-NEXT:    pand 48(%rdi), %xmm1
; SSE41-NEXT:    pand 32(%rdi), %xmm0
; SSE41-NEXT:    pand %xmm1, %xmm0
; SSE41-NEXT:    pcmpeqd %xmm1, %xmm1
; SSE41-NEXT:    ptest %xmm1, %xmm0
; SSE41-NEXT:    setb %al
; SSE41-NEXT:    retq
;
; AVX1-LABEL: allbits_i512_load_arg:
; AVX1:       # %bb.0:
; AVX1-NEXT:    vmovups (%rdi), %ymm0
; AVX1-NEXT:    vandps 32(%rdi), %ymm0, %ymm0
; AVX1-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1-NEXT:    vcmptrueps %ymm1, %ymm1, %ymm1
; AVX1-NEXT:    vptest %ymm1, %ymm0
; AVX1-NEXT:    setb %al
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: allbits_i512_load_arg:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovdqu (%rdi), %ymm0
; AVX2-NEXT:    vpand 32(%rdi), %ymm0, %ymm0
; AVX2-NEXT:    vpcmpeqd %ymm1, %ymm1, %ymm1
; AVX2-NEXT:    vptest %ymm1, %ymm0
; AVX2-NEXT:    setb %al
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: allbits_i512_load_arg:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpternlogd {{.*#+}} zmm0 = -1
; AVX512-NEXT:    vpcmpneqd (%rdi), %zmm0, %k0
; AVX512-NEXT:    kortestw %k0, %k0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ld = load i512, ptr %w
  %cmp = icmp eq i512 %ld, -1
  ret i1 %cmp
}
