; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=i686-unknown-unknown -mattr=+sse2 | FileCheck %s --check-prefixes=X86
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx | FileCheck %s --check-prefixes=X64

;; Use cttz to test if we properly prove never-zero. There is a very
;; simple transform from cttz -> cttz_zero_undef if its operand is
;; known never zero.
declare i32 @llvm.cttz.i32(i32, i1)
declare i32 @llvm.uadd.sat.i32(i32, i32)
declare i32 @llvm.umax.i32(i32, i32)
declare i32 @llvm.umin.i32(i32, i32)
declare i32 @llvm.smin.i32(i32, i32)
declare <4 x i32> @llvm.smin.v4i32(<4 x i32>, <4 x i32>)
declare i32 @llvm.smax.i32(i32, i32)
declare <4 x i32> @llvm.smax.v4i32(<4 x i32>, <4 x i32>)
declare i32 @llvm.bswap.i32(i32)
declare i32 @llvm.bitreverse.i32(i32)
declare i32 @llvm.ctpop.i32(i32)
declare <4 x i32> @llvm.ctpop.v4i32(<4 x i32>)
declare i32 @llvm.abs.i32(i32, i1)
declare i32 @llvm.fshl.i32(i32, i32, i32)
declare i32 @llvm.fshr.i32(i32, i32, i32)

define i32 @or_known_nonzero(i32 %x) {
; X86-LABEL: or_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: or_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = or i32 %x, 1
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @or_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: or_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: or_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    orl %esi, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = or i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @select_known_nonzero(i1 %c, i32 %x) {
; X86-LABEL: select_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    testb $1, {{[0-9]+}}(%esp)
; X86-NEXT:    movl $122, %ecx
; X86-NEXT:    cmovnel %eax, %ecx
; X86-NEXT:    rep bsfl %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: select_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %esi
; X64-NEXT:    testb $1, %dil
; X64-NEXT:    movl $122, %eax
; X64-NEXT:    cmovnel %esi, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %y = or i32 %x, 1
  %z = select i1 %c, i32 %y, i32 122
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @select_maybe_zero(i1 %c, i32 %x) {
; X86-LABEL: select_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    xorl %ecx, %ecx
; X86-NEXT:    testb $1, {{[0-9]+}}(%esp)
; X86-NEXT:    cmovnel %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: select_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %esi
; X64-NEXT:    xorl %ecx, %ecx
; X64-NEXT:    testb $1, %dil
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %y = or i32 %x, 1
  %z = select i1 %c, i32 %y, i32 0
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @shl_known_nonzero_1s_bit_set(i32 %x) {
; X86-LABEL: shl_known_nonzero_1s_bit_set:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $123, %eax
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: shl_known_nonzero_1s_bit_set:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $123, %eax
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = shl i32 123, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @shl_known_nonzero_nsw(i32 %x, i32 %yy) {
; X86-LABEL: shl_known_nonzero_nsw:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: shl_known_nonzero_nsw:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = shl nsw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @shl_known_nonzero_nuw(i32 %x, i32 %yy) {
; X86-LABEL: shl_known_nonzero_nuw:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: shl_known_nonzero_nuw:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = shl nuw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @shl_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: shl_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: shl_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %z = shl nuw nsw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @uaddsat_known_nonzero(i32 %x) {
; X86-LABEL: uaddsat_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    incl %eax
; X86-NEXT:    movl $-1, %ecx
; X86-NEXT:    cmovnel %eax, %ecx
; X86-NEXT:    rep bsfl %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: uaddsat_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    incl %edi
; X64-NEXT:    movl $-1, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.uadd.sat.i32(i32 %x, i32 1)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @uaddsat_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: uaddsat_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    addl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl $-1, %ecx
; X86-NEXT:    cmovael %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: uaddsat_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    addl %esi, %edi
; X64-NEXT:    movl $-1, %ecx
; X64-NEXT:    cmovael %edi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.uadd.sat.i32(i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @umax_known_nonzero(i32 %x, i32 %y) {
; X86-LABEL: umax_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $4, %edx
; X86-NEXT:    shll %cl, %edx
; X86-NEXT:    cmpl %edx, %eax
; X86-NEXT:    cmoval %eax, %edx
; X86-NEXT:    rep bsfl %edx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: umax_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    movl $4, %eax
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    cmpl %eax, %edi
; X64-NEXT:    cmoval %edi, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %yy = shl nuw i32 4, %y
  %z = call i32 @llvm.umax.i32(i32 %x, i32 %yy)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @umax_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: umax_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    cmpl %eax, %ecx
; X86-NEXT:    cmoval %ecx, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: umax_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    cmpl %esi, %edi
; X64-NEXT:    cmoval %edi, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.umax.i32(i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @umin_known_nonzero(i32 %xx, i32 %yy) {
; X86-LABEL: umin_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $4, %edx
; X86-NEXT:    shll %cl, %edx
; X86-NEXT:    addl $4, %eax
; X86-NEXT:    cmpl %eax, %edx
; X86-NEXT:    cmovbl %edx, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: umin_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $4, %eax
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    addl $4, %esi
; X64-NEXT:    cmpl %esi, %eax
; X64-NEXT:    cmovbl %eax, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %x = shl nuw i32 4, %xx
  %y = add nuw nsw i32 %yy, 4
  %z = call i32 @llvm.umin.i32(i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @umin_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: umin_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cmpl $54, %eax
; X86-NEXT:    movl $54, %ecx
; X86-NEXT:    cmovbl %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: umin_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    cmpl $54, %edi
; X64-NEXT:    movl $54, %ecx
; X64-NEXT:    cmovbl %edi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.umin.i32(i32 %x, i32 54)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @smin_known_nonzero(i32 %xx, i32 %yy) {
; X86-LABEL: smin_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $4, %edx
; X86-NEXT:    shll %cl, %edx
; X86-NEXT:    addl $4, %eax
; X86-NEXT:    cmpl %eax, %edx
; X86-NEXT:    cmovll %edx, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smin_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $4, %eax
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    addl $4, %esi
; X64-NEXT:    cmpl %esi, %eax
; X64-NEXT:    cmovll %eax, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %x = shl nuw i32 4, %xx
  %y = add nuw nsw i32 %yy, 4
  %z = call i32 @llvm.smin.i32(i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @smin_known_zero(i32 %x, i32 %y) {
; X86-LABEL: smin_known_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cmpl $-54, %eax
; X86-NEXT:    movl $-54, %ecx
; X86-NEXT:    cmovll %eax, %ecx
; X86-NEXT:    rep bsfl %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smin_known_zero:
; X64:       # %bb.0:
; X64-NEXT:    cmpl $-54, %edi
; X64-NEXT:    movl $-54, %eax
; X64-NEXT:    cmovll %edi, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.smin.i32(i32 %x, i32 -54)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define <4 x i32> @smin_known_zero_vec(<4 x i32> %x, <4 x i32> %y) {
; X86-LABEL: smin_known_zero_vec:
; X86:       # %bb.0:
; X86-NEXT:    movdqa {{.*#+}} xmm1 = [4294967242,4294967273,4294967284,4294967295]
; X86-NEXT:    movdqa %xmm1, %xmm2
; X86-NEXT:    pcmpgtd %xmm0, %xmm2
; X86-NEXT:    pand %xmm2, %xmm0
; X86-NEXT:    pandn %xmm1, %xmm2
; X86-NEXT:    por %xmm2, %xmm0
; X86-NEXT:    pcmpeqd %xmm1, %xmm1
; X86-NEXT:    paddd %xmm0, %xmm1
; X86-NEXT:    pand %xmm1, %xmm0
; X86-NEXT:    pxor %xmm1, %xmm1
; X86-NEXT:    pcmpeqd %xmm1, %xmm0
; X86-NEXT:    psrld $31, %xmm0
; X86-NEXT:    retl
;
; X64-LABEL: smin_known_zero_vec:
; X64:       # %bb.0:
; X64-NEXT:    vpminsd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; X64-NEXT:    vpcmpeqd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vpaddd %xmm1, %xmm0, %xmm1
; X64-NEXT:    vpand %xmm1, %xmm0, %xmm0
; X64-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; X64-NEXT:    vpcmpeqd %xmm1, %xmm0, %xmm0
; X64-NEXT:    vpsrld $31, %xmm0, %xmm0
; X64-NEXT:    retq
  %z = call <4 x i32> @llvm.smin.v4i32(<4 x i32> %x, <4 x i32> <i32 -54, i32 -23, i32 -12, i32 -1>)
  %r = call <4 x i32> @llvm.ctpop.v4i32(<4 x i32> %z)
  %3 = icmp eq <4 x i32> %r, <i32 1, i32 1, i32 1, i32 1>
  %ret = zext <4 x i1> %3 to <4 x i32>
  ret <4 x i32> %ret
}

define i32 @smin_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: smin_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cmpl $54, %eax
; X86-NEXT:    movl $54, %ecx
; X86-NEXT:    cmovll %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smin_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    cmpl $54, %edi
; X64-NEXT:    movl $54, %ecx
; X64-NEXT:    cmovll %edi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.smin.i32(i32 %x, i32 54)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @smax_known_nonzero(i32 %xx, i32 %yy) {
; X86-LABEL: smax_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $4, %edx
; X86-NEXT:    shll %cl, %edx
; X86-NEXT:    addl $4, %eax
; X86-NEXT:    cmpl %eax, %edx
; X86-NEXT:    cmovgl %edx, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smax_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $4, %eax
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    addl $4, %esi
; X64-NEXT:    cmpl %esi, %eax
; X64-NEXT:    cmovgl %eax, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %x = shl nuw i32 4, %xx
  %y = add nuw nsw i32 %yy, 4
  %z = call i32 @llvm.smax.i32(i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @smax_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: smax_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cmpl $55, %eax
; X86-NEXT:    movl $54, %ecx
; X86-NEXT:    cmovgel %eax, %ecx
; X86-NEXT:    rep bsfl %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smax_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    cmpl $55, %edi
; X64-NEXT:    movl $54, %eax
; X64-NEXT:    cmovgel %edi, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.smax.i32(i32 %x, i32 54)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define <4 x i32> @smax_known_zero_vec(<4 x i32> %x, <4 x i32> %y) {
; X86-LABEL: smax_known_zero_vec:
; X86:       # %bb.0:
; X86-NEXT:    movdqa {{.*#+}} xmm1 = [54,23,12,1]
; X86-NEXT:    movdqa %xmm0, %xmm2
; X86-NEXT:    pcmpgtd %xmm1, %xmm2
; X86-NEXT:    pand %xmm2, %xmm0
; X86-NEXT:    pandn %xmm1, %xmm2
; X86-NEXT:    por %xmm2, %xmm0
; X86-NEXT:    pcmpeqd %xmm1, %xmm1
; X86-NEXT:    paddd %xmm0, %xmm1
; X86-NEXT:    pand %xmm1, %xmm0
; X86-NEXT:    pxor %xmm1, %xmm1
; X86-NEXT:    pcmpeqd %xmm1, %xmm0
; X86-NEXT:    psrld $31, %xmm0
; X86-NEXT:    retl
;
; X64-LABEL: smax_known_zero_vec:
; X64:       # %bb.0:
; X64-NEXT:    vpmaxsd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; X64-NEXT:    vpcmpeqd %xmm1, %xmm1, %xmm1
; X64-NEXT:    vpaddd %xmm1, %xmm0, %xmm1
; X64-NEXT:    vpand %xmm1, %xmm0, %xmm0
; X64-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; X64-NEXT:    vpcmpeqd %xmm1, %xmm0, %xmm0
; X64-NEXT:    vpsrld $31, %xmm0, %xmm0
; X64-NEXT:    retq
  %z = call <4 x i32> @llvm.smax.v4i32(<4 x i32> %x, <4 x i32> <i32 54, i32 23, i32 12, i32 1>)
  %r = call <4 x i32> @llvm.ctpop.v4i32(<4 x i32> %z)
  %3 = icmp eq <4 x i32> %r, <i32 1, i32 1, i32 1, i32 1>
  %ret = zext <4 x i1> %3 to <4 x i32>
  ret <4 x i32> %ret
}

define i32 @smax_known_zero(i32 %x, i32 %y) {
; X86-LABEL: smax_known_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    testl %eax, %eax
; X86-NEXT:    movl $-1, %ecx
; X86-NEXT:    cmovnsl %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: smax_known_zero:
; X64:       # %bb.0:
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    movl $-1, %ecx
; X64-NEXT:    cmovnsl %edi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.smax.i32(i32 %x, i32 -1)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotr_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: rotr_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    rorl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotr_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    orl $256, %edi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    rorl %cl, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 256
  %shr = lshr i32 %x, %y
  %sub = sub i32 32, %y
  %shl = shl i32 %x, %sub
  %z = or i32 %shl, %shr
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotr_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: rotr_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    rorl %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotr_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    rorl %cl, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %shr = lshr i32 %x, %y
  %sub = sub i32 32, %y
  %shl = shl i32 %x, %sub
  %z = or i32 %shl, %shr
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotr_with_fshr_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: rotr_with_fshr_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    rorl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotr_with_fshr_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    orl $256, %edi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    rorl %cl, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 256
  %z = call i32 @llvm.fshr.i32(i32 %x, i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotr_with_fshr_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: rotr_with_fshr_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    rorl %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotr_with_fshr_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    rorl %cl, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.fshr.i32(i32 %x, i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotl_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: rotl_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    roll %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotl_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    orl $256, %edi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    roll %cl, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 256
  %shl = shl i32 %x, %y
  %sub = sub i32 32, %y
  %shr = lshr i32 %x, %sub
  %z = or i32 %shr, %shl
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotl_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: rotl_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    roll %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotl_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    roll %cl, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %shl = shl i32 %x, %y
  %sub = sub i32 32, %y
  %shr = lshr i32 %x, %sub
  %z = or i32 %shr, %shl
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotl_with_fshl_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: rotl_with_fshl_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    roll %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotl_with_fshl_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    orl $256, %edi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    roll %cl, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 256
  %z = call i32 @llvm.fshl.i32(i32 %x, i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @rotl_with_fshl_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: rotl_with_fshl_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    roll %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: rotl_with_fshl_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %esi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    roll %cl, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = call i32 @llvm.fshl.i32(i32 %x, i32 %x, i32 %y)
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sra_known_nonzero_sign_bit_set(i32 %x) {
; X86-LABEL: sra_known_nonzero_sign_bit_set:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $-2147360405, %eax # imm = 0x8001E16B
; X86-NEXT:    sarl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sra_known_nonzero_sign_bit_set:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $-2147360405, %eax # imm = 0x8001E16B
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    sarl %cl, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = ashr i32 2147606891, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sra_known_nonzero_exact(i32 %x, i32 %yy) {
; X86-LABEL: sra_known_nonzero_exact:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    sarl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sra_known_nonzero_exact:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    sarl %cl, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = ashr exact i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sra_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: sra_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    sarl %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sra_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    sarl %cl, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %z = ashr exact i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @srl_known_nonzero_sign_bit_set(i32 %x) {
; X86-LABEL: srl_known_nonzero_sign_bit_set:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $-2147360405, %eax # imm = 0x8001E16B
; X86-NEXT:    shrl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: srl_known_nonzero_sign_bit_set:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $-2147360405, %eax # imm = 0x8001E16B
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shrl %cl, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %z = lshr i32 2147606891, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @srl_known_nonzero_exact(i32 %x, i32 %yy) {
; X86-LABEL: srl_known_nonzero_exact:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shrl %cl, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: srl_known_nonzero_exact:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shrl %cl, %esi
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = lshr exact i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @srl_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: srl_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shrl %cl, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: srl_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shrl %cl, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %z = lshr exact i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: udiv_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $64, %eax
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl {{[0-9]+}}(%esp)
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $64, %eax
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %esi
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 64
  %z = udiv exact i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: udiv_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl {{[0-9]+}}(%esp)
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %esi
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %z = udiv exact i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: sdiv_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $64, %eax
; X86-NEXT:    cltd
; X86-NEXT:    idivl {{[0-9]+}}(%esp)
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $64, %eax
; X64-NEXT:    cltd
; X64-NEXT:    idivl %esi
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 64
  %z = sdiv exact i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: sdiv_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cltd
; X86-NEXT:    idivl {{[0-9]+}}(%esp)
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    cltd
; X64-NEXT:    idivl %esi
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %z = sdiv exact i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @add_known_nonzero(i32 %xx, i32 %y) {
; X86-LABEL: add_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    addl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: add_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %edi
; X64-NEXT:    addl %esi, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 1
  %z = add nuw i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @add_maybe_zero(i32 %xx, i32 %y) {
; X86-LABEL: add_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    addl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: add_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %edi
; X64-NEXT:    addl %esi, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 1
  %z = add nsw i32 %x, %y
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sub_known_nonzero_neg_case(i32 %xx) {
; X86-LABEL: sub_known_nonzero_neg_case:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    negl %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sub_known_nonzero_neg_case:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $256, %eax # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    negl %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = shl nuw nsw i32 256, %xx
  %z = sub i32 0, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sub_known_nonzero_ne_case(i32 %xx, i32 %yy) {
; X86-LABEL: sub_known_nonzero_ne_case:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl %eax, %ecx
; X86-NEXT:    orl $64, %ecx
; X86-NEXT:    andl $-65, %eax
; X86-NEXT:    subl %ecx, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sub_known_nonzero_ne_case:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $64, %eax
; X64-NEXT:    andl $-65, %edi
; X64-NEXT:    subl %eax, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %x = or i32 %xx, 64
  %y = and i32 %xx, -65
  %z = sub i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sub_maybe_zero(i32 %x) {
; X86-LABEL: sub_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl %eax, %ecx
; X86-NEXT:    orl $64, %ecx
; X86-NEXT:    subl %eax, %ecx
; X86-NEXT:    bsfl %ecx, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sub_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    orl $64, %ecx
; X64-NEXT:    subl %edi, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %y = or i32 %x, 64
  %z = sub i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sub_maybe_zero2(i32 %x) {
; X86-LABEL: sub_maybe_zero2:
; X86:       # %bb.0:
; X86-NEXT:    xorl %eax, %eax
; X86-NEXT:    subl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sub_maybe_zero2:
; X64:       # %bb.0:
; X64-NEXT:    negl %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = sub i32 0, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @mul_known_nonzero_nsw(i32 %x, i32 %yy) {
; X86-LABEL: mul_known_nonzero_nsw:
; X86:       # %bb.0:
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    imull {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: mul_known_nonzero_nsw:
; X64:       # %bb.0:
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    imull %edi, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = mul nsw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @mul_known_nonzero_nuw(i32 %x, i32 %yy) {
; X86-LABEL: mul_known_nonzero_nuw:
; X86:       # %bb.0:
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    imull {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: mul_known_nonzero_nuw:
; X64:       # %bb.0:
; X64-NEXT:    orl $256, %esi # imm = 0x100
; X64-NEXT:    imull %edi, %esi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %esi, %eax
; X64-NEXT:    retq
  %y = or i32 %yy, 256
  %z = mul nuw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @mul_maybe_zero(i32 %x, i32 %y) {
; X86-LABEL: mul_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    imull {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: mul_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    imull %esi, %edi
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %z = mul nuw nsw i32 %y, %x
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @bitcast_known_nonzero(<2 x i16> %xx) {
; X86-LABEL: bitcast_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    punpcklwd {{.*#+}} xmm0 = xmm0[0,0,1,1,2,2,3,3]
; X86-NEXT:    pslld $23, %xmm0
; X86-NEXT:    paddd {{\.?LCPI[0-9]+_[0-9]+}}, %xmm0
; X86-NEXT:    cvttps2dq %xmm0, %xmm0
; X86-NEXT:    pshuflw {{.*#+}} xmm0 = xmm0[0,2,2,3,4,5,6,7]
; X86-NEXT:    pmullw {{\.?LCPI[0-9]+_[0-9]+}}, %xmm0 # [256,256,u,u,u,u,u,u]
; X86-NEXT:    movd %xmm0, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: bitcast_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; X64-NEXT:    vpslld $23, %xmm0, %xmm0
; X64-NEXT:    vpaddd {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; X64-NEXT:    vcvttps2dq %xmm0, %xmm0
; X64-NEXT:    vpackusdw %xmm0, %xmm0, %xmm0
; X64-NEXT:    vpmullw {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0 # [256,256,u,u,u,u,u,u]
; X64-NEXT:    vmovd %xmm0, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %x = shl nuw nsw <2 x i16> <i16 256, i16 256>, %xx
  %z = bitcast <2 x i16> %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @bitcast_maybe_zero(<2 x i16> %x) {
; X86-LABEL: bitcast_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movd %xmm0, %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: bitcast_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    vmovd %xmm0, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = bitcast <2 x i16> %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @bitcast_from_float(float %x) {
; X86-LABEL: bitcast_from_float:
; X86:       # %bb.0:
; X86-NEXT:    bsfl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: bitcast_from_float:
; X64:       # %bb.0:
; X64-NEXT:    vmovd %xmm0, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = bitcast float %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @zext_known_nonzero(i16 %xx) {
; X86-LABEL: zext_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    movzwl %ax, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: zext_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $256, %eax # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    movzwl %ax, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = shl nuw nsw i16 256, %xx
  %z = zext i16 %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @zext_maybe_zero(i16 %x) {
; X86-LABEL: zext_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movzwl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: zext_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movzwl %di, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = zext i16 %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sext_known_nonzero(i16 %xx) {
; X86-LABEL: sext_known_nonzero:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    shll %cl, %eax
; X86-NEXT:    movzwl %ax, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sext_known_nonzero:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %ecx
; X64-NEXT:    movl $256, %eax # imm = 0x100
; X64-NEXT:    # kill: def $cl killed $cl killed $ecx
; X64-NEXT:    shll %cl, %eax
; X64-NEXT:    movzwl %ax, %eax
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x = shl nuw nsw i16 256, %xx
  %z = sext i16 %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sext_maybe_zero(i16 %x) {
; X86-LABEL: sext_maybe_zero:
; X86:       # %bb.0:
; X86-NEXT:    movswl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sext_maybe_zero:
; X64:       # %bb.0:
; X64-NEXT:    movswl %di, %ecx
; X64-NEXT:    movl $32, %eax
; X64-NEXT:    rep bsfl %ecx, %eax
; X64-NEXT:    retq
  %z = sext i16 %x to i32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_dividend_gt_divisor(i32 %x, i32 %y) {
; X86-LABEL: udiv_dividend_gt_divisor:
; X86:       # %bb.0:
; X86-NEXT:    pushl %edi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 12
; X86-NEXT:    .cfi_offset %esi, -12
; X86-NEXT:    .cfi_offset %edi, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    cmpl $32, %eax
; X86-NEXT:    setae %dl
; X86-NEXT:    cmpl $16, %ecx
; X86-NEXT:    setb %dh
; X86-NEXT:    testb %dh, %dl
; X86-NEXT:    movl $32, %esi
; X86-NEXT:    cmovel %esi, %eax
; X86-NEXT:    movl $16, %edi
; X86-NEXT:    cmovnel %ecx, %edi
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %edi
; X86-NEXT:    bsfl %eax, %eax
; X86-NEXT:    cmovel %esi, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    popl %edi
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: udiv_dividend_gt_divisor:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    cmpl $32, %edi
; X64-NEXT:    setae %cl
; X64-NEXT:    cmpl $16, %esi
; X64-NEXT:    setb %dl
; X64-NEXT:    testb %dl, %cl
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    cmovel %ecx, %eax
; X64-NEXT:    movl $16, %edi
; X64-NEXT:    cmovnel %esi, %edi
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %edi
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %cond1 = icmp uge i32 %x, 32
  %cond2 = icmp ult i32 %y, 16
  %both = and i1 %cond1, %cond2
  %dividend = select i1 %both, i32 %x, i32 32
  %divisor = select i1 %both, i32 %y, i32 16
  %z = udiv i32 %dividend, %divisor
  ; If dividend >= 32 and divisor < 16, then result is >= 2, so never zero
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_exact_never_zero(i32 %x) {
; X86-LABEL: udiv_exact_never_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl $256, %eax # imm = 0x100
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    shrl $3, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_exact_never_zero:
; X64:       # %bb.0:
; X64-NEXT:    orl $256, %edi # imm = 0x100
; X64-NEXT:    shrl $3, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %dividend = or i32 %x, 256
  %z = udiv exact i32 %dividend, 8
  ; If the dividend is a multiple of 8 and at least 256, the result is at least 32
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_different_signs(i32 %x, i32 %y) {
; X86-LABEL: sdiv_different_signs:
; X86:       # %bb.0:
; X86-NEXT:    movl $128, %eax
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    movl %eax, %ecx
; X86-NEXT:    negl %ecx
; X86-NEXT:    cltd
; X86-NEXT:    idivl %ecx
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_different_signs:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $128, %eax
; X64-NEXT:    movl %eax, %ecx
; X64-NEXT:    negl %ecx
; X64-NEXT:    cltd
; X64-NEXT:    idivl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %positive = or i32 %x, 128
  %negative = sub i32 0, %positive
  %z = sdiv i32 %positive, %negative
  ; If the operands have different signs and both are non-zero, the result is negative and non-zero
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_exact_never_zero(i32 %x) {
; X86-LABEL: sdiv_exact_never_zero:
; X86:       # %bb.0:
; X86-NEXT:    movl $512, %eax # imm = 0x200
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    sarl $2, %eax
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_exact_never_zero:
; X64:       # %bb.0:
; X64-NEXT:    orl $512, %edi # imm = 0x200
; X64-NEXT:    sarl $2, %edi
; X64-NEXT:    rep bsfl %edi, %eax
; X64-NEXT:    retq
  %dividend = or i32 %x, 512
  %z = sdiv exact i32 %dividend, 4
  ; If the dividend is a multiple of 4 and at least 512, the result is at least 128
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_dividend_higher_bits(i32 %x) {
; X86-LABEL: udiv_dividend_higher_bits:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl %ecx, %eax
; X86-NEXT:    orl $65536, %eax # imm = 0x10000
; X86-NEXT:    orl $1, %ecx
; X86-NEXT:    movzbl %cl, %ecx
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %ecx
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_dividend_higher_bits:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $65536, %eax # imm = 0x10000
; X64-NEXT:    orl $1, %edi
; X64-NEXT:    movzbl %dil, %ecx
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %dividend = or i32 %x, 65536
  %divisor = and i32 %x, 255
  %divisor_safe = or i32 %divisor, 1 ; ensure divisor is at least 1
  %z = udiv i32 %dividend, %divisor_safe
  ; If dividend has bits set higher than any bit in the divisor, the result must be non-zero
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_known_nonzero_gt(i32 %x, i32 %y) {
; X86-LABEL: udiv_known_nonzero_gt:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebx
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 12
; X86-NEXT:    .cfi_offset %esi, -12
; X86-NEXT:    .cfi_offset %ebx, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    cmpl %ecx, %edx
; X86-NEXT:    seta %al
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    setne %ah
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    setne %bl
; X86-NEXT:    andb %ah, %bl
; X86-NEXT:    testb %al, %bl
; X86-NEXT:    movl $64, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    movl $32, %esi
; X86-NEXT:    cmovel %esi, %ecx
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %ecx
; X86-NEXT:    bsfl %eax, %eax
; X86-NEXT:    cmovel %esi, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    popl %ebx
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: udiv_known_nonzero_gt:
; X64:       # %bb.0:
; X64-NEXT:    cmpl %esi, %edi
; X64-NEXT:    seta %al
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    setne %cl
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    setne %dl
; X64-NEXT:    andb %cl, %dl
; X64-NEXT:    testb %al, %dl
; X64-NEXT:    movl $64, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    cmovel %ecx, %esi
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %esi
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %gt = icmp ugt i32 %x, %y    ; Ensure x > y
  %x_gt_0 = icmp ne i32 %x, 0  ; Ensure x > 0
  %y_gt_0 = icmp ne i32 %y, 0  ; Ensure y > 0
  %cond1 = and i1 %gt, %x_gt_0
  %cond = and i1 %cond1, %y_gt_0
  %dividend = select i1 %cond, i32 %x, i32 64
  %divisor = select i1 %cond, i32 %y, i32 32
  %z = udiv i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_exact_known_nonzero_dividend(i32 %x, i32 %y) {
; X86-LABEL: udiv_exact_known_nonzero_dividend:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %eax
; X86-NEXT:    orl $1, %ecx
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %ecx
; X86-NEXT:    rep bsfl %eax, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_exact_known_nonzero_dividend:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $1, %eax
; X64-NEXT:    orl $1, %esi
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %esi
; X64-NEXT:    rep bsfl %eax, %eax
; X64-NEXT:    retq
  %x_nz = or i32 %x, 1
  ; Make y variable but non-zero
  %y_nz = or i32 %y, 1
  %z = udiv exact i32 %x_nz, %y_nz
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_known_nonzero_diff_signs(i32 %a, i32 %b) {
  ; Ensure a is positive by making it >= 1
; X86-LABEL: sdiv_known_nonzero_diff_signs:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    orl $1, %ecx
; X86-NEXT:    movl $-2147483648, %edx # imm = 0x80000000
; X86-NEXT:    orl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    negl %edx
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    setg %al
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    sets %ah
; X86-NEXT:    testb %ah, %al
; X86-NEXT:    movl $10, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    movl $-5, %ecx
; X86-NEXT:    cmovnel %edx, %ecx
; X86-NEXT:    cltd
; X86-NEXT:    idivl %ecx
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_known_nonzero_diff_signs:
; X64:       # %bb.0:
; X64-NEXT:    orl $1, %edi
; X64-NEXT:    orl $-2147483648, %esi # imm = 0x80000000
; X64-NEXT:    negl %esi
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    setg %al
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    sets %cl
; X64-NEXT:    testb %cl, %al
; X64-NEXT:    movl $10, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    movl $-5, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    cltd
; X64-NEXT:    idivl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %a_pos = or i32 %a, 1
  ; Ensure b is negative by making it <= -1
  %b_neg = or i32 %b, -2147483648  ; Set the sign bit
  %b_neg2 = sub i32 0, %b_neg      ; Force negative value

  ; Create a condition to select the correct values
  %a_is_pos = icmp sgt i32 %a_pos, 0
  %b_is_neg = icmp slt i32 %b_neg2, 0
  %diff_signs = and i1 %a_is_pos, %b_is_neg

  ; Select the dividend and divisor based on the condition
  %dividend = select i1 %diff_signs, i32 %a_pos, i32 10
  %divisor = select i1 %diff_signs, i32 %b_neg2, i32 -5

  %z = sdiv i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @sdiv_exact_known_nonzero_dividend(i32 %x, i32 %y) {
; X86-LABEL: sdiv_exact_known_nonzero_dividend:
; X86:       # %bb.0:
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    movl $16, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    orl $1, %ecx
; X86-NEXT:    cltd
; X86-NEXT:    idivl %ecx
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_exact_known_nonzero_dividend:
; X64:       # %bb.0:
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    movl $16, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    orl $1, %esi
; X64-NEXT:    cltd
; X64-NEXT:    idivl %esi
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %cond = icmp eq i32 %x, 0
  %x_nz = select i1 %cond, i32 16, i32 %x
  %y_nz = or i32 %y, 1
  %z = sdiv exact i32 %x_nz, %y_nz
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

define i32 @udiv_known_nonzero_higher_bits(i32 %x, i32 %y) {
; X86-LABEL: udiv_known_nonzero_higher_bits:
; X86:       # %bb.0:
; X86-NEXT:    movzbl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl $65536, %eax # imm = 0x10000
; X86-NEXT:    orl {{[0-9]+}}(%esp), %eax
; X86-NEXT:    orl $1, %ecx
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %ecx
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    retl
;
; X64-LABEL: udiv_known_nonzero_higher_bits:
; X64:       # %bb.0:
; X64-NEXT:    movl %edi, %eax
; X64-NEXT:    orl $65536, %eax # imm = 0x10000
; X64-NEXT:    orl $1, %esi
; X64-NEXT:    movzbl %sil, %ecx
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %x_high = or i32 %x, 65536
  %y_low = and i32 %y, 255
  %y_safe = or i32 %y_low, 1
  %z = udiv i32 %x_high, %y_safe
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

; Test the exact condition for UDIV: dividend > divisor and both > 0
define i32 @udiv_improved_condition(i32 %x, i32 %y) {
; X86-LABEL: udiv_improved_condition:
; X86:       # %bb.0:
; X86-NEXT:    pushl %ebx
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 12
; X86-NEXT:    .cfi_offset %esi, -12
; X86-NEXT:    .cfi_offset %ebx, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    cmpl %ecx, %edx
; X86-NEXT:    seta %al
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    setne %ah
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    setne %bl
; X86-NEXT:    andb %ah, %bl
; X86-NEXT:    testb %al, %bl
; X86-NEXT:    movl $4, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    movl $1, %esi
; X86-NEXT:    cmovnel %ecx, %esi
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %esi
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    popl %ebx
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: udiv_improved_condition:
; X64:       # %bb.0:
; X64-NEXT:    cmpl %esi, %edi
; X64-NEXT:    seta %al
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    setne %cl
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    setne %dl
; X64-NEXT:    andb %cl, %dl
; X64-NEXT:    testb %al, %dl
; X64-NEXT:    movl $4, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    movl $1, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %is_x_gt_y = icmp ugt i32 %x, %y
  %is_x_gt_0 = icmp ne i32 %x, 0
  %is_y_gt_0 = icmp ne i32 %y, 0
  %all_true = and i1 %is_x_gt_y, %is_x_gt_0
  %all_cond = and i1 %all_true, %is_y_gt_0

  ; Force the exact condition: Op0 > Op1 && Op0 > 0 && Op1 > 0
  %dividend = select i1 %all_cond, i32 %x, i32 4
  %divisor = select i1 %all_cond, i32 %y, i32 1

  %z = udiv i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

; Test that SDIV with different signs produces a non-zero result
define i32 @sdiv_different_signs_improved(i32 %x, i32 %y) {
  ; Ensure x is positive
; X86-LABEL: sdiv_different_signs_improved:
; X86:       # %bb.0:
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %esi, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    setg %al
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    sets %ah
; X86-NEXT:    testb %ah, %al
; X86-NEXT:    movl $1, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    movl $-1, %esi
; X86-NEXT:    cmovnel %ecx, %esi
; X86-NEXT:    cltd
; X86-NEXT:    idivl %esi
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_different_signs_improved:
; X64:       # %bb.0:
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    setg %al
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    sets %cl
; X64-NEXT:    testb %cl, %al
; X64-NEXT:    movl $1, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    movl $-1, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    cltd
; X64-NEXT:    idivl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %x_is_pos = icmp sgt i32 %x, 0
  ; Ensure y is negative
  %y_is_neg = icmp slt i32 %y, 0
  ; Both conditions must be true
  %opposite_signs = and i1 %x_is_pos, %y_is_neg

  ; When signs are different, use inputs; otherwise use constants
  %dividend = select i1 %opposite_signs, i32 %x, i32 1
  %divisor = select i1 %opposite_signs, i32 %y, i32 -1

  %z = sdiv i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

; Test SDIV with exact flag and non-zero dividend
define i32 @sdiv_exact_nonzero_dividend(i32 %x, i32 %y) {
  ; Ensure x is non-zero
; X86-LABEL: sdiv_exact_nonzero_dividend:
; X86:       # %bb.0:
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %esi, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    movl $128, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    movl $4, %esi
; X86-NEXT:    cmovnel %ecx, %esi
; X86-NEXT:    cltd
; X86-NEXT:    idivl %esi
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: sdiv_exact_nonzero_dividend:
; X64:       # %bb.0:
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    movl $128, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    movl $4, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    cltd
; X64-NEXT:    idivl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %x_is_nz = icmp ne i32 %x, 0
  %dividend = select i1 %x_is_nz, i32 %x, i32 128

  ; Ensure y is non-zero
  %y_is_nz = icmp ne i32 %y, 0
  %divisor = select i1 %y_is_nz, i32 %y, i32 4

  %z = sdiv exact i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

; Test UDIV with exact flag and non-zero dividend
define i32 @udiv_exact_nonzero_dividend(i32 %x, i32 %y) {
  ; Ensure x is non-zero
; X86-LABEL: udiv_exact_nonzero_dividend:
; X86:       # %bb.0:
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %esi, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    testl %edx, %edx
; X86-NEXT:    movl $128, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    movl $4, %esi
; X86-NEXT:    cmovnel %ecx, %esi
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %esi
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: udiv_exact_nonzero_dividend:
; X64:       # %bb.0:
; X64-NEXT:    testl %edi, %edi
; X64-NEXT:    movl $128, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    movl $4, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %x_is_nz = icmp ne i32 %x, 0
  %dividend = select i1 %x_is_nz, i32 %x, i32 128

  ; Ensure y is non-zero
  %y_is_nz = icmp ne i32 %y, 0
  %divisor = select i1 %y_is_nz, i32 %y, i32 4

  %z = udiv exact i32 %dividend, %divisor
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}

; Test the exact condition we added:
; "If Op0 >= Op1, then the result is at least 1, and therefore not 0."
define i32 @udiv_dividend_ge_divisor(i32 %x, i32 %y) {
  ; Create a condition where x >= y
; X86-LABEL: udiv_dividend_ge_divisor:
; X86:       # %bb.0:
; X86-NEXT:    pushl %esi
; X86-NEXT:    .cfi_def_cfa_offset 8
; X86-NEXT:    .cfi_offset %esi, -8
; X86-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-NEXT:    cmpl %ecx, %edx
; X86-NEXT:    setae %al
; X86-NEXT:    testl %ecx, %ecx
; X86-NEXT:    setne %ah
; X86-NEXT:    testb %ah, %al
; X86-NEXT:    movl $10, %eax
; X86-NEXT:    cmovnel %edx, %eax
; X86-NEXT:    movl $20, %esi
; X86-NEXT:    cmovnel %ecx, %esi
; X86-NEXT:    xorl %edx, %edx
; X86-NEXT:    divl %esi
; X86-NEXT:    bsfl %eax, %ecx
; X86-NEXT:    movl $32, %eax
; X86-NEXT:    cmovnel %ecx, %eax
; X86-NEXT:    popl %esi
; X86-NEXT:    .cfi_def_cfa_offset 4
; X86-NEXT:    retl
;
; X64-LABEL: udiv_dividend_ge_divisor:
; X64:       # %bb.0:
; X64-NEXT:    cmpl %esi, %edi
; X64-NEXT:    setae %al
; X64-NEXT:    testl %esi, %esi
; X64-NEXT:    setne %cl
; X64-NEXT:    testb %cl, %al
; X64-NEXT:    movl $10, %eax
; X64-NEXT:    cmovnel %edi, %eax
; X64-NEXT:    movl $20, %ecx
; X64-NEXT:    cmovnel %esi, %ecx
; X64-NEXT:    xorl %edx, %edx
; X64-NEXT:    divl %ecx
; X64-NEXT:    movl $32, %ecx
; X64-NEXT:    rep bsfl %eax, %ecx
; X64-NEXT:    movl %ecx, %eax
; X64-NEXT:    retq
  %is_x_ge_y = icmp uge i32 %x, %y

  ; Ensure y is not zero to avoid division by zero
  %is_y_nz = icmp ne i32 %y, 0

  ; Both conditions must be satisfied
  %valid_div = and i1 %is_x_ge_y, %is_y_nz

  ; When the condition is true, use x and y; otherwise use constants
  %dividend = select i1 %valid_div, i32 %x, i32 10
  %divisor = select i1 %valid_div, i32 %y, i32 20

  ; This division must produce a non-zero result when dividend >= divisor
  %z = udiv i32 %dividend, %divisor

  ; CTTZ is used to check if the compiler knows the result is never zero
  ; If it knows, it will use rep bsf instead of conditional cmovne
  %r = call i32 @llvm.cttz.i32(i32 %z, i1 false)
  ret i32 %r
}
