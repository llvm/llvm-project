; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -mtriple=x86_64-unknown-linux-gnu -mattr=+sse4.1 < %s | FileCheck %s --check-prefix=SSE
; RUN: llc -mtriple=x86_64-unknown-linux-gnu -mattr=+avx2 < %s | FileCheck %s --check-prefix=AVX

; Test that @llvm.speculative.load is lowered to a regular load
; in SelectionDAG.

define <4 x i32> @speculative_load_v4i32(ptr %ptr) {
; SSE-LABEL: speculative_load_v4i32:
; SSE:       # %bb.0:
; SSE-NEXT:    movaps (%rdi), %xmm0
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_v4i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovaps (%rdi), %xmm0
; AVX-NEXT:    retq
  %load = call <4 x i32> @llvm.speculative.load.v4i32.p0(ptr align 16 %ptr)
  ret <4 x i32> %load
}

define <8 x i32> @speculative_load_v8i32(ptr %ptr) {
; SSE-LABEL: speculative_load_v8i32:
; SSE:       # %bb.0:
; SSE-NEXT:    movaps (%rdi), %xmm0
; SSE-NEXT:    movaps 16(%rdi), %xmm1
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovaps (%rdi), %ymm0
; AVX-NEXT:    retq
  %load = call <8 x i32> @llvm.speculative.load.v8i32.p0(ptr align 32 %ptr)
  ret <8 x i32> %load
}

define <2 x i64> @speculative_load_v2i64(ptr %ptr) {
; SSE-LABEL: speculative_load_v2i64:
; SSE:       # %bb.0:
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_v2i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovups (%rdi), %xmm0
; AVX-NEXT:    retq
  %load = call <2 x i64> @llvm.speculative.load.v2i64.p0(ptr %ptr)
  ret <2 x i64> %load
}

define <4 x float> @speculative_load_v4f32(ptr %ptr) {
; SSE-LABEL: speculative_load_v4f32:
; SSE:       # %bb.0:
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_v4f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovups (%rdi), %xmm0
; AVX-NEXT:    retq
  %load = call <4 x float> @llvm.speculative.load.v4f32.p0(ptr align 8 %ptr)
  ret <4 x float> %load
}

define <2 x double> @speculative_load_v2f64(ptr %ptr) {
; SSE-LABEL: speculative_load_v2f64:
; SSE:       # %bb.0:
; SSE-NEXT:    movaps (%rdi), %xmm0
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_v2f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovaps (%rdi), %xmm0
; AVX-NEXT:    retq
  %load = call <2 x double> @llvm.speculative.load.v2f64.p0(ptr align 16 %ptr)
  ret <2 x double> %load
}

declare <4 x i32> @llvm.speculative.load.v4i32.p0(ptr)
declare <8 x i32> @llvm.speculative.load.v8i32.p0(ptr)
declare <2 x i64> @llvm.speculative.load.v2i64.p0(ptr)
declare <4 x float> @llvm.speculative.load.v4f32.p0(ptr)
declare <2 x double> @llvm.speculative.load.v2f64.p0(ptr)

; Scalar type tests

define i32 @speculative_load_i32(ptr %ptr) {
; SSE-LABEL: speculative_load_i32:
; SSE:       # %bb.0:
; SSE-NEXT:    movl (%rdi), %eax
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_i32:
; AVX:       # %bb.0:
; AVX-NEXT:    movl (%rdi), %eax
; AVX-NEXT:    retq
  %load = call i32 @llvm.speculative.load.i32.p0(ptr %ptr)
  ret i32 %load
}

define i64 @speculative_load_i64(ptr %ptr) {
; SSE-LABEL: speculative_load_i64:
; SSE:       # %bb.0:
; SSE-NEXT:    movq (%rdi), %rax
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_i64:
; AVX:       # %bb.0:
; AVX-NEXT:    movq (%rdi), %rax
; AVX-NEXT:    retq
  %load = call i64 @llvm.speculative.load.i64.p0(ptr align 8 %ptr)
  ret i64 %load
}

define float @speculative_load_f32(ptr %ptr) {
; SSE-LABEL: speculative_load_f32:
; SSE:       # %bb.0:
; SSE-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; AVX-NEXT:    retq
  %load = call float @llvm.speculative.load.f32.p0(ptr align 4 %ptr)
  ret float %load
}

define double @speculative_load_f64(ptr %ptr) {
; SSE-LABEL: speculative_load_f64:
; SSE:       # %bb.0:
; SSE-NEXT:    movsd {{.*#+}} xmm0 = mem[0],zero
; SSE-NEXT:    retq
;
; AVX-LABEL: speculative_load_f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; AVX-NEXT:    retq
  %load = call double @llvm.speculative.load.f64.p0(ptr %ptr)
  ret double %load
}

declare i32 @llvm.speculative.load.i32.p0(ptr)
declare i64 @llvm.speculative.load.i64.p0(ptr)
declare float @llvm.speculative.load.f32.p0(ptr)
declare double @llvm.speculative.load.f64.p0(ptr)
