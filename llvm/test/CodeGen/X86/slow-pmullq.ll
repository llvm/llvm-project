; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mcpu=cannonlake | FileCheck %s --check-prefix=CNL
; RUN: llc < %s -mtriple=x86_64-unknown-linux-gnu -mcpu=cannonlake -mattr=-avx512vl | FileCheck %s --check-prefix=NOVLX

; ============================================================================
; Case 1: 52-bit Optimization (vpmadd52luq)
; ============================================================================

define <8 x i64> @test_mul_52bit_fits(<8 x i64> %a, <8 x i64> %b) {
; CNL-LABEL: test_mul_52bit_fits:
; CNL:       # %bb.0:
; CNL-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; CNL-NEXT:    vpmadd52luq %zmm1, %zmm0, %zmm2
; CNL-NEXT:    vmovdqa64 %zmm2, %zmm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_52bit_fits:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; NOVLX-NEXT:    vpmadd52luq %zmm1, %zmm0, %zmm2
; NOVLX-NEXT:    vmovdqa64 %zmm2, %zmm0
; NOVLX-NEXT:    retq
  %a_masked = and <8 x i64> %a,  splat (i64 4503599627370495)
  %b_masked = and <8 x i64> %b,  splat (i64 4503599627370495)

  %res = mul <8 x i64> %a_masked, %b_masked
  ret <8 x i64> %res
}

; ============================================================================
; Case 1.5: Non-constant test (using Logical Shift Right to clear high bits)
; ============================================================================

define <8 x i64> @test_mul_shift_high_bits(<8 x i64> %a, <8 x i64> %b) {
; CNL-LABEL: test_mul_shift_high_bits:
; CNL:       # %bb.0:
; CNL-NEXT:    vpsrlq $12, %zmm0, %zmm2
; CNL-NEXT:    vpsrlq $12, %zmm1, %zmm1
; CNL-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; CNL-NEXT:    vpmadd52luq %zmm1, %zmm2, %zmm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_shift_high_bits:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpsrlq $12, %zmm0, %zmm2
; NOVLX-NEXT:    vpsrlq $12, %zmm1, %zmm1
; NOVLX-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; NOVLX-NEXT:    vpmadd52luq %zmm1, %zmm2, %zmm0
; NOVLX-NEXT:    retq
  %a_shifted = lshr <8 x i64> %a, <i64 12, i64 12, i64 12, i64 12, i64 12, i64 12, i64 12, i64 12>
  %b_shifted = lshr <8 x i64> %b, <i64 12, i64 12, i64 12, i64 12, i64 12, i64 12, i64 12, i64 12>

  %res = mul <8 x i64> %a_shifted, %b_shifted
  ret <8 x i64> %res
}

; ============================================================================
; Case 2: 32-bit Optimization (vpmuludq)
; ============================================================================

define <8 x i64> @test_mul_32bit_fits(<8 x i64> %a, <8 x i64> %b) {
; CNL-LABEL: test_mul_32bit_fits:
; CNL:       # %bb.0:
; CNL-NEXT:    vpmuludq %zmm1, %zmm0, %zmm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_32bit_fits:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpmuludq %zmm1, %zmm0, %zmm0
; NOVLX-NEXT:    retq

  %a_masked = and <8 x i64> %a,  splat (i64 4294967295)
  %b_masked = and <8 x i64> %b,  splat (i64 4294967295)

  %res = mul <8 x i64> %a_masked, %b_masked
  ret <8 x i64> %res
}

; ============================================================================
; Case 3: No Optimization (Full 64-bit)
; ============================================================================

define <8 x i64> @test_mul_full_64bit(<8 x i64> %a, <8 x i64> %b) {
; CNL-LABEL: test_mul_full_64bit:
; CNL:       # %bb.0:
; CNL-NEXT:    vpmullq %zmm1, %zmm0, %zmm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_full_64bit:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpmullq %zmm1, %zmm0, %zmm0
; NOVLX-NEXT:    retq
  %res = mul <8 x i64> %a, %b
  ret <8 x i64> %res
}

; ============================================================================
; Case 4: Vector Width Variety (Check 256-bit / YMM)
; ============================================================================

define <4 x i64> @test_mul_52bit_ymm(<4 x i64> %a, <4 x i64> %b) {
; CNL-LABEL: test_mul_52bit_ymm:
; CNL:       # %bb.0:
; CNL-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; CNL-NEXT:    vpmadd52luq %ymm1, %ymm0, %ymm2
; CNL-NEXT:    vmovdqa %ymm2, %ymm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_52bit_ymm:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpbroadcastq {{.*#+}} ymm2 = [4503599627370495,4503599627370495,4503599627370495,4503599627370495]
; NOVLX-NEXT:    vpand %ymm2, %ymm0, %ymm0
; NOVLX-NEXT:    vpand %ymm2, %ymm1, %ymm1
; NOVLX-NEXT:    vpmullq %zmm1, %zmm0, %zmm0
; NOVLX-NEXT:    # kill: def $ymm0 killed $ymm0 killed $zmm0
; NOVLX-NEXT:    retq

  %a_masked = and <4 x i64> %a,  splat (i64 4503599627370495)
  %b_masked = and <4 x i64> %b,  splat (i64 4503599627370495)

  %res = mul <4 x i64> %a_masked, %b_masked
  ret <4 x i64> %res
}

; ============================================================================
; Case 1.5: 32-bit Signed Optimization (vpmuldq)
; ============================================================================

define <8 x i64> @test_mul_32bit_signed(<8 x i32> %a, <8 x i32> %b) {
; CNL-LABEL: test_mul_32bit_signed:
; CNL:       # %bb.0:
; CNL-NEXT:    vpmovzxdq {{.*#+}} zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero
; CNL-NEXT:    vpmovzxdq {{.*#+}} zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero
; CNL-NEXT:    vpmuldq %zmm1, %zmm0, %zmm0
; CNL-NEXT:    retq
;
; NOVLX-LABEL: test_mul_32bit_signed:
; NOVLX:       # %bb.0:
; NOVLX-NEXT:    vpmovzxdq {{.*#+}} zmm0 = ymm0[0],zero,ymm0[1],zero,ymm0[2],zero,ymm0[3],zero,ymm0[4],zero,ymm0[5],zero,ymm0[6],zero,ymm0[7],zero
; NOVLX-NEXT:    vpmovzxdq {{.*#+}} zmm1 = ymm1[0],zero,ymm1[1],zero,ymm1[2],zero,ymm1[3],zero,ymm1[4],zero,ymm1[5],zero,ymm1[6],zero,ymm1[7],zero
; NOVLX-NEXT:    vpmuldq %zmm1, %zmm0, %zmm0
; NOVLX-NEXT:    retq
  %a_ = sext <8 x i32> %a to <8 x i64>
  %b_ = sext <8 x i32> %b to <8 x i64>

  %res = mul <8 x i64> %a_, %b_
  ret <8 x i64> %res
}
