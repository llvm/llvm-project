; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v2 | FileCheck %s --check-prefixes=CHECK,NOBMI
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v3 | FileCheck %s --check-prefixes=CHECK,BMI

define i64 @foo(i64 %w, i64 %x, i64 %y, i64 %z) {
; NOBMI-LABEL: foo:
; NOBMI:       # %bb.0: # %Entry
; NOBMI-NEXT:    movq %rcx, %rax
; NOBMI-NEXT:    andq %rdx, %rsi
; NOBMI-NEXT:    notq %rsi
; NOBMI-NEXT:    andq %rdi, %rsi
; NOBMI-NEXT:    notq %rax
; NOBMI-NEXT:    orq %rdx, %rax
; NOBMI-NEXT:    andq %rsi, %rax
; NOBMI-NEXT:    retq
;
; BMI-LABEL: foo:
; BMI:       # %bb.0: # %Entry
; BMI-NEXT:    andq %rdx, %rsi
; BMI-NEXT:    andnq %rdi, %rsi, %rax
; BMI-NEXT:    andnq %rcx, %rdx, %rcx
; BMI-NEXT:    andnq %rax, %rcx, %rax
; BMI-NEXT:    retq
Entry:
  %and1 = and i64 %y, %x
  %xor1 = xor i64 %and1, -1
  %and2 = and i64 %xor1, %w
  %.not = xor i64 %z, -1
  %or1 = or i64 %.not, %y
  %and3 = and i64 %and2, %or1
  ret i64 %and3
}

define <16 x i8> @fooVec(<16 x i8> %w, <16 x i8> %x, <16 x i8> %y, <16 x i8> %z) {
; NOBMI-LABEL: fooVec:
; NOBMI:       # %bb.0: # %Entry
; NOBMI-NEXT:    andps %xmm2, %xmm1
; NOBMI-NEXT:    andnps %xmm0, %xmm1
; NOBMI-NEXT:    andnps %xmm3, %xmm2
; NOBMI-NEXT:    andnps %xmm1, %xmm2
; NOBMI-NEXT:    movaps %xmm2, %xmm0
; NOBMI-NEXT:    retq
;
; BMI-LABEL: fooVec:
; BMI:       # %bb.0: # %Entry
; BMI-NEXT:    vandps %xmm1, %xmm2, %xmm1
; BMI-NEXT:    vandnps %xmm0, %xmm1, %xmm0
; BMI-NEXT:    vandnps %xmm3, %xmm2, %xmm1
; BMI-NEXT:    vandnps %xmm0, %xmm1, %xmm0
; BMI-NEXT:    retq
Entry:
  %and1 = and <16 x i8> %y, %x
  %xor1 = xor <16 x i8> %and1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %and2 = and <16 x i8> %xor1, %w
  %.not = xor <16 x i8> %z, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %or1 = or <16 x i8> %.not, %y
  %and3 = and <16 x i8> %and2, %or1
  ret <16 x i8> %and3
}

; PR112347 - don't fold if we'd be inverting a constant, as demorgan normalisation will invert it back again.
define void @PR112347(ptr %p0, ptr %p1, ptr %p2) {
; CHECK-LABEL: PR112347:
; CHECK:       # %bb.0:
; CHECK-NEXT:    movl (%rdi), %eax
; CHECK-NEXT:    notl %eax
; CHECK-NEXT:    orl $-16777204, %eax # imm = 0xFF00000C
; CHECK-NEXT:    andl (%rsi), %eax
; CHECK-NEXT:    movl %eax, (%rdx)
; CHECK-NEXT:    retq
  %load0 = load i32, ptr %p0, align 1
  %load1 = load i32, ptr %p1, align 4
  %not = xor i32 %load0, -1
  %top = or i32 %not, -16777204
  %mask = and i32 %load1, %top
  store i32 %mask, ptr %p2, align 4
  ret void
}

