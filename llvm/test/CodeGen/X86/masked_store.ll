; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse2    | FileCheck %s --check-prefixes=SSE,SSE2
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=sse4.2  | FileCheck %s --check-prefixes=SSE,SSE4
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx     | FileCheck %s --check-prefixes=AVX,AVX1OR2,AVX1
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx2    | FileCheck %s --check-prefixes=AVX,AVX1OR2,AVX2
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx512f | FileCheck %s --check-prefixes=AVX,AVX512,AVX512F
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx512f,avx512dq,avx512vl | FileCheck %s --check-prefixes=AVX,AVX512,AVX512VL,AVX512VLDQ
; RUN: llc < %s -mtriple=x86_64-apple-darwin -mattr=avx512f,avx512bw,avx512vl | FileCheck %s --check-prefixes=AVX,AVX512,AVX512VL,AVX512VLBW
; RUN: llc < %s -mtriple=i686-apple-darwin -mattr=avx512f,avx512bw,avx512dq,avx512vl | FileCheck %s --check-prefixes=X86-AVX512

;
; vXf64
;

define void @store_v1f64_i1(i1%trigger, ptr %addr, <1 x double> %val) nounwind {
; SSE-LABEL: store_v1f64_i1:
; SSE:       ## %bb.0:
; SSE-NEXT:    testb $1, %dil
; SSE-NEXT:    je LBB0_2
; SSE-NEXT:  ## %bb.1: ## %cond.store
; SSE-NEXT:    movsd %xmm0, (%rsi)
; SSE-NEXT:  LBB0_2: ## %else
; SSE-NEXT:    retq
;
; AVX-LABEL: store_v1f64_i1:
; AVX:       ## %bb.0:
; AVX-NEXT:    testb $1, %dil
; AVX-NEXT:    je LBB0_2
; AVX-NEXT:  ## %bb.1: ## %cond.store
; AVX-NEXT:    vmovsd %xmm0, (%rsi)
; AVX-NEXT:  LBB0_2: ## %else
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: store_v1f64_i1:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    testb $1, {{[0-9]+}}(%esp)
; X86-AVX512-NEXT:    je LBB0_2
; X86-AVX512-NEXT:  ## %bb.1: ## %cond.store
; X86-AVX512-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovsd %xmm0, (%eax)
; X86-AVX512-NEXT:  LBB0_2: ## %else
; X86-AVX512-NEXT:    retl
  %mask = bitcast i1 %trigger to <1 x i1>
  call void @llvm.masked.store.v1f64.p0(<1 x double> %val, ptr %addr, i32 4, <1 x i1> %mask)
  ret void
}

define void @store_v1f64_v1i64(<1 x i64> %trigger, ptr %addr, <1 x double> %val) nounwind {
; SSE-LABEL: store_v1f64_v1i64:
; SSE:       ## %bb.0:
; SSE-NEXT:    testq %rdi, %rdi
; SSE-NEXT:    jns LBB1_2
; SSE-NEXT:  ## %bb.1: ## %cond.store
; SSE-NEXT:    movsd %xmm0, (%rsi)
; SSE-NEXT:  LBB1_2: ## %else
; SSE-NEXT:    retq
;
; AVX-LABEL: store_v1f64_v1i64:
; AVX:       ## %bb.0:
; AVX-NEXT:    testq %rdi, %rdi
; AVX-NEXT:    jns LBB1_2
; AVX-NEXT:  ## %bb.1: ## %cond.store
; AVX-NEXT:    vmovsd %xmm0, (%rsi)
; AVX-NEXT:  LBB1_2: ## %else
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: store_v1f64_v1i64:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    cmpl $0, {{[0-9]+}}(%esp)
; X86-AVX512-NEXT:    jns LBB1_2
; X86-AVX512-NEXT:  ## %bb.1: ## %cond.store
; X86-AVX512-NEXT:    vmovsd {{.*#+}} xmm0 = mem[0],zero
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovsd %xmm0, (%eax)
; X86-AVX512-NEXT:  LBB1_2: ## %else
; X86-AVX512-NEXT:    retl
  %mask = icmp slt <1 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v1f64.p0(<1 x double> %val, ptr %addr, i32 4, <1 x i1> %mask)
  ret void
}

define void @store_v2f64_i2(i2 %trigger, ptr %addr, <2 x double> %val) nounwind {
; SSE-LABEL: store_v2f64_i2:
; SSE:       ## %bb.0:
; SSE-NEXT:    testb $1, %dil
; SSE-NEXT:    jne LBB2_1
; SSE-NEXT:  ## %bb.2: ## %else
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    jne LBB2_3
; SSE-NEXT:  LBB2_4: ## %else2
; SSE-NEXT:    retq
; SSE-NEXT:  LBB2_1: ## %cond.store
; SSE-NEXT:    movlps %xmm0, (%rsi)
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    je LBB2_4
; SSE-NEXT:  LBB2_3: ## %cond.store1
; SSE-NEXT:    movhps %xmm0, 8(%rsi)
; SSE-NEXT:    retq
;
; AVX1-LABEL: store_v2f64_i2:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovd %edi, %xmm1
; AVX1-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[0,1,0,1]
; AVX1-NEXT:    vpmovsxbq {{.*#+}} xmm2 = [1,2]
; AVX1-NEXT:    vpand %xmm2, %xmm1, %xmm1
; AVX1-NEXT:    vpcmpeqq %xmm2, %xmm1, %xmm1
; AVX1-NEXT:    vmaskmovpd %xmm0, %xmm1, (%rsi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v2f64_i2:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovd %edi, %xmm1
; AVX2-NEXT:    vpbroadcastd %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxbq {{.*#+}} xmm2 = [1,2]
; AVX2-NEXT:    vpand %xmm2, %xmm1, %xmm1
; AVX2-NEXT:    vpcmpeqq %xmm2, %xmm1, %xmm1
; AVX2-NEXT:    vmaskmovpd %xmm0, %xmm1, (%rsi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v2f64_i2:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    kmovw %edi, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovupd %zmm0, (%rsi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2f64_i2:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %edi, %k1
; AVX512VLDQ-NEXT:    vmovupd %xmm0, (%rsi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2f64_i2:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %edi, %k1
; AVX512VLBW-NEXT:    vmovupd %xmm0, (%rsi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2f64_i2:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    vmovupd %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = bitcast i2 %trigger to <2 x i1>
  call void @llvm.masked.store.v2f64.p0(<2 x double> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v2f64_v2i64(<2 x i64> %trigger, ptr %addr, <2 x double> %val) nounwind {
; SSE-LABEL: store_v2f64_v2i64:
; SSE:       ## %bb.0:
; SSE-NEXT:    movmskpd %xmm0, %eax
; SSE-NEXT:    testb $1, %al
; SSE-NEXT:    jne LBB3_1
; SSE-NEXT:  ## %bb.2: ## %else
; SSE-NEXT:    testb $2, %al
; SSE-NEXT:    jne LBB3_3
; SSE-NEXT:  LBB3_4: ## %else2
; SSE-NEXT:    retq
; SSE-NEXT:  LBB3_1: ## %cond.store
; SSE-NEXT:    movlps %xmm1, (%rdi)
; SSE-NEXT:    testb $2, %al
; SSE-NEXT:    je LBB3_4
; SSE-NEXT:  LBB3_3: ## %cond.store1
; SSE-NEXT:    movhps %xmm1, 8(%rdi)
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: store_v2f64_v2i64:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovpd %xmm1, %xmm0, (%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v2f64_v2i64:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpgtq %zmm0, %zmm2, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovupd %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2f64_v2i64:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovq2m %xmm0, %k1
; AVX512VLDQ-NEXT:    vmovupd %xmm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2f64_v2i64:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLBW-NEXT:    vpcmpgtq %xmm0, %xmm2, %k1
; AVX512VLBW-NEXT:    vmovupd %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2f64_v2i64:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovq2m %xmm0, %k1
; X86-AVX512-NEXT:    vmovupd %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp slt <2 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v2f64.p0(<2 x double> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v4f64_i4(i4 %trigger, ptr %addr, <4 x double> %val) nounwind {
; SSE-LABEL: store_v4f64_i4:
; SSE:       ## %bb.0:
; SSE-NEXT:    testb $1, %dil
; SSE-NEXT:    jne LBB4_1
; SSE-NEXT:  ## %bb.2: ## %else
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    jne LBB4_3
; SSE-NEXT:  LBB4_4: ## %else2
; SSE-NEXT:    testb $4, %dil
; SSE-NEXT:    jne LBB4_5
; SSE-NEXT:  LBB4_6: ## %else4
; SSE-NEXT:    testb $8, %dil
; SSE-NEXT:    jne LBB4_7
; SSE-NEXT:  LBB4_8: ## %else6
; SSE-NEXT:    retq
; SSE-NEXT:  LBB4_1: ## %cond.store
; SSE-NEXT:    movlps %xmm0, (%rsi)
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    je LBB4_4
; SSE-NEXT:  LBB4_3: ## %cond.store1
; SSE-NEXT:    movhps %xmm0, 8(%rsi)
; SSE-NEXT:    testb $4, %dil
; SSE-NEXT:    je LBB4_6
; SSE-NEXT:  LBB4_5: ## %cond.store3
; SSE-NEXT:    movlps %xmm1, 16(%rsi)
; SSE-NEXT:    testb $8, %dil
; SSE-NEXT:    je LBB4_8
; SSE-NEXT:  LBB4_7: ## %cond.store5
; SSE-NEXT:    movhps %xmm1, 24(%rsi)
; SSE-NEXT:    retq
;
; AVX1-LABEL: store_v4f64_i4:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovd %edi, %xmm1
; AVX1-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[0,1,0,1]
; AVX1-NEXT:    vinsertf128 $1, %xmm1, %ymm1, %ymm1
; AVX1-NEXT:    vmovaps {{.*#+}} ymm2 = [1,2,4,8]
; AVX1-NEXT:    vandps %ymm2, %ymm1, %ymm1
; AVX1-NEXT:    vpcmpeqq %xmm2, %xmm1, %xmm2
; AVX1-NEXT:    vextractf128 $1, %ymm1, %xmm1
; AVX1-NEXT:    vpcmpeqq {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm1
; AVX1-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX1-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rsi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v4f64_i4:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovd %edi, %xmm1
; AVX2-NEXT:    vpbroadcastd %xmm1, %ymm1
; AVX2-NEXT:    vpmovsxbq {{.*#+}} ymm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpcmpeqq %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rsi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v4f64_i4:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    kmovw %edi, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovupd %zmm0, (%rsi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v4f64_i4:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %edi, %k1
; AVX512VLDQ-NEXT:    vmovupd %ymm0, (%rsi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v4f64_i4:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %edi, %k1
; AVX512VLBW-NEXT:    vmovupd %ymm0, (%rsi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4f64_i4:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    vmovupd %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = bitcast i4 %trigger to <4 x i1>
  call void @llvm.masked.store.v4f64.p0(<4 x double> %val, ptr %addr, i32 4, <4 x i1> %mask)
  ret void
}

define void @store_v4f64_v4i64(<4 x i64> %trigger, ptr %addr, <4 x double> %val) nounwind {
; SSE2-LABEL: store_v4f64_v4i64:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,3],xmm1[1,3]
; SSE2-NEXT:    movmskps %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB5_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB5_3
; SSE2-NEXT:  LBB5_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB5_5
; SSE2-NEXT:  LBB5_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB5_7
; SSE2-NEXT:  LBB5_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB5_1: ## %cond.store
; SSE2-NEXT:    movlps %xmm2, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB5_4
; SSE2-NEXT:  LBB5_3: ## %cond.store1
; SSE2-NEXT:    movhps %xmm2, 8(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB5_6
; SSE2-NEXT:  LBB5_5: ## %cond.store3
; SSE2-NEXT:    movlps %xmm3, 16(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB5_8
; SSE2-NEXT:  LBB5_7: ## %cond.store5
; SSE2-NEXT:    movhps %xmm3, 24(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v4f64_v4i64:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    packssdw %xmm1, %xmm0
; SSE4-NEXT:    movmskps %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB5_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB5_3
; SSE4-NEXT:  LBB5_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB5_5
; SSE4-NEXT:  LBB5_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB5_7
; SSE4-NEXT:  LBB5_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB5_1: ## %cond.store
; SSE4-NEXT:    movlps %xmm2, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB5_4
; SSE4-NEXT:  LBB5_3: ## %cond.store1
; SSE4-NEXT:    movhps %xmm2, 8(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB5_6
; SSE4-NEXT:  LBB5_5: ## %cond.store3
; SSE4-NEXT:    movlps %xmm3, 16(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB5_8
; SSE4-NEXT:  LBB5_7: ## %cond.store5
; SSE4-NEXT:    movhps %xmm3, 24(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v4f64_v4i64:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovpd %ymm1, %ymm0, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v4f64_v4i64:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm1 killed $ymm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpgtq %zmm0, %zmm2, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovupd %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v4f64_v4i64:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovq2m %ymm0, %k1
; AVX512VLDQ-NEXT:    vmovupd %ymm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v4f64_v4i64:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLBW-NEXT:    vpcmpgtq %ymm0, %ymm2, %k1
; AVX512VLBW-NEXT:    vmovupd %ymm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4f64_v4i64:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovq2m %ymm0, %k1
; X86-AVX512-NEXT:    vmovupd %ymm1, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp slt <4 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v4f64.p0(<4 x double> %val, ptr %addr, i32 4, <4 x i1> %mask)
  ret void
}

;
; vXf32
;

define void @store_v2f32_i2(i2 %trigger, ptr %addr, <2 x float> %val) nounwind {
; SSE2-LABEL: store_v2f32_i2:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    testb $1, %dil
; SSE2-NEXT:    jne LBB6_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %dil
; SSE2-NEXT:    jne LBB6_3
; SSE2-NEXT:  LBB6_4: ## %else2
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB6_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rsi)
; SSE2-NEXT:    testb $2, %dil
; SSE2-NEXT:    je LBB6_4
; SSE2-NEXT:  LBB6_3: ## %cond.store1
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1,1,1]
; SSE2-NEXT:    movss %xmm0, 4(%rsi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v2f32_i2:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    testb $1, %dil
; SSE4-NEXT:    jne LBB6_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %dil
; SSE4-NEXT:    jne LBB6_3
; SSE4-NEXT:  LBB6_4: ## %else2
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB6_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rsi)
; SSE4-NEXT:    testb $2, %dil
; SSE4-NEXT:    je LBB6_4
; SSE4-NEXT:  LBB6_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rsi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v2f32_i2:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    movl %edi, %eax
; AVX1OR2-NEXT:    andb $2, %al
; AVX1OR2-NEXT:    shrb %al
; AVX1OR2-NEXT:    andb $1, %dil
; AVX1OR2-NEXT:    vmovd %edi, %xmm1
; AVX1OR2-NEXT:    vpinsrb $8, %eax, %xmm1, %xmm1
; AVX1OR2-NEXT:    vinsertps {{.*#+}} xmm1 = xmm1[0,2],zero,zero
; AVX1OR2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX1OR2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rsi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v2f32_i2:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    kmovw %edi, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rsi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2f32_i2:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %edi, %k0
; AVX512VLDQ-NEXT:    kshiftlb $6, %k0, %k0
; AVX512VLDQ-NEXT:    kshiftrb $6, %k0, %k1
; AVX512VLDQ-NEXT:    vmovups %xmm0, (%rsi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2f32_i2:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %edi, %k0
; AVX512VLBW-NEXT:    kshiftlw $14, %k0, %k0
; AVX512VLBW-NEXT:    kshiftrw $14, %k0, %k1
; AVX512VLBW-NEXT:    vmovups %xmm0, (%rsi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2f32_i2:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k0
; X86-AVX512-NEXT:    kshiftlb $6, %k0, %k0
; X86-AVX512-NEXT:    kshiftrb $6, %k0, %k1
; X86-AVX512-NEXT:    vmovups %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = bitcast i2 %trigger to <2 x i1>
  call void @llvm.masked.store.v2f32.p0(<2 x float> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v2f32_v2i32(<2 x i32> %trigger, ptr %addr, <2 x float> %val) nounwind {
; SSE2-LABEL: store_v2f32_v2i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,1,1]
; SSE2-NEXT:    pxor %xmm2, %xmm2
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE2-NEXT:    movmskpd %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB7_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB7_3
; SSE2-NEXT:  LBB7_4: ## %else2
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB7_1: ## %cond.store
; SSE2-NEXT:    movss %xmm1, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB7_4
; SSE2-NEXT:  LBB7_3: ## %cond.store1
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1,1,1]
; SSE2-NEXT:    movss %xmm1, 4(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v2f32_v2i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm2, %xmm2
; SSE4-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE4-NEXT:    pmovsxdq %xmm2, %xmm0
; SSE4-NEXT:    movmskpd %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB7_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB7_3
; SSE4-NEXT:  LBB7_4: ## %else2
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB7_1: ## %cond.store
; SSE4-NEXT:    movss %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB7_4
; SSE4-NEXT:  LBB7_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm1, 4(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v2f32_v2i32:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1OR2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
; AVX1OR2-NEXT:    vmovq {{.*#+}} xmm0 = xmm0[0],zero
; AVX1OR2-NEXT:    vmaskmovps %xmm1, %xmm0, (%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v2f32_v2i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vptestnmd %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2f32_v2i32:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; AVX512VLDQ-NEXT:    kshiftlb $6, %k0, %k0
; AVX512VLDQ-NEXT:    kshiftrb $6, %k0, %k1
; AVX512VLDQ-NEXT:    vmovups %xmm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2f32_v2i32:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; AVX512VLBW-NEXT:    kshiftlw $14, %k0, %k0
; AVX512VLBW-NEXT:    kshiftrw $14, %k0, %k1
; AVX512VLBW-NEXT:    vmovups %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2f32_v2i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; X86-AVX512-NEXT:    kshiftlb $6, %k0, %k0
; X86-AVX512-NEXT:    kshiftrb $6, %k0, %k1
; X86-AVX512-NEXT:    vmovups %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <2 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v2f32.p0(<2 x float> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v4f32_i4(<4 x float> %x, ptr %ptr, <4 x float> %y, i4 %trigger) nounwind {
; SSE2-LABEL: store_v4f32_i4:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    testb $1, %sil
; SSE2-NEXT:    jne LBB8_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    jne LBB8_3
; SSE2-NEXT:  LBB8_4: ## %else2
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    jne LBB8_5
; SSE2-NEXT:  LBB8_6: ## %else4
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    jne LBB8_7
; SSE2-NEXT:  LBB8_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB8_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    je LBB8_4
; SSE2-NEXT:  LBB8_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm1, 4(%rdi)
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    je LBB8_6
; SSE2-NEXT:  LBB8_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm1 = xmm1[1],xmm0[1]
; SSE2-NEXT:    movss %xmm1, 8(%rdi)
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    je LBB8_8
; SSE2-NEXT:  LBB8_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v4f32_i4:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    testb $1, %sil
; SSE4-NEXT:    jne LBB8_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    jne LBB8_3
; SSE4-NEXT:  LBB8_4: ## %else2
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    jne LBB8_5
; SSE4-NEXT:  LBB8_6: ## %else4
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    jne LBB8_7
; SSE4-NEXT:  LBB8_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB8_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    je LBB8_4
; SSE4-NEXT:  LBB8_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    je LBB8_6
; SSE4-NEXT:  LBB8_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    je LBB8_8
; SSE4-NEXT:  LBB8_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v4f32_i4:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovd %esi, %xmm1
; AVX1-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[0,0,0,0]
; AVX1-NEXT:    vpmovsxbd {{.*#+}} xmm2 = [1,2,4,8]
; AVX1-NEXT:    vpand %xmm2, %xmm1, %xmm1
; AVX1-NEXT:    vpcmpeqd %xmm2, %xmm1, %xmm1
; AVX1-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v4f32_i4:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovd %esi, %xmm1
; AVX2-NEXT:    vpbroadcastd %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxbd {{.*#+}} xmm2 = [1,2,4,8]
; AVX2-NEXT:    vpand %xmm2, %xmm1, %xmm1
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm1, %xmm1
; AVX2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v4f32_i4:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    kmovw %esi, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v4f32_i4:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %esi, %k1
; AVX512VLDQ-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v4f32_i4:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %esi, %k1
; AVX512VLBW-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4f32_i4:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    vmovups %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = bitcast i4 %trigger to <4 x i1>
  call void @llvm.masked.store.v4f32.p0(<4 x float> %x, ptr %ptr, i32 1, <4 x i1> %mask)
  ret void
}

define void @store_v4f32_v4i32(<4 x float> %x, ptr %ptr, <4 x float> %y, <4 x i32> %mask) nounwind {
; SSE2-LABEL: store_v4f32_v4i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movmskps %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB9_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB9_3
; SSE2-NEXT:  LBB9_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB9_5
; SSE2-NEXT:  LBB9_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB9_7
; SSE2-NEXT:  LBB9_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB9_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB9_4
; SSE2-NEXT:  LBB9_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm1, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB9_6
; SSE2-NEXT:  LBB9_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm1 = xmm1[1],xmm0[1]
; SSE2-NEXT:    movss %xmm1, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB9_8
; SSE2-NEXT:  LBB9_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v4f32_v4i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movmskps %xmm2, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB9_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB9_3
; SSE4-NEXT:  LBB9_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB9_5
; SSE4-NEXT:  LBB9_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB9_7
; SSE4-NEXT:  LBB9_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB9_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB9_4
; SSE4-NEXT:  LBB9_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB9_6
; SSE4-NEXT:  LBB9_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB9_8
; SSE4-NEXT:  LBB9_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v4f32_v4i32:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovps %xmm0, %xmm2, (%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v4f32_v4i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm2 killed $xmm2 def $zmm2
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512F-NEXT:    vpcmpgtd %zmm2, %zmm1, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v4f32_v4i32:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovd2m %xmm2, %k1
; AVX512VLDQ-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v4f32_v4i32:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512VLBW-NEXT:    vpcmpgtd %xmm2, %xmm1, %k1
; AVX512VLBW-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4f32_v4i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovd2m %xmm2, %k1
; X86-AVX512-NEXT:    vmovups %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %bool_mask = icmp slt <4 x i32> %mask, zeroinitializer
  call void @llvm.masked.store.v4f32.p0(<4 x float> %x, ptr %ptr, i32 1, <4 x i1> %bool_mask)
  ret void
}

define void @store_v8f32_i8(<8 x float> %x, ptr %ptr, <8 x float> %y, i8 %trigger) nounwind {
; SSE2-LABEL: store_v8f32_i8:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    testb $1, %sil
; SSE2-NEXT:    jne LBB10_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    jne LBB10_3
; SSE2-NEXT:  LBB10_4: ## %else2
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    jne LBB10_5
; SSE2-NEXT:  LBB10_6: ## %else4
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    jne LBB10_7
; SSE2-NEXT:  LBB10_8: ## %else6
; SSE2-NEXT:    testb $16, %sil
; SSE2-NEXT:    jne LBB10_9
; SSE2-NEXT:  LBB10_10: ## %else8
; SSE2-NEXT:    testb $32, %sil
; SSE2-NEXT:    jne LBB10_11
; SSE2-NEXT:  LBB10_12: ## %else10
; SSE2-NEXT:    testb $64, %sil
; SSE2-NEXT:    jne LBB10_13
; SSE2-NEXT:  LBB10_14: ## %else12
; SSE2-NEXT:    testb $-128, %sil
; SSE2-NEXT:    jne LBB10_15
; SSE2-NEXT:  LBB10_16: ## %else14
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB10_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    je LBB10_4
; SSE2-NEXT:  LBB10_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm2
; SSE2-NEXT:    shufps {{.*#+}} xmm2 = xmm2[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm2, 4(%rdi)
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    je LBB10_6
; SSE2-NEXT:  LBB10_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm2
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm2 = xmm2[1],xmm0[1]
; SSE2-NEXT:    movss %xmm2, 8(%rdi)
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    je LBB10_8
; SSE2-NEXT:  LBB10_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $16, %sil
; SSE2-NEXT:    je LBB10_10
; SSE2-NEXT:  LBB10_9: ## %cond.store7
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    testb $32, %sil
; SSE2-NEXT:    je LBB10_12
; SSE2-NEXT:  LBB10_11: ## %cond.store9
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm1[1,1]
; SSE2-NEXT:    movss %xmm0, 20(%rdi)
; SSE2-NEXT:    testb $64, %sil
; SSE2-NEXT:    je LBB10_14
; SSE2-NEXT:  LBB10_13: ## %cond.store11
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm1[1]
; SSE2-NEXT:    movss %xmm0, 24(%rdi)
; SSE2-NEXT:    testb $-128, %sil
; SSE2-NEXT:    je LBB10_16
; SSE2-NEXT:  LBB10_15: ## %cond.store13
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; SSE2-NEXT:    movss %xmm1, 28(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v8f32_i8:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    testb $1, %sil
; SSE4-NEXT:    jne LBB10_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    jne LBB10_3
; SSE4-NEXT:  LBB10_4: ## %else2
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    jne LBB10_5
; SSE4-NEXT:  LBB10_6: ## %else4
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    jne LBB10_7
; SSE4-NEXT:  LBB10_8: ## %else6
; SSE4-NEXT:    testb $16, %sil
; SSE4-NEXT:    jne LBB10_9
; SSE4-NEXT:  LBB10_10: ## %else8
; SSE4-NEXT:    testb $32, %sil
; SSE4-NEXT:    jne LBB10_11
; SSE4-NEXT:  LBB10_12: ## %else10
; SSE4-NEXT:    testb $64, %sil
; SSE4-NEXT:    jne LBB10_13
; SSE4-NEXT:  LBB10_14: ## %else12
; SSE4-NEXT:    testb $-128, %sil
; SSE4-NEXT:    jne LBB10_15
; SSE4-NEXT:  LBB10_16: ## %else14
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB10_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    je LBB10_4
; SSE4-NEXT:  LBB10_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    je LBB10_6
; SSE4-NEXT:  LBB10_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    je LBB10_8
; SSE4-NEXT:  LBB10_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    testb $16, %sil
; SSE4-NEXT:    je LBB10_10
; SSE4-NEXT:  LBB10_9: ## %cond.store7
; SSE4-NEXT:    movss %xmm1, 16(%rdi)
; SSE4-NEXT:    testb $32, %sil
; SSE4-NEXT:    je LBB10_12
; SSE4-NEXT:  LBB10_11: ## %cond.store9
; SSE4-NEXT:    extractps $1, %xmm1, 20(%rdi)
; SSE4-NEXT:    testb $64, %sil
; SSE4-NEXT:    je LBB10_14
; SSE4-NEXT:  LBB10_13: ## %cond.store11
; SSE4-NEXT:    extractps $2, %xmm1, 24(%rdi)
; SSE4-NEXT:    testb $-128, %sil
; SSE4-NEXT:    je LBB10_16
; SSE4-NEXT:  LBB10_15: ## %cond.store13
; SSE4-NEXT:    extractps $3, %xmm1, 28(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v8f32_i8:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovd %esi, %xmm1
; AVX1-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[0,0,0,0]
; AVX1-NEXT:    vinsertf128 $1, %xmm1, %ymm1, %ymm1
; AVX1-NEXT:    vandps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; AVX1-NEXT:    vcvtdq2ps %ymm1, %ymm1
; AVX1-NEXT:    vcmpeqps {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm1, %ymm1
; AVX1-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v8f32_i8:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovd %esi, %xmm1
; AVX2-NEXT:    vpbroadcastb %xmm1, %ymm1
; AVX2-NEXT:    vpmovzxbd {{.*#+}} ymm2 = [1,2,4,8,16,32,64,128]
; AVX2-NEXT:    vpand %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vpcmpeqd %ymm2, %ymm1, %ymm1
; AVX2-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v8f32_i8:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    kmovw %esi, %k0
; AVX512F-NEXT:    kshiftlw $8, %k0, %k0
; AVX512F-NEXT:    kshiftrw $8, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v8f32_i8:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %esi, %k1
; AVX512VLDQ-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v8f32_i8:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %esi, %k1
; AVX512VLBW-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v8f32_i8:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    vmovups %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = bitcast i8 %trigger to <8 x i1>
  call void @llvm.masked.store.v8f32.p0(<8 x float> %x, ptr %ptr, i32 1, <8 x i1> %mask)
  ret void
}

define void @store_v8f32_v8i32(<8 x float> %x, ptr %ptr, <8 x float> %y, <8 x i32> %mask) nounwind {
; SSE2-LABEL: store_v8f32_v8i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    packssdw %xmm5, %xmm4
; SSE2-NEXT:    packsswb %xmm4, %xmm4
; SSE2-NEXT:    pmovmskb %xmm4, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB11_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB11_3
; SSE2-NEXT:  LBB11_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB11_5
; SSE2-NEXT:  LBB11_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB11_7
; SSE2-NEXT:  LBB11_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB11_9
; SSE2-NEXT:  LBB11_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB11_11
; SSE2-NEXT:  LBB11_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB11_13
; SSE2-NEXT:  LBB11_14: ## %else12
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    jne LBB11_15
; SSE2-NEXT:  LBB11_16: ## %else14
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB11_1: ## %cond.store
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB11_4
; SSE2-NEXT:  LBB11_3: ## %cond.store1
; SSE2-NEXT:    movdqa %xmm0, %xmm2
; SSE2-NEXT:    shufps {{.*#+}} xmm2 = xmm2[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm2, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB11_6
; SSE2-NEXT:  LBB11_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm2
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm2 = xmm2[1],xmm0[1]
; SSE2-NEXT:    movss %xmm2, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB11_8
; SSE2-NEXT:  LBB11_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB11_10
; SSE2-NEXT:  LBB11_9: ## %cond.store7
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB11_12
; SSE2-NEXT:  LBB11_11: ## %cond.store9
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm1[1,1]
; SSE2-NEXT:    movss %xmm0, 20(%rdi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB11_14
; SSE2-NEXT:  LBB11_13: ## %cond.store11
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm1[1]
; SSE2-NEXT:    movss %xmm0, 24(%rdi)
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    je LBB11_16
; SSE2-NEXT:  LBB11_15: ## %cond.store13
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; SSE2-NEXT:    movss %xmm1, 28(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v8f32_v8i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    packssdw %xmm5, %xmm4
; SSE4-NEXT:    packsswb %xmm4, %xmm4
; SSE4-NEXT:    pmovmskb %xmm4, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB11_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB11_3
; SSE4-NEXT:  LBB11_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB11_5
; SSE4-NEXT:  LBB11_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB11_7
; SSE4-NEXT:  LBB11_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB11_9
; SSE4-NEXT:  LBB11_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB11_11
; SSE4-NEXT:  LBB11_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB11_13
; SSE4-NEXT:  LBB11_14: ## %else12
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    jne LBB11_15
; SSE4-NEXT:  LBB11_16: ## %else14
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB11_1: ## %cond.store
; SSE4-NEXT:    movd %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB11_4
; SSE4-NEXT:  LBB11_3: ## %cond.store1
; SSE4-NEXT:    pextrd $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB11_6
; SSE4-NEXT:  LBB11_5: ## %cond.store3
; SSE4-NEXT:    pextrd $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB11_8
; SSE4-NEXT:  LBB11_7: ## %cond.store5
; SSE4-NEXT:    pextrd $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB11_10
; SSE4-NEXT:  LBB11_9: ## %cond.store7
; SSE4-NEXT:    movss %xmm1, 16(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB11_12
; SSE4-NEXT:  LBB11_11: ## %cond.store9
; SSE4-NEXT:    extractps $1, %xmm1, 20(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB11_14
; SSE4-NEXT:  LBB11_13: ## %cond.store11
; SSE4-NEXT:    extractps $2, %xmm1, 24(%rdi)
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    je LBB11_16
; SSE4-NEXT:  LBB11_15: ## %cond.store13
; SSE4-NEXT:    extractps $3, %xmm1, 28(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v8f32_v8i32:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovps %ymm0, %ymm2, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v8f32_v8i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm2 killed $ymm2 def $zmm2
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512F-NEXT:    vpcmpgtd %zmm2, %zmm1, %k0
; AVX512F-NEXT:    kshiftlw $8, %k0, %k0
; AVX512F-NEXT:    kshiftrw $8, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v8f32_v8i32:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovd2m %ymm2, %k1
; AVX512VLDQ-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v8f32_v8i32:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512VLBW-NEXT:    vpcmpgtd %ymm2, %ymm1, %k1
; AVX512VLBW-NEXT:    vmovups %ymm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v8f32_v8i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovd2m %ymm2, %k1
; X86-AVX512-NEXT:    vmovups %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %bool_mask = icmp slt <8 x i32> %mask, zeroinitializer
  call void @llvm.masked.store.v8f32.p0(<8 x float> %x, ptr %ptr, i32 1, <8 x i1> %bool_mask)
  ret void
}

define void @store_v16f32_i16(<16 x float> %x, ptr %ptr, <16 x float> %y, i16 %trigger) nounwind {
; SSE2-LABEL: store_v16f32_i16:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    testb $1, %sil
; SSE2-NEXT:    jne LBB12_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    jne LBB12_3
; SSE2-NEXT:  LBB12_4: ## %else2
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    jne LBB12_5
; SSE2-NEXT:  LBB12_6: ## %else4
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    jne LBB12_7
; SSE2-NEXT:  LBB12_8: ## %else6
; SSE2-NEXT:    testb $16, %sil
; SSE2-NEXT:    jne LBB12_9
; SSE2-NEXT:  LBB12_10: ## %else8
; SSE2-NEXT:    testb $32, %sil
; SSE2-NEXT:    jne LBB12_11
; SSE2-NEXT:  LBB12_12: ## %else10
; SSE2-NEXT:    testb $64, %sil
; SSE2-NEXT:    jne LBB12_13
; SSE2-NEXT:  LBB12_14: ## %else12
; SSE2-NEXT:    testb %sil, %sil
; SSE2-NEXT:    js LBB12_15
; SSE2-NEXT:  LBB12_16: ## %else14
; SSE2-NEXT:    testl $256, %esi ## imm = 0x100
; SSE2-NEXT:    jne LBB12_17
; SSE2-NEXT:  LBB12_18: ## %else16
; SSE2-NEXT:    testl $512, %esi ## imm = 0x200
; SSE2-NEXT:    jne LBB12_19
; SSE2-NEXT:  LBB12_20: ## %else18
; SSE2-NEXT:    testl $1024, %esi ## imm = 0x400
; SSE2-NEXT:    jne LBB12_21
; SSE2-NEXT:  LBB12_22: ## %else20
; SSE2-NEXT:    testl $2048, %esi ## imm = 0x800
; SSE2-NEXT:    jne LBB12_23
; SSE2-NEXT:  LBB12_24: ## %else22
; SSE2-NEXT:    testl $4096, %esi ## imm = 0x1000
; SSE2-NEXT:    jne LBB12_25
; SSE2-NEXT:  LBB12_26: ## %else24
; SSE2-NEXT:    testl $8192, %esi ## imm = 0x2000
; SSE2-NEXT:    jne LBB12_27
; SSE2-NEXT:  LBB12_28: ## %else26
; SSE2-NEXT:    testl $16384, %esi ## imm = 0x4000
; SSE2-NEXT:    jne LBB12_29
; SSE2-NEXT:  LBB12_30: ## %else28
; SSE2-NEXT:    testl $32768, %esi ## imm = 0x8000
; SSE2-NEXT:    jne LBB12_31
; SSE2-NEXT:  LBB12_32: ## %else30
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB12_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %sil
; SSE2-NEXT:    je LBB12_4
; SSE2-NEXT:  LBB12_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm4
; SSE2-NEXT:    shufps {{.*#+}} xmm4 = xmm4[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm4, 4(%rdi)
; SSE2-NEXT:    testb $4, %sil
; SSE2-NEXT:    je LBB12_6
; SSE2-NEXT:  LBB12_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm4
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm4 = xmm4[1],xmm0[1]
; SSE2-NEXT:    movss %xmm4, 8(%rdi)
; SSE2-NEXT:    testb $8, %sil
; SSE2-NEXT:    je LBB12_8
; SSE2-NEXT:  LBB12_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $16, %sil
; SSE2-NEXT:    je LBB12_10
; SSE2-NEXT:  LBB12_9: ## %cond.store7
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    testb $32, %sil
; SSE2-NEXT:    je LBB12_12
; SSE2-NEXT:  LBB12_11: ## %cond.store9
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm1[1,1]
; SSE2-NEXT:    movss %xmm0, 20(%rdi)
; SSE2-NEXT:    testb $64, %sil
; SSE2-NEXT:    je LBB12_14
; SSE2-NEXT:  LBB12_13: ## %cond.store11
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm1[1]
; SSE2-NEXT:    movss %xmm0, 24(%rdi)
; SSE2-NEXT:    testb %sil, %sil
; SSE2-NEXT:    jns LBB12_16
; SSE2-NEXT:  LBB12_15: ## %cond.store13
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; SSE2-NEXT:    movss %xmm1, 28(%rdi)
; SSE2-NEXT:    testl $256, %esi ## imm = 0x100
; SSE2-NEXT:    je LBB12_18
; SSE2-NEXT:  LBB12_17: ## %cond.store15
; SSE2-NEXT:    movss %xmm2, 32(%rdi)
; SSE2-NEXT:    testl $512, %esi ## imm = 0x200
; SSE2-NEXT:    je LBB12_20
; SSE2-NEXT:  LBB12_19: ## %cond.store17
; SSE2-NEXT:    movaps %xmm2, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm2[1,1]
; SSE2-NEXT:    movss %xmm0, 36(%rdi)
; SSE2-NEXT:    testl $1024, %esi ## imm = 0x400
; SSE2-NEXT:    je LBB12_22
; SSE2-NEXT:  LBB12_21: ## %cond.store19
; SSE2-NEXT:    movaps %xmm2, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm2[1]
; SSE2-NEXT:    movss %xmm0, 40(%rdi)
; SSE2-NEXT:    testl $2048, %esi ## imm = 0x800
; SSE2-NEXT:    je LBB12_24
; SSE2-NEXT:  LBB12_23: ## %cond.store21
; SSE2-NEXT:    shufps {{.*#+}} xmm2 = xmm2[3,3,3,3]
; SSE2-NEXT:    movss %xmm2, 44(%rdi)
; SSE2-NEXT:    testl $4096, %esi ## imm = 0x1000
; SSE2-NEXT:    je LBB12_26
; SSE2-NEXT:  LBB12_25: ## %cond.store23
; SSE2-NEXT:    movss %xmm3, 48(%rdi)
; SSE2-NEXT:    testl $8192, %esi ## imm = 0x2000
; SSE2-NEXT:    je LBB12_28
; SSE2-NEXT:  LBB12_27: ## %cond.store25
; SSE2-NEXT:    movaps %xmm3, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm3[1,1]
; SSE2-NEXT:    movss %xmm0, 52(%rdi)
; SSE2-NEXT:    testl $16384, %esi ## imm = 0x4000
; SSE2-NEXT:    je LBB12_30
; SSE2-NEXT:  LBB12_29: ## %cond.store27
; SSE2-NEXT:    movaps %xmm3, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm3[1]
; SSE2-NEXT:    movss %xmm0, 56(%rdi)
; SSE2-NEXT:    testl $32768, %esi ## imm = 0x8000
; SSE2-NEXT:    je LBB12_32
; SSE2-NEXT:  LBB12_31: ## %cond.store29
; SSE2-NEXT:    shufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; SSE2-NEXT:    movss %xmm3, 60(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v16f32_i16:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    testb $1, %sil
; SSE4-NEXT:    jne LBB12_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    jne LBB12_3
; SSE4-NEXT:  LBB12_4: ## %else2
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    jne LBB12_5
; SSE4-NEXT:  LBB12_6: ## %else4
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    jne LBB12_7
; SSE4-NEXT:  LBB12_8: ## %else6
; SSE4-NEXT:    testb $16, %sil
; SSE4-NEXT:    jne LBB12_9
; SSE4-NEXT:  LBB12_10: ## %else8
; SSE4-NEXT:    testb $32, %sil
; SSE4-NEXT:    jne LBB12_11
; SSE4-NEXT:  LBB12_12: ## %else10
; SSE4-NEXT:    testb $64, %sil
; SSE4-NEXT:    jne LBB12_13
; SSE4-NEXT:  LBB12_14: ## %else12
; SSE4-NEXT:    testb %sil, %sil
; SSE4-NEXT:    js LBB12_15
; SSE4-NEXT:  LBB12_16: ## %else14
; SSE4-NEXT:    testl $256, %esi ## imm = 0x100
; SSE4-NEXT:    jne LBB12_17
; SSE4-NEXT:  LBB12_18: ## %else16
; SSE4-NEXT:    testl $512, %esi ## imm = 0x200
; SSE4-NEXT:    jne LBB12_19
; SSE4-NEXT:  LBB12_20: ## %else18
; SSE4-NEXT:    testl $1024, %esi ## imm = 0x400
; SSE4-NEXT:    jne LBB12_21
; SSE4-NEXT:  LBB12_22: ## %else20
; SSE4-NEXT:    testl $2048, %esi ## imm = 0x800
; SSE4-NEXT:    jne LBB12_23
; SSE4-NEXT:  LBB12_24: ## %else22
; SSE4-NEXT:    testl $4096, %esi ## imm = 0x1000
; SSE4-NEXT:    jne LBB12_25
; SSE4-NEXT:  LBB12_26: ## %else24
; SSE4-NEXT:    testl $8192, %esi ## imm = 0x2000
; SSE4-NEXT:    jne LBB12_27
; SSE4-NEXT:  LBB12_28: ## %else26
; SSE4-NEXT:    testl $16384, %esi ## imm = 0x4000
; SSE4-NEXT:    jne LBB12_29
; SSE4-NEXT:  LBB12_30: ## %else28
; SSE4-NEXT:    testl $32768, %esi ## imm = 0x8000
; SSE4-NEXT:    jne LBB12_31
; SSE4-NEXT:  LBB12_32: ## %else30
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB12_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %sil
; SSE4-NEXT:    je LBB12_4
; SSE4-NEXT:  LBB12_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %sil
; SSE4-NEXT:    je LBB12_6
; SSE4-NEXT:  LBB12_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %sil
; SSE4-NEXT:    je LBB12_8
; SSE4-NEXT:  LBB12_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    testb $16, %sil
; SSE4-NEXT:    je LBB12_10
; SSE4-NEXT:  LBB12_9: ## %cond.store7
; SSE4-NEXT:    movss %xmm1, 16(%rdi)
; SSE4-NEXT:    testb $32, %sil
; SSE4-NEXT:    je LBB12_12
; SSE4-NEXT:  LBB12_11: ## %cond.store9
; SSE4-NEXT:    extractps $1, %xmm1, 20(%rdi)
; SSE4-NEXT:    testb $64, %sil
; SSE4-NEXT:    je LBB12_14
; SSE4-NEXT:  LBB12_13: ## %cond.store11
; SSE4-NEXT:    extractps $2, %xmm1, 24(%rdi)
; SSE4-NEXT:    testb %sil, %sil
; SSE4-NEXT:    jns LBB12_16
; SSE4-NEXT:  LBB12_15: ## %cond.store13
; SSE4-NEXT:    extractps $3, %xmm1, 28(%rdi)
; SSE4-NEXT:    testl $256, %esi ## imm = 0x100
; SSE4-NEXT:    je LBB12_18
; SSE4-NEXT:  LBB12_17: ## %cond.store15
; SSE4-NEXT:    movss %xmm2, 32(%rdi)
; SSE4-NEXT:    testl $512, %esi ## imm = 0x200
; SSE4-NEXT:    je LBB12_20
; SSE4-NEXT:  LBB12_19: ## %cond.store17
; SSE4-NEXT:    extractps $1, %xmm2, 36(%rdi)
; SSE4-NEXT:    testl $1024, %esi ## imm = 0x400
; SSE4-NEXT:    je LBB12_22
; SSE4-NEXT:  LBB12_21: ## %cond.store19
; SSE4-NEXT:    extractps $2, %xmm2, 40(%rdi)
; SSE4-NEXT:    testl $2048, %esi ## imm = 0x800
; SSE4-NEXT:    je LBB12_24
; SSE4-NEXT:  LBB12_23: ## %cond.store21
; SSE4-NEXT:    extractps $3, %xmm2, 44(%rdi)
; SSE4-NEXT:    testl $4096, %esi ## imm = 0x1000
; SSE4-NEXT:    je LBB12_26
; SSE4-NEXT:  LBB12_25: ## %cond.store23
; SSE4-NEXT:    movss %xmm3, 48(%rdi)
; SSE4-NEXT:    testl $8192, %esi ## imm = 0x2000
; SSE4-NEXT:    je LBB12_28
; SSE4-NEXT:  LBB12_27: ## %cond.store25
; SSE4-NEXT:    extractps $1, %xmm3, 52(%rdi)
; SSE4-NEXT:    testl $16384, %esi ## imm = 0x4000
; SSE4-NEXT:    je LBB12_30
; SSE4-NEXT:  LBB12_29: ## %cond.store27
; SSE4-NEXT:    extractps $2, %xmm3, 56(%rdi)
; SSE4-NEXT:    testl $32768, %esi ## imm = 0x8000
; SSE4-NEXT:    je LBB12_32
; SSE4-NEXT:  LBB12_31: ## %cond.store29
; SSE4-NEXT:    extractps $3, %xmm3, 60(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v16f32_i16:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    pushq %rbp
; AVX1-NEXT:    pushq %r15
; AVX1-NEXT:    pushq %r14
; AVX1-NEXT:    pushq %r13
; AVX1-NEXT:    pushq %r12
; AVX1-NEXT:    pushq %rbx
; AVX1-NEXT:    movl %esi, %eax
; AVX1-NEXT:    andl $1, %eax
; AVX1-NEXT:    vmovd %eax, %xmm2
; AVX1-NEXT:    movl %esi, %eax
; AVX1-NEXT:    movl %esi, %ecx
; AVX1-NEXT:    movl %esi, %edx
; AVX1-NEXT:    movl %esi, %r8d
; AVX1-NEXT:    movl %esi, %r9d
; AVX1-NEXT:    movl %esi, %r10d
; AVX1-NEXT:    movl %esi, %r11d
; AVX1-NEXT:    movl %esi, %ebx
; AVX1-NEXT:    movl %esi, %ebp
; AVX1-NEXT:    movl %esi, %r14d
; AVX1-NEXT:    movl %esi, %r15d
; AVX1-NEXT:    movl %esi, %r12d
; AVX1-NEXT:    movl %esi, %r13d
; AVX1-NEXT:    shrl %r13d
; AVX1-NEXT:    andl $1, %r13d
; AVX1-NEXT:    vpinsrb $1, %r13d, %xmm2, %xmm2
; AVX1-NEXT:    movl %esi, %r13d
; AVX1-NEXT:    movzwl %si, %esi
; AVX1-NEXT:    shrl $2, %eax
; AVX1-NEXT:    andl $1, %eax
; AVX1-NEXT:    vpinsrb $2, %eax, %xmm2, %xmm2
; AVX1-NEXT:    shrl $3, %ecx
; AVX1-NEXT:    andl $1, %ecx
; AVX1-NEXT:    vpinsrb $3, %ecx, %xmm2, %xmm2
; AVX1-NEXT:    shrl $4, %edx
; AVX1-NEXT:    andl $1, %edx
; AVX1-NEXT:    vpinsrb $4, %edx, %xmm2, %xmm3
; AVX1-NEXT:    shrl $5, %r8d
; AVX1-NEXT:    andl $1, %r8d
; AVX1-NEXT:    vpinsrb $5, %r8d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $6, %r9d
; AVX1-NEXT:    andl $1, %r9d
; AVX1-NEXT:    vpinsrb $6, %r9d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $7, %r10d
; AVX1-NEXT:    andl $1, %r10d
; AVX1-NEXT:    vpinsrb $7, %r10d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $8, %r11d
; AVX1-NEXT:    andl $1, %r11d
; AVX1-NEXT:    vpinsrb $8, %r11d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $9, %ebx
; AVX1-NEXT:    andl $1, %ebx
; AVX1-NEXT:    vpinsrb $9, %ebx, %xmm3, %xmm3
; AVX1-NEXT:    shrl $10, %ebp
; AVX1-NEXT:    andl $1, %ebp
; AVX1-NEXT:    vpinsrb $10, %ebp, %xmm3, %xmm3
; AVX1-NEXT:    shrl $11, %r14d
; AVX1-NEXT:    andl $1, %r14d
; AVX1-NEXT:    vpinsrb $11, %r14d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $12, %r15d
; AVX1-NEXT:    andl $1, %r15d
; AVX1-NEXT:    vpinsrb $12, %r15d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $13, %r12d
; AVX1-NEXT:    andl $1, %r12d
; AVX1-NEXT:    vpinsrb $13, %r12d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $14, %r13d
; AVX1-NEXT:    andl $1, %r13d
; AVX1-NEXT:    vpinsrb $14, %r13d, %xmm3, %xmm3
; AVX1-NEXT:    shrl $15, %esi
; AVX1-NEXT:    vpinsrb $15, %esi, %xmm3, %xmm3
; AVX1-NEXT:    vpmovzxbd {{.*#+}} xmm2 = xmm2[0],zero,zero,zero,xmm2[1],zero,zero,zero,xmm2[2],zero,zero,zero,xmm2[3],zero,zero,zero
; AVX1-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX1-NEXT:    vpshufd {{.*#+}} xmm4 = xmm3[1,1,1,1]
; AVX1-NEXT:    vpmovzxbd {{.*#+}} xmm4 = xmm4[0],zero,zero,zero,xmm4[1],zero,zero,zero,xmm4[2],zero,zero,zero,xmm4[3],zero,zero,zero
; AVX1-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX1-NEXT:    vinsertf128 $1, %xmm4, %ymm2, %ymm2
; AVX1-NEXT:    vmaskmovps %ymm0, %ymm2, (%rdi)
; AVX1-NEXT:    vpunpckhbw {{.*#+}} xmm0 = xmm3[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
; AVX1-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX1-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX1-NEXT:    vpunpckhwd {{.*#+}} xmm0 = xmm0[4,4,5,5,6,6,7,7]
; AVX1-NEXT:    vpslld $31, %xmm0, %xmm0
; AVX1-NEXT:    vinsertf128 $1, %xmm0, %ymm2, %ymm0
; AVX1-NEXT:    vmaskmovps %ymm1, %ymm0, 32(%rdi)
; AVX1-NEXT:    popq %rbx
; AVX1-NEXT:    popq %r12
; AVX1-NEXT:    popq %r13
; AVX1-NEXT:    popq %r14
; AVX1-NEXT:    popq %r15
; AVX1-NEXT:    popq %rbp
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v16f32_i16:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    pushq %rbp
; AVX2-NEXT:    pushq %r15
; AVX2-NEXT:    pushq %r14
; AVX2-NEXT:    pushq %r13
; AVX2-NEXT:    pushq %r12
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    movl %esi, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    vmovd %eax, %xmm2
; AVX2-NEXT:    movl %esi, %eax
; AVX2-NEXT:    movl %esi, %ecx
; AVX2-NEXT:    movl %esi, %edx
; AVX2-NEXT:    movl %esi, %r8d
; AVX2-NEXT:    movl %esi, %r9d
; AVX2-NEXT:    movl %esi, %r10d
; AVX2-NEXT:    movl %esi, %r11d
; AVX2-NEXT:    movl %esi, %ebx
; AVX2-NEXT:    movl %esi, %ebp
; AVX2-NEXT:    movl %esi, %r14d
; AVX2-NEXT:    movl %esi, %r15d
; AVX2-NEXT:    movl %esi, %r12d
; AVX2-NEXT:    movl %esi, %r13d
; AVX2-NEXT:    shrl %r13d
; AVX2-NEXT:    andl $1, %r13d
; AVX2-NEXT:    vpinsrb $1, %r13d, %xmm2, %xmm2
; AVX2-NEXT:    movl %esi, %r13d
; AVX2-NEXT:    movzwl %si, %esi
; AVX2-NEXT:    shrl $2, %eax
; AVX2-NEXT:    andl $1, %eax
; AVX2-NEXT:    vpinsrb $2, %eax, %xmm2, %xmm2
; AVX2-NEXT:    shrl $3, %ecx
; AVX2-NEXT:    andl $1, %ecx
; AVX2-NEXT:    vpinsrb $3, %ecx, %xmm2, %xmm2
; AVX2-NEXT:    shrl $4, %edx
; AVX2-NEXT:    andl $1, %edx
; AVX2-NEXT:    vpinsrb $4, %edx, %xmm2, %xmm2
; AVX2-NEXT:    shrl $5, %r8d
; AVX2-NEXT:    andl $1, %r8d
; AVX2-NEXT:    vpinsrb $5, %r8d, %xmm2, %xmm2
; AVX2-NEXT:    shrl $6, %r9d
; AVX2-NEXT:    andl $1, %r9d
; AVX2-NEXT:    vpinsrb $6, %r9d, %xmm2, %xmm2
; AVX2-NEXT:    shrl $7, %r10d
; AVX2-NEXT:    andl $1, %r10d
; AVX2-NEXT:    vpinsrb $7, %r10d, %xmm2, %xmm2
; AVX2-NEXT:    shrl $8, %r11d
; AVX2-NEXT:    andl $1, %r11d
; AVX2-NEXT:    vpinsrb $8, %r11d, %xmm2, %xmm3
; AVX2-NEXT:    shrl $9, %ebx
; AVX2-NEXT:    andl $1, %ebx
; AVX2-NEXT:    vpinsrb $9, %ebx, %xmm3, %xmm3
; AVX2-NEXT:    shrl $10, %ebp
; AVX2-NEXT:    andl $1, %ebp
; AVX2-NEXT:    vpinsrb $10, %ebp, %xmm3, %xmm3
; AVX2-NEXT:    shrl $11, %r14d
; AVX2-NEXT:    andl $1, %r14d
; AVX2-NEXT:    vpinsrb $11, %r14d, %xmm3, %xmm3
; AVX2-NEXT:    shrl $12, %r15d
; AVX2-NEXT:    andl $1, %r15d
; AVX2-NEXT:    vpinsrb $12, %r15d, %xmm3, %xmm3
; AVX2-NEXT:    shrl $13, %r12d
; AVX2-NEXT:    andl $1, %r12d
; AVX2-NEXT:    vpinsrb $13, %r12d, %xmm3, %xmm3
; AVX2-NEXT:    shrl $14, %r13d
; AVX2-NEXT:    andl $1, %r13d
; AVX2-NEXT:    vpinsrb $14, %r13d, %xmm3, %xmm3
; AVX2-NEXT:    shrl $15, %esi
; AVX2-NEXT:    vpinsrb $15, %esi, %xmm3, %xmm3
; AVX2-NEXT:    vpmovzxbd {{.*#+}} ymm2 = xmm2[0],zero,zero,zero,xmm2[1],zero,zero,zero,xmm2[2],zero,zero,zero,xmm2[3],zero,zero,zero,xmm2[4],zero,zero,zero,xmm2[5],zero,zero,zero,xmm2[6],zero,zero,zero,xmm2[7],zero,zero,zero
; AVX2-NEXT:    vpslld $31, %ymm2, %ymm2
; AVX2-NEXT:    vmaskmovps %ymm0, %ymm2, (%rdi)
; AVX2-NEXT:    vpunpckhbw {{.*#+}} xmm0 = xmm3[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
; AVX2-NEXT:    vpslld $31, %ymm0, %ymm0
; AVX2-NEXT:    vmaskmovps %ymm1, %ymm0, 32(%rdi)
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    popq %r12
; AVX2-NEXT:    popq %r13
; AVX2-NEXT:    popq %r14
; AVX2-NEXT:    popq %r15
; AVX2-NEXT:    popq %rbp
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v16f32_i16:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    kmovw %esi, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v16f32_i16:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %esi, %k1
; AVX512VLDQ-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v16f32_i16:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    kmovd %esi, %k1
; AVX512VLBW-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v16f32_i16:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    kmovw {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    vmovups %zmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = bitcast i16 %trigger to <16 x i1>
  call void @llvm.masked.store.v16f32.p0(<16 x float> %x, ptr %ptr, i32 1, <16 x i1> %mask)
  ret void
}

define void @store_v16f32_v16i32(<16 x float> %x, ptr %ptr, <16 x float> %y, <16 x i32> %mask) nounwind {
; SSE2-LABEL: store_v16f32_v16i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm4
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm5
; SSE2-NEXT:    packssdw {{[0-9]+}}(%rsp), %xmm5
; SSE2-NEXT:    packssdw {{[0-9]+}}(%rsp), %xmm4
; SSE2-NEXT:    packsswb %xmm5, %xmm4
; SSE2-NEXT:    pmovmskb %xmm4, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB13_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB13_3
; SSE2-NEXT:  LBB13_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB13_5
; SSE2-NEXT:  LBB13_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB13_7
; SSE2-NEXT:  LBB13_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB13_9
; SSE2-NEXT:  LBB13_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB13_11
; SSE2-NEXT:  LBB13_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB13_13
; SSE2-NEXT:  LBB13_14: ## %else12
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    js LBB13_15
; SSE2-NEXT:  LBB13_16: ## %else14
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    jne LBB13_17
; SSE2-NEXT:  LBB13_18: ## %else16
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    jne LBB13_19
; SSE2-NEXT:  LBB13_20: ## %else18
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    jne LBB13_21
; SSE2-NEXT:  LBB13_22: ## %else20
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    jne LBB13_23
; SSE2-NEXT:  LBB13_24: ## %else22
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    jne LBB13_25
; SSE2-NEXT:  LBB13_26: ## %else24
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    jne LBB13_27
; SSE2-NEXT:  LBB13_28: ## %else26
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    jne LBB13_29
; SSE2-NEXT:  LBB13_30: ## %else28
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    jne LBB13_31
; SSE2-NEXT:  LBB13_32: ## %else30
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB13_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB13_4
; SSE2-NEXT:  LBB13_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm4
; SSE2-NEXT:    shufps {{.*#+}} xmm4 = xmm4[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm4, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB13_6
; SSE2-NEXT:  LBB13_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm4
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm4 = xmm4[1],xmm0[1]
; SSE2-NEXT:    movss %xmm4, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB13_8
; SSE2-NEXT:  LBB13_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB13_10
; SSE2-NEXT:  LBB13_9: ## %cond.store7
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB13_12
; SSE2-NEXT:  LBB13_11: ## %cond.store9
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm1[1,1]
; SSE2-NEXT:    movss %xmm0, 20(%rdi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB13_14
; SSE2-NEXT:  LBB13_13: ## %cond.store11
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm1[1]
; SSE2-NEXT:    movss %xmm0, 24(%rdi)
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    jns LBB13_16
; SSE2-NEXT:  LBB13_15: ## %cond.store13
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; SSE2-NEXT:    movss %xmm1, 28(%rdi)
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    je LBB13_18
; SSE2-NEXT:  LBB13_17: ## %cond.store15
; SSE2-NEXT:    movss %xmm2, 32(%rdi)
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    je LBB13_20
; SSE2-NEXT:  LBB13_19: ## %cond.store17
; SSE2-NEXT:    movaps %xmm2, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm2[1,1]
; SSE2-NEXT:    movss %xmm0, 36(%rdi)
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    je LBB13_22
; SSE2-NEXT:  LBB13_21: ## %cond.store19
; SSE2-NEXT:    movaps %xmm2, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm2[1]
; SSE2-NEXT:    movss %xmm0, 40(%rdi)
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    je LBB13_24
; SSE2-NEXT:  LBB13_23: ## %cond.store21
; SSE2-NEXT:    shufps {{.*#+}} xmm2 = xmm2[3,3,3,3]
; SSE2-NEXT:    movss %xmm2, 44(%rdi)
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    je LBB13_26
; SSE2-NEXT:  LBB13_25: ## %cond.store23
; SSE2-NEXT:    movss %xmm3, 48(%rdi)
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    je LBB13_28
; SSE2-NEXT:  LBB13_27: ## %cond.store25
; SSE2-NEXT:    movaps %xmm3, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm3[1,1]
; SSE2-NEXT:    movss %xmm0, 52(%rdi)
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    je LBB13_30
; SSE2-NEXT:  LBB13_29: ## %cond.store27
; SSE2-NEXT:    movaps %xmm3, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm3[1]
; SSE2-NEXT:    movss %xmm0, 56(%rdi)
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    je LBB13_32
; SSE2-NEXT:  LBB13_31: ## %cond.store29
; SSE2-NEXT:    shufps {{.*#+}} xmm3 = xmm3[3,3,3,3]
; SSE2-NEXT:    movss %xmm3, 60(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v16f32_v16i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm4
; SSE4-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm5
; SSE4-NEXT:    packssdw {{[0-9]+}}(%rsp), %xmm5
; SSE4-NEXT:    packssdw {{[0-9]+}}(%rsp), %xmm4
; SSE4-NEXT:    packsswb %xmm5, %xmm4
; SSE4-NEXT:    pmovmskb %xmm4, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB13_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB13_3
; SSE4-NEXT:  LBB13_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB13_5
; SSE4-NEXT:  LBB13_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB13_7
; SSE4-NEXT:  LBB13_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB13_9
; SSE4-NEXT:  LBB13_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB13_11
; SSE4-NEXT:  LBB13_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB13_13
; SSE4-NEXT:  LBB13_14: ## %else12
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    js LBB13_15
; SSE4-NEXT:  LBB13_16: ## %else14
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    jne LBB13_17
; SSE4-NEXT:  LBB13_18: ## %else16
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    jne LBB13_19
; SSE4-NEXT:  LBB13_20: ## %else18
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    jne LBB13_21
; SSE4-NEXT:  LBB13_22: ## %else20
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    jne LBB13_23
; SSE4-NEXT:  LBB13_24: ## %else22
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    jne LBB13_25
; SSE4-NEXT:  LBB13_26: ## %else24
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    jne LBB13_27
; SSE4-NEXT:  LBB13_28: ## %else26
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    jne LBB13_29
; SSE4-NEXT:  LBB13_30: ## %else28
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    jne LBB13_31
; SSE4-NEXT:  LBB13_32: ## %else30
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB13_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB13_4
; SSE4-NEXT:  LBB13_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB13_6
; SSE4-NEXT:  LBB13_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB13_8
; SSE4-NEXT:  LBB13_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB13_10
; SSE4-NEXT:  LBB13_9: ## %cond.store7
; SSE4-NEXT:    movss %xmm1, 16(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB13_12
; SSE4-NEXT:  LBB13_11: ## %cond.store9
; SSE4-NEXT:    extractps $1, %xmm1, 20(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB13_14
; SSE4-NEXT:  LBB13_13: ## %cond.store11
; SSE4-NEXT:    extractps $2, %xmm1, 24(%rdi)
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    jns LBB13_16
; SSE4-NEXT:  LBB13_15: ## %cond.store13
; SSE4-NEXT:    extractps $3, %xmm1, 28(%rdi)
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    je LBB13_18
; SSE4-NEXT:  LBB13_17: ## %cond.store15
; SSE4-NEXT:    movss %xmm2, 32(%rdi)
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    je LBB13_20
; SSE4-NEXT:  LBB13_19: ## %cond.store17
; SSE4-NEXT:    extractps $1, %xmm2, 36(%rdi)
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    je LBB13_22
; SSE4-NEXT:  LBB13_21: ## %cond.store19
; SSE4-NEXT:    extractps $2, %xmm2, 40(%rdi)
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    je LBB13_24
; SSE4-NEXT:  LBB13_23: ## %cond.store21
; SSE4-NEXT:    extractps $3, %xmm2, 44(%rdi)
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    je LBB13_26
; SSE4-NEXT:  LBB13_25: ## %cond.store23
; SSE4-NEXT:    movss %xmm3, 48(%rdi)
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    je LBB13_28
; SSE4-NEXT:  LBB13_27: ## %cond.store25
; SSE4-NEXT:    extractps $1, %xmm3, 52(%rdi)
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    je LBB13_30
; SSE4-NEXT:  LBB13_29: ## %cond.store27
; SSE4-NEXT:    extractps $2, %xmm3, 56(%rdi)
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    je LBB13_32
; SSE4-NEXT:  LBB13_31: ## %cond.store29
; SSE4-NEXT:    extractps $3, %xmm3, 60(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v16f32_v16i32:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovps %ymm1, %ymm5, 32(%rdi)
; AVX1OR2-NEXT:    vmaskmovps %ymm0, %ymm4, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v16f32_v16i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512F-NEXT:    vpcmpgtd %zmm2, %zmm1, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v16f32_v16i32:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovd2m %zmm2, %k1
; AVX512VLDQ-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v16f32_v16i32:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512VLBW-NEXT:    vpcmpgtd %zmm2, %zmm1, %k1
; AVX512VLBW-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v16f32_v16i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovd2m %zmm2, %k1
; X86-AVX512-NEXT:    vmovups %zmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %bool_mask = icmp slt <16 x i32> %mask, zeroinitializer
  call void @llvm.masked.store.v16f32.p0(<16 x float> %x, ptr %ptr, i32 1, <16 x i1> %bool_mask)
  ret void
}

;
; vXi64
;

define void @store_v2i64_v2i64(<2 x i64> %trigger, ptr %addr, <2 x i64> %val) nounwind {
; SSE2-LABEL: store_v2i64_v2i64:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movmskpd %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB14_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB14_3
; SSE2-NEXT:  LBB14_4: ## %else2
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB14_1: ## %cond.store
; SSE2-NEXT:    movq %xmm1, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB14_4
; SSE2-NEXT:  LBB14_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, 8(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v2i64_v2i64:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movmskpd %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB14_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB14_3
; SSE4-NEXT:  LBB14_4: ## %else2
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB14_1: ## %cond.store
; SSE4-NEXT:    movq %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB14_4
; SSE4-NEXT:  LBB14_3: ## %cond.store1
; SSE4-NEXT:    pextrq $1, %xmm1, 8(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v2i64_v2i64:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmaskmovpd %xmm1, %xmm0, (%rdi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v2i64_v2i64:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpmaskmovq %xmm1, %xmm0, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v2i64_v2i64:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpgtq %zmm0, %zmm2, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovdqu64 %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2i64_v2i64:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovq2m %xmm0, %k1
; AVX512VLDQ-NEXT:    vmovdqu64 %xmm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2i64_v2i64:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLBW-NEXT:    vpcmpgtq %xmm0, %xmm2, %k1
; AVX512VLBW-NEXT:    vmovdqu64 %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2i64_v2i64:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovq2m %xmm0, %k1
; X86-AVX512-NEXT:    vmovdqu64 %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp slt <2 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v2i64.p0(<2 x i64> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v4i64_v4i64(<4 x i64> %trigger, ptr %addr, <4 x i64> %val) nounwind {
; SSE2-LABEL: store_v4i64_v4i64:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,3],xmm1[1,3]
; SSE2-NEXT:    movmskps %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB15_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB15_3
; SSE2-NEXT:  LBB15_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB15_5
; SSE2-NEXT:  LBB15_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB15_7
; SSE2-NEXT:  LBB15_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB15_1: ## %cond.store
; SSE2-NEXT:    movq %xmm2, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB15_4
; SSE2-NEXT:  LBB15_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, 8(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB15_6
; SSE2-NEXT:  LBB15_5: ## %cond.store3
; SSE2-NEXT:    movq %xmm3, 16(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB15_8
; SSE2-NEXT:  LBB15_7: ## %cond.store5
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, 24(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v4i64_v4i64:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    packssdw %xmm1, %xmm0
; SSE4-NEXT:    movmskps %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB15_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB15_3
; SSE4-NEXT:  LBB15_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB15_5
; SSE4-NEXT:  LBB15_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB15_7
; SSE4-NEXT:  LBB15_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB15_1: ## %cond.store
; SSE4-NEXT:    movq %xmm2, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB15_4
; SSE4-NEXT:  LBB15_3: ## %cond.store1
; SSE4-NEXT:    pextrq $1, %xmm2, 8(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB15_6
; SSE4-NEXT:  LBB15_5: ## %cond.store3
; SSE4-NEXT:    movq %xmm3, 16(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB15_8
; SSE4-NEXT:  LBB15_7: ## %cond.store5
; SSE4-NEXT:    pextrq $1, %xmm3, 24(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v4i64_v4i64:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmaskmovpd %ymm1, %ymm0, (%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v4i64_v4i64:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpmaskmovq %ymm1, %ymm0, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v4i64_v4i64:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm1 killed $ymm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpgtq %zmm0, %zmm2, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovdqu64 %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v4i64_v4i64:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovq2m %ymm0, %k1
; AVX512VLDQ-NEXT:    vmovdqu64 %ymm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v4i64_v4i64:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLBW-NEXT:    vpcmpgtq %ymm0, %ymm2, %k1
; AVX512VLBW-NEXT:    vmovdqu64 %ymm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4i64_v4i64:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovq2m %ymm0, %k1
; X86-AVX512-NEXT:    vmovdqu64 %ymm1, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp slt <4 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v4i64.p0(<4 x i64> %val, ptr %addr, i32 4, <4 x i1> %mask)
  ret void
}

;
; vXi32
;

define void @store_v1i32_v1i32(<1 x i32> %trigger, ptr %addr, <1 x i32> %val) nounwind {
; SSE-LABEL: store_v1i32_v1i32:
; SSE:       ## %bb.0:
; SSE-NEXT:    testl %edi, %edi
; SSE-NEXT:    jne LBB16_2
; SSE-NEXT:  ## %bb.1: ## %cond.store
; SSE-NEXT:    movl %edx, (%rsi)
; SSE-NEXT:  LBB16_2: ## %else
; SSE-NEXT:    retq
;
; AVX-LABEL: store_v1i32_v1i32:
; AVX:       ## %bb.0:
; AVX-NEXT:    testl %edi, %edi
; AVX-NEXT:    jne LBB16_2
; AVX-NEXT:  ## %bb.1: ## %cond.store
; AVX-NEXT:    movl %edx, (%rsi)
; AVX-NEXT:  LBB16_2: ## %else
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: store_v1i32_v1i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    cmpl $0, {{[0-9]+}}(%esp)
; X86-AVX512-NEXT:    jne LBB16_2
; X86-AVX512-NEXT:  ## %bb.1: ## %cond.store
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-AVX512-NEXT:    movl %eax, (%ecx)
; X86-AVX512-NEXT:  LBB16_2: ## %else
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <1 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v1i32.p0(<1 x i32> %val, ptr %addr, i32 4, <1 x i1> %mask)
  ret void
}

define void @store_v2i32_v2i32(<2 x i32> %trigger, ptr %addr, <2 x i32> %val) nounwind {
; SSE2-LABEL: store_v2i32_v2i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[0,0,1,1]
; SSE2-NEXT:    pxor %xmm2, %xmm2
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE2-NEXT:    movmskpd %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB17_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB17_3
; SSE2-NEXT:  LBB17_4: ## %else2
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB17_1: ## %cond.store
; SSE2-NEXT:    movd %xmm1, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB17_4
; SSE2-NEXT:  LBB17_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[1,1,1,1]
; SSE2-NEXT:    movd %xmm0, 4(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v2i32_v2i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm2, %xmm2
; SSE4-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE4-NEXT:    pmovsxdq %xmm2, %xmm0
; SSE4-NEXT:    movmskpd %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB17_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB17_3
; SSE4-NEXT:  LBB17_4: ## %else2
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB17_1: ## %cond.store
; SSE4-NEXT:    movss %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB17_4
; SSE4-NEXT:  LBB17_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm1, 4(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v2i32_v2i32:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vmovq {{.*#+}} xmm0 = xmm0[0],zero
; AVX1-NEXT:    vmaskmovps %xmm1, %xmm0, (%rdi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v2i32_v2i32:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vmovq {{.*#+}} xmm0 = xmm0[0],zero
; AVX2-NEXT:    vpmaskmovd %xmm1, %xmm0, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v2i32_v2i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vptestnmd %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kshiftlw $14, %k0, %k0
; AVX512F-NEXT:    kshiftrw $14, %k0, %k1
; AVX512F-NEXT:    vmovdqu32 %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v2i32_v2i32:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; AVX512VLDQ-NEXT:    kshiftlb $6, %k0, %k0
; AVX512VLDQ-NEXT:    kshiftrb $6, %k0, %k1
; AVX512VLDQ-NEXT:    vmovdqu32 %xmm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v2i32_v2i32:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; AVX512VLBW-NEXT:    kshiftlw $14, %k0, %k0
; AVX512VLBW-NEXT:    kshiftrw $14, %k0, %k1
; AVX512VLBW-NEXT:    vmovdqu32 %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v2i32_v2i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmd %xmm0, %xmm0, %k0
; X86-AVX512-NEXT:    kshiftlb $6, %k0, %k0
; X86-AVX512-NEXT:    kshiftrb $6, %k0, %k1
; X86-AVX512-NEXT:    vmovdqu32 %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <2 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v2i32.p0(<2 x i32> %val, ptr %addr, i32 4, <2 x i1> %mask)
  ret void
}

define void @store_v4i32_v4i32(<4 x i32> %trigger, ptr %addr, <4 x i32> %val) nounwind {
; SSE2-LABEL: store_v4i32_v4i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm2, %xmm2
; SSE2-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE2-NEXT:    movmskps %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB18_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB18_3
; SSE2-NEXT:  LBB18_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB18_5
; SSE2-NEXT:  LBB18_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB18_7
; SSE2-NEXT:  LBB18_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB18_1: ## %cond.store
; SSE2-NEXT:    movd %xmm1, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB18_4
; SSE2-NEXT:  LBB18_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[1,1,1,1]
; SSE2-NEXT:    movd %xmm0, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB18_6
; SSE2-NEXT:  LBB18_5: ## %cond.store3
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[2,3,2,3]
; SSE2-NEXT:    movd %xmm0, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB18_8
; SSE2-NEXT:  LBB18_7: ## %cond.store5
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm1[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v4i32_v4i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm2, %xmm2
; SSE4-NEXT:    pcmpeqd %xmm0, %xmm2
; SSE4-NEXT:    movmskps %xmm2, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB18_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB18_3
; SSE4-NEXT:  LBB18_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB18_5
; SSE4-NEXT:  LBB18_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB18_7
; SSE4-NEXT:  LBB18_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB18_1: ## %cond.store
; SSE4-NEXT:    movss %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB18_4
; SSE4-NEXT:  LBB18_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm1, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB18_6
; SSE4-NEXT:  LBB18_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm1, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB18_8
; SSE4-NEXT:  LBB18_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm1, 12(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v4i32_v4i32:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vmaskmovps %xmm1, %xmm0, (%rdi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v4i32_v4i32:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX2-NEXT:    vpcmpeqd %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpmaskmovd %xmm1, %xmm0, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v4i32_v4i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vptestnmd %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovdqu32 %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: store_v4i32_v4i32:
; AVX512VL:       ## %bb.0:
; AVX512VL-NEXT:    vptestnmd %xmm0, %xmm0, %k1
; AVX512VL-NEXT:    vmovdqu32 %xmm1, (%rdi) {%k1}
; AVX512VL-NEXT:    retq
;
; X86-AVX512-LABEL: store_v4i32_v4i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmd %xmm0, %xmm0, %k1
; X86-AVX512-NEXT:    vmovdqu32 %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <4 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v4i32.p0(<4 x i32> %val, ptr %addr, i32 4, <4 x i1> %mask)
  ret void
}

define void @store_v8i32_v8i32(<8 x i32> %trigger, ptr %addr, <8 x i32> %val) nounwind {
; SSE2-LABEL: store_v8i32_v8i32:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm4, %xmm4
; SSE2-NEXT:    pcmpeqd %xmm4, %xmm1
; SSE2-NEXT:    pcmpeqd %xmm4, %xmm0
; SSE2-NEXT:    packssdw %xmm1, %xmm0
; SSE2-NEXT:    packsswb %xmm0, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB19_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB19_3
; SSE2-NEXT:  LBB19_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB19_5
; SSE2-NEXT:  LBB19_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB19_7
; SSE2-NEXT:  LBB19_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB19_9
; SSE2-NEXT:  LBB19_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB19_11
; SSE2-NEXT:  LBB19_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB19_13
; SSE2-NEXT:  LBB19_14: ## %else12
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    jne LBB19_15
; SSE2-NEXT:  LBB19_16: ## %else14
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB19_1: ## %cond.store
; SSE2-NEXT:    movd %xmm2, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB19_4
; SSE2-NEXT:  LBB19_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[1,1,1,1]
; SSE2-NEXT:    movd %xmm0, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB19_6
; SSE2-NEXT:  LBB19_5: ## %cond.store3
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[2,3,2,3]
; SSE2-NEXT:    movd %xmm0, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB19_8
; SSE2-NEXT:  LBB19_7: ## %cond.store5
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm2[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB19_10
; SSE2-NEXT:  LBB19_9: ## %cond.store7
; SSE2-NEXT:    movd %xmm3, 16(%rdi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB19_12
; SSE2-NEXT:  LBB19_11: ## %cond.store9
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[1,1,1,1]
; SSE2-NEXT:    movd %xmm0, 20(%rdi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB19_14
; SSE2-NEXT:  LBB19_13: ## %cond.store11
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[2,3,2,3]
; SSE2-NEXT:    movd %xmm0, 24(%rdi)
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    je LBB19_16
; SSE2-NEXT:  LBB19_15: ## %cond.store13
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm3[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 28(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v8i32_v8i32:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm4, %xmm4
; SSE4-NEXT:    pcmpeqd %xmm4, %xmm1
; SSE4-NEXT:    pcmpeqd %xmm4, %xmm0
; SSE4-NEXT:    packssdw %xmm1, %xmm0
; SSE4-NEXT:    packsswb %xmm0, %xmm0
; SSE4-NEXT:    pmovmskb %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB19_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB19_3
; SSE4-NEXT:  LBB19_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB19_5
; SSE4-NEXT:  LBB19_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB19_7
; SSE4-NEXT:  LBB19_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB19_9
; SSE4-NEXT:  LBB19_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB19_11
; SSE4-NEXT:  LBB19_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB19_13
; SSE4-NEXT:  LBB19_14: ## %else12
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    jne LBB19_15
; SSE4-NEXT:  LBB19_16: ## %else14
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB19_1: ## %cond.store
; SSE4-NEXT:    movss %xmm2, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB19_4
; SSE4-NEXT:  LBB19_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm2, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB19_6
; SSE4-NEXT:  LBB19_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm2, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB19_8
; SSE4-NEXT:  LBB19_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm2, 12(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB19_10
; SSE4-NEXT:  LBB19_9: ## %cond.store7
; SSE4-NEXT:    movss %xmm3, 16(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB19_12
; SSE4-NEXT:  LBB19_11: ## %cond.store9
; SSE4-NEXT:    extractps $1, %xmm3, 20(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB19_14
; SSE4-NEXT:  LBB19_13: ## %cond.store11
; SSE4-NEXT:    extractps $2, %xmm3, 24(%rdi)
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    je LBB19_16
; SSE4-NEXT:  LBB19_15: ## %cond.store13
; SSE4-NEXT:    extractps $3, %xmm3, 28(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v8i32_v8i32:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vextractf128 $1, %ymm0, %xmm2
; AVX1-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX1-NEXT:    vpcmpeqd %xmm3, %xmm2, %xmm2
; AVX1-NEXT:    vpcmpeqd %xmm3, %xmm0, %xmm0
; AVX1-NEXT:    vinsertf128 $1, %xmm2, %ymm0, %ymm0
; AVX1-NEXT:    vmaskmovps %ymm1, %ymm0, (%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v8i32_v8i32:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX2-NEXT:    vpcmpeqd %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpmaskmovd %ymm1, %ymm0, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v8i32_v8i32:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm1 killed $ymm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    vptestnmd %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kshiftlw $8, %k0, %k0
; AVX512F-NEXT:    kshiftrw $8, %k0, %k1
; AVX512F-NEXT:    vmovdqu32 %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: store_v8i32_v8i32:
; AVX512VL:       ## %bb.0:
; AVX512VL-NEXT:    vptestnmd %ymm0, %ymm0, %k1
; AVX512VL-NEXT:    vmovdqu32 %ymm1, (%rdi) {%k1}
; AVX512VL-NEXT:    vzeroupper
; AVX512VL-NEXT:    retq
;
; X86-AVX512-LABEL: store_v8i32_v8i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmd %ymm0, %ymm0, %k1
; X86-AVX512-NEXT:    vmovdqu32 %ymm1, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <8 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v8i32.p0(<8 x i32> %val, ptr %addr, i32 4, <8 x i1> %mask)
  ret void
}

;
; vXi16
;

define void @store_v8i16_v8i16(<8 x i16> %trigger, ptr %addr, <8 x i16> %val) nounwind {
; SSE2-LABEL: store_v8i16_v8i16:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm2, %xmm2
; SSE2-NEXT:    pcmpeqw %xmm0, %xmm2
; SSE2-NEXT:    packsswb %xmm2, %xmm2
; SSE2-NEXT:    pmovmskb %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB20_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB20_3
; SSE2-NEXT:  LBB20_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB20_5
; SSE2-NEXT:  LBB20_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB20_7
; SSE2-NEXT:  LBB20_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB20_9
; SSE2-NEXT:  LBB20_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB20_11
; SSE2-NEXT:  LBB20_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB20_13
; SSE2-NEXT:  LBB20_14: ## %else12
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    jne LBB20_15
; SSE2-NEXT:  LBB20_16: ## %else14
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB20_1: ## %cond.store
; SSE2-NEXT:    movd %xmm1, %ecx
; SSE2-NEXT:    movw %cx, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB20_4
; SSE2-NEXT:  LBB20_3: ## %cond.store1
; SSE2-NEXT:    pextrw $1, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 2(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB20_6
; SSE2-NEXT:  LBB20_5: ## %cond.store3
; SSE2-NEXT:    pextrw $2, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 4(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB20_8
; SSE2-NEXT:  LBB20_7: ## %cond.store5
; SSE2-NEXT:    pextrw $3, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 6(%rdi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB20_10
; SSE2-NEXT:  LBB20_9: ## %cond.store7
; SSE2-NEXT:    pextrw $4, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 8(%rdi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB20_12
; SSE2-NEXT:  LBB20_11: ## %cond.store9
; SSE2-NEXT:    pextrw $5, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 10(%rdi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB20_14
; SSE2-NEXT:  LBB20_13: ## %cond.store11
; SSE2-NEXT:    pextrw $6, %xmm1, %ecx
; SSE2-NEXT:    movw %cx, 12(%rdi)
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    je LBB20_16
; SSE2-NEXT:  LBB20_15: ## %cond.store13
; SSE2-NEXT:    pextrw $7, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 14(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v8i16_v8i16:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm2, %xmm2
; SSE4-NEXT:    pcmpeqw %xmm0, %xmm2
; SSE4-NEXT:    packsswb %xmm2, %xmm2
; SSE4-NEXT:    pmovmskb %xmm2, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB20_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB20_3
; SSE4-NEXT:  LBB20_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB20_5
; SSE4-NEXT:  LBB20_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB20_7
; SSE4-NEXT:  LBB20_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB20_9
; SSE4-NEXT:  LBB20_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB20_11
; SSE4-NEXT:  LBB20_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB20_13
; SSE4-NEXT:  LBB20_14: ## %else12
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    jne LBB20_15
; SSE4-NEXT:  LBB20_16: ## %else14
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB20_1: ## %cond.store
; SSE4-NEXT:    pextrw $0, %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB20_4
; SSE4-NEXT:  LBB20_3: ## %cond.store1
; SSE4-NEXT:    pextrw $1, %xmm1, 2(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB20_6
; SSE4-NEXT:  LBB20_5: ## %cond.store3
; SSE4-NEXT:    pextrw $2, %xmm1, 4(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB20_8
; SSE4-NEXT:  LBB20_7: ## %cond.store5
; SSE4-NEXT:    pextrw $3, %xmm1, 6(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB20_10
; SSE4-NEXT:  LBB20_9: ## %cond.store7
; SSE4-NEXT:    pextrw $4, %xmm1, 8(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB20_12
; SSE4-NEXT:  LBB20_11: ## %cond.store9
; SSE4-NEXT:    pextrw $5, %xmm1, 10(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB20_14
; SSE4-NEXT:  LBB20_13: ## %cond.store11
; SSE4-NEXT:    pextrw $6, %xmm1, 12(%rdi)
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    je LBB20_16
; SSE4-NEXT:  LBB20_15: ## %cond.store13
; SSE4-NEXT:    pextrw $7, %xmm1, 14(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v8i16_v8i16:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1OR2-NEXT:    vpcmpeqw %xmm2, %xmm0, %xmm0
; AVX1OR2-NEXT:    vpacksswb %xmm0, %xmm0, %xmm0
; AVX1OR2-NEXT:    vpmovmskb %xmm0, %eax
; AVX1OR2-NEXT:    testb $1, %al
; AVX1OR2-NEXT:    jne LBB20_1
; AVX1OR2-NEXT:  ## %bb.2: ## %else
; AVX1OR2-NEXT:    testb $2, %al
; AVX1OR2-NEXT:    jne LBB20_3
; AVX1OR2-NEXT:  LBB20_4: ## %else2
; AVX1OR2-NEXT:    testb $4, %al
; AVX1OR2-NEXT:    jne LBB20_5
; AVX1OR2-NEXT:  LBB20_6: ## %else4
; AVX1OR2-NEXT:    testb $8, %al
; AVX1OR2-NEXT:    jne LBB20_7
; AVX1OR2-NEXT:  LBB20_8: ## %else6
; AVX1OR2-NEXT:    testb $16, %al
; AVX1OR2-NEXT:    jne LBB20_9
; AVX1OR2-NEXT:  LBB20_10: ## %else8
; AVX1OR2-NEXT:    testb $32, %al
; AVX1OR2-NEXT:    jne LBB20_11
; AVX1OR2-NEXT:  LBB20_12: ## %else10
; AVX1OR2-NEXT:    testb $64, %al
; AVX1OR2-NEXT:    jne LBB20_13
; AVX1OR2-NEXT:  LBB20_14: ## %else12
; AVX1OR2-NEXT:    testb $-128, %al
; AVX1OR2-NEXT:    jne LBB20_15
; AVX1OR2-NEXT:  LBB20_16: ## %else14
; AVX1OR2-NEXT:    retq
; AVX1OR2-NEXT:  LBB20_1: ## %cond.store
; AVX1OR2-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX1OR2-NEXT:    testb $2, %al
; AVX1OR2-NEXT:    je LBB20_4
; AVX1OR2-NEXT:  LBB20_3: ## %cond.store1
; AVX1OR2-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX1OR2-NEXT:    testb $4, %al
; AVX1OR2-NEXT:    je LBB20_6
; AVX1OR2-NEXT:  LBB20_5: ## %cond.store3
; AVX1OR2-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX1OR2-NEXT:    testb $8, %al
; AVX1OR2-NEXT:    je LBB20_8
; AVX1OR2-NEXT:  LBB20_7: ## %cond.store5
; AVX1OR2-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX1OR2-NEXT:    testb $16, %al
; AVX1OR2-NEXT:    je LBB20_10
; AVX1OR2-NEXT:  LBB20_9: ## %cond.store7
; AVX1OR2-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX1OR2-NEXT:    testb $32, %al
; AVX1OR2-NEXT:    je LBB20_12
; AVX1OR2-NEXT:  LBB20_11: ## %cond.store9
; AVX1OR2-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX1OR2-NEXT:    testb $64, %al
; AVX1OR2-NEXT:    je LBB20_14
; AVX1OR2-NEXT:  LBB20_13: ## %cond.store11
; AVX1OR2-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX1OR2-NEXT:    testb $-128, %al
; AVX1OR2-NEXT:    je LBB20_16
; AVX1OR2-NEXT:  LBB20_15: ## %cond.store13
; AVX1OR2-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v8i16_v8i16:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpeqw %xmm2, %xmm0, %xmm0
; AVX512F-NEXT:    vpmovsxwq %xmm0, %zmm0
; AVX512F-NEXT:    vptestmq %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kmovw %k0, %eax
; AVX512F-NEXT:    testb $1, %al
; AVX512F-NEXT:    jne LBB20_1
; AVX512F-NEXT:  ## %bb.2: ## %else
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    jne LBB20_3
; AVX512F-NEXT:  LBB20_4: ## %else2
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    jne LBB20_5
; AVX512F-NEXT:  LBB20_6: ## %else4
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    jne LBB20_7
; AVX512F-NEXT:  LBB20_8: ## %else6
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    jne LBB20_9
; AVX512F-NEXT:  LBB20_10: ## %else8
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    jne LBB20_11
; AVX512F-NEXT:  LBB20_12: ## %else10
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    jne LBB20_13
; AVX512F-NEXT:  LBB20_14: ## %else12
; AVX512F-NEXT:    testb $-128, %al
; AVX512F-NEXT:    jne LBB20_15
; AVX512F-NEXT:  LBB20_16: ## %else14
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
; AVX512F-NEXT:  LBB20_1: ## %cond.store
; AVX512F-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    je LBB20_4
; AVX512F-NEXT:  LBB20_3: ## %cond.store1
; AVX512F-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    je LBB20_6
; AVX512F-NEXT:  LBB20_5: ## %cond.store3
; AVX512F-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    je LBB20_8
; AVX512F-NEXT:  LBB20_7: ## %cond.store5
; AVX512F-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    je LBB20_10
; AVX512F-NEXT:  LBB20_9: ## %cond.store7
; AVX512F-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    je LBB20_12
; AVX512F-NEXT:  LBB20_11: ## %cond.store9
; AVX512F-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    je LBB20_14
; AVX512F-NEXT:  LBB20_13: ## %cond.store11
; AVX512F-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX512F-NEXT:    testb $-128, %al
; AVX512F-NEXT:    je LBB20_16
; AVX512F-NEXT:  LBB20_15: ## %cond.store13
; AVX512F-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v8i16_v8i16:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLDQ-NEXT:    vpcmpeqw %xmm2, %xmm0, %xmm0
; AVX512VLDQ-NEXT:    vpmovsxwd %xmm0, %ymm0
; AVX512VLDQ-NEXT:    vmovmskps %ymm0, %eax
; AVX512VLDQ-NEXT:    testb $1, %al
; AVX512VLDQ-NEXT:    jne LBB20_1
; AVX512VLDQ-NEXT:  ## %bb.2: ## %else
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    jne LBB20_3
; AVX512VLDQ-NEXT:  LBB20_4: ## %else2
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    jne LBB20_5
; AVX512VLDQ-NEXT:  LBB20_6: ## %else4
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    jne LBB20_7
; AVX512VLDQ-NEXT:  LBB20_8: ## %else6
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    jne LBB20_9
; AVX512VLDQ-NEXT:  LBB20_10: ## %else8
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    jne LBB20_11
; AVX512VLDQ-NEXT:  LBB20_12: ## %else10
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    jne LBB20_13
; AVX512VLDQ-NEXT:  LBB20_14: ## %else12
; AVX512VLDQ-NEXT:    testb $-128, %al
; AVX512VLDQ-NEXT:    jne LBB20_15
; AVX512VLDQ-NEXT:  LBB20_16: ## %else14
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
; AVX512VLDQ-NEXT:  LBB20_1: ## %cond.store
; AVX512VLDQ-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    je LBB20_4
; AVX512VLDQ-NEXT:  LBB20_3: ## %cond.store1
; AVX512VLDQ-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    je LBB20_6
; AVX512VLDQ-NEXT:  LBB20_5: ## %cond.store3
; AVX512VLDQ-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    je LBB20_8
; AVX512VLDQ-NEXT:  LBB20_7: ## %cond.store5
; AVX512VLDQ-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    je LBB20_10
; AVX512VLDQ-NEXT:  LBB20_9: ## %cond.store7
; AVX512VLDQ-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    je LBB20_12
; AVX512VLDQ-NEXT:  LBB20_11: ## %cond.store9
; AVX512VLDQ-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    je LBB20_14
; AVX512VLDQ-NEXT:  LBB20_13: ## %cond.store11
; AVX512VLDQ-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX512VLDQ-NEXT:    testb $-128, %al
; AVX512VLDQ-NEXT:    je LBB20_16
; AVX512VLDQ-NEXT:  LBB20_15: ## %cond.store13
; AVX512VLDQ-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v8i16_v8i16:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmw %xmm0, %xmm0, %k1
; AVX512VLBW-NEXT:    vmovdqu16 %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v8i16_v8i16:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmw %xmm0, %xmm0, %k1
; X86-AVX512-NEXT:    vmovdqu16 %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <8 x i16> %trigger, zeroinitializer
  call void @llvm.masked.store.v8i16.p0(<8 x i16> %val, ptr %addr, i32 4, <8 x i1> %mask)
  ret void
}

define void @store_v16i16_v16i16(<16 x i16> %trigger, ptr %addr, <16 x i16> %val) nounwind {
; SSE2-LABEL: store_v16i16_v16i16:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm4, %xmm4
; SSE2-NEXT:    pcmpeqw %xmm4, %xmm1
; SSE2-NEXT:    pcmpeqw %xmm4, %xmm0
; SSE2-NEXT:    packsswb %xmm1, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB21_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB21_3
; SSE2-NEXT:  LBB21_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB21_5
; SSE2-NEXT:  LBB21_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB21_7
; SSE2-NEXT:  LBB21_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB21_9
; SSE2-NEXT:  LBB21_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB21_11
; SSE2-NEXT:  LBB21_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB21_13
; SSE2-NEXT:  LBB21_14: ## %else12
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    js LBB21_15
; SSE2-NEXT:  LBB21_16: ## %else14
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    jne LBB21_17
; SSE2-NEXT:  LBB21_18: ## %else16
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    jne LBB21_19
; SSE2-NEXT:  LBB21_20: ## %else18
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    jne LBB21_21
; SSE2-NEXT:  LBB21_22: ## %else20
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    jne LBB21_23
; SSE2-NEXT:  LBB21_24: ## %else22
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    jne LBB21_25
; SSE2-NEXT:  LBB21_26: ## %else24
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    jne LBB21_27
; SSE2-NEXT:  LBB21_28: ## %else26
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    jne LBB21_29
; SSE2-NEXT:  LBB21_30: ## %else28
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    jne LBB21_31
; SSE2-NEXT:  LBB21_32: ## %else30
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB21_1: ## %cond.store
; SSE2-NEXT:    movd %xmm2, %ecx
; SSE2-NEXT:    movw %cx, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB21_4
; SSE2-NEXT:  LBB21_3: ## %cond.store1
; SSE2-NEXT:    pextrw $1, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 2(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB21_6
; SSE2-NEXT:  LBB21_5: ## %cond.store3
; SSE2-NEXT:    pextrw $2, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 4(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB21_8
; SSE2-NEXT:  LBB21_7: ## %cond.store5
; SSE2-NEXT:    pextrw $3, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 6(%rdi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB21_10
; SSE2-NEXT:  LBB21_9: ## %cond.store7
; SSE2-NEXT:    pextrw $4, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 8(%rdi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB21_12
; SSE2-NEXT:  LBB21_11: ## %cond.store9
; SSE2-NEXT:    pextrw $5, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 10(%rdi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB21_14
; SSE2-NEXT:  LBB21_13: ## %cond.store11
; SSE2-NEXT:    pextrw $6, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 12(%rdi)
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    jns LBB21_16
; SSE2-NEXT:  LBB21_15: ## %cond.store13
; SSE2-NEXT:    pextrw $7, %xmm2, %ecx
; SSE2-NEXT:    movw %cx, 14(%rdi)
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    je LBB21_18
; SSE2-NEXT:  LBB21_17: ## %cond.store15
; SSE2-NEXT:    movd %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 16(%rdi)
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    je LBB21_20
; SSE2-NEXT:  LBB21_19: ## %cond.store17
; SSE2-NEXT:    pextrw $1, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 18(%rdi)
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    je LBB21_22
; SSE2-NEXT:  LBB21_21: ## %cond.store19
; SSE2-NEXT:    pextrw $2, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 20(%rdi)
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    je LBB21_24
; SSE2-NEXT:  LBB21_23: ## %cond.store21
; SSE2-NEXT:    pextrw $3, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 22(%rdi)
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    je LBB21_26
; SSE2-NEXT:  LBB21_25: ## %cond.store23
; SSE2-NEXT:    pextrw $4, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 24(%rdi)
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    je LBB21_28
; SSE2-NEXT:  LBB21_27: ## %cond.store25
; SSE2-NEXT:    pextrw $5, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 26(%rdi)
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    je LBB21_30
; SSE2-NEXT:  LBB21_29: ## %cond.store27
; SSE2-NEXT:    pextrw $6, %xmm3, %ecx
; SSE2-NEXT:    movw %cx, 28(%rdi)
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    je LBB21_32
; SSE2-NEXT:  LBB21_31: ## %cond.store29
; SSE2-NEXT:    pextrw $7, %xmm3, %eax
; SSE2-NEXT:    movw %ax, 30(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v16i16_v16i16:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm4, %xmm4
; SSE4-NEXT:    pcmpeqw %xmm4, %xmm1
; SSE4-NEXT:    pcmpeqw %xmm4, %xmm0
; SSE4-NEXT:    packsswb %xmm1, %xmm0
; SSE4-NEXT:    pmovmskb %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB21_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB21_3
; SSE4-NEXT:  LBB21_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB21_5
; SSE4-NEXT:  LBB21_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB21_7
; SSE4-NEXT:  LBB21_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB21_9
; SSE4-NEXT:  LBB21_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB21_11
; SSE4-NEXT:  LBB21_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB21_13
; SSE4-NEXT:  LBB21_14: ## %else12
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    js LBB21_15
; SSE4-NEXT:  LBB21_16: ## %else14
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    jne LBB21_17
; SSE4-NEXT:  LBB21_18: ## %else16
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    jne LBB21_19
; SSE4-NEXT:  LBB21_20: ## %else18
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    jne LBB21_21
; SSE4-NEXT:  LBB21_22: ## %else20
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    jne LBB21_23
; SSE4-NEXT:  LBB21_24: ## %else22
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    jne LBB21_25
; SSE4-NEXT:  LBB21_26: ## %else24
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    jne LBB21_27
; SSE4-NEXT:  LBB21_28: ## %else26
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    jne LBB21_29
; SSE4-NEXT:  LBB21_30: ## %else28
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    jne LBB21_31
; SSE4-NEXT:  LBB21_32: ## %else30
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB21_1: ## %cond.store
; SSE4-NEXT:    pextrw $0, %xmm2, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB21_4
; SSE4-NEXT:  LBB21_3: ## %cond.store1
; SSE4-NEXT:    pextrw $1, %xmm2, 2(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB21_6
; SSE4-NEXT:  LBB21_5: ## %cond.store3
; SSE4-NEXT:    pextrw $2, %xmm2, 4(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB21_8
; SSE4-NEXT:  LBB21_7: ## %cond.store5
; SSE4-NEXT:    pextrw $3, %xmm2, 6(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB21_10
; SSE4-NEXT:  LBB21_9: ## %cond.store7
; SSE4-NEXT:    pextrw $4, %xmm2, 8(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB21_12
; SSE4-NEXT:  LBB21_11: ## %cond.store9
; SSE4-NEXT:    pextrw $5, %xmm2, 10(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB21_14
; SSE4-NEXT:  LBB21_13: ## %cond.store11
; SSE4-NEXT:    pextrw $6, %xmm2, 12(%rdi)
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    jns LBB21_16
; SSE4-NEXT:  LBB21_15: ## %cond.store13
; SSE4-NEXT:    pextrw $7, %xmm2, 14(%rdi)
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    je LBB21_18
; SSE4-NEXT:  LBB21_17: ## %cond.store15
; SSE4-NEXT:    pextrw $0, %xmm3, 16(%rdi)
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    je LBB21_20
; SSE4-NEXT:  LBB21_19: ## %cond.store17
; SSE4-NEXT:    pextrw $1, %xmm3, 18(%rdi)
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    je LBB21_22
; SSE4-NEXT:  LBB21_21: ## %cond.store19
; SSE4-NEXT:    pextrw $2, %xmm3, 20(%rdi)
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    je LBB21_24
; SSE4-NEXT:  LBB21_23: ## %cond.store21
; SSE4-NEXT:    pextrw $3, %xmm3, 22(%rdi)
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    je LBB21_26
; SSE4-NEXT:  LBB21_25: ## %cond.store23
; SSE4-NEXT:    pextrw $4, %xmm3, 24(%rdi)
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    je LBB21_28
; SSE4-NEXT:  LBB21_27: ## %cond.store25
; SSE4-NEXT:    pextrw $5, %xmm3, 26(%rdi)
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    je LBB21_30
; SSE4-NEXT:  LBB21_29: ## %cond.store27
; SSE4-NEXT:    pextrw $6, %xmm3, 28(%rdi)
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    je LBB21_32
; SSE4-NEXT:  LBB21_31: ## %cond.store29
; SSE4-NEXT:    pextrw $7, %xmm3, 30(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v16i16_v16i16:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vextractf128 $1, %ymm0, %xmm2
; AVX1-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX1-NEXT:    vpcmpeqw %xmm3, %xmm2, %xmm2
; AVX1-NEXT:    vpcmpeqw %xmm3, %xmm0, %xmm0
; AVX1-NEXT:    vpacksswb %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vpmovmskb %xmm0, %eax
; AVX1-NEXT:    testb $1, %al
; AVX1-NEXT:    jne LBB21_1
; AVX1-NEXT:  ## %bb.2: ## %else
; AVX1-NEXT:    testb $2, %al
; AVX1-NEXT:    jne LBB21_3
; AVX1-NEXT:  LBB21_4: ## %else2
; AVX1-NEXT:    testb $4, %al
; AVX1-NEXT:    jne LBB21_5
; AVX1-NEXT:  LBB21_6: ## %else4
; AVX1-NEXT:    testb $8, %al
; AVX1-NEXT:    jne LBB21_7
; AVX1-NEXT:  LBB21_8: ## %else6
; AVX1-NEXT:    testb $16, %al
; AVX1-NEXT:    jne LBB21_9
; AVX1-NEXT:  LBB21_10: ## %else8
; AVX1-NEXT:    testb $32, %al
; AVX1-NEXT:    jne LBB21_11
; AVX1-NEXT:  LBB21_12: ## %else10
; AVX1-NEXT:    testb $64, %al
; AVX1-NEXT:    jne LBB21_13
; AVX1-NEXT:  LBB21_14: ## %else12
; AVX1-NEXT:    testb %al, %al
; AVX1-NEXT:    jns LBB21_16
; AVX1-NEXT:  LBB21_15: ## %cond.store13
; AVX1-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX1-NEXT:  LBB21_16: ## %else14
; AVX1-NEXT:    testl $256, %eax ## imm = 0x100
; AVX1-NEXT:    vextractf128 $1, %ymm1, %xmm0
; AVX1-NEXT:    jne LBB21_17
; AVX1-NEXT:  ## %bb.18: ## %else16
; AVX1-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1-NEXT:    jne LBB21_19
; AVX1-NEXT:  LBB21_20: ## %else18
; AVX1-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1-NEXT:    jne LBB21_21
; AVX1-NEXT:  LBB21_22: ## %else20
; AVX1-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1-NEXT:    jne LBB21_23
; AVX1-NEXT:  LBB21_24: ## %else22
; AVX1-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1-NEXT:    jne LBB21_25
; AVX1-NEXT:  LBB21_26: ## %else24
; AVX1-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1-NEXT:    jne LBB21_27
; AVX1-NEXT:  LBB21_28: ## %else26
; AVX1-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1-NEXT:    jne LBB21_29
; AVX1-NEXT:  LBB21_30: ## %else28
; AVX1-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX1-NEXT:    jne LBB21_31
; AVX1-NEXT:  LBB21_32: ## %else30
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
; AVX1-NEXT:  LBB21_1: ## %cond.store
; AVX1-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX1-NEXT:    testb $2, %al
; AVX1-NEXT:    je LBB21_4
; AVX1-NEXT:  LBB21_3: ## %cond.store1
; AVX1-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX1-NEXT:    testb $4, %al
; AVX1-NEXT:    je LBB21_6
; AVX1-NEXT:  LBB21_5: ## %cond.store3
; AVX1-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX1-NEXT:    testb $8, %al
; AVX1-NEXT:    je LBB21_8
; AVX1-NEXT:  LBB21_7: ## %cond.store5
; AVX1-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX1-NEXT:    testb $16, %al
; AVX1-NEXT:    je LBB21_10
; AVX1-NEXT:  LBB21_9: ## %cond.store7
; AVX1-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX1-NEXT:    testb $32, %al
; AVX1-NEXT:    je LBB21_12
; AVX1-NEXT:  LBB21_11: ## %cond.store9
; AVX1-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX1-NEXT:    testb $64, %al
; AVX1-NEXT:    je LBB21_14
; AVX1-NEXT:  LBB21_13: ## %cond.store11
; AVX1-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX1-NEXT:    testb %al, %al
; AVX1-NEXT:    js LBB21_15
; AVX1-NEXT:    jmp LBB21_16
; AVX1-NEXT:  LBB21_17: ## %cond.store15
; AVX1-NEXT:    vpextrw $0, %xmm0, 16(%rdi)
; AVX1-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1-NEXT:    je LBB21_20
; AVX1-NEXT:  LBB21_19: ## %cond.store17
; AVX1-NEXT:    vpextrw $1, %xmm0, 18(%rdi)
; AVX1-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1-NEXT:    je LBB21_22
; AVX1-NEXT:  LBB21_21: ## %cond.store19
; AVX1-NEXT:    vpextrw $2, %xmm0, 20(%rdi)
; AVX1-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1-NEXT:    je LBB21_24
; AVX1-NEXT:  LBB21_23: ## %cond.store21
; AVX1-NEXT:    vpextrw $3, %xmm0, 22(%rdi)
; AVX1-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1-NEXT:    je LBB21_26
; AVX1-NEXT:  LBB21_25: ## %cond.store23
; AVX1-NEXT:    vpextrw $4, %xmm0, 24(%rdi)
; AVX1-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1-NEXT:    je LBB21_28
; AVX1-NEXT:  LBB21_27: ## %cond.store25
; AVX1-NEXT:    vpextrw $5, %xmm0, 26(%rdi)
; AVX1-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1-NEXT:    je LBB21_30
; AVX1-NEXT:  LBB21_29: ## %cond.store27
; AVX1-NEXT:    vpextrw $6, %xmm0, 28(%rdi)
; AVX1-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX1-NEXT:    je LBB21_32
; AVX1-NEXT:  LBB21_31: ## %cond.store29
; AVX1-NEXT:    vpextrw $7, %xmm0, 30(%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v16i16_v16i16:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX2-NEXT:    vpcmpeqw %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vextracti128 $1, %ymm0, %xmm2
; AVX2-NEXT:    vpacksswb %xmm2, %xmm0, %xmm0
; AVX2-NEXT:    vpmovmskb %xmm0, %eax
; AVX2-NEXT:    testb $1, %al
; AVX2-NEXT:    jne LBB21_1
; AVX2-NEXT:  ## %bb.2: ## %else
; AVX2-NEXT:    testb $2, %al
; AVX2-NEXT:    jne LBB21_3
; AVX2-NEXT:  LBB21_4: ## %else2
; AVX2-NEXT:    testb $4, %al
; AVX2-NEXT:    jne LBB21_5
; AVX2-NEXT:  LBB21_6: ## %else4
; AVX2-NEXT:    testb $8, %al
; AVX2-NEXT:    jne LBB21_7
; AVX2-NEXT:  LBB21_8: ## %else6
; AVX2-NEXT:    testb $16, %al
; AVX2-NEXT:    jne LBB21_9
; AVX2-NEXT:  LBB21_10: ## %else8
; AVX2-NEXT:    testb $32, %al
; AVX2-NEXT:    jne LBB21_11
; AVX2-NEXT:  LBB21_12: ## %else10
; AVX2-NEXT:    testb $64, %al
; AVX2-NEXT:    jne LBB21_13
; AVX2-NEXT:  LBB21_14: ## %else12
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    jns LBB21_16
; AVX2-NEXT:  LBB21_15: ## %cond.store13
; AVX2-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX2-NEXT:  LBB21_16: ## %else14
; AVX2-NEXT:    testl $256, %eax ## imm = 0x100
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX2-NEXT:    jne LBB21_17
; AVX2-NEXT:  ## %bb.18: ## %else16
; AVX2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX2-NEXT:    jne LBB21_19
; AVX2-NEXT:  LBB21_20: ## %else18
; AVX2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX2-NEXT:    jne LBB21_21
; AVX2-NEXT:  LBB21_22: ## %else20
; AVX2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX2-NEXT:    jne LBB21_23
; AVX2-NEXT:  LBB21_24: ## %else22
; AVX2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX2-NEXT:    jne LBB21_25
; AVX2-NEXT:  LBB21_26: ## %else24
; AVX2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX2-NEXT:    jne LBB21_27
; AVX2-NEXT:  LBB21_28: ## %else26
; AVX2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX2-NEXT:    jne LBB21_29
; AVX2-NEXT:  LBB21_30: ## %else28
; AVX2-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX2-NEXT:    jne LBB21_31
; AVX2-NEXT:  LBB21_32: ## %else30
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
; AVX2-NEXT:  LBB21_1: ## %cond.store
; AVX2-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX2-NEXT:    testb $2, %al
; AVX2-NEXT:    je LBB21_4
; AVX2-NEXT:  LBB21_3: ## %cond.store1
; AVX2-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX2-NEXT:    testb $4, %al
; AVX2-NEXT:    je LBB21_6
; AVX2-NEXT:  LBB21_5: ## %cond.store3
; AVX2-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX2-NEXT:    testb $8, %al
; AVX2-NEXT:    je LBB21_8
; AVX2-NEXT:  LBB21_7: ## %cond.store5
; AVX2-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX2-NEXT:    testb $16, %al
; AVX2-NEXT:    je LBB21_10
; AVX2-NEXT:  LBB21_9: ## %cond.store7
; AVX2-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX2-NEXT:    testb $32, %al
; AVX2-NEXT:    je LBB21_12
; AVX2-NEXT:  LBB21_11: ## %cond.store9
; AVX2-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX2-NEXT:    testb $64, %al
; AVX2-NEXT:    je LBB21_14
; AVX2-NEXT:  LBB21_13: ## %cond.store11
; AVX2-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    js LBB21_15
; AVX2-NEXT:    jmp LBB21_16
; AVX2-NEXT:  LBB21_17: ## %cond.store15
; AVX2-NEXT:    vpextrw $0, %xmm0, 16(%rdi)
; AVX2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX2-NEXT:    je LBB21_20
; AVX2-NEXT:  LBB21_19: ## %cond.store17
; AVX2-NEXT:    vpextrw $1, %xmm0, 18(%rdi)
; AVX2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX2-NEXT:    je LBB21_22
; AVX2-NEXT:  LBB21_21: ## %cond.store19
; AVX2-NEXT:    vpextrw $2, %xmm0, 20(%rdi)
; AVX2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX2-NEXT:    je LBB21_24
; AVX2-NEXT:  LBB21_23: ## %cond.store21
; AVX2-NEXT:    vpextrw $3, %xmm0, 22(%rdi)
; AVX2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX2-NEXT:    je LBB21_26
; AVX2-NEXT:  LBB21_25: ## %cond.store23
; AVX2-NEXT:    vpextrw $4, %xmm0, 24(%rdi)
; AVX2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX2-NEXT:    je LBB21_28
; AVX2-NEXT:  LBB21_27: ## %cond.store25
; AVX2-NEXT:    vpextrw $5, %xmm0, 26(%rdi)
; AVX2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX2-NEXT:    je LBB21_30
; AVX2-NEXT:  LBB21_29: ## %cond.store27
; AVX2-NEXT:    vpextrw $6, %xmm0, 28(%rdi)
; AVX2-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX2-NEXT:    je LBB21_32
; AVX2-NEXT:  LBB21_31: ## %cond.store29
; AVX2-NEXT:    vpextrw $7, %xmm0, 30(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v16i16_v16i16:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpeqw %ymm2, %ymm0, %ymm0
; AVX512F-NEXT:    vpmovsxwd %ymm0, %zmm0
; AVX512F-NEXT:    vptestmd %zmm0, %zmm0, %k0
; AVX512F-NEXT:    kmovw %k0, %eax
; AVX512F-NEXT:    testb $1, %al
; AVX512F-NEXT:    jne LBB21_1
; AVX512F-NEXT:  ## %bb.2: ## %else
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    jne LBB21_3
; AVX512F-NEXT:  LBB21_4: ## %else2
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    jne LBB21_5
; AVX512F-NEXT:  LBB21_6: ## %else4
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    jne LBB21_7
; AVX512F-NEXT:  LBB21_8: ## %else6
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    jne LBB21_9
; AVX512F-NEXT:  LBB21_10: ## %else8
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    jne LBB21_11
; AVX512F-NEXT:  LBB21_12: ## %else10
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    jne LBB21_13
; AVX512F-NEXT:  LBB21_14: ## %else12
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    jns LBB21_16
; AVX512F-NEXT:  LBB21_15: ## %cond.store13
; AVX512F-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX512F-NEXT:  LBB21_16: ## %else14
; AVX512F-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512F-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512F-NEXT:    jne LBB21_17
; AVX512F-NEXT:  ## %bb.18: ## %else16
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    jne LBB21_19
; AVX512F-NEXT:  LBB21_20: ## %else18
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    jne LBB21_21
; AVX512F-NEXT:  LBB21_22: ## %else20
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    jne LBB21_23
; AVX512F-NEXT:  LBB21_24: ## %else22
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    jne LBB21_25
; AVX512F-NEXT:  LBB21_26: ## %else24
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    jne LBB21_27
; AVX512F-NEXT:  LBB21_28: ## %else26
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    jne LBB21_29
; AVX512F-NEXT:  LBB21_30: ## %else28
; AVX512F-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512F-NEXT:    jne LBB21_31
; AVX512F-NEXT:  LBB21_32: ## %else30
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
; AVX512F-NEXT:  LBB21_1: ## %cond.store
; AVX512F-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    je LBB21_4
; AVX512F-NEXT:  LBB21_3: ## %cond.store1
; AVX512F-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    je LBB21_6
; AVX512F-NEXT:  LBB21_5: ## %cond.store3
; AVX512F-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    je LBB21_8
; AVX512F-NEXT:  LBB21_7: ## %cond.store5
; AVX512F-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    je LBB21_10
; AVX512F-NEXT:  LBB21_9: ## %cond.store7
; AVX512F-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    je LBB21_12
; AVX512F-NEXT:  LBB21_11: ## %cond.store9
; AVX512F-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    je LBB21_14
; AVX512F-NEXT:  LBB21_13: ## %cond.store11
; AVX512F-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    js LBB21_15
; AVX512F-NEXT:    jmp LBB21_16
; AVX512F-NEXT:  LBB21_17: ## %cond.store15
; AVX512F-NEXT:    vpextrw $0, %xmm0, 16(%rdi)
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    je LBB21_20
; AVX512F-NEXT:  LBB21_19: ## %cond.store17
; AVX512F-NEXT:    vpextrw $1, %xmm0, 18(%rdi)
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    je LBB21_22
; AVX512F-NEXT:  LBB21_21: ## %cond.store19
; AVX512F-NEXT:    vpextrw $2, %xmm0, 20(%rdi)
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    je LBB21_24
; AVX512F-NEXT:  LBB21_23: ## %cond.store21
; AVX512F-NEXT:    vpextrw $3, %xmm0, 22(%rdi)
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    je LBB21_26
; AVX512F-NEXT:  LBB21_25: ## %cond.store23
; AVX512F-NEXT:    vpextrw $4, %xmm0, 24(%rdi)
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    je LBB21_28
; AVX512F-NEXT:  LBB21_27: ## %cond.store25
; AVX512F-NEXT:    vpextrw $5, %xmm0, 26(%rdi)
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    je LBB21_30
; AVX512F-NEXT:  LBB21_29: ## %cond.store27
; AVX512F-NEXT:    vpextrw $6, %xmm0, 28(%rdi)
; AVX512F-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512F-NEXT:    je LBB21_32
; AVX512F-NEXT:  LBB21_31: ## %cond.store29
; AVX512F-NEXT:    vpextrw $7, %xmm0, 30(%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v16i16_v16i16:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLDQ-NEXT:    vpcmpeqw %ymm2, %ymm0, %ymm0
; AVX512VLDQ-NEXT:    vpmovsxwd %ymm0, %zmm0
; AVX512VLDQ-NEXT:    vpmovd2m %zmm0, %k0
; AVX512VLDQ-NEXT:    kmovw %k0, %eax
; AVX512VLDQ-NEXT:    testb $1, %al
; AVX512VLDQ-NEXT:    jne LBB21_1
; AVX512VLDQ-NEXT:  ## %bb.2: ## %else
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    jne LBB21_3
; AVX512VLDQ-NEXT:  LBB21_4: ## %else2
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    jne LBB21_5
; AVX512VLDQ-NEXT:  LBB21_6: ## %else4
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    jne LBB21_7
; AVX512VLDQ-NEXT:  LBB21_8: ## %else6
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    jne LBB21_9
; AVX512VLDQ-NEXT:  LBB21_10: ## %else8
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    jne LBB21_11
; AVX512VLDQ-NEXT:  LBB21_12: ## %else10
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    jne LBB21_13
; AVX512VLDQ-NEXT:  LBB21_14: ## %else12
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    jns LBB21_16
; AVX512VLDQ-NEXT:  LBB21_15: ## %cond.store13
; AVX512VLDQ-NEXT:    vpextrw $7, %xmm1, 14(%rdi)
; AVX512VLDQ-NEXT:  LBB21_16: ## %else14
; AVX512VLDQ-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512VLDQ-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VLDQ-NEXT:    jne LBB21_17
; AVX512VLDQ-NEXT:  ## %bb.18: ## %else16
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    jne LBB21_19
; AVX512VLDQ-NEXT:  LBB21_20: ## %else18
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    jne LBB21_21
; AVX512VLDQ-NEXT:  LBB21_22: ## %else20
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    jne LBB21_23
; AVX512VLDQ-NEXT:  LBB21_24: ## %else22
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    jne LBB21_25
; AVX512VLDQ-NEXT:  LBB21_26: ## %else24
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    jne LBB21_27
; AVX512VLDQ-NEXT:  LBB21_28: ## %else26
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    jne LBB21_29
; AVX512VLDQ-NEXT:  LBB21_30: ## %else28
; AVX512VLDQ-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512VLDQ-NEXT:    jne LBB21_31
; AVX512VLDQ-NEXT:  LBB21_32: ## %else30
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
; AVX512VLDQ-NEXT:  LBB21_1: ## %cond.store
; AVX512VLDQ-NEXT:    vpextrw $0, %xmm1, (%rdi)
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    je LBB21_4
; AVX512VLDQ-NEXT:  LBB21_3: ## %cond.store1
; AVX512VLDQ-NEXT:    vpextrw $1, %xmm1, 2(%rdi)
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    je LBB21_6
; AVX512VLDQ-NEXT:  LBB21_5: ## %cond.store3
; AVX512VLDQ-NEXT:    vpextrw $2, %xmm1, 4(%rdi)
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    je LBB21_8
; AVX512VLDQ-NEXT:  LBB21_7: ## %cond.store5
; AVX512VLDQ-NEXT:    vpextrw $3, %xmm1, 6(%rdi)
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    je LBB21_10
; AVX512VLDQ-NEXT:  LBB21_9: ## %cond.store7
; AVX512VLDQ-NEXT:    vpextrw $4, %xmm1, 8(%rdi)
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    je LBB21_12
; AVX512VLDQ-NEXT:  LBB21_11: ## %cond.store9
; AVX512VLDQ-NEXT:    vpextrw $5, %xmm1, 10(%rdi)
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    je LBB21_14
; AVX512VLDQ-NEXT:  LBB21_13: ## %cond.store11
; AVX512VLDQ-NEXT:    vpextrw $6, %xmm1, 12(%rdi)
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    js LBB21_15
; AVX512VLDQ-NEXT:    jmp LBB21_16
; AVX512VLDQ-NEXT:  LBB21_17: ## %cond.store15
; AVX512VLDQ-NEXT:    vpextrw $0, %xmm0, 16(%rdi)
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    je LBB21_20
; AVX512VLDQ-NEXT:  LBB21_19: ## %cond.store17
; AVX512VLDQ-NEXT:    vpextrw $1, %xmm0, 18(%rdi)
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    je LBB21_22
; AVX512VLDQ-NEXT:  LBB21_21: ## %cond.store19
; AVX512VLDQ-NEXT:    vpextrw $2, %xmm0, 20(%rdi)
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    je LBB21_24
; AVX512VLDQ-NEXT:  LBB21_23: ## %cond.store21
; AVX512VLDQ-NEXT:    vpextrw $3, %xmm0, 22(%rdi)
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    je LBB21_26
; AVX512VLDQ-NEXT:  LBB21_25: ## %cond.store23
; AVX512VLDQ-NEXT:    vpextrw $4, %xmm0, 24(%rdi)
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    je LBB21_28
; AVX512VLDQ-NEXT:  LBB21_27: ## %cond.store25
; AVX512VLDQ-NEXT:    vpextrw $5, %xmm0, 26(%rdi)
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    je LBB21_30
; AVX512VLDQ-NEXT:  LBB21_29: ## %cond.store27
; AVX512VLDQ-NEXT:    vpextrw $6, %xmm0, 28(%rdi)
; AVX512VLDQ-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512VLDQ-NEXT:    je LBB21_32
; AVX512VLDQ-NEXT:  LBB21_31: ## %cond.store29
; AVX512VLDQ-NEXT:    vpextrw $7, %xmm0, 30(%rdi)
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v16i16_v16i16:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmw %ymm0, %ymm0, %k1
; AVX512VLBW-NEXT:    vmovdqu16 %ymm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v16i16_v16i16:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmw %ymm0, %ymm0, %k1
; X86-AVX512-NEXT:    vmovdqu16 %ymm1, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <16 x i16> %trigger, zeroinitializer
  call void @llvm.masked.store.v16i16.p0(<16 x i16> %val, ptr %addr, i32 4, <16 x i1> %mask)
  ret void
}

;
; vXi8
;

define void @store_v16i8_v16i8(<16 x i8> %trigger, ptr %addr, <16 x i8> %val) nounwind {
; SSE2-LABEL: store_v16i8_v16i8:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm2, %xmm2
; SSE2-NEXT:    pcmpeqb %xmm0, %xmm2
; SSE2-NEXT:    pmovmskb %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    movd %xmm1, %ecx
; SSE2-NEXT:    jne LBB22_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB22_3
; SSE2-NEXT:  LBB22_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB22_5
; SSE2-NEXT:  LBB22_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB22_8
; SSE2-NEXT:  LBB22_7: ## %cond.store5
; SSE2-NEXT:    shrl $24, %ecx
; SSE2-NEXT:    movb %cl, 3(%rdi)
; SSE2-NEXT:  LBB22_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    pextrw $2, %xmm1, %ecx
; SSE2-NEXT:    je LBB22_10
; SSE2-NEXT:  ## %bb.9: ## %cond.store7
; SSE2-NEXT:    movb %cl, 4(%rdi)
; SSE2-NEXT:  LBB22_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB22_12
; SSE2-NEXT:  ## %bb.11: ## %cond.store9
; SSE2-NEXT:    movb %ch, 5(%rdi)
; SSE2-NEXT:  LBB22_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    pextrw $3, %xmm1, %ecx
; SSE2-NEXT:    je LBB22_14
; SSE2-NEXT:  ## %bb.13: ## %cond.store11
; SSE2-NEXT:    movb %cl, 6(%rdi)
; SSE2-NEXT:  LBB22_14: ## %else12
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    jns LBB22_16
; SSE2-NEXT:  ## %bb.15: ## %cond.store13
; SSE2-NEXT:    movb %ch, 7(%rdi)
; SSE2-NEXT:  LBB22_16: ## %else14
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    pextrw $4, %xmm1, %ecx
; SSE2-NEXT:    je LBB22_18
; SSE2-NEXT:  ## %bb.17: ## %cond.store15
; SSE2-NEXT:    movb %cl, 8(%rdi)
; SSE2-NEXT:  LBB22_18: ## %else16
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    je LBB22_20
; SSE2-NEXT:  ## %bb.19: ## %cond.store17
; SSE2-NEXT:    movb %ch, 9(%rdi)
; SSE2-NEXT:  LBB22_20: ## %else18
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    pextrw $5, %xmm1, %ecx
; SSE2-NEXT:    je LBB22_22
; SSE2-NEXT:  ## %bb.21: ## %cond.store19
; SSE2-NEXT:    movb %cl, 10(%rdi)
; SSE2-NEXT:  LBB22_22: ## %else20
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    je LBB22_24
; SSE2-NEXT:  ## %bb.23: ## %cond.store21
; SSE2-NEXT:    movb %ch, 11(%rdi)
; SSE2-NEXT:  LBB22_24: ## %else22
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    pextrw $6, %xmm1, %ecx
; SSE2-NEXT:    je LBB22_26
; SSE2-NEXT:  ## %bb.25: ## %cond.store23
; SSE2-NEXT:    movb %cl, 12(%rdi)
; SSE2-NEXT:  LBB22_26: ## %else24
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    je LBB22_28
; SSE2-NEXT:  ## %bb.27: ## %cond.store25
; SSE2-NEXT:    movb %ch, 13(%rdi)
; SSE2-NEXT:  LBB22_28: ## %else26
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    pextrw $7, %xmm1, %ecx
; SSE2-NEXT:    jne LBB22_29
; SSE2-NEXT:  ## %bb.30: ## %else28
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    jne LBB22_31
; SSE2-NEXT:  LBB22_32: ## %else30
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB22_1: ## %cond.store
; SSE2-NEXT:    movb %cl, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB22_4
; SSE2-NEXT:  LBB22_3: ## %cond.store1
; SSE2-NEXT:    movb %ch, 1(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB22_6
; SSE2-NEXT:  LBB22_5: ## %cond.store3
; SSE2-NEXT:    movl %ecx, %edx
; SSE2-NEXT:    shrl $16, %edx
; SSE2-NEXT:    movb %dl, 2(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB22_7
; SSE2-NEXT:    jmp LBB22_8
; SSE2-NEXT:  LBB22_29: ## %cond.store27
; SSE2-NEXT:    movb %cl, 14(%rdi)
; SSE2-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE2-NEXT:    je LBB22_32
; SSE2-NEXT:  LBB22_31: ## %cond.store29
; SSE2-NEXT:    movb %ch, 15(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v16i8_v16i8:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm2, %xmm2
; SSE4-NEXT:    pcmpeqb %xmm0, %xmm2
; SSE4-NEXT:    pmovmskb %xmm2, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB22_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB22_3
; SSE4-NEXT:  LBB22_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB22_5
; SSE4-NEXT:  LBB22_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB22_7
; SSE4-NEXT:  LBB22_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB22_9
; SSE4-NEXT:  LBB22_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB22_11
; SSE4-NEXT:  LBB22_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB22_13
; SSE4-NEXT:  LBB22_14: ## %else12
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    js LBB22_15
; SSE4-NEXT:  LBB22_16: ## %else14
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    jne LBB22_17
; SSE4-NEXT:  LBB22_18: ## %else16
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    jne LBB22_19
; SSE4-NEXT:  LBB22_20: ## %else18
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    jne LBB22_21
; SSE4-NEXT:  LBB22_22: ## %else20
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    jne LBB22_23
; SSE4-NEXT:  LBB22_24: ## %else22
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    jne LBB22_25
; SSE4-NEXT:  LBB22_26: ## %else24
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    jne LBB22_27
; SSE4-NEXT:  LBB22_28: ## %else26
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    jne LBB22_29
; SSE4-NEXT:  LBB22_30: ## %else28
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    jne LBB22_31
; SSE4-NEXT:  LBB22_32: ## %else30
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB22_1: ## %cond.store
; SSE4-NEXT:    pextrb $0, %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB22_4
; SSE4-NEXT:  LBB22_3: ## %cond.store1
; SSE4-NEXT:    pextrb $1, %xmm1, 1(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB22_6
; SSE4-NEXT:  LBB22_5: ## %cond.store3
; SSE4-NEXT:    pextrb $2, %xmm1, 2(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB22_8
; SSE4-NEXT:  LBB22_7: ## %cond.store5
; SSE4-NEXT:    pextrb $3, %xmm1, 3(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB22_10
; SSE4-NEXT:  LBB22_9: ## %cond.store7
; SSE4-NEXT:    pextrb $4, %xmm1, 4(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB22_12
; SSE4-NEXT:  LBB22_11: ## %cond.store9
; SSE4-NEXT:    pextrb $5, %xmm1, 5(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB22_14
; SSE4-NEXT:  LBB22_13: ## %cond.store11
; SSE4-NEXT:    pextrb $6, %xmm1, 6(%rdi)
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    jns LBB22_16
; SSE4-NEXT:  LBB22_15: ## %cond.store13
; SSE4-NEXT:    pextrb $7, %xmm1, 7(%rdi)
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    je LBB22_18
; SSE4-NEXT:  LBB22_17: ## %cond.store15
; SSE4-NEXT:    pextrb $8, %xmm1, 8(%rdi)
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    je LBB22_20
; SSE4-NEXT:  LBB22_19: ## %cond.store17
; SSE4-NEXT:    pextrb $9, %xmm1, 9(%rdi)
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    je LBB22_22
; SSE4-NEXT:  LBB22_21: ## %cond.store19
; SSE4-NEXT:    pextrb $10, %xmm1, 10(%rdi)
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    je LBB22_24
; SSE4-NEXT:  LBB22_23: ## %cond.store21
; SSE4-NEXT:    pextrb $11, %xmm1, 11(%rdi)
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    je LBB22_26
; SSE4-NEXT:  LBB22_25: ## %cond.store23
; SSE4-NEXT:    pextrb $12, %xmm1, 12(%rdi)
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    je LBB22_28
; SSE4-NEXT:  LBB22_27: ## %cond.store25
; SSE4-NEXT:    pextrb $13, %xmm1, 13(%rdi)
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    je LBB22_30
; SSE4-NEXT:  LBB22_29: ## %cond.store27
; SSE4-NEXT:    pextrb $14, %xmm1, 14(%rdi)
; SSE4-NEXT:    testl $32768, %eax ## imm = 0x8000
; SSE4-NEXT:    je LBB22_32
; SSE4-NEXT:  LBB22_31: ## %cond.store29
; SSE4-NEXT:    pextrb $15, %xmm1, 15(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: store_v16i8_v16i8:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1OR2-NEXT:    vpcmpeqb %xmm2, %xmm0, %xmm0
; AVX1OR2-NEXT:    vpmovmskb %xmm0, %eax
; AVX1OR2-NEXT:    testb $1, %al
; AVX1OR2-NEXT:    jne LBB22_1
; AVX1OR2-NEXT:  ## %bb.2: ## %else
; AVX1OR2-NEXT:    testb $2, %al
; AVX1OR2-NEXT:    jne LBB22_3
; AVX1OR2-NEXT:  LBB22_4: ## %else2
; AVX1OR2-NEXT:    testb $4, %al
; AVX1OR2-NEXT:    jne LBB22_5
; AVX1OR2-NEXT:  LBB22_6: ## %else4
; AVX1OR2-NEXT:    testb $8, %al
; AVX1OR2-NEXT:    jne LBB22_7
; AVX1OR2-NEXT:  LBB22_8: ## %else6
; AVX1OR2-NEXT:    testb $16, %al
; AVX1OR2-NEXT:    jne LBB22_9
; AVX1OR2-NEXT:  LBB22_10: ## %else8
; AVX1OR2-NEXT:    testb $32, %al
; AVX1OR2-NEXT:    jne LBB22_11
; AVX1OR2-NEXT:  LBB22_12: ## %else10
; AVX1OR2-NEXT:    testb $64, %al
; AVX1OR2-NEXT:    jne LBB22_13
; AVX1OR2-NEXT:  LBB22_14: ## %else12
; AVX1OR2-NEXT:    testb %al, %al
; AVX1OR2-NEXT:    js LBB22_15
; AVX1OR2-NEXT:  LBB22_16: ## %else14
; AVX1OR2-NEXT:    testl $256, %eax ## imm = 0x100
; AVX1OR2-NEXT:    jne LBB22_17
; AVX1OR2-NEXT:  LBB22_18: ## %else16
; AVX1OR2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1OR2-NEXT:    jne LBB22_19
; AVX1OR2-NEXT:  LBB22_20: ## %else18
; AVX1OR2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1OR2-NEXT:    jne LBB22_21
; AVX1OR2-NEXT:  LBB22_22: ## %else20
; AVX1OR2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1OR2-NEXT:    jne LBB22_23
; AVX1OR2-NEXT:  LBB22_24: ## %else22
; AVX1OR2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1OR2-NEXT:    jne LBB22_25
; AVX1OR2-NEXT:  LBB22_26: ## %else24
; AVX1OR2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1OR2-NEXT:    jne LBB22_27
; AVX1OR2-NEXT:  LBB22_28: ## %else26
; AVX1OR2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1OR2-NEXT:    jne LBB22_29
; AVX1OR2-NEXT:  LBB22_30: ## %else28
; AVX1OR2-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX1OR2-NEXT:    jne LBB22_31
; AVX1OR2-NEXT:  LBB22_32: ## %else30
; AVX1OR2-NEXT:    retq
; AVX1OR2-NEXT:  LBB22_1: ## %cond.store
; AVX1OR2-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX1OR2-NEXT:    testb $2, %al
; AVX1OR2-NEXT:    je LBB22_4
; AVX1OR2-NEXT:  LBB22_3: ## %cond.store1
; AVX1OR2-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX1OR2-NEXT:    testb $4, %al
; AVX1OR2-NEXT:    je LBB22_6
; AVX1OR2-NEXT:  LBB22_5: ## %cond.store3
; AVX1OR2-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX1OR2-NEXT:    testb $8, %al
; AVX1OR2-NEXT:    je LBB22_8
; AVX1OR2-NEXT:  LBB22_7: ## %cond.store5
; AVX1OR2-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX1OR2-NEXT:    testb $16, %al
; AVX1OR2-NEXT:    je LBB22_10
; AVX1OR2-NEXT:  LBB22_9: ## %cond.store7
; AVX1OR2-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX1OR2-NEXT:    testb $32, %al
; AVX1OR2-NEXT:    je LBB22_12
; AVX1OR2-NEXT:  LBB22_11: ## %cond.store9
; AVX1OR2-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX1OR2-NEXT:    testb $64, %al
; AVX1OR2-NEXT:    je LBB22_14
; AVX1OR2-NEXT:  LBB22_13: ## %cond.store11
; AVX1OR2-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX1OR2-NEXT:    testb %al, %al
; AVX1OR2-NEXT:    jns LBB22_16
; AVX1OR2-NEXT:  LBB22_15: ## %cond.store13
; AVX1OR2-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX1OR2-NEXT:    testl $256, %eax ## imm = 0x100
; AVX1OR2-NEXT:    je LBB22_18
; AVX1OR2-NEXT:  LBB22_17: ## %cond.store15
; AVX1OR2-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX1OR2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1OR2-NEXT:    je LBB22_20
; AVX1OR2-NEXT:  LBB22_19: ## %cond.store17
; AVX1OR2-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX1OR2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1OR2-NEXT:    je LBB22_22
; AVX1OR2-NEXT:  LBB22_21: ## %cond.store19
; AVX1OR2-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX1OR2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1OR2-NEXT:    je LBB22_24
; AVX1OR2-NEXT:  LBB22_23: ## %cond.store21
; AVX1OR2-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX1OR2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1OR2-NEXT:    je LBB22_26
; AVX1OR2-NEXT:  LBB22_25: ## %cond.store23
; AVX1OR2-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX1OR2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1OR2-NEXT:    je LBB22_28
; AVX1OR2-NEXT:  LBB22_27: ## %cond.store25
; AVX1OR2-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX1OR2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1OR2-NEXT:    je LBB22_30
; AVX1OR2-NEXT:  LBB22_29: ## %cond.store27
; AVX1OR2-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX1OR2-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX1OR2-NEXT:    je LBB22_32
; AVX1OR2-NEXT:  LBB22_31: ## %cond.store29
; AVX1OR2-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: store_v16i8_v16i8:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpeqb %xmm2, %xmm0, %xmm0
; AVX512F-NEXT:    vpmovmskb %xmm0, %eax
; AVX512F-NEXT:    testb $1, %al
; AVX512F-NEXT:    jne LBB22_1
; AVX512F-NEXT:  ## %bb.2: ## %else
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    jne LBB22_3
; AVX512F-NEXT:  LBB22_4: ## %else2
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    jne LBB22_5
; AVX512F-NEXT:  LBB22_6: ## %else4
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    jne LBB22_7
; AVX512F-NEXT:  LBB22_8: ## %else6
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    jne LBB22_9
; AVX512F-NEXT:  LBB22_10: ## %else8
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    jne LBB22_11
; AVX512F-NEXT:  LBB22_12: ## %else10
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    jne LBB22_13
; AVX512F-NEXT:  LBB22_14: ## %else12
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    js LBB22_15
; AVX512F-NEXT:  LBB22_16: ## %else14
; AVX512F-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512F-NEXT:    jne LBB22_17
; AVX512F-NEXT:  LBB22_18: ## %else16
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    jne LBB22_19
; AVX512F-NEXT:  LBB22_20: ## %else18
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    jne LBB22_21
; AVX512F-NEXT:  LBB22_22: ## %else20
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    jne LBB22_23
; AVX512F-NEXT:  LBB22_24: ## %else22
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    jne LBB22_25
; AVX512F-NEXT:  LBB22_26: ## %else24
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    jne LBB22_27
; AVX512F-NEXT:  LBB22_28: ## %else26
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    jne LBB22_29
; AVX512F-NEXT:  LBB22_30: ## %else28
; AVX512F-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512F-NEXT:    jne LBB22_31
; AVX512F-NEXT:  LBB22_32: ## %else30
; AVX512F-NEXT:    retq
; AVX512F-NEXT:  LBB22_1: ## %cond.store
; AVX512F-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    je LBB22_4
; AVX512F-NEXT:  LBB22_3: ## %cond.store1
; AVX512F-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    je LBB22_6
; AVX512F-NEXT:  LBB22_5: ## %cond.store3
; AVX512F-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    je LBB22_8
; AVX512F-NEXT:  LBB22_7: ## %cond.store5
; AVX512F-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    je LBB22_10
; AVX512F-NEXT:  LBB22_9: ## %cond.store7
; AVX512F-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    je LBB22_12
; AVX512F-NEXT:  LBB22_11: ## %cond.store9
; AVX512F-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    je LBB22_14
; AVX512F-NEXT:  LBB22_13: ## %cond.store11
; AVX512F-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    jns LBB22_16
; AVX512F-NEXT:  LBB22_15: ## %cond.store13
; AVX512F-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX512F-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512F-NEXT:    je LBB22_18
; AVX512F-NEXT:  LBB22_17: ## %cond.store15
; AVX512F-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    je LBB22_20
; AVX512F-NEXT:  LBB22_19: ## %cond.store17
; AVX512F-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    je LBB22_22
; AVX512F-NEXT:  LBB22_21: ## %cond.store19
; AVX512F-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    je LBB22_24
; AVX512F-NEXT:  LBB22_23: ## %cond.store21
; AVX512F-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    je LBB22_26
; AVX512F-NEXT:  LBB22_25: ## %cond.store23
; AVX512F-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    je LBB22_28
; AVX512F-NEXT:  LBB22_27: ## %cond.store25
; AVX512F-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    je LBB22_30
; AVX512F-NEXT:  LBB22_29: ## %cond.store27
; AVX512F-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX512F-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512F-NEXT:    je LBB22_32
; AVX512F-NEXT:  LBB22_31: ## %cond.store29
; AVX512F-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v16i8_v16i8:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLDQ-NEXT:    vpcmpeqb %xmm2, %xmm0, %xmm0
; AVX512VLDQ-NEXT:    vpmovmskb %xmm0, %eax
; AVX512VLDQ-NEXT:    testb $1, %al
; AVX512VLDQ-NEXT:    jne LBB22_1
; AVX512VLDQ-NEXT:  ## %bb.2: ## %else
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    jne LBB22_3
; AVX512VLDQ-NEXT:  LBB22_4: ## %else2
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    jne LBB22_5
; AVX512VLDQ-NEXT:  LBB22_6: ## %else4
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    jne LBB22_7
; AVX512VLDQ-NEXT:  LBB22_8: ## %else6
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    jne LBB22_9
; AVX512VLDQ-NEXT:  LBB22_10: ## %else8
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    jne LBB22_11
; AVX512VLDQ-NEXT:  LBB22_12: ## %else10
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    jne LBB22_13
; AVX512VLDQ-NEXT:  LBB22_14: ## %else12
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    js LBB22_15
; AVX512VLDQ-NEXT:  LBB22_16: ## %else14
; AVX512VLDQ-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512VLDQ-NEXT:    jne LBB22_17
; AVX512VLDQ-NEXT:  LBB22_18: ## %else16
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    jne LBB22_19
; AVX512VLDQ-NEXT:  LBB22_20: ## %else18
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    jne LBB22_21
; AVX512VLDQ-NEXT:  LBB22_22: ## %else20
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    jne LBB22_23
; AVX512VLDQ-NEXT:  LBB22_24: ## %else22
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    jne LBB22_25
; AVX512VLDQ-NEXT:  LBB22_26: ## %else24
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    jne LBB22_27
; AVX512VLDQ-NEXT:  LBB22_28: ## %else26
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    jne LBB22_29
; AVX512VLDQ-NEXT:  LBB22_30: ## %else28
; AVX512VLDQ-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512VLDQ-NEXT:    jne LBB22_31
; AVX512VLDQ-NEXT:  LBB22_32: ## %else30
; AVX512VLDQ-NEXT:    retq
; AVX512VLDQ-NEXT:  LBB22_1: ## %cond.store
; AVX512VLDQ-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    je LBB22_4
; AVX512VLDQ-NEXT:  LBB22_3: ## %cond.store1
; AVX512VLDQ-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    je LBB22_6
; AVX512VLDQ-NEXT:  LBB22_5: ## %cond.store3
; AVX512VLDQ-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    je LBB22_8
; AVX512VLDQ-NEXT:  LBB22_7: ## %cond.store5
; AVX512VLDQ-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    je LBB22_10
; AVX512VLDQ-NEXT:  LBB22_9: ## %cond.store7
; AVX512VLDQ-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    je LBB22_12
; AVX512VLDQ-NEXT:  LBB22_11: ## %cond.store9
; AVX512VLDQ-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    je LBB22_14
; AVX512VLDQ-NEXT:  LBB22_13: ## %cond.store11
; AVX512VLDQ-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    jns LBB22_16
; AVX512VLDQ-NEXT:  LBB22_15: ## %cond.store13
; AVX512VLDQ-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX512VLDQ-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512VLDQ-NEXT:    je LBB22_18
; AVX512VLDQ-NEXT:  LBB22_17: ## %cond.store15
; AVX512VLDQ-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    je LBB22_20
; AVX512VLDQ-NEXT:  LBB22_19: ## %cond.store17
; AVX512VLDQ-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    je LBB22_22
; AVX512VLDQ-NEXT:  LBB22_21: ## %cond.store19
; AVX512VLDQ-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    je LBB22_24
; AVX512VLDQ-NEXT:  LBB22_23: ## %cond.store21
; AVX512VLDQ-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    je LBB22_26
; AVX512VLDQ-NEXT:  LBB22_25: ## %cond.store23
; AVX512VLDQ-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    je LBB22_28
; AVX512VLDQ-NEXT:  LBB22_27: ## %cond.store25
; AVX512VLDQ-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    je LBB22_30
; AVX512VLDQ-NEXT:  LBB22_29: ## %cond.store27
; AVX512VLDQ-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX512VLDQ-NEXT:    testl $32768, %eax ## imm = 0x8000
; AVX512VLDQ-NEXT:    je LBB22_32
; AVX512VLDQ-NEXT:  LBB22_31: ## %cond.store29
; AVX512VLDQ-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v16i8_v16i8:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmb %xmm0, %xmm0, %k1
; AVX512VLBW-NEXT:    vmovdqu8 %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v16i8_v16i8:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmb %xmm0, %xmm0, %k1
; X86-AVX512-NEXT:    vmovdqu8 %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <16 x i8> %trigger, zeroinitializer
  call void @llvm.masked.store.v16i8.p0(<16 x i8> %val, ptr %addr, i32 4, <16 x i1> %mask)
  ret void
}

define void @store_v32i8_v32i8(<32 x i8> %trigger, ptr %addr, <32 x i8> %val) nounwind {
; SSE2-LABEL: store_v32i8_v32i8:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pxor %xmm4, %xmm4
; SSE2-NEXT:    pcmpeqb %xmm4, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %ecx
; SSE2-NEXT:    pcmpeqb %xmm4, %xmm1
; SSE2-NEXT:    pmovmskb %xmm1, %eax
; SSE2-NEXT:    shll $16, %eax
; SSE2-NEXT:    orl %ecx, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    movd %xmm2, %ecx
; SSE2-NEXT:    jne LBB23_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB23_3
; SSE2-NEXT:  LBB23_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB23_5
; SSE2-NEXT:  LBB23_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB23_8
; SSE2-NEXT:  LBB23_7: ## %cond.store5
; SSE2-NEXT:    shrl $24, %ecx
; SSE2-NEXT:    movb %cl, 3(%rdi)
; SSE2-NEXT:  LBB23_8: ## %else6
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    pextrw $2, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_10
; SSE2-NEXT:  ## %bb.9: ## %cond.store7
; SSE2-NEXT:    movb %cl, 4(%rdi)
; SSE2-NEXT:  LBB23_10: ## %else8
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB23_12
; SSE2-NEXT:  ## %bb.11: ## %cond.store9
; SSE2-NEXT:    movb %ch, 5(%rdi)
; SSE2-NEXT:  LBB23_12: ## %else10
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    pextrw $3, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_14
; SSE2-NEXT:  ## %bb.13: ## %cond.store11
; SSE2-NEXT:    movb %cl, 6(%rdi)
; SSE2-NEXT:  LBB23_14: ## %else12
; SSE2-NEXT:    testb %al, %al
; SSE2-NEXT:    jns LBB23_16
; SSE2-NEXT:  ## %bb.15: ## %cond.store13
; SSE2-NEXT:    movb %ch, 7(%rdi)
; SSE2-NEXT:  LBB23_16: ## %else14
; SSE2-NEXT:    testl $256, %eax ## imm = 0x100
; SSE2-NEXT:    pextrw $4, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_18
; SSE2-NEXT:  ## %bb.17: ## %cond.store15
; SSE2-NEXT:    movb %cl, 8(%rdi)
; SSE2-NEXT:  LBB23_18: ## %else16
; SSE2-NEXT:    testl $512, %eax ## imm = 0x200
; SSE2-NEXT:    je LBB23_20
; SSE2-NEXT:  ## %bb.19: ## %cond.store17
; SSE2-NEXT:    movb %ch, 9(%rdi)
; SSE2-NEXT:  LBB23_20: ## %else18
; SSE2-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE2-NEXT:    pextrw $5, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_22
; SSE2-NEXT:  ## %bb.21: ## %cond.store19
; SSE2-NEXT:    movb %cl, 10(%rdi)
; SSE2-NEXT:  LBB23_22: ## %else20
; SSE2-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE2-NEXT:    je LBB23_24
; SSE2-NEXT:  ## %bb.23: ## %cond.store21
; SSE2-NEXT:    movb %ch, 11(%rdi)
; SSE2-NEXT:  LBB23_24: ## %else22
; SSE2-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE2-NEXT:    pextrw $6, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_26
; SSE2-NEXT:  ## %bb.25: ## %cond.store23
; SSE2-NEXT:    movb %cl, 12(%rdi)
; SSE2-NEXT:  LBB23_26: ## %else24
; SSE2-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE2-NEXT:    je LBB23_28
; SSE2-NEXT:  ## %bb.27: ## %cond.store25
; SSE2-NEXT:    movb %ch, 13(%rdi)
; SSE2-NEXT:  LBB23_28: ## %else26
; SSE2-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE2-NEXT:    pextrw $7, %xmm2, %ecx
; SSE2-NEXT:    je LBB23_30
; SSE2-NEXT:  ## %bb.29: ## %cond.store27
; SSE2-NEXT:    movb %cl, 14(%rdi)
; SSE2-NEXT:  LBB23_30: ## %else28
; SSE2-NEXT:    testw %ax, %ax
; SSE2-NEXT:    jns LBB23_32
; SSE2-NEXT:  ## %bb.31: ## %cond.store29
; SSE2-NEXT:    movb %ch, 15(%rdi)
; SSE2-NEXT:  LBB23_32: ## %else30
; SSE2-NEXT:    testl $65536, %eax ## imm = 0x10000
; SSE2-NEXT:    movd %xmm3, %ecx
; SSE2-NEXT:    jne LBB23_33
; SSE2-NEXT:  ## %bb.34: ## %else32
; SSE2-NEXT:    testl $131072, %eax ## imm = 0x20000
; SSE2-NEXT:    jne LBB23_35
; SSE2-NEXT:  LBB23_36: ## %else34
; SSE2-NEXT:    testl $262144, %eax ## imm = 0x40000
; SSE2-NEXT:    jne LBB23_37
; SSE2-NEXT:  LBB23_38: ## %else36
; SSE2-NEXT:    testl $524288, %eax ## imm = 0x80000
; SSE2-NEXT:    je LBB23_40
; SSE2-NEXT:  LBB23_39: ## %cond.store37
; SSE2-NEXT:    shrl $24, %ecx
; SSE2-NEXT:    movb %cl, 19(%rdi)
; SSE2-NEXT:  LBB23_40: ## %else38
; SSE2-NEXT:    testl $1048576, %eax ## imm = 0x100000
; SSE2-NEXT:    pextrw $2, %xmm3, %ecx
; SSE2-NEXT:    je LBB23_42
; SSE2-NEXT:  ## %bb.41: ## %cond.store39
; SSE2-NEXT:    movb %cl, 20(%rdi)
; SSE2-NEXT:  LBB23_42: ## %else40
; SSE2-NEXT:    testl $2097152, %eax ## imm = 0x200000
; SSE2-NEXT:    je LBB23_44
; SSE2-NEXT:  ## %bb.43: ## %cond.store41
; SSE2-NEXT:    movb %ch, 21(%rdi)
; SSE2-NEXT:  LBB23_44: ## %else42
; SSE2-NEXT:    testl $4194304, %eax ## imm = 0x400000
; SSE2-NEXT:    pextrw $3, %xmm3, %ecx
; SSE2-NEXT:    je LBB23_46
; SSE2-NEXT:  ## %bb.45: ## %cond.store43
; SSE2-NEXT:    movb %cl, 22(%rdi)
; SSE2-NEXT:  LBB23_46: ## %else44
; SSE2-NEXT:    testl $8388608, %eax ## imm = 0x800000
; SSE2-NEXT:    je LBB23_48
; SSE2-NEXT:  ## %bb.47: ## %cond.store45
; SSE2-NEXT:    movb %ch, 23(%rdi)
; SSE2-NEXT:  LBB23_48: ## %else46
; SSE2-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; SSE2-NEXT:    pextrw $4, %xmm3, %ecx
; SSE2-NEXT:    je LBB23_50
; SSE2-NEXT:  ## %bb.49: ## %cond.store47
; SSE2-NEXT:    movb %cl, 24(%rdi)
; SSE2-NEXT:  LBB23_50: ## %else48
; SSE2-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; SSE2-NEXT:    je LBB23_52
; SSE2-NEXT:  ## %bb.51: ## %cond.store49
; SSE2-NEXT:    movb %ch, 25(%rdi)
; SSE2-NEXT:  LBB23_52: ## %else50
; SSE2-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; SSE2-NEXT:    pextrw $5, %xmm3, %ecx
; SSE2-NEXT:    je LBB23_54
; SSE2-NEXT:  ## %bb.53: ## %cond.store51
; SSE2-NEXT:    movb %cl, 26(%rdi)
; SSE2-NEXT:  LBB23_54: ## %else52
; SSE2-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; SSE2-NEXT:    je LBB23_56
; SSE2-NEXT:  ## %bb.55: ## %cond.store53
; SSE2-NEXT:    movb %ch, 27(%rdi)
; SSE2-NEXT:  LBB23_56: ## %else54
; SSE2-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; SSE2-NEXT:    pextrw $6, %xmm3, %ecx
; SSE2-NEXT:    je LBB23_58
; SSE2-NEXT:  ## %bb.57: ## %cond.store55
; SSE2-NEXT:    movb %cl, 28(%rdi)
; SSE2-NEXT:  LBB23_58: ## %else56
; SSE2-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; SSE2-NEXT:    je LBB23_60
; SSE2-NEXT:  ## %bb.59: ## %cond.store57
; SSE2-NEXT:    movb %ch, 29(%rdi)
; SSE2-NEXT:  LBB23_60: ## %else58
; SSE2-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; SSE2-NEXT:    pextrw $7, %xmm3, %ecx
; SSE2-NEXT:    jne LBB23_61
; SSE2-NEXT:  ## %bb.62: ## %else60
; SSE2-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; SSE2-NEXT:    jne LBB23_63
; SSE2-NEXT:  LBB23_64: ## %else62
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB23_1: ## %cond.store
; SSE2-NEXT:    movb %cl, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB23_4
; SSE2-NEXT:  LBB23_3: ## %cond.store1
; SSE2-NEXT:    movb %ch, 1(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB23_6
; SSE2-NEXT:  LBB23_5: ## %cond.store3
; SSE2-NEXT:    movl %ecx, %edx
; SSE2-NEXT:    shrl $16, %edx
; SSE2-NEXT:    movb %dl, 2(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB23_7
; SSE2-NEXT:    jmp LBB23_8
; SSE2-NEXT:  LBB23_33: ## %cond.store31
; SSE2-NEXT:    movb %cl, 16(%rdi)
; SSE2-NEXT:    testl $131072, %eax ## imm = 0x20000
; SSE2-NEXT:    je LBB23_36
; SSE2-NEXT:  LBB23_35: ## %cond.store33
; SSE2-NEXT:    movb %ch, 17(%rdi)
; SSE2-NEXT:    testl $262144, %eax ## imm = 0x40000
; SSE2-NEXT:    je LBB23_38
; SSE2-NEXT:  LBB23_37: ## %cond.store35
; SSE2-NEXT:    movl %ecx, %edx
; SSE2-NEXT:    shrl $16, %edx
; SSE2-NEXT:    movb %dl, 18(%rdi)
; SSE2-NEXT:    testl $524288, %eax ## imm = 0x80000
; SSE2-NEXT:    jne LBB23_39
; SSE2-NEXT:    jmp LBB23_40
; SSE2-NEXT:  LBB23_61: ## %cond.store59
; SSE2-NEXT:    movb %cl, 30(%rdi)
; SSE2-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; SSE2-NEXT:    je LBB23_64
; SSE2-NEXT:  LBB23_63: ## %cond.store61
; SSE2-NEXT:    movb %ch, 31(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: store_v32i8_v32i8:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    pxor %xmm4, %xmm4
; SSE4-NEXT:    pcmpeqb %xmm4, %xmm0
; SSE4-NEXT:    pmovmskb %xmm0, %ecx
; SSE4-NEXT:    pcmpeqb %xmm4, %xmm1
; SSE4-NEXT:    pmovmskb %xmm1, %eax
; SSE4-NEXT:    shll $16, %eax
; SSE4-NEXT:    orl %ecx, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB23_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB23_3
; SSE4-NEXT:  LBB23_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB23_5
; SSE4-NEXT:  LBB23_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB23_7
; SSE4-NEXT:  LBB23_8: ## %else6
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB23_9
; SSE4-NEXT:  LBB23_10: ## %else8
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB23_11
; SSE4-NEXT:  LBB23_12: ## %else10
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB23_13
; SSE4-NEXT:  LBB23_14: ## %else12
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    js LBB23_15
; SSE4-NEXT:  LBB23_16: ## %else14
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    jne LBB23_17
; SSE4-NEXT:  LBB23_18: ## %else16
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    jne LBB23_19
; SSE4-NEXT:  LBB23_20: ## %else18
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    jne LBB23_21
; SSE4-NEXT:  LBB23_22: ## %else20
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    jne LBB23_23
; SSE4-NEXT:  LBB23_24: ## %else22
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    jne LBB23_25
; SSE4-NEXT:  LBB23_26: ## %else24
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    jne LBB23_27
; SSE4-NEXT:  LBB23_28: ## %else26
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    jne LBB23_29
; SSE4-NEXT:  LBB23_30: ## %else28
; SSE4-NEXT:    testw %ax, %ax
; SSE4-NEXT:    js LBB23_31
; SSE4-NEXT:  LBB23_32: ## %else30
; SSE4-NEXT:    testl $65536, %eax ## imm = 0x10000
; SSE4-NEXT:    jne LBB23_33
; SSE4-NEXT:  LBB23_34: ## %else32
; SSE4-NEXT:    testl $131072, %eax ## imm = 0x20000
; SSE4-NEXT:    jne LBB23_35
; SSE4-NEXT:  LBB23_36: ## %else34
; SSE4-NEXT:    testl $262144, %eax ## imm = 0x40000
; SSE4-NEXT:    jne LBB23_37
; SSE4-NEXT:  LBB23_38: ## %else36
; SSE4-NEXT:    testl $524288, %eax ## imm = 0x80000
; SSE4-NEXT:    jne LBB23_39
; SSE4-NEXT:  LBB23_40: ## %else38
; SSE4-NEXT:    testl $1048576, %eax ## imm = 0x100000
; SSE4-NEXT:    jne LBB23_41
; SSE4-NEXT:  LBB23_42: ## %else40
; SSE4-NEXT:    testl $2097152, %eax ## imm = 0x200000
; SSE4-NEXT:    jne LBB23_43
; SSE4-NEXT:  LBB23_44: ## %else42
; SSE4-NEXT:    testl $4194304, %eax ## imm = 0x400000
; SSE4-NEXT:    jne LBB23_45
; SSE4-NEXT:  LBB23_46: ## %else44
; SSE4-NEXT:    testl $8388608, %eax ## imm = 0x800000
; SSE4-NEXT:    jne LBB23_47
; SSE4-NEXT:  LBB23_48: ## %else46
; SSE4-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; SSE4-NEXT:    jne LBB23_49
; SSE4-NEXT:  LBB23_50: ## %else48
; SSE4-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; SSE4-NEXT:    jne LBB23_51
; SSE4-NEXT:  LBB23_52: ## %else50
; SSE4-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; SSE4-NEXT:    jne LBB23_53
; SSE4-NEXT:  LBB23_54: ## %else52
; SSE4-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; SSE4-NEXT:    jne LBB23_55
; SSE4-NEXT:  LBB23_56: ## %else54
; SSE4-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; SSE4-NEXT:    jne LBB23_57
; SSE4-NEXT:  LBB23_58: ## %else56
; SSE4-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; SSE4-NEXT:    jne LBB23_59
; SSE4-NEXT:  LBB23_60: ## %else58
; SSE4-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; SSE4-NEXT:    jne LBB23_61
; SSE4-NEXT:  LBB23_62: ## %else60
; SSE4-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; SSE4-NEXT:    jne LBB23_63
; SSE4-NEXT:  LBB23_64: ## %else62
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB23_1: ## %cond.store
; SSE4-NEXT:    pextrb $0, %xmm2, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB23_4
; SSE4-NEXT:  LBB23_3: ## %cond.store1
; SSE4-NEXT:    pextrb $1, %xmm2, 1(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB23_6
; SSE4-NEXT:  LBB23_5: ## %cond.store3
; SSE4-NEXT:    pextrb $2, %xmm2, 2(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB23_8
; SSE4-NEXT:  LBB23_7: ## %cond.store5
; SSE4-NEXT:    pextrb $3, %xmm2, 3(%rdi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB23_10
; SSE4-NEXT:  LBB23_9: ## %cond.store7
; SSE4-NEXT:    pextrb $4, %xmm2, 4(%rdi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB23_12
; SSE4-NEXT:  LBB23_11: ## %cond.store9
; SSE4-NEXT:    pextrb $5, %xmm2, 5(%rdi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB23_14
; SSE4-NEXT:  LBB23_13: ## %cond.store11
; SSE4-NEXT:    pextrb $6, %xmm2, 6(%rdi)
; SSE4-NEXT:    testb %al, %al
; SSE4-NEXT:    jns LBB23_16
; SSE4-NEXT:  LBB23_15: ## %cond.store13
; SSE4-NEXT:    pextrb $7, %xmm2, 7(%rdi)
; SSE4-NEXT:    testl $256, %eax ## imm = 0x100
; SSE4-NEXT:    je LBB23_18
; SSE4-NEXT:  LBB23_17: ## %cond.store15
; SSE4-NEXT:    pextrb $8, %xmm2, 8(%rdi)
; SSE4-NEXT:    testl $512, %eax ## imm = 0x200
; SSE4-NEXT:    je LBB23_20
; SSE4-NEXT:  LBB23_19: ## %cond.store17
; SSE4-NEXT:    pextrb $9, %xmm2, 9(%rdi)
; SSE4-NEXT:    testl $1024, %eax ## imm = 0x400
; SSE4-NEXT:    je LBB23_22
; SSE4-NEXT:  LBB23_21: ## %cond.store19
; SSE4-NEXT:    pextrb $10, %xmm2, 10(%rdi)
; SSE4-NEXT:    testl $2048, %eax ## imm = 0x800
; SSE4-NEXT:    je LBB23_24
; SSE4-NEXT:  LBB23_23: ## %cond.store21
; SSE4-NEXT:    pextrb $11, %xmm2, 11(%rdi)
; SSE4-NEXT:    testl $4096, %eax ## imm = 0x1000
; SSE4-NEXT:    je LBB23_26
; SSE4-NEXT:  LBB23_25: ## %cond.store23
; SSE4-NEXT:    pextrb $12, %xmm2, 12(%rdi)
; SSE4-NEXT:    testl $8192, %eax ## imm = 0x2000
; SSE4-NEXT:    je LBB23_28
; SSE4-NEXT:  LBB23_27: ## %cond.store25
; SSE4-NEXT:    pextrb $13, %xmm2, 13(%rdi)
; SSE4-NEXT:    testl $16384, %eax ## imm = 0x4000
; SSE4-NEXT:    je LBB23_30
; SSE4-NEXT:  LBB23_29: ## %cond.store27
; SSE4-NEXT:    pextrb $14, %xmm2, 14(%rdi)
; SSE4-NEXT:    testw %ax, %ax
; SSE4-NEXT:    jns LBB23_32
; SSE4-NEXT:  LBB23_31: ## %cond.store29
; SSE4-NEXT:    pextrb $15, %xmm2, 15(%rdi)
; SSE4-NEXT:    testl $65536, %eax ## imm = 0x10000
; SSE4-NEXT:    je LBB23_34
; SSE4-NEXT:  LBB23_33: ## %cond.store31
; SSE4-NEXT:    pextrb $0, %xmm3, 16(%rdi)
; SSE4-NEXT:    testl $131072, %eax ## imm = 0x20000
; SSE4-NEXT:    je LBB23_36
; SSE4-NEXT:  LBB23_35: ## %cond.store33
; SSE4-NEXT:    pextrb $1, %xmm3, 17(%rdi)
; SSE4-NEXT:    testl $262144, %eax ## imm = 0x40000
; SSE4-NEXT:    je LBB23_38
; SSE4-NEXT:  LBB23_37: ## %cond.store35
; SSE4-NEXT:    pextrb $2, %xmm3, 18(%rdi)
; SSE4-NEXT:    testl $524288, %eax ## imm = 0x80000
; SSE4-NEXT:    je LBB23_40
; SSE4-NEXT:  LBB23_39: ## %cond.store37
; SSE4-NEXT:    pextrb $3, %xmm3, 19(%rdi)
; SSE4-NEXT:    testl $1048576, %eax ## imm = 0x100000
; SSE4-NEXT:    je LBB23_42
; SSE4-NEXT:  LBB23_41: ## %cond.store39
; SSE4-NEXT:    pextrb $4, %xmm3, 20(%rdi)
; SSE4-NEXT:    testl $2097152, %eax ## imm = 0x200000
; SSE4-NEXT:    je LBB23_44
; SSE4-NEXT:  LBB23_43: ## %cond.store41
; SSE4-NEXT:    pextrb $5, %xmm3, 21(%rdi)
; SSE4-NEXT:    testl $4194304, %eax ## imm = 0x400000
; SSE4-NEXT:    je LBB23_46
; SSE4-NEXT:  LBB23_45: ## %cond.store43
; SSE4-NEXT:    pextrb $6, %xmm3, 22(%rdi)
; SSE4-NEXT:    testl $8388608, %eax ## imm = 0x800000
; SSE4-NEXT:    je LBB23_48
; SSE4-NEXT:  LBB23_47: ## %cond.store45
; SSE4-NEXT:    pextrb $7, %xmm3, 23(%rdi)
; SSE4-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; SSE4-NEXT:    je LBB23_50
; SSE4-NEXT:  LBB23_49: ## %cond.store47
; SSE4-NEXT:    pextrb $8, %xmm3, 24(%rdi)
; SSE4-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; SSE4-NEXT:    je LBB23_52
; SSE4-NEXT:  LBB23_51: ## %cond.store49
; SSE4-NEXT:    pextrb $9, %xmm3, 25(%rdi)
; SSE4-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; SSE4-NEXT:    je LBB23_54
; SSE4-NEXT:  LBB23_53: ## %cond.store51
; SSE4-NEXT:    pextrb $10, %xmm3, 26(%rdi)
; SSE4-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; SSE4-NEXT:    je LBB23_56
; SSE4-NEXT:  LBB23_55: ## %cond.store53
; SSE4-NEXT:    pextrb $11, %xmm3, 27(%rdi)
; SSE4-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; SSE4-NEXT:    je LBB23_58
; SSE4-NEXT:  LBB23_57: ## %cond.store55
; SSE4-NEXT:    pextrb $12, %xmm3, 28(%rdi)
; SSE4-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; SSE4-NEXT:    je LBB23_60
; SSE4-NEXT:  LBB23_59: ## %cond.store57
; SSE4-NEXT:    pextrb $13, %xmm3, 29(%rdi)
; SSE4-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; SSE4-NEXT:    je LBB23_62
; SSE4-NEXT:  LBB23_61: ## %cond.store59
; SSE4-NEXT:    pextrb $14, %xmm3, 30(%rdi)
; SSE4-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; SSE4-NEXT:    je LBB23_64
; SSE4-NEXT:  LBB23_63: ## %cond.store61
; SSE4-NEXT:    pextrb $15, %xmm3, 31(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: store_v32i8_v32i8:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX1-NEXT:    vpcmpeqb %xmm2, %xmm0, %xmm3
; AVX1-NEXT:    vpmovmskb %xmm3, %ecx
; AVX1-NEXT:    vextractf128 $1, %ymm0, %xmm0
; AVX1-NEXT:    vpcmpeqb %xmm2, %xmm0, %xmm0
; AVX1-NEXT:    vpmovmskb %xmm0, %eax
; AVX1-NEXT:    shll $16, %eax
; AVX1-NEXT:    orl %ecx, %eax
; AVX1-NEXT:    testb $1, %al
; AVX1-NEXT:    jne LBB23_1
; AVX1-NEXT:  ## %bb.2: ## %else
; AVX1-NEXT:    testb $2, %al
; AVX1-NEXT:    jne LBB23_3
; AVX1-NEXT:  LBB23_4: ## %else2
; AVX1-NEXT:    testb $4, %al
; AVX1-NEXT:    jne LBB23_5
; AVX1-NEXT:  LBB23_6: ## %else4
; AVX1-NEXT:    testb $8, %al
; AVX1-NEXT:    jne LBB23_7
; AVX1-NEXT:  LBB23_8: ## %else6
; AVX1-NEXT:    testb $16, %al
; AVX1-NEXT:    jne LBB23_9
; AVX1-NEXT:  LBB23_10: ## %else8
; AVX1-NEXT:    testb $32, %al
; AVX1-NEXT:    jne LBB23_11
; AVX1-NEXT:  LBB23_12: ## %else10
; AVX1-NEXT:    testb $64, %al
; AVX1-NEXT:    jne LBB23_13
; AVX1-NEXT:  LBB23_14: ## %else12
; AVX1-NEXT:    testb %al, %al
; AVX1-NEXT:    js LBB23_15
; AVX1-NEXT:  LBB23_16: ## %else14
; AVX1-NEXT:    testl $256, %eax ## imm = 0x100
; AVX1-NEXT:    jne LBB23_17
; AVX1-NEXT:  LBB23_18: ## %else16
; AVX1-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1-NEXT:    jne LBB23_19
; AVX1-NEXT:  LBB23_20: ## %else18
; AVX1-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1-NEXT:    jne LBB23_21
; AVX1-NEXT:  LBB23_22: ## %else20
; AVX1-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1-NEXT:    jne LBB23_23
; AVX1-NEXT:  LBB23_24: ## %else22
; AVX1-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1-NEXT:    jne LBB23_25
; AVX1-NEXT:  LBB23_26: ## %else24
; AVX1-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1-NEXT:    jne LBB23_27
; AVX1-NEXT:  LBB23_28: ## %else26
; AVX1-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1-NEXT:    jne LBB23_29
; AVX1-NEXT:  LBB23_30: ## %else28
; AVX1-NEXT:    testw %ax, %ax
; AVX1-NEXT:    jns LBB23_32
; AVX1-NEXT:  LBB23_31: ## %cond.store29
; AVX1-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX1-NEXT:  LBB23_32: ## %else30
; AVX1-NEXT:    testl $65536, %eax ## imm = 0x10000
; AVX1-NEXT:    vextractf128 $1, %ymm1, %xmm0
; AVX1-NEXT:    jne LBB23_33
; AVX1-NEXT:  ## %bb.34: ## %else32
; AVX1-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX1-NEXT:    jne LBB23_35
; AVX1-NEXT:  LBB23_36: ## %else34
; AVX1-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX1-NEXT:    jne LBB23_37
; AVX1-NEXT:  LBB23_38: ## %else36
; AVX1-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX1-NEXT:    jne LBB23_39
; AVX1-NEXT:  LBB23_40: ## %else38
; AVX1-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX1-NEXT:    jne LBB23_41
; AVX1-NEXT:  LBB23_42: ## %else40
; AVX1-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX1-NEXT:    jne LBB23_43
; AVX1-NEXT:  LBB23_44: ## %else42
; AVX1-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX1-NEXT:    jne LBB23_45
; AVX1-NEXT:  LBB23_46: ## %else44
; AVX1-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX1-NEXT:    jne LBB23_47
; AVX1-NEXT:  LBB23_48: ## %else46
; AVX1-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX1-NEXT:    jne LBB23_49
; AVX1-NEXT:  LBB23_50: ## %else48
; AVX1-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX1-NEXT:    jne LBB23_51
; AVX1-NEXT:  LBB23_52: ## %else50
; AVX1-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX1-NEXT:    jne LBB23_53
; AVX1-NEXT:  LBB23_54: ## %else52
; AVX1-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX1-NEXT:    jne LBB23_55
; AVX1-NEXT:  LBB23_56: ## %else54
; AVX1-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX1-NEXT:    jne LBB23_57
; AVX1-NEXT:  LBB23_58: ## %else56
; AVX1-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX1-NEXT:    jne LBB23_59
; AVX1-NEXT:  LBB23_60: ## %else58
; AVX1-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX1-NEXT:    jne LBB23_61
; AVX1-NEXT:  LBB23_62: ## %else60
; AVX1-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX1-NEXT:    jne LBB23_63
; AVX1-NEXT:  LBB23_64: ## %else62
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
; AVX1-NEXT:  LBB23_1: ## %cond.store
; AVX1-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX1-NEXT:    testb $2, %al
; AVX1-NEXT:    je LBB23_4
; AVX1-NEXT:  LBB23_3: ## %cond.store1
; AVX1-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX1-NEXT:    testb $4, %al
; AVX1-NEXT:    je LBB23_6
; AVX1-NEXT:  LBB23_5: ## %cond.store3
; AVX1-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX1-NEXT:    testb $8, %al
; AVX1-NEXT:    je LBB23_8
; AVX1-NEXT:  LBB23_7: ## %cond.store5
; AVX1-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX1-NEXT:    testb $16, %al
; AVX1-NEXT:    je LBB23_10
; AVX1-NEXT:  LBB23_9: ## %cond.store7
; AVX1-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX1-NEXT:    testb $32, %al
; AVX1-NEXT:    je LBB23_12
; AVX1-NEXT:  LBB23_11: ## %cond.store9
; AVX1-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX1-NEXT:    testb $64, %al
; AVX1-NEXT:    je LBB23_14
; AVX1-NEXT:  LBB23_13: ## %cond.store11
; AVX1-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX1-NEXT:    testb %al, %al
; AVX1-NEXT:    jns LBB23_16
; AVX1-NEXT:  LBB23_15: ## %cond.store13
; AVX1-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX1-NEXT:    testl $256, %eax ## imm = 0x100
; AVX1-NEXT:    je LBB23_18
; AVX1-NEXT:  LBB23_17: ## %cond.store15
; AVX1-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX1-NEXT:    testl $512, %eax ## imm = 0x200
; AVX1-NEXT:    je LBB23_20
; AVX1-NEXT:  LBB23_19: ## %cond.store17
; AVX1-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX1-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX1-NEXT:    je LBB23_22
; AVX1-NEXT:  LBB23_21: ## %cond.store19
; AVX1-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX1-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX1-NEXT:    je LBB23_24
; AVX1-NEXT:  LBB23_23: ## %cond.store21
; AVX1-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX1-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX1-NEXT:    je LBB23_26
; AVX1-NEXT:  LBB23_25: ## %cond.store23
; AVX1-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX1-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX1-NEXT:    je LBB23_28
; AVX1-NEXT:  LBB23_27: ## %cond.store25
; AVX1-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX1-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX1-NEXT:    je LBB23_30
; AVX1-NEXT:  LBB23_29: ## %cond.store27
; AVX1-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX1-NEXT:    testw %ax, %ax
; AVX1-NEXT:    js LBB23_31
; AVX1-NEXT:    jmp LBB23_32
; AVX1-NEXT:  LBB23_33: ## %cond.store31
; AVX1-NEXT:    vpextrb $0, %xmm0, 16(%rdi)
; AVX1-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX1-NEXT:    je LBB23_36
; AVX1-NEXT:  LBB23_35: ## %cond.store33
; AVX1-NEXT:    vpextrb $1, %xmm0, 17(%rdi)
; AVX1-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX1-NEXT:    je LBB23_38
; AVX1-NEXT:  LBB23_37: ## %cond.store35
; AVX1-NEXT:    vpextrb $2, %xmm0, 18(%rdi)
; AVX1-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX1-NEXT:    je LBB23_40
; AVX1-NEXT:  LBB23_39: ## %cond.store37
; AVX1-NEXT:    vpextrb $3, %xmm0, 19(%rdi)
; AVX1-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX1-NEXT:    je LBB23_42
; AVX1-NEXT:  LBB23_41: ## %cond.store39
; AVX1-NEXT:    vpextrb $4, %xmm0, 20(%rdi)
; AVX1-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX1-NEXT:    je LBB23_44
; AVX1-NEXT:  LBB23_43: ## %cond.store41
; AVX1-NEXT:    vpextrb $5, %xmm0, 21(%rdi)
; AVX1-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX1-NEXT:    je LBB23_46
; AVX1-NEXT:  LBB23_45: ## %cond.store43
; AVX1-NEXT:    vpextrb $6, %xmm0, 22(%rdi)
; AVX1-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX1-NEXT:    je LBB23_48
; AVX1-NEXT:  LBB23_47: ## %cond.store45
; AVX1-NEXT:    vpextrb $7, %xmm0, 23(%rdi)
; AVX1-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX1-NEXT:    je LBB23_50
; AVX1-NEXT:  LBB23_49: ## %cond.store47
; AVX1-NEXT:    vpextrb $8, %xmm0, 24(%rdi)
; AVX1-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX1-NEXT:    je LBB23_52
; AVX1-NEXT:  LBB23_51: ## %cond.store49
; AVX1-NEXT:    vpextrb $9, %xmm0, 25(%rdi)
; AVX1-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX1-NEXT:    je LBB23_54
; AVX1-NEXT:  LBB23_53: ## %cond.store51
; AVX1-NEXT:    vpextrb $10, %xmm0, 26(%rdi)
; AVX1-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX1-NEXT:    je LBB23_56
; AVX1-NEXT:  LBB23_55: ## %cond.store53
; AVX1-NEXT:    vpextrb $11, %xmm0, 27(%rdi)
; AVX1-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX1-NEXT:    je LBB23_58
; AVX1-NEXT:  LBB23_57: ## %cond.store55
; AVX1-NEXT:    vpextrb $12, %xmm0, 28(%rdi)
; AVX1-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX1-NEXT:    je LBB23_60
; AVX1-NEXT:  LBB23_59: ## %cond.store57
; AVX1-NEXT:    vpextrb $13, %xmm0, 29(%rdi)
; AVX1-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX1-NEXT:    je LBB23_62
; AVX1-NEXT:  LBB23_61: ## %cond.store59
; AVX1-NEXT:    vpextrb $14, %xmm0, 30(%rdi)
; AVX1-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX1-NEXT:    je LBB23_64
; AVX1-NEXT:  LBB23_63: ## %cond.store61
; AVX1-NEXT:    vpextrb $15, %xmm0, 31(%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v32i8_v32i8:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX2-NEXT:    vpcmpeqb %ymm2, %ymm0, %ymm0
; AVX2-NEXT:    vpmovmskb %ymm0, %eax
; AVX2-NEXT:    testb $1, %al
; AVX2-NEXT:    jne LBB23_1
; AVX2-NEXT:  ## %bb.2: ## %else
; AVX2-NEXT:    testb $2, %al
; AVX2-NEXT:    jne LBB23_3
; AVX2-NEXT:  LBB23_4: ## %else2
; AVX2-NEXT:    testb $4, %al
; AVX2-NEXT:    jne LBB23_5
; AVX2-NEXT:  LBB23_6: ## %else4
; AVX2-NEXT:    testb $8, %al
; AVX2-NEXT:    jne LBB23_7
; AVX2-NEXT:  LBB23_8: ## %else6
; AVX2-NEXT:    testb $16, %al
; AVX2-NEXT:    jne LBB23_9
; AVX2-NEXT:  LBB23_10: ## %else8
; AVX2-NEXT:    testb $32, %al
; AVX2-NEXT:    jne LBB23_11
; AVX2-NEXT:  LBB23_12: ## %else10
; AVX2-NEXT:    testb $64, %al
; AVX2-NEXT:    jne LBB23_13
; AVX2-NEXT:  LBB23_14: ## %else12
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    js LBB23_15
; AVX2-NEXT:  LBB23_16: ## %else14
; AVX2-NEXT:    testl $256, %eax ## imm = 0x100
; AVX2-NEXT:    jne LBB23_17
; AVX2-NEXT:  LBB23_18: ## %else16
; AVX2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX2-NEXT:    jne LBB23_19
; AVX2-NEXT:  LBB23_20: ## %else18
; AVX2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX2-NEXT:    jne LBB23_21
; AVX2-NEXT:  LBB23_22: ## %else20
; AVX2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX2-NEXT:    jne LBB23_23
; AVX2-NEXT:  LBB23_24: ## %else22
; AVX2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX2-NEXT:    jne LBB23_25
; AVX2-NEXT:  LBB23_26: ## %else24
; AVX2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX2-NEXT:    jne LBB23_27
; AVX2-NEXT:  LBB23_28: ## %else26
; AVX2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX2-NEXT:    jne LBB23_29
; AVX2-NEXT:  LBB23_30: ## %else28
; AVX2-NEXT:    testw %ax, %ax
; AVX2-NEXT:    jns LBB23_32
; AVX2-NEXT:  LBB23_31: ## %cond.store29
; AVX2-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX2-NEXT:  LBB23_32: ## %else30
; AVX2-NEXT:    testl $65536, %eax ## imm = 0x10000
; AVX2-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX2-NEXT:    jne LBB23_33
; AVX2-NEXT:  ## %bb.34: ## %else32
; AVX2-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX2-NEXT:    jne LBB23_35
; AVX2-NEXT:  LBB23_36: ## %else34
; AVX2-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX2-NEXT:    jne LBB23_37
; AVX2-NEXT:  LBB23_38: ## %else36
; AVX2-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX2-NEXT:    jne LBB23_39
; AVX2-NEXT:  LBB23_40: ## %else38
; AVX2-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX2-NEXT:    jne LBB23_41
; AVX2-NEXT:  LBB23_42: ## %else40
; AVX2-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX2-NEXT:    jne LBB23_43
; AVX2-NEXT:  LBB23_44: ## %else42
; AVX2-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX2-NEXT:    jne LBB23_45
; AVX2-NEXT:  LBB23_46: ## %else44
; AVX2-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX2-NEXT:    jne LBB23_47
; AVX2-NEXT:  LBB23_48: ## %else46
; AVX2-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX2-NEXT:    jne LBB23_49
; AVX2-NEXT:  LBB23_50: ## %else48
; AVX2-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX2-NEXT:    jne LBB23_51
; AVX2-NEXT:  LBB23_52: ## %else50
; AVX2-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX2-NEXT:    jne LBB23_53
; AVX2-NEXT:  LBB23_54: ## %else52
; AVX2-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX2-NEXT:    jne LBB23_55
; AVX2-NEXT:  LBB23_56: ## %else54
; AVX2-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX2-NEXT:    jne LBB23_57
; AVX2-NEXT:  LBB23_58: ## %else56
; AVX2-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX2-NEXT:    jne LBB23_59
; AVX2-NEXT:  LBB23_60: ## %else58
; AVX2-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX2-NEXT:    jne LBB23_61
; AVX2-NEXT:  LBB23_62: ## %else60
; AVX2-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX2-NEXT:    jne LBB23_63
; AVX2-NEXT:  LBB23_64: ## %else62
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
; AVX2-NEXT:  LBB23_1: ## %cond.store
; AVX2-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX2-NEXT:    testb $2, %al
; AVX2-NEXT:    je LBB23_4
; AVX2-NEXT:  LBB23_3: ## %cond.store1
; AVX2-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX2-NEXT:    testb $4, %al
; AVX2-NEXT:    je LBB23_6
; AVX2-NEXT:  LBB23_5: ## %cond.store3
; AVX2-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX2-NEXT:    testb $8, %al
; AVX2-NEXT:    je LBB23_8
; AVX2-NEXT:  LBB23_7: ## %cond.store5
; AVX2-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX2-NEXT:    testb $16, %al
; AVX2-NEXT:    je LBB23_10
; AVX2-NEXT:  LBB23_9: ## %cond.store7
; AVX2-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX2-NEXT:    testb $32, %al
; AVX2-NEXT:    je LBB23_12
; AVX2-NEXT:  LBB23_11: ## %cond.store9
; AVX2-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX2-NEXT:    testb $64, %al
; AVX2-NEXT:    je LBB23_14
; AVX2-NEXT:  LBB23_13: ## %cond.store11
; AVX2-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX2-NEXT:    testb %al, %al
; AVX2-NEXT:    jns LBB23_16
; AVX2-NEXT:  LBB23_15: ## %cond.store13
; AVX2-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX2-NEXT:    testl $256, %eax ## imm = 0x100
; AVX2-NEXT:    je LBB23_18
; AVX2-NEXT:  LBB23_17: ## %cond.store15
; AVX2-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX2-NEXT:    testl $512, %eax ## imm = 0x200
; AVX2-NEXT:    je LBB23_20
; AVX2-NEXT:  LBB23_19: ## %cond.store17
; AVX2-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX2-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX2-NEXT:    je LBB23_22
; AVX2-NEXT:  LBB23_21: ## %cond.store19
; AVX2-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX2-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX2-NEXT:    je LBB23_24
; AVX2-NEXT:  LBB23_23: ## %cond.store21
; AVX2-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX2-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX2-NEXT:    je LBB23_26
; AVX2-NEXT:  LBB23_25: ## %cond.store23
; AVX2-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX2-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX2-NEXT:    je LBB23_28
; AVX2-NEXT:  LBB23_27: ## %cond.store25
; AVX2-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX2-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX2-NEXT:    je LBB23_30
; AVX2-NEXT:  LBB23_29: ## %cond.store27
; AVX2-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX2-NEXT:    testw %ax, %ax
; AVX2-NEXT:    js LBB23_31
; AVX2-NEXT:    jmp LBB23_32
; AVX2-NEXT:  LBB23_33: ## %cond.store31
; AVX2-NEXT:    vpextrb $0, %xmm0, 16(%rdi)
; AVX2-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX2-NEXT:    je LBB23_36
; AVX2-NEXT:  LBB23_35: ## %cond.store33
; AVX2-NEXT:    vpextrb $1, %xmm0, 17(%rdi)
; AVX2-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX2-NEXT:    je LBB23_38
; AVX2-NEXT:  LBB23_37: ## %cond.store35
; AVX2-NEXT:    vpextrb $2, %xmm0, 18(%rdi)
; AVX2-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX2-NEXT:    je LBB23_40
; AVX2-NEXT:  LBB23_39: ## %cond.store37
; AVX2-NEXT:    vpextrb $3, %xmm0, 19(%rdi)
; AVX2-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX2-NEXT:    je LBB23_42
; AVX2-NEXT:  LBB23_41: ## %cond.store39
; AVX2-NEXT:    vpextrb $4, %xmm0, 20(%rdi)
; AVX2-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX2-NEXT:    je LBB23_44
; AVX2-NEXT:  LBB23_43: ## %cond.store41
; AVX2-NEXT:    vpextrb $5, %xmm0, 21(%rdi)
; AVX2-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX2-NEXT:    je LBB23_46
; AVX2-NEXT:  LBB23_45: ## %cond.store43
; AVX2-NEXT:    vpextrb $6, %xmm0, 22(%rdi)
; AVX2-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX2-NEXT:    je LBB23_48
; AVX2-NEXT:  LBB23_47: ## %cond.store45
; AVX2-NEXT:    vpextrb $7, %xmm0, 23(%rdi)
; AVX2-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX2-NEXT:    je LBB23_50
; AVX2-NEXT:  LBB23_49: ## %cond.store47
; AVX2-NEXT:    vpextrb $8, %xmm0, 24(%rdi)
; AVX2-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX2-NEXT:    je LBB23_52
; AVX2-NEXT:  LBB23_51: ## %cond.store49
; AVX2-NEXT:    vpextrb $9, %xmm0, 25(%rdi)
; AVX2-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX2-NEXT:    je LBB23_54
; AVX2-NEXT:  LBB23_53: ## %cond.store51
; AVX2-NEXT:    vpextrb $10, %xmm0, 26(%rdi)
; AVX2-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX2-NEXT:    je LBB23_56
; AVX2-NEXT:  LBB23_55: ## %cond.store53
; AVX2-NEXT:    vpextrb $11, %xmm0, 27(%rdi)
; AVX2-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX2-NEXT:    je LBB23_58
; AVX2-NEXT:  LBB23_57: ## %cond.store55
; AVX2-NEXT:    vpextrb $12, %xmm0, 28(%rdi)
; AVX2-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX2-NEXT:    je LBB23_60
; AVX2-NEXT:  LBB23_59: ## %cond.store57
; AVX2-NEXT:    vpextrb $13, %xmm0, 29(%rdi)
; AVX2-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX2-NEXT:    je LBB23_62
; AVX2-NEXT:  LBB23_61: ## %cond.store59
; AVX2-NEXT:    vpextrb $14, %xmm0, 30(%rdi)
; AVX2-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX2-NEXT:    je LBB23_64
; AVX2-NEXT:  LBB23_63: ## %cond.store61
; AVX2-NEXT:    vpextrb $15, %xmm0, 31(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v32i8_v32i8:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    vpcmpeqb %ymm2, %ymm0, %ymm0
; AVX512F-NEXT:    vpmovmskb %ymm0, %eax
; AVX512F-NEXT:    testb $1, %al
; AVX512F-NEXT:    jne LBB23_1
; AVX512F-NEXT:  ## %bb.2: ## %else
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    jne LBB23_3
; AVX512F-NEXT:  LBB23_4: ## %else2
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    jne LBB23_5
; AVX512F-NEXT:  LBB23_6: ## %else4
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    jne LBB23_7
; AVX512F-NEXT:  LBB23_8: ## %else6
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    jne LBB23_9
; AVX512F-NEXT:  LBB23_10: ## %else8
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    jne LBB23_11
; AVX512F-NEXT:  LBB23_12: ## %else10
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    jne LBB23_13
; AVX512F-NEXT:  LBB23_14: ## %else12
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    js LBB23_15
; AVX512F-NEXT:  LBB23_16: ## %else14
; AVX512F-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512F-NEXT:    jne LBB23_17
; AVX512F-NEXT:  LBB23_18: ## %else16
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    jne LBB23_19
; AVX512F-NEXT:  LBB23_20: ## %else18
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    jne LBB23_21
; AVX512F-NEXT:  LBB23_22: ## %else20
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    jne LBB23_23
; AVX512F-NEXT:  LBB23_24: ## %else22
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    jne LBB23_25
; AVX512F-NEXT:  LBB23_26: ## %else24
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    jne LBB23_27
; AVX512F-NEXT:  LBB23_28: ## %else26
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    jne LBB23_29
; AVX512F-NEXT:  LBB23_30: ## %else28
; AVX512F-NEXT:    testw %ax, %ax
; AVX512F-NEXT:    jns LBB23_32
; AVX512F-NEXT:  LBB23_31: ## %cond.store29
; AVX512F-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX512F-NEXT:  LBB23_32: ## %else30
; AVX512F-NEXT:    testl $65536, %eax ## imm = 0x10000
; AVX512F-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512F-NEXT:    jne LBB23_33
; AVX512F-NEXT:  ## %bb.34: ## %else32
; AVX512F-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX512F-NEXT:    jne LBB23_35
; AVX512F-NEXT:  LBB23_36: ## %else34
; AVX512F-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX512F-NEXT:    jne LBB23_37
; AVX512F-NEXT:  LBB23_38: ## %else36
; AVX512F-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX512F-NEXT:    jne LBB23_39
; AVX512F-NEXT:  LBB23_40: ## %else38
; AVX512F-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX512F-NEXT:    jne LBB23_41
; AVX512F-NEXT:  LBB23_42: ## %else40
; AVX512F-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX512F-NEXT:    jne LBB23_43
; AVX512F-NEXT:  LBB23_44: ## %else42
; AVX512F-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX512F-NEXT:    jne LBB23_45
; AVX512F-NEXT:  LBB23_46: ## %else44
; AVX512F-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX512F-NEXT:    jne LBB23_47
; AVX512F-NEXT:  LBB23_48: ## %else46
; AVX512F-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX512F-NEXT:    jne LBB23_49
; AVX512F-NEXT:  LBB23_50: ## %else48
; AVX512F-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX512F-NEXT:    jne LBB23_51
; AVX512F-NEXT:  LBB23_52: ## %else50
; AVX512F-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX512F-NEXT:    jne LBB23_53
; AVX512F-NEXT:  LBB23_54: ## %else52
; AVX512F-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX512F-NEXT:    jne LBB23_55
; AVX512F-NEXT:  LBB23_56: ## %else54
; AVX512F-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX512F-NEXT:    jne LBB23_57
; AVX512F-NEXT:  LBB23_58: ## %else56
; AVX512F-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX512F-NEXT:    jne LBB23_59
; AVX512F-NEXT:  LBB23_60: ## %else58
; AVX512F-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX512F-NEXT:    jne LBB23_61
; AVX512F-NEXT:  LBB23_62: ## %else60
; AVX512F-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX512F-NEXT:    jne LBB23_63
; AVX512F-NEXT:  LBB23_64: ## %else62
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
; AVX512F-NEXT:  LBB23_1: ## %cond.store
; AVX512F-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX512F-NEXT:    testb $2, %al
; AVX512F-NEXT:    je LBB23_4
; AVX512F-NEXT:  LBB23_3: ## %cond.store1
; AVX512F-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX512F-NEXT:    testb $4, %al
; AVX512F-NEXT:    je LBB23_6
; AVX512F-NEXT:  LBB23_5: ## %cond.store3
; AVX512F-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX512F-NEXT:    testb $8, %al
; AVX512F-NEXT:    je LBB23_8
; AVX512F-NEXT:  LBB23_7: ## %cond.store5
; AVX512F-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX512F-NEXT:    testb $16, %al
; AVX512F-NEXT:    je LBB23_10
; AVX512F-NEXT:  LBB23_9: ## %cond.store7
; AVX512F-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX512F-NEXT:    testb $32, %al
; AVX512F-NEXT:    je LBB23_12
; AVX512F-NEXT:  LBB23_11: ## %cond.store9
; AVX512F-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX512F-NEXT:    testb $64, %al
; AVX512F-NEXT:    je LBB23_14
; AVX512F-NEXT:  LBB23_13: ## %cond.store11
; AVX512F-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX512F-NEXT:    testb %al, %al
; AVX512F-NEXT:    jns LBB23_16
; AVX512F-NEXT:  LBB23_15: ## %cond.store13
; AVX512F-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX512F-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512F-NEXT:    je LBB23_18
; AVX512F-NEXT:  LBB23_17: ## %cond.store15
; AVX512F-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX512F-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512F-NEXT:    je LBB23_20
; AVX512F-NEXT:  LBB23_19: ## %cond.store17
; AVX512F-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX512F-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512F-NEXT:    je LBB23_22
; AVX512F-NEXT:  LBB23_21: ## %cond.store19
; AVX512F-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX512F-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512F-NEXT:    je LBB23_24
; AVX512F-NEXT:  LBB23_23: ## %cond.store21
; AVX512F-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX512F-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512F-NEXT:    je LBB23_26
; AVX512F-NEXT:  LBB23_25: ## %cond.store23
; AVX512F-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX512F-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512F-NEXT:    je LBB23_28
; AVX512F-NEXT:  LBB23_27: ## %cond.store25
; AVX512F-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX512F-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512F-NEXT:    je LBB23_30
; AVX512F-NEXT:  LBB23_29: ## %cond.store27
; AVX512F-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX512F-NEXT:    testw %ax, %ax
; AVX512F-NEXT:    js LBB23_31
; AVX512F-NEXT:    jmp LBB23_32
; AVX512F-NEXT:  LBB23_33: ## %cond.store31
; AVX512F-NEXT:    vpextrb $0, %xmm0, 16(%rdi)
; AVX512F-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX512F-NEXT:    je LBB23_36
; AVX512F-NEXT:  LBB23_35: ## %cond.store33
; AVX512F-NEXT:    vpextrb $1, %xmm0, 17(%rdi)
; AVX512F-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX512F-NEXT:    je LBB23_38
; AVX512F-NEXT:  LBB23_37: ## %cond.store35
; AVX512F-NEXT:    vpextrb $2, %xmm0, 18(%rdi)
; AVX512F-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX512F-NEXT:    je LBB23_40
; AVX512F-NEXT:  LBB23_39: ## %cond.store37
; AVX512F-NEXT:    vpextrb $3, %xmm0, 19(%rdi)
; AVX512F-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX512F-NEXT:    je LBB23_42
; AVX512F-NEXT:  LBB23_41: ## %cond.store39
; AVX512F-NEXT:    vpextrb $4, %xmm0, 20(%rdi)
; AVX512F-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX512F-NEXT:    je LBB23_44
; AVX512F-NEXT:  LBB23_43: ## %cond.store41
; AVX512F-NEXT:    vpextrb $5, %xmm0, 21(%rdi)
; AVX512F-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX512F-NEXT:    je LBB23_46
; AVX512F-NEXT:  LBB23_45: ## %cond.store43
; AVX512F-NEXT:    vpextrb $6, %xmm0, 22(%rdi)
; AVX512F-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX512F-NEXT:    je LBB23_48
; AVX512F-NEXT:  LBB23_47: ## %cond.store45
; AVX512F-NEXT:    vpextrb $7, %xmm0, 23(%rdi)
; AVX512F-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX512F-NEXT:    je LBB23_50
; AVX512F-NEXT:  LBB23_49: ## %cond.store47
; AVX512F-NEXT:    vpextrb $8, %xmm0, 24(%rdi)
; AVX512F-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX512F-NEXT:    je LBB23_52
; AVX512F-NEXT:  LBB23_51: ## %cond.store49
; AVX512F-NEXT:    vpextrb $9, %xmm0, 25(%rdi)
; AVX512F-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX512F-NEXT:    je LBB23_54
; AVX512F-NEXT:  LBB23_53: ## %cond.store51
; AVX512F-NEXT:    vpextrb $10, %xmm0, 26(%rdi)
; AVX512F-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX512F-NEXT:    je LBB23_56
; AVX512F-NEXT:  LBB23_55: ## %cond.store53
; AVX512F-NEXT:    vpextrb $11, %xmm0, 27(%rdi)
; AVX512F-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX512F-NEXT:    je LBB23_58
; AVX512F-NEXT:  LBB23_57: ## %cond.store55
; AVX512F-NEXT:    vpextrb $12, %xmm0, 28(%rdi)
; AVX512F-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX512F-NEXT:    je LBB23_60
; AVX512F-NEXT:  LBB23_59: ## %cond.store57
; AVX512F-NEXT:    vpextrb $13, %xmm0, 29(%rdi)
; AVX512F-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX512F-NEXT:    je LBB23_62
; AVX512F-NEXT:  LBB23_61: ## %cond.store59
; AVX512F-NEXT:    vpextrb $14, %xmm0, 30(%rdi)
; AVX512F-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX512F-NEXT:    je LBB23_64
; AVX512F-NEXT:  LBB23_63: ## %cond.store61
; AVX512F-NEXT:    vpextrb $15, %xmm0, 31(%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v32i8_v32i8:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLDQ-NEXT:    vpcmpeqb %ymm2, %ymm0, %ymm0
; AVX512VLDQ-NEXT:    vpmovmskb %ymm0, %eax
; AVX512VLDQ-NEXT:    testb $1, %al
; AVX512VLDQ-NEXT:    jne LBB23_1
; AVX512VLDQ-NEXT:  ## %bb.2: ## %else
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    jne LBB23_3
; AVX512VLDQ-NEXT:  LBB23_4: ## %else2
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    jne LBB23_5
; AVX512VLDQ-NEXT:  LBB23_6: ## %else4
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    jne LBB23_7
; AVX512VLDQ-NEXT:  LBB23_8: ## %else6
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    jne LBB23_9
; AVX512VLDQ-NEXT:  LBB23_10: ## %else8
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    jne LBB23_11
; AVX512VLDQ-NEXT:  LBB23_12: ## %else10
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    jne LBB23_13
; AVX512VLDQ-NEXT:  LBB23_14: ## %else12
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    js LBB23_15
; AVX512VLDQ-NEXT:  LBB23_16: ## %else14
; AVX512VLDQ-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512VLDQ-NEXT:    jne LBB23_17
; AVX512VLDQ-NEXT:  LBB23_18: ## %else16
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    jne LBB23_19
; AVX512VLDQ-NEXT:  LBB23_20: ## %else18
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    jne LBB23_21
; AVX512VLDQ-NEXT:  LBB23_22: ## %else20
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    jne LBB23_23
; AVX512VLDQ-NEXT:  LBB23_24: ## %else22
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    jne LBB23_25
; AVX512VLDQ-NEXT:  LBB23_26: ## %else24
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    jne LBB23_27
; AVX512VLDQ-NEXT:  LBB23_28: ## %else26
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    jne LBB23_29
; AVX512VLDQ-NEXT:  LBB23_30: ## %else28
; AVX512VLDQ-NEXT:    testw %ax, %ax
; AVX512VLDQ-NEXT:    jns LBB23_32
; AVX512VLDQ-NEXT:  LBB23_31: ## %cond.store29
; AVX512VLDQ-NEXT:    vpextrb $15, %xmm1, 15(%rdi)
; AVX512VLDQ-NEXT:  LBB23_32: ## %else30
; AVX512VLDQ-NEXT:    testl $65536, %eax ## imm = 0x10000
; AVX512VLDQ-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512VLDQ-NEXT:    jne LBB23_33
; AVX512VLDQ-NEXT:  ## %bb.34: ## %else32
; AVX512VLDQ-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX512VLDQ-NEXT:    jne LBB23_35
; AVX512VLDQ-NEXT:  LBB23_36: ## %else34
; AVX512VLDQ-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX512VLDQ-NEXT:    jne LBB23_37
; AVX512VLDQ-NEXT:  LBB23_38: ## %else36
; AVX512VLDQ-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX512VLDQ-NEXT:    jne LBB23_39
; AVX512VLDQ-NEXT:  LBB23_40: ## %else38
; AVX512VLDQ-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX512VLDQ-NEXT:    jne LBB23_41
; AVX512VLDQ-NEXT:  LBB23_42: ## %else40
; AVX512VLDQ-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX512VLDQ-NEXT:    jne LBB23_43
; AVX512VLDQ-NEXT:  LBB23_44: ## %else42
; AVX512VLDQ-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX512VLDQ-NEXT:    jne LBB23_45
; AVX512VLDQ-NEXT:  LBB23_46: ## %else44
; AVX512VLDQ-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX512VLDQ-NEXT:    jne LBB23_47
; AVX512VLDQ-NEXT:  LBB23_48: ## %else46
; AVX512VLDQ-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX512VLDQ-NEXT:    jne LBB23_49
; AVX512VLDQ-NEXT:  LBB23_50: ## %else48
; AVX512VLDQ-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX512VLDQ-NEXT:    jne LBB23_51
; AVX512VLDQ-NEXT:  LBB23_52: ## %else50
; AVX512VLDQ-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX512VLDQ-NEXT:    jne LBB23_53
; AVX512VLDQ-NEXT:  LBB23_54: ## %else52
; AVX512VLDQ-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX512VLDQ-NEXT:    jne LBB23_55
; AVX512VLDQ-NEXT:  LBB23_56: ## %else54
; AVX512VLDQ-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX512VLDQ-NEXT:    jne LBB23_57
; AVX512VLDQ-NEXT:  LBB23_58: ## %else56
; AVX512VLDQ-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX512VLDQ-NEXT:    jne LBB23_59
; AVX512VLDQ-NEXT:  LBB23_60: ## %else58
; AVX512VLDQ-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX512VLDQ-NEXT:    jne LBB23_61
; AVX512VLDQ-NEXT:  LBB23_62: ## %else60
; AVX512VLDQ-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX512VLDQ-NEXT:    jne LBB23_63
; AVX512VLDQ-NEXT:  LBB23_64: ## %else62
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
; AVX512VLDQ-NEXT:  LBB23_1: ## %cond.store
; AVX512VLDQ-NEXT:    vpextrb $0, %xmm1, (%rdi)
; AVX512VLDQ-NEXT:    testb $2, %al
; AVX512VLDQ-NEXT:    je LBB23_4
; AVX512VLDQ-NEXT:  LBB23_3: ## %cond.store1
; AVX512VLDQ-NEXT:    vpextrb $1, %xmm1, 1(%rdi)
; AVX512VLDQ-NEXT:    testb $4, %al
; AVX512VLDQ-NEXT:    je LBB23_6
; AVX512VLDQ-NEXT:  LBB23_5: ## %cond.store3
; AVX512VLDQ-NEXT:    vpextrb $2, %xmm1, 2(%rdi)
; AVX512VLDQ-NEXT:    testb $8, %al
; AVX512VLDQ-NEXT:    je LBB23_8
; AVX512VLDQ-NEXT:  LBB23_7: ## %cond.store5
; AVX512VLDQ-NEXT:    vpextrb $3, %xmm1, 3(%rdi)
; AVX512VLDQ-NEXT:    testb $16, %al
; AVX512VLDQ-NEXT:    je LBB23_10
; AVX512VLDQ-NEXT:  LBB23_9: ## %cond.store7
; AVX512VLDQ-NEXT:    vpextrb $4, %xmm1, 4(%rdi)
; AVX512VLDQ-NEXT:    testb $32, %al
; AVX512VLDQ-NEXT:    je LBB23_12
; AVX512VLDQ-NEXT:  LBB23_11: ## %cond.store9
; AVX512VLDQ-NEXT:    vpextrb $5, %xmm1, 5(%rdi)
; AVX512VLDQ-NEXT:    testb $64, %al
; AVX512VLDQ-NEXT:    je LBB23_14
; AVX512VLDQ-NEXT:  LBB23_13: ## %cond.store11
; AVX512VLDQ-NEXT:    vpextrb $6, %xmm1, 6(%rdi)
; AVX512VLDQ-NEXT:    testb %al, %al
; AVX512VLDQ-NEXT:    jns LBB23_16
; AVX512VLDQ-NEXT:  LBB23_15: ## %cond.store13
; AVX512VLDQ-NEXT:    vpextrb $7, %xmm1, 7(%rdi)
; AVX512VLDQ-NEXT:    testl $256, %eax ## imm = 0x100
; AVX512VLDQ-NEXT:    je LBB23_18
; AVX512VLDQ-NEXT:  LBB23_17: ## %cond.store15
; AVX512VLDQ-NEXT:    vpextrb $8, %xmm1, 8(%rdi)
; AVX512VLDQ-NEXT:    testl $512, %eax ## imm = 0x200
; AVX512VLDQ-NEXT:    je LBB23_20
; AVX512VLDQ-NEXT:  LBB23_19: ## %cond.store17
; AVX512VLDQ-NEXT:    vpextrb $9, %xmm1, 9(%rdi)
; AVX512VLDQ-NEXT:    testl $1024, %eax ## imm = 0x400
; AVX512VLDQ-NEXT:    je LBB23_22
; AVX512VLDQ-NEXT:  LBB23_21: ## %cond.store19
; AVX512VLDQ-NEXT:    vpextrb $10, %xmm1, 10(%rdi)
; AVX512VLDQ-NEXT:    testl $2048, %eax ## imm = 0x800
; AVX512VLDQ-NEXT:    je LBB23_24
; AVX512VLDQ-NEXT:  LBB23_23: ## %cond.store21
; AVX512VLDQ-NEXT:    vpextrb $11, %xmm1, 11(%rdi)
; AVX512VLDQ-NEXT:    testl $4096, %eax ## imm = 0x1000
; AVX512VLDQ-NEXT:    je LBB23_26
; AVX512VLDQ-NEXT:  LBB23_25: ## %cond.store23
; AVX512VLDQ-NEXT:    vpextrb $12, %xmm1, 12(%rdi)
; AVX512VLDQ-NEXT:    testl $8192, %eax ## imm = 0x2000
; AVX512VLDQ-NEXT:    je LBB23_28
; AVX512VLDQ-NEXT:  LBB23_27: ## %cond.store25
; AVX512VLDQ-NEXT:    vpextrb $13, %xmm1, 13(%rdi)
; AVX512VLDQ-NEXT:    testl $16384, %eax ## imm = 0x4000
; AVX512VLDQ-NEXT:    je LBB23_30
; AVX512VLDQ-NEXT:  LBB23_29: ## %cond.store27
; AVX512VLDQ-NEXT:    vpextrb $14, %xmm1, 14(%rdi)
; AVX512VLDQ-NEXT:    testw %ax, %ax
; AVX512VLDQ-NEXT:    js LBB23_31
; AVX512VLDQ-NEXT:    jmp LBB23_32
; AVX512VLDQ-NEXT:  LBB23_33: ## %cond.store31
; AVX512VLDQ-NEXT:    vpextrb $0, %xmm0, 16(%rdi)
; AVX512VLDQ-NEXT:    testl $131072, %eax ## imm = 0x20000
; AVX512VLDQ-NEXT:    je LBB23_36
; AVX512VLDQ-NEXT:  LBB23_35: ## %cond.store33
; AVX512VLDQ-NEXT:    vpextrb $1, %xmm0, 17(%rdi)
; AVX512VLDQ-NEXT:    testl $262144, %eax ## imm = 0x40000
; AVX512VLDQ-NEXT:    je LBB23_38
; AVX512VLDQ-NEXT:  LBB23_37: ## %cond.store35
; AVX512VLDQ-NEXT:    vpextrb $2, %xmm0, 18(%rdi)
; AVX512VLDQ-NEXT:    testl $524288, %eax ## imm = 0x80000
; AVX512VLDQ-NEXT:    je LBB23_40
; AVX512VLDQ-NEXT:  LBB23_39: ## %cond.store37
; AVX512VLDQ-NEXT:    vpextrb $3, %xmm0, 19(%rdi)
; AVX512VLDQ-NEXT:    testl $1048576, %eax ## imm = 0x100000
; AVX512VLDQ-NEXT:    je LBB23_42
; AVX512VLDQ-NEXT:  LBB23_41: ## %cond.store39
; AVX512VLDQ-NEXT:    vpextrb $4, %xmm0, 20(%rdi)
; AVX512VLDQ-NEXT:    testl $2097152, %eax ## imm = 0x200000
; AVX512VLDQ-NEXT:    je LBB23_44
; AVX512VLDQ-NEXT:  LBB23_43: ## %cond.store41
; AVX512VLDQ-NEXT:    vpextrb $5, %xmm0, 21(%rdi)
; AVX512VLDQ-NEXT:    testl $4194304, %eax ## imm = 0x400000
; AVX512VLDQ-NEXT:    je LBB23_46
; AVX512VLDQ-NEXT:  LBB23_45: ## %cond.store43
; AVX512VLDQ-NEXT:    vpextrb $6, %xmm0, 22(%rdi)
; AVX512VLDQ-NEXT:    testl $8388608, %eax ## imm = 0x800000
; AVX512VLDQ-NEXT:    je LBB23_48
; AVX512VLDQ-NEXT:  LBB23_47: ## %cond.store45
; AVX512VLDQ-NEXT:    vpextrb $7, %xmm0, 23(%rdi)
; AVX512VLDQ-NEXT:    testl $16777216, %eax ## imm = 0x1000000
; AVX512VLDQ-NEXT:    je LBB23_50
; AVX512VLDQ-NEXT:  LBB23_49: ## %cond.store47
; AVX512VLDQ-NEXT:    vpextrb $8, %xmm0, 24(%rdi)
; AVX512VLDQ-NEXT:    testl $33554432, %eax ## imm = 0x2000000
; AVX512VLDQ-NEXT:    je LBB23_52
; AVX512VLDQ-NEXT:  LBB23_51: ## %cond.store49
; AVX512VLDQ-NEXT:    vpextrb $9, %xmm0, 25(%rdi)
; AVX512VLDQ-NEXT:    testl $67108864, %eax ## imm = 0x4000000
; AVX512VLDQ-NEXT:    je LBB23_54
; AVX512VLDQ-NEXT:  LBB23_53: ## %cond.store51
; AVX512VLDQ-NEXT:    vpextrb $10, %xmm0, 26(%rdi)
; AVX512VLDQ-NEXT:    testl $134217728, %eax ## imm = 0x8000000
; AVX512VLDQ-NEXT:    je LBB23_56
; AVX512VLDQ-NEXT:  LBB23_55: ## %cond.store53
; AVX512VLDQ-NEXT:    vpextrb $11, %xmm0, 27(%rdi)
; AVX512VLDQ-NEXT:    testl $268435456, %eax ## imm = 0x10000000
; AVX512VLDQ-NEXT:    je LBB23_58
; AVX512VLDQ-NEXT:  LBB23_57: ## %cond.store55
; AVX512VLDQ-NEXT:    vpextrb $12, %xmm0, 28(%rdi)
; AVX512VLDQ-NEXT:    testl $536870912, %eax ## imm = 0x20000000
; AVX512VLDQ-NEXT:    je LBB23_60
; AVX512VLDQ-NEXT:  LBB23_59: ## %cond.store57
; AVX512VLDQ-NEXT:    vpextrb $13, %xmm0, 29(%rdi)
; AVX512VLDQ-NEXT:    testl $1073741824, %eax ## imm = 0x40000000
; AVX512VLDQ-NEXT:    je LBB23_62
; AVX512VLDQ-NEXT:  LBB23_61: ## %cond.store59
; AVX512VLDQ-NEXT:    vpextrb $14, %xmm0, 30(%rdi)
; AVX512VLDQ-NEXT:    testl $-2147483648, %eax ## imm = 0x80000000
; AVX512VLDQ-NEXT:    je LBB23_64
; AVX512VLDQ-NEXT:  LBB23_63: ## %cond.store61
; AVX512VLDQ-NEXT:    vpextrb $15, %xmm0, 31(%rdi)
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v32i8_v32i8:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vptestnmb %ymm0, %ymm0, %k1
; AVX512VLBW-NEXT:    vmovdqu8 %ymm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v32i8_v32i8:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestnmb %ymm0, %ymm0, %k1
; X86-AVX512-NEXT:    vmovdqu8 %ymm1, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <32 x i8> %trigger, zeroinitializer
  call void @llvm.masked.store.v32i8.p0(<32 x i8> %val, ptr %addr, i32 4, <32 x i1> %mask)
  ret void
}

;;; Stores with Constant Masks

define void @mstore_constmask_v4i32_v4i32(<4 x i32> %trigger, ptr %addr, <4 x i32> %val) nounwind {
; SSE-LABEL: mstore_constmask_v4i32_v4i32:
; SSE:       ## %bb.0:
; SSE-NEXT:    movups %xmm1, (%rdi)
; SSE-NEXT:    retq
;
; AVX-LABEL: mstore_constmask_v4i32_v4i32:
; AVX:       ## %bb.0:
; AVX-NEXT:    vmovups %xmm1, (%rdi)
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: mstore_constmask_v4i32_v4i32:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovups %xmm1, (%eax)
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <4 x i32> %trigger, zeroinitializer
  call void @llvm.masked.store.v4i32.p0(<4 x i32> %val, ptr %addr, i32 4, <4 x i1><i1 true, i1 true, i1 true, i1 true>)
  ret void
}

; Make sure we are able to detect all ones constant mask after type legalization
; to avoid masked stores.
define void @mstore_constmask_allones_split(<16 x i64> %trigger, ptr %addr, <16 x i64> %val) nounwind {
; SSE2-LABEL: mstore_constmask_allones_split:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm0
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm1
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm2
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm3
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm4
; SSE2-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm5
; SSE2-NEXT:    movq %xmm5, (%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = xmm5[2,3,2,3]
; SSE2-NEXT:    movq %xmm5, 8(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm5 = mem[2,3,2,3]
; SSE2-NEXT:    movq %xmm5, 24(%rdi)
; SSE2-NEXT:    movq {{[0-9]+}}(%rsp), %rax
; SSE2-NEXT:    movq %rax, 32(%rdi)
; SSE2-NEXT:    movq %xmm4, 48(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm4 = xmm4[2,3,2,3]
; SSE2-NEXT:    movq %xmm4, 56(%rdi)
; SSE2-NEXT:    movq %xmm3, 64(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; SSE2-NEXT:    movq %xmm3, 72(%rdi)
; SSE2-NEXT:    movq %xmm2, 80(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm2 = xmm2[2,3,2,3]
; SSE2-NEXT:    movq %xmm2, 88(%rdi)
; SSE2-NEXT:    movq %xmm1, 96(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm1 = xmm1[2,3,2,3]
; SSE2-NEXT:    movq %xmm1, 104(%rdi)
; SSE2-NEXT:    movq %xmm0, 112(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, 120(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: mstore_constmask_allones_split:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm0
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm1
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm2
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm3
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm4
; SSE4-NEXT:    movdqa {{[0-9]+}}(%rsp), %xmm5
; SSE4-NEXT:    movaps {{[0-9]+}}(%rsp), %xmm6
; SSE4-NEXT:    movups %xmm6, (%rdi)
; SSE4-NEXT:    palignr {{.*#+}} xmm5 = mem[8,9,10,11,12,13,14,15],xmm5[0,1,2,3,4,5,6,7]
; SSE4-NEXT:    movdqu %xmm5, 24(%rdi)
; SSE4-NEXT:    movups %xmm4, 48(%rdi)
; SSE4-NEXT:    movups %xmm3, 64(%rdi)
; SSE4-NEXT:    movups %xmm2, 80(%rdi)
; SSE4-NEXT:    movups %xmm1, 96(%rdi)
; SSE4-NEXT:    movups %xmm0, 112(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: mstore_constmask_allones_split:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovapd {{.*#+}} ymm0 = [18446744073709551615,0,18446744073709551615,18446744073709551615]
; AVX1-NEXT:    vmaskmovpd %ymm5, %ymm0, 32(%rdi)
; AVX1-NEXT:    vmovapd {{.*#+}} ymm0 = [18446744073709551615,18446744073709551615,0,18446744073709551615]
; AVX1-NEXT:    vmaskmovpd %ymm4, %ymm0, (%rdi)
; AVX1-NEXT:    vmovups %ymm7, 96(%rdi)
; AVX1-NEXT:    vmovups %ymm6, 64(%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: mstore_constmask_allones_split:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpmovsxbq {{.*#+}} ymm0 = [18446744073709551615,0,18446744073709551615,18446744073709551615]
; AVX2-NEXT:    vpmaskmovq %ymm5, %ymm0, 32(%rdi)
; AVX2-NEXT:    vpmovsxbq {{.*#+}} ymm0 = [18446744073709551615,18446744073709551615,0,18446744073709551615]
; AVX2-NEXT:    vpmaskmovq %ymm4, %ymm0, (%rdi)
; AVX2-NEXT:    vmovups %ymm7, 96(%rdi)
; AVX2-NEXT:    vmovups %ymm6, 64(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: mstore_constmask_allones_split:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movb $-37, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovdqu64 %zmm2, (%rdi) {%k1}
; AVX512F-NEXT:    vmovups %zmm3, 64(%rdi)
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mstore_constmask_allones_split:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $-37, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovdqu64 %zmm2, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vmovups %zmm3, 64(%rdi)
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mstore_constmask_allones_split:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $-37, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovdqu64 %zmm2, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vmovups %zmm3, 64(%rdi)
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mstore_constmask_allones_split:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $-37, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovdqu64 %zmm2, (%eax) {%k1}
; X86-AVX512-NEXT:    vmovups %zmm3, 64(%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %mask = icmp eq <16 x i64> %trigger, zeroinitializer
  call void @llvm.masked.store.v16i64.p0(<16 x i64> %val, ptr %addr, i32 4, <16 x i1><i1 true, i1 true, i1 false, i1 true, i1 true, i1 false, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>)
  ret void
}

;  When only one element of the mask is set, reduce to a scalar store.

define void @one_mask_bit_set1(ptr %addr, <4 x i32> %val) nounwind {
; SSE-LABEL: one_mask_bit_set1:
; SSE:       ## %bb.0:
; SSE-NEXT:    movss %xmm0, (%rdi)
; SSE-NEXT:    retq
;
; AVX-LABEL: one_mask_bit_set1:
; AVX:       ## %bb.0:
; AVX-NEXT:    vmovss %xmm0, (%rdi)
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set1:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovss %xmm0, (%eax)
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v4i32.p0(<4 x i32> %val, ptr %addr, i32 4, <4 x i1><i1 true, i1 false, i1 false, i1 false>)
  ret void
}

; Choose a different element to show that the correct address offset is produced.

define void @one_mask_bit_set2(ptr %addr, <4 x float> %val) nounwind {
; SSE2-LABEL: one_mask_bit_set2:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movhlps {{.*#+}} xmm0 = xmm0[1,1]
; SSE2-NEXT:    movss %xmm0, 8(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: one_mask_bit_set2:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    retq
;
; AVX-LABEL: one_mask_bit_set2:
; AVX:       ## %bb.0:
; AVX-NEXT:    vextractps $2, %xmm0, 8(%rdi)
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set2:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vextractps $2, %xmm0, 8(%eax)
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v4f32.p0(<4 x float> %val, ptr %addr, i32 4, <4 x i1><i1 false, i1 false, i1 true, i1 false>)
  ret void
}

; Choose a different scalar type and a high element of a 256-bit vector because AVX doesn't support those evenly.

define void @one_mask_bit_set3(ptr %addr, <4 x i64> %val) nounwind {
; SSE-LABEL: one_mask_bit_set3:
; SSE:       ## %bb.0:
; SSE-NEXT:    movlps %xmm1, 16(%rdi)
; SSE-NEXT:    retq
;
; AVX-LABEL: one_mask_bit_set3:
; AVX:       ## %bb.0:
; AVX-NEXT:    vextractf128 $1, %ymm0, %xmm0
; AVX-NEXT:    vmovlps %xmm0, 16(%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set3:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm0
; X86-AVX512-NEXT:    vmovlps %xmm0, 16(%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v4i64.p0(<4 x i64> %val, ptr %addr, i32 4, <4 x i1><i1 false, i1 false, i1 true, i1 false>)
  ret void
}

; Choose a different scalar type and a high element of a 256-bit vector because AVX doesn't support those evenly.

define void @one_mask_bit_set4(ptr %addr, <4 x double> %val) nounwind {
; SSE-LABEL: one_mask_bit_set4:
; SSE:       ## %bb.0:
; SSE-NEXT:    movhps %xmm1, 24(%rdi)
; SSE-NEXT:    retq
;
; AVX-LABEL: one_mask_bit_set4:
; AVX:       ## %bb.0:
; AVX-NEXT:    vextractf128 $1, %ymm0, %xmm0
; AVX-NEXT:    vmovhps %xmm0, 24(%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set4:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vextractf128 $1, %ymm0, %xmm0
; X86-AVX512-NEXT:    vmovhps %xmm0, 24(%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v4f64.p0(<4 x double> %val, ptr %addr, i32 4, <4 x i1><i1 false, i1 false, i1 false, i1 true>)
  ret void
}

; Try a 512-bit vector to make sure AVX doesn't die and AVX512 works as expected.

define void @one_mask_bit_set5(ptr %addr, <8 x double> %val) nounwind {
; SSE-LABEL: one_mask_bit_set5:
; SSE:       ## %bb.0:
; SSE-NEXT:    movlps %xmm3, 48(%rdi)
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: one_mask_bit_set5:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vextractf128 $1, %ymm1, %xmm0
; AVX1OR2-NEXT:    vmovlps %xmm0, 48(%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512-LABEL: one_mask_bit_set5:
; AVX512:       ## %bb.0:
; AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm0
; AVX512-NEXT:    vmovlps %xmm0, 48(%rdi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set5:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm0
; X86-AVX512-NEXT:    vmovlps %xmm0, 48(%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v8f64.p0(<8 x double> %val, ptr %addr, i32 4, <8 x i1><i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 true, i1 false>)
  ret void
}

; Try one elt in each half of a vector that needs to split
define void @one_mask_bit_set6(ptr %addr, <16 x i64> %val) nounwind {
; SSE2-LABEL: one_mask_bit_set6:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movlps %xmm3, 48(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm5[2,3,2,3]
; SSE2-NEXT:    movq %xmm0, 88(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: one_mask_bit_set6:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movlps %xmm3, 48(%rdi)
; SSE4-NEXT:    pextrq $1, %xmm5, 88(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: one_mask_bit_set6:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovapd {{.*#+}} ymm0 = [0,0,0,18446744073709551615]
; AVX1-NEXT:    vmaskmovpd %ymm2, %ymm0, 64(%rdi)
; AVX1-NEXT:    vmovapd {{.*#+}} ymm0 = [0,0,18446744073709551615,0]
; AVX1-NEXT:    vmaskmovpd %ymm1, %ymm0, 32(%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: one_mask_bit_set6:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpmovsxbq {{.*#+}} ymm0 = [0,0,0,18446744073709551615]
; AVX2-NEXT:    vpmaskmovq %ymm2, %ymm0, 64(%rdi)
; AVX2-NEXT:    vpmovsxbq {{.*#+}} ymm0 = [0,0,18446744073709551615,0]
; AVX2-NEXT:    vpmaskmovq %ymm1, %ymm0, 32(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: one_mask_bit_set6:
; AVX512:       ## %bb.0:
; AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm0
; AVX512-NEXT:    vmovlps %xmm0, 48(%rdi)
; AVX512-NEXT:    vextracti128 $1, %ymm1, %xmm0
; AVX512-NEXT:    vpextrq $1, %xmm0, 88(%rdi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set6:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vextractf32x4 $3, %zmm0, %xmm0
; X86-AVX512-NEXT:    vmovlps %xmm0, 48(%eax)
; X86-AVX512-NEXT:    vextractf128 $1, %ymm1, %xmm0
; X86-AVX512-NEXT:    vshufps {{.*#+}} xmm0 = xmm0[2,3,0,1]
; X86-AVX512-NEXT:    vmovlps %xmm0, 88(%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v16i64.p0(<16 x i64> %val, ptr %addr, i32 4, <16 x i1><i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @top_bits_unset_stack() nounwind {
; SSE-LABEL: top_bits_unset_stack:
; SSE:       ## %bb.0: ## %entry
; SSE-NEXT:    xorps %xmm0, %xmm0
; SSE-NEXT:    movaps %xmm0, -{{[0-9]+}}(%rsp)
; SSE-NEXT:    movaps %xmm0, -{{[0-9]+}}(%rsp)
; SSE-NEXT:    movaps %xmm0, -{{[0-9]+}}(%rsp)
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: top_bits_unset_stack:
; AVX1OR2:       ## %bb.0: ## %entry
; AVX1OR2-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; AVX1OR2-NEXT:    vmovapd {{.*#+}} ymm1 = [18446744073709551615,18446744073709551615,0,0]
; AVX1OR2-NEXT:    vmaskmovpd %ymm0, %ymm1, -{{[0-9]+}}(%rsp)
; AVX1OR2-NEXT:    vmovupd %ymm0, -{{[0-9]+}}(%rsp)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: top_bits_unset_stack:
; AVX512F:       ## %bb.0: ## %entry
; AVX512F-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; AVX512F-NEXT:    movb $63, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovupd %zmm0, -{{[0-9]+}}(%rsp) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: top_bits_unset_stack:
; AVX512VLDQ:       ## %bb.0: ## %entry
; AVX512VLDQ-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; AVX512VLDQ-NEXT:    movb $63, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovupd %zmm0, -{{[0-9]+}}(%rsp) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: top_bits_unset_stack:
; AVX512VLBW:       ## %bb.0: ## %entry
; AVX512VLBW-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; AVX512VLBW-NEXT:    movb $63, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovupd %zmm0, -{{[0-9]+}}(%rsp) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: top_bits_unset_stack:
; X86-AVX512:       ## %bb.0: ## %entry
; X86-AVX512-NEXT:    subl $76, %esp
; X86-AVX512-NEXT:    vxorpd %xmm0, %xmm0, %xmm0
; X86-AVX512-NEXT:    movb $63, %al
; X86-AVX512-NEXT:    kmovd %eax, %k1
; X86-AVX512-NEXT:    vmovupd %zmm0, (%esp) {%k1}
; X86-AVX512-NEXT:    addl $76, %esp
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
entry:
  %P.i150.i.i = alloca [3 x [3 x double]], align 16
  call void @llvm.masked.store.v8f64.p0(<8 x double> zeroinitializer, ptr %P.i150.i.i, i32 8, <8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false>)
  ret void
}


; SimplifyDemandedBits eliminates an ashr here.

define void @masked_store_bool_mask_demand_trunc_sext(<4 x double> %x, ptr %p, <4 x i32> %masksrc) nounwind {
; SSE-LABEL: masked_store_bool_mask_demand_trunc_sext:
; SSE:       ## %bb.0:
; SSE-NEXT:    pslld $31, %xmm2
; SSE-NEXT:    movmskps %xmm2, %eax
; SSE-NEXT:    testb $1, %al
; SSE-NEXT:    jne LBB33_1
; SSE-NEXT:  ## %bb.2: ## %else
; SSE-NEXT:    testb $2, %al
; SSE-NEXT:    jne LBB33_3
; SSE-NEXT:  LBB33_4: ## %else2
; SSE-NEXT:    testb $4, %al
; SSE-NEXT:    jne LBB33_5
; SSE-NEXT:  LBB33_6: ## %else4
; SSE-NEXT:    testb $8, %al
; SSE-NEXT:    jne LBB33_7
; SSE-NEXT:  LBB33_8: ## %else6
; SSE-NEXT:    retq
; SSE-NEXT:  LBB33_1: ## %cond.store
; SSE-NEXT:    movlps %xmm0, (%rdi)
; SSE-NEXT:    testb $2, %al
; SSE-NEXT:    je LBB33_4
; SSE-NEXT:  LBB33_3: ## %cond.store1
; SSE-NEXT:    movhps %xmm0, 8(%rdi)
; SSE-NEXT:    testb $4, %al
; SSE-NEXT:    je LBB33_6
; SSE-NEXT:  LBB33_5: ## %cond.store3
; SSE-NEXT:    movlps %xmm1, 16(%rdi)
; SSE-NEXT:    testb $8, %al
; SSE-NEXT:    je LBB33_8
; SSE-NEXT:  LBB33_7: ## %cond.store5
; SSE-NEXT:    movhps %xmm1, 24(%rdi)
; SSE-NEXT:    retq
;
; AVX1-LABEL: masked_store_bool_mask_demand_trunc_sext:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX1-NEXT:    vpmovsxdq %xmm1, %xmm2
; AVX1-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[2,3,2,3]
; AVX1-NEXT:    vpmovsxdq %xmm1, %xmm1
; AVX1-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX1-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rdi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: masked_store_bool_mask_demand_trunc_sext:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxdq %xmm1, %ymm1
; AVX2-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: masked_store_bool_mask_demand_trunc_sext:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512F-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovupd %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: masked_store_bool_mask_demand_trunc_sext:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512VLDQ-NEXT:    vpmovd2m %xmm1, %k1
; AVX512VLDQ-NEXT:    vmovupd %ymm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: masked_store_bool_mask_demand_trunc_sext:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512VLBW-NEXT:    vptestmd %xmm1, %xmm1, %k1
; AVX512VLBW-NEXT:    vmovupd %ymm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: masked_store_bool_mask_demand_trunc_sext:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; X86-AVX512-NEXT:    vpmovd2m %xmm1, %k1
; X86-AVX512-NEXT:    vmovupd %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %sext = sext <4 x i32> %masksrc to <4 x i64>
  %boolmask = trunc <4 x i64> %sext to <4 x i1>
  call void @llvm.masked.store.v4f64.p0(<4 x double> %x, ptr %p, i32 4, <4 x i1> %boolmask)
  ret void
}

; PR26697

define void @one_mask_bit_set1_variable(ptr %addr, <4 x float> %val, <4 x i32> %mask) nounwind {
; SSE2-LABEL: one_mask_bit_set1_variable:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movmskps %xmm1, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB34_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB34_3
; SSE2-NEXT:  LBB34_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB34_5
; SSE2-NEXT:  LBB34_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB34_7
; SSE2-NEXT:  LBB34_8: ## %else6
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB34_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB34_4
; SSE2-NEXT:  LBB34_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm1, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB34_6
; SSE2-NEXT:  LBB34_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm1
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm1 = xmm1[1],xmm0[1]
; SSE2-NEXT:    movss %xmm1, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB34_8
; SSE2-NEXT:  LBB34_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: one_mask_bit_set1_variable:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movmskps %xmm1, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB34_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB34_3
; SSE4-NEXT:  LBB34_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB34_5
; SSE4-NEXT:  LBB34_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB34_7
; SSE4-NEXT:  LBB34_8: ## %else6
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB34_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB34_4
; SSE4-NEXT:  LBB34_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB34_6
; SSE4-NEXT:  LBB34_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB34_8
; SSE4-NEXT:  LBB34_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: one_mask_bit_set1_variable:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: one_mask_bit_set1_variable:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    vptestmd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VL-LABEL: one_mask_bit_set1_variable:
; AVX512VL:       ## %bb.0:
; AVX512VL-NEXT:    vptestmd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to4}, %xmm1, %k1
; AVX512VL-NEXT:    vmovups %xmm0, (%rdi) {%k1}
; AVX512VL-NEXT:    retq
;
; X86-AVX512-LABEL: one_mask_bit_set1_variable:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vptestmd {{\.?LCPI[0-9]+_[0-9]+}}{1to4}, %xmm1, %k1
; X86-AVX512-NEXT:    vmovups %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %mask_signbit = and <4 x i32> %mask, <i32 2147483648, i32 2147483648, i32 2147483648, i32 2147483648>
  %mask_bool = icmp ne <4 x i32> %mask_signbit, zeroinitializer
  call void @llvm.masked.store.v4f32.p0(<4 x float> %val, ptr %addr, i32 1, <4 x i1> %mask_bool)
  ret void
}

; This needs to be widened to v4i32.
; This used to assert in type legalization. PR38436
; FIXME: The codegen for AVX512 should use KSHIFT to zero the upper bits of the mask.
define void @widen_masked_store(<3 x i32> %v, ptr %p, <3 x i1> %mask) nounwind {
; SSE2-LABEL: widen_masked_store:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    andb $1, %sil
; SSE2-NEXT:    andb $1, %dl
; SSE2-NEXT:    addb %dl, %dl
; SSE2-NEXT:    orb %sil, %dl
; SSE2-NEXT:    andb $1, %cl
; SSE2-NEXT:    shlb $2, %cl
; SSE2-NEXT:    orb %dl, %cl
; SSE2-NEXT:    testb $1, %cl
; SSE2-NEXT:    jne LBB35_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %cl
; SSE2-NEXT:    jne LBB35_3
; SSE2-NEXT:  LBB35_4: ## %else2
; SSE2-NEXT:    testb $4, %cl
; SSE2-NEXT:    jne LBB35_5
; SSE2-NEXT:  LBB35_6: ## %else4
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB35_1: ## %cond.store
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %cl
; SSE2-NEXT:    je LBB35_4
; SSE2-NEXT:  LBB35_3: ## %cond.store1
; SSE2-NEXT:    pshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; SSE2-NEXT:    movd %xmm1, 4(%rdi)
; SSE2-NEXT:    testb $4, %cl
; SSE2-NEXT:    je LBB35_6
; SSE2-NEXT:  LBB35_5: ## %cond.store3
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[2,3,2,3]
; SSE2-NEXT:    movd %xmm0, 8(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: widen_masked_store:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    andb $1, %sil
; SSE4-NEXT:    andb $1, %dl
; SSE4-NEXT:    addb %dl, %dl
; SSE4-NEXT:    orb %sil, %dl
; SSE4-NEXT:    andb $1, %cl
; SSE4-NEXT:    shlb $2, %cl
; SSE4-NEXT:    orb %dl, %cl
; SSE4-NEXT:    testb $1, %cl
; SSE4-NEXT:    jne LBB35_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %cl
; SSE4-NEXT:    jne LBB35_3
; SSE4-NEXT:  LBB35_4: ## %else2
; SSE4-NEXT:    testb $4, %cl
; SSE4-NEXT:    jne LBB35_5
; SSE4-NEXT:  LBB35_6: ## %else4
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB35_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %cl
; SSE4-NEXT:    je LBB35_4
; SSE4-NEXT:  LBB35_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %cl
; SSE4-NEXT:    je LBB35_6
; SSE4-NEXT:  LBB35_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: widen_masked_store:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovd %edx, %xmm1
; AVX1-NEXT:    vmovd %esi, %xmm2
; AVX1-NEXT:    vpunpckldq {{.*#+}} xmm1 = xmm2[0],xmm1[0],xmm2[1],xmm1[1]
; AVX1-NEXT:    vmovd %ecx, %xmm2
; AVX1-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm2[0]
; AVX1-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX1-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX1-NEXT:    retq
;
; AVX2-LABEL: widen_masked_store:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovd %edx, %xmm1
; AVX2-NEXT:    vmovd %esi, %xmm2
; AVX2-NEXT:    vpunpckldq {{.*#+}} xmm1 = xmm2[0],xmm1[0],xmm2[1],xmm1[1]
; AVX2-NEXT:    vmovd %ecx, %xmm2
; AVX2-NEXT:    vpunpcklqdq {{.*#+}} xmm1 = xmm1[0],xmm2[0]
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmaskmovd %xmm0, %xmm1, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: widen_masked_store:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512F-NEXT:    andl $1, %esi
; AVX512F-NEXT:    kmovw %esi, %k0
; AVX512F-NEXT:    kmovw %edx, %k1
; AVX512F-NEXT:    kshiftlw $15, %k1, %k1
; AVX512F-NEXT:    kshiftrw $14, %k1, %k1
; AVX512F-NEXT:    korw %k1, %k0, %k0
; AVX512F-NEXT:    movw $-5, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    kandw %k1, %k0, %k0
; AVX512F-NEXT:    kmovw %ecx, %k1
; AVX512F-NEXT:    kshiftlw $15, %k1, %k1
; AVX512F-NEXT:    kshiftrw $13, %k1, %k1
; AVX512F-NEXT:    korw %k1, %k0, %k0
; AVX512F-NEXT:    movb $7, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    kandw %k1, %k0, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovdqu32 %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: widen_masked_store:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    kmovw %edx, %k0
; AVX512VLDQ-NEXT:    kshiftlb $7, %k0, %k0
; AVX512VLDQ-NEXT:    kshiftrb $6, %k0, %k0
; AVX512VLDQ-NEXT:    kmovw %esi, %k1
; AVX512VLDQ-NEXT:    kshiftlb $7, %k1, %k1
; AVX512VLDQ-NEXT:    kshiftrb $7, %k1, %k1
; AVX512VLDQ-NEXT:    korw %k0, %k1, %k0
; AVX512VLDQ-NEXT:    movb $-5, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    kandw %k1, %k0, %k0
; AVX512VLDQ-NEXT:    kmovw %ecx, %k1
; AVX512VLDQ-NEXT:    kshiftlb $7, %k1, %k1
; AVX512VLDQ-NEXT:    kshiftrb $5, %k1, %k1
; AVX512VLDQ-NEXT:    korw %k1, %k0, %k0
; AVX512VLDQ-NEXT:    movb $7, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    kandw %k1, %k0, %k1
; AVX512VLDQ-NEXT:    vmovdqa32 %xmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: widen_masked_store:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    andl $1, %esi
; AVX512VLBW-NEXT:    kmovw %esi, %k0
; AVX512VLBW-NEXT:    kmovd %edx, %k1
; AVX512VLBW-NEXT:    kshiftlw $15, %k1, %k1
; AVX512VLBW-NEXT:    kshiftrw $14, %k1, %k1
; AVX512VLBW-NEXT:    korw %k1, %k0, %k0
; AVX512VLBW-NEXT:    movw $-5, %ax
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    kandw %k1, %k0, %k0
; AVX512VLBW-NEXT:    kmovd %ecx, %k1
; AVX512VLBW-NEXT:    kshiftlw $15, %k1, %k1
; AVX512VLBW-NEXT:    kshiftrw $13, %k1, %k1
; AVX512VLBW-NEXT:    korw %k1, %k0, %k0
; AVX512VLBW-NEXT:    movb $7, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    kandw %k1, %k0, %k1
; AVX512VLBW-NEXT:    vmovdqa32 %xmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: widen_masked_store:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k0
; X86-AVX512-NEXT:    kshiftlb $7, %k0, %k0
; X86-AVX512-NEXT:    kshiftrb $6, %k0, %k0
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    kshiftlb $7, %k1, %k1
; X86-AVX512-NEXT:    kshiftrb $7, %k1, %k1
; X86-AVX512-NEXT:    korw %k0, %k1, %k0
; X86-AVX512-NEXT:    movb $-5, %al
; X86-AVX512-NEXT:    kmovd %eax, %k1
; X86-AVX512-NEXT:    kandw %k1, %k0, %k0
; X86-AVX512-NEXT:    kmovb {{[0-9]+}}(%esp), %k1
; X86-AVX512-NEXT:    kshiftlb $7, %k1, %k1
; X86-AVX512-NEXT:    kshiftrb $5, %k1, %k1
; X86-AVX512-NEXT:    korw %k1, %k0, %k0
; X86-AVX512-NEXT:    movb $7, %al
; X86-AVX512-NEXT:    kmovd %eax, %k1
; X86-AVX512-NEXT:    kandw %k1, %k0, %k1
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovdqa32 %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v3i32.p0(<3 x i32> %v, ptr %p, i32 16, <3 x i1> %mask)
  ret void
}

define void @zero_mask(ptr %addr, <2 x double> %val) nounwind {
; SSE-LABEL: zero_mask:
; SSE:       ## %bb.0:
; SSE-NEXT:    retq
;
; AVX-LABEL: zero_mask:
; AVX:       ## %bb.0:
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: zero_mask:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    retl
  call void @llvm.masked.store.v2f64.p0(<2 x double> %val, ptr %addr, i32 4, <2 x i1> zeroinitializer)
  ret void
}

define void @PR11210(<4 x float> %x, ptr %ptr, <4 x float> %y, <2 x i64> %mask) nounwind {
; SSE2-LABEL: PR11210:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movmskps %xmm2, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB37_1
; SSE2-NEXT:  ## %bb.2: ## %else
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB37_3
; SSE2-NEXT:  LBB37_4: ## %else2
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB37_5
; SSE2-NEXT:  LBB37_6: ## %else4
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB37_7
; SSE2-NEXT:  LBB37_8: ## %else6
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB37_9
; SSE2-NEXT:  LBB37_10: ## %else9
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB37_11
; SSE2-NEXT:  LBB37_12: ## %else11
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB37_13
; SSE2-NEXT:  LBB37_14: ## %else13
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB37_15
; SSE2-NEXT:  LBB37_16: ## %else15
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB37_1: ## %cond.store
; SSE2-NEXT:    movss %xmm0, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB37_4
; SSE2-NEXT:  LBB37_3: ## %cond.store1
; SSE2-NEXT:    movaps %xmm0, %xmm2
; SSE2-NEXT:    shufps {{.*#+}} xmm2 = xmm2[1,1],xmm0[1,1]
; SSE2-NEXT:    movss %xmm2, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB37_6
; SSE2-NEXT:  LBB37_5: ## %cond.store3
; SSE2-NEXT:    movaps %xmm0, %xmm2
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm2 = xmm2[1],xmm0[1]
; SSE2-NEXT:    movss %xmm2, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB37_8
; SSE2-NEXT:  LBB37_7: ## %cond.store5
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movss %xmm0, 12(%rdi)
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    je LBB37_10
; SSE2-NEXT:  LBB37_9: ## %cond.store8
; SSE2-NEXT:    movss %xmm1, (%rdi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB37_12
; SSE2-NEXT:  LBB37_11: ## %cond.store10
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    shufps {{.*#+}} xmm0 = xmm0[1,1],xmm1[1,1]
; SSE2-NEXT:    movss %xmm0, 4(%rdi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB37_14
; SSE2-NEXT:  LBB37_13: ## %cond.store12
; SSE2-NEXT:    movaps %xmm1, %xmm0
; SSE2-NEXT:    unpckhpd {{.*#+}} xmm0 = xmm0[1],xmm1[1]
; SSE2-NEXT:    movss %xmm0, 8(%rdi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB37_16
; SSE2-NEXT:  LBB37_15: ## %cond.store14
; SSE2-NEXT:    shufps {{.*#+}} xmm1 = xmm1[3,3,3,3]
; SSE2-NEXT:    movss %xmm1, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: PR11210:
; SSE4:       ## %bb.0:
; SSE4-NEXT:    movmskps %xmm2, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB37_1
; SSE4-NEXT:  ## %bb.2: ## %else
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB37_3
; SSE4-NEXT:  LBB37_4: ## %else2
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB37_5
; SSE4-NEXT:  LBB37_6: ## %else4
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB37_7
; SSE4-NEXT:  LBB37_8: ## %else6
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB37_9
; SSE4-NEXT:  LBB37_10: ## %else9
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB37_11
; SSE4-NEXT:  LBB37_12: ## %else11
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB37_13
; SSE4-NEXT:  LBB37_14: ## %else13
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB37_15
; SSE4-NEXT:  LBB37_16: ## %else15
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB37_1: ## %cond.store
; SSE4-NEXT:    movss %xmm0, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB37_4
; SSE4-NEXT:  LBB37_3: ## %cond.store1
; SSE4-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB37_6
; SSE4-NEXT:  LBB37_5: ## %cond.store3
; SSE4-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB37_8
; SSE4-NEXT:  LBB37_7: ## %cond.store5
; SSE4-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    je LBB37_10
; SSE4-NEXT:  LBB37_9: ## %cond.store8
; SSE4-NEXT:    movss %xmm1, (%rdi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB37_12
; SSE4-NEXT:  LBB37_11: ## %cond.store10
; SSE4-NEXT:    extractps $1, %xmm1, 4(%rdi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB37_14
; SSE4-NEXT:  LBB37_13: ## %cond.store12
; SSE4-NEXT:    extractps $2, %xmm1, 8(%rdi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB37_16
; SSE4-NEXT:  LBB37_15: ## %cond.store14
; SSE4-NEXT:    extractps $3, %xmm1, 12(%rdi)
; SSE4-NEXT:    retq
;
; AVX1OR2-LABEL: PR11210:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmaskmovps %xmm1, %xmm2, (%rdi)
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: PR11210:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $xmm2 killed $xmm2 def $zmm2
; AVX512F-NEXT:    ## kill: def $xmm1 killed $xmm1 def $zmm1
; AVX512F-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512F-NEXT:    vpcmpgtd %zmm2, %zmm0, %k0
; AVX512F-NEXT:    kshiftlw $12, %k0, %k0
; AVX512F-NEXT:    kshiftrw $12, %k0, %k1
; AVX512F-NEXT:    vmovups %zmm1, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: PR11210:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovd2m %xmm2, %k1
; AVX512VLDQ-NEXT:    vmovups %xmm1, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: PR11210:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512VLBW-NEXT:    vpcmpgtd %xmm2, %xmm0, %k1
; AVX512VLBW-NEXT:    vmovups %xmm1, (%rdi) {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: PR11210:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vpmovd2m %xmm2, %k1
; X86-AVX512-NEXT:    vmovups %xmm1, (%eax) {%k1}
; X86-AVX512-NEXT:    retl
  %bc = bitcast <2 x i64> %mask to <4 x i32>
  %trunc = icmp slt <4 x i32> %bc, zeroinitializer
  call void @llvm.masked.store.v4f32.p0(<4 x float> %x, ptr %ptr, i32 1, <4 x i1> %trunc)
  call void @llvm.masked.store.v4f32.p0(<4 x float> %y, ptr %ptr, i32 1, <4 x i1> %trunc)
  ret void
}

define void @store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts(ptr %trigger.ptr, ptr %val.ptr, ptr %dst) nounwind {
; SSE-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; SSE:       ## %bb.0:
; SSE-NEXT:    pushq %rbp
; SSE-NEXT:    pushq %r15
; SSE-NEXT:    pushq %r14
; SSE-NEXT:    pushq %r13
; SSE-NEXT:    pushq %r12
; SSE-NEXT:    pushq %rbx
; SSE-NEXT:    movdqa (%rdi), %xmm1
; SSE-NEXT:    movdqa 32(%rdi), %xmm2
; SSE-NEXT:    movdqa 64(%rdi), %xmm0
; SSE-NEXT:    movl 92(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 88(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 84(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 80(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 76(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 72(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 68(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 64(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 60(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 56(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    movl 52(%rsi), %eax
; SSE-NEXT:    movl %eax, {{[-0-9]+}}(%r{{[sb]}}p) ## 4-byte Spill
; SSE-NEXT:    packssdw 48(%rdi), %xmm2
; SSE-NEXT:    packssdw 16(%rdi), %xmm1
; SSE-NEXT:    packsswb %xmm2, %xmm1
; SSE-NEXT:    packssdw 80(%rdi), %xmm0
; SSE-NEXT:    packsswb %xmm0, %xmm0
; SSE-NEXT:    pmovmskb %xmm1, %eax
; SSE-NEXT:    andl $21845, %eax ## imm = 0x5555
; SSE-NEXT:    pmovmskb %xmm0, %edi
; SSE-NEXT:    andl $85, %edi
; SSE-NEXT:    shll $16, %edi
; SSE-NEXT:    orl %eax, %edi
; SSE-NEXT:    movl 48(%rsi), %r13d
; SSE-NEXT:    testb $1, %dil
; SSE-NEXT:    movl 44(%rsi), %eax
; SSE-NEXT:    movl 40(%rsi), %ecx
; SSE-NEXT:    movl 36(%rsi), %r8d
; SSE-NEXT:    movl 32(%rsi), %r9d
; SSE-NEXT:    movl 28(%rsi), %r10d
; SSE-NEXT:    movl 24(%rsi), %r11d
; SSE-NEXT:    movl 20(%rsi), %ebx
; SSE-NEXT:    movl 16(%rsi), %ebp
; SSE-NEXT:    movl 12(%rsi), %r14d
; SSE-NEXT:    movl 8(%rsi), %r15d
; SSE-NEXT:    movl 4(%rsi), %r12d
; SSE-NEXT:    jne LBB38_1
; SSE-NEXT:  ## %bb.2: ## %else
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    jne LBB38_3
; SSE-NEXT:  LBB38_4: ## %else2
; SSE-NEXT:    testb $4, %dil
; SSE-NEXT:    jne LBB38_5
; SSE-NEXT:  LBB38_6: ## %else4
; SSE-NEXT:    testb $8, %dil
; SSE-NEXT:    jne LBB38_7
; SSE-NEXT:  LBB38_8: ## %else6
; SSE-NEXT:    testb $16, %dil
; SSE-NEXT:    jne LBB38_9
; SSE-NEXT:  LBB38_10: ## %else8
; SSE-NEXT:    testb $32, %dil
; SSE-NEXT:    jne LBB38_11
; SSE-NEXT:  LBB38_12: ## %else10
; SSE-NEXT:    testb $64, %dil
; SSE-NEXT:    jne LBB38_13
; SSE-NEXT:  LBB38_14: ## %else12
; SSE-NEXT:    testb %dil, %dil
; SSE-NEXT:    js LBB38_15
; SSE-NEXT:  LBB38_16: ## %else14
; SSE-NEXT:    testl $256, %edi ## imm = 0x100
; SSE-NEXT:    jne LBB38_17
; SSE-NEXT:  LBB38_18: ## %else16
; SSE-NEXT:    testl $512, %edi ## imm = 0x200
; SSE-NEXT:    jne LBB38_19
; SSE-NEXT:  LBB38_20: ## %else18
; SSE-NEXT:    testl $1024, %edi ## imm = 0x400
; SSE-NEXT:    jne LBB38_21
; SSE-NEXT:  LBB38_22: ## %else20
; SSE-NEXT:    testl $2048, %edi ## imm = 0x800
; SSE-NEXT:    jne LBB38_23
; SSE-NEXT:  LBB38_24: ## %else22
; SSE-NEXT:    testl $4096, %edi ## imm = 0x1000
; SSE-NEXT:    jne LBB38_25
; SSE-NEXT:  LBB38_26: ## %else24
; SSE-NEXT:    testl $8192, %edi ## imm = 0x2000
; SSE-NEXT:    jne LBB38_27
; SSE-NEXT:  LBB38_28: ## %else26
; SSE-NEXT:    testl $16384, %edi ## imm = 0x4000
; SSE-NEXT:    jne LBB38_29
; SSE-NEXT:  LBB38_30: ## %else28
; SSE-NEXT:    testw %di, %di
; SSE-NEXT:    js LBB38_31
; SSE-NEXT:  LBB38_32: ## %else30
; SSE-NEXT:    testl $65536, %edi ## imm = 0x10000
; SSE-NEXT:    jne LBB38_33
; SSE-NEXT:  LBB38_34: ## %else32
; SSE-NEXT:    testl $131072, %edi ## imm = 0x20000
; SSE-NEXT:    jne LBB38_35
; SSE-NEXT:  LBB38_36: ## %else34
; SSE-NEXT:    testl $262144, %edi ## imm = 0x40000
; SSE-NEXT:    jne LBB38_37
; SSE-NEXT:  LBB38_38: ## %else36
; SSE-NEXT:    testl $524288, %edi ## imm = 0x80000
; SSE-NEXT:    jne LBB38_39
; SSE-NEXT:  LBB38_40: ## %else38
; SSE-NEXT:    testl $1048576, %edi ## imm = 0x100000
; SSE-NEXT:    jne LBB38_41
; SSE-NEXT:  LBB38_42: ## %else40
; SSE-NEXT:    testl $2097152, %edi ## imm = 0x200000
; SSE-NEXT:    jne LBB38_43
; SSE-NEXT:  LBB38_44: ## %else42
; SSE-NEXT:    testl $4194304, %edi ## imm = 0x400000
; SSE-NEXT:    je LBB38_46
; SSE-NEXT:  LBB38_45: ## %cond.store43
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 88(%rdx)
; SSE-NEXT:  LBB38_46: ## %else44
; SSE-NEXT:    movb $1, %al
; SSE-NEXT:    testb %al, %al
; SSE-NEXT:    jne LBB38_48
; SSE-NEXT:  ## %bb.47: ## %cond.store45
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 92(%rdx)
; SSE-NEXT:  LBB38_48: ## %else46
; SSE-NEXT:    popq %rbx
; SSE-NEXT:    popq %r12
; SSE-NEXT:    popq %r13
; SSE-NEXT:    popq %r14
; SSE-NEXT:    popq %r15
; SSE-NEXT:    popq %rbp
; SSE-NEXT:    retq
; SSE-NEXT:  LBB38_1: ## %cond.store
; SSE-NEXT:    movl (%rsi), %esi
; SSE-NEXT:    movl %esi, (%rdx)
; SSE-NEXT:    testb $2, %dil
; SSE-NEXT:    je LBB38_4
; SSE-NEXT:  LBB38_3: ## %cond.store1
; SSE-NEXT:    movl %r12d, 4(%rdx)
; SSE-NEXT:    testb $4, %dil
; SSE-NEXT:    je LBB38_6
; SSE-NEXT:  LBB38_5: ## %cond.store3
; SSE-NEXT:    movl %r15d, 8(%rdx)
; SSE-NEXT:    testb $8, %dil
; SSE-NEXT:    je LBB38_8
; SSE-NEXT:  LBB38_7: ## %cond.store5
; SSE-NEXT:    movl %r14d, 12(%rdx)
; SSE-NEXT:    testb $16, %dil
; SSE-NEXT:    je LBB38_10
; SSE-NEXT:  LBB38_9: ## %cond.store7
; SSE-NEXT:    movl %ebp, 16(%rdx)
; SSE-NEXT:    testb $32, %dil
; SSE-NEXT:    je LBB38_12
; SSE-NEXT:  LBB38_11: ## %cond.store9
; SSE-NEXT:    movl %ebx, 20(%rdx)
; SSE-NEXT:    testb $64, %dil
; SSE-NEXT:    je LBB38_14
; SSE-NEXT:  LBB38_13: ## %cond.store11
; SSE-NEXT:    movl %r11d, 24(%rdx)
; SSE-NEXT:    testb %dil, %dil
; SSE-NEXT:    jns LBB38_16
; SSE-NEXT:  LBB38_15: ## %cond.store13
; SSE-NEXT:    movl %r10d, 28(%rdx)
; SSE-NEXT:    testl $256, %edi ## imm = 0x100
; SSE-NEXT:    je LBB38_18
; SSE-NEXT:  LBB38_17: ## %cond.store15
; SSE-NEXT:    movl %r9d, 32(%rdx)
; SSE-NEXT:    testl $512, %edi ## imm = 0x200
; SSE-NEXT:    je LBB38_20
; SSE-NEXT:  LBB38_19: ## %cond.store17
; SSE-NEXT:    movl %r8d, 36(%rdx)
; SSE-NEXT:    testl $1024, %edi ## imm = 0x400
; SSE-NEXT:    je LBB38_22
; SSE-NEXT:  LBB38_21: ## %cond.store19
; SSE-NEXT:    movl %ecx, 40(%rdx)
; SSE-NEXT:    testl $2048, %edi ## imm = 0x800
; SSE-NEXT:    je LBB38_24
; SSE-NEXT:  LBB38_23: ## %cond.store21
; SSE-NEXT:    movl %eax, 44(%rdx)
; SSE-NEXT:    testl $4096, %edi ## imm = 0x1000
; SSE-NEXT:    je LBB38_26
; SSE-NEXT:  LBB38_25: ## %cond.store23
; SSE-NEXT:    movl %r13d, 48(%rdx)
; SSE-NEXT:    testl $8192, %edi ## imm = 0x2000
; SSE-NEXT:    je LBB38_28
; SSE-NEXT:  LBB38_27: ## %cond.store25
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 52(%rdx)
; SSE-NEXT:    testl $16384, %edi ## imm = 0x4000
; SSE-NEXT:    je LBB38_30
; SSE-NEXT:  LBB38_29: ## %cond.store27
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 56(%rdx)
; SSE-NEXT:    testw %di, %di
; SSE-NEXT:    jns LBB38_32
; SSE-NEXT:  LBB38_31: ## %cond.store29
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 60(%rdx)
; SSE-NEXT:    testl $65536, %edi ## imm = 0x10000
; SSE-NEXT:    je LBB38_34
; SSE-NEXT:  LBB38_33: ## %cond.store31
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 64(%rdx)
; SSE-NEXT:    testl $131072, %edi ## imm = 0x20000
; SSE-NEXT:    je LBB38_36
; SSE-NEXT:  LBB38_35: ## %cond.store33
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 68(%rdx)
; SSE-NEXT:    testl $262144, %edi ## imm = 0x40000
; SSE-NEXT:    je LBB38_38
; SSE-NEXT:  LBB38_37: ## %cond.store35
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 72(%rdx)
; SSE-NEXT:    testl $524288, %edi ## imm = 0x80000
; SSE-NEXT:    je LBB38_40
; SSE-NEXT:  LBB38_39: ## %cond.store37
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 76(%rdx)
; SSE-NEXT:    testl $1048576, %edi ## imm = 0x100000
; SSE-NEXT:    je LBB38_42
; SSE-NEXT:  LBB38_41: ## %cond.store39
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 80(%rdx)
; SSE-NEXT:    testl $2097152, %edi ## imm = 0x200000
; SSE-NEXT:    je LBB38_44
; SSE-NEXT:  LBB38_43: ## %cond.store41
; SSE-NEXT:    movl {{[-0-9]+}}(%r{{[sb]}}p), %eax ## 4-byte Reload
; SSE-NEXT:    movl %eax, 84(%rdx)
; SSE-NEXT:    testl $4194304, %edi ## imm = 0x400000
; SSE-NEXT:    jne LBB38_45
; SSE-NEXT:    jmp LBB38_46
;
; AVX1-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vmovaps (%rsi), %ymm1
; AVX1-NEXT:    vmovaps 32(%rsi), %ymm2
; AVX1-NEXT:    vmovdqa 64(%rsi), %ymm0
; AVX1-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX1-NEXT:    vpcmpgtd 48(%rdi), %xmm3, %xmm4
; AVX1-NEXT:    vpcmpgtd 32(%rdi), %xmm3, %xmm5
; AVX1-NEXT:    vpackssdw %xmm4, %xmm5, %xmm4
; AVX1-NEXT:    vpacksswb %xmm4, %xmm4, %xmm4
; AVX1-NEXT:    vpcmpgtd 80(%rdi), %xmm3, %xmm5
; AVX1-NEXT:    vpcmpgtd 64(%rdi), %xmm3, %xmm6
; AVX1-NEXT:    vpcmpgtd 16(%rdi), %xmm3, %xmm7
; AVX1-NEXT:    vpcmpgtd (%rdi), %xmm3, %xmm8
; AVX1-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm4[4],xmm3[4],xmm4[5],xmm3[5],xmm4[6],xmm3[6],xmm4[7],xmm3[7]
; AVX1-NEXT:    vpmovzxwd {{.*#+}} xmm4 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero
; AVX1-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX1-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm3[4,4,5,5,6,6,7,7]
; AVX1-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX1-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX1-NEXT:    vmaskmovps %ymm2, %ymm3, 32(%rdx)
; AVX1-NEXT:    vinsertf128 $1, %xmm7, %ymm8, %ymm2
; AVX1-NEXT:    vxorps %xmm3, %xmm3, %xmm3
; AVX1-NEXT:    vblendps {{.*#+}} ymm2 = ymm2[0],ymm3[1],ymm2[2],ymm3[3],ymm2[4],ymm3[5],ymm2[6],ymm3[7]
; AVX1-NEXT:    vpslld $31, %xmm2, %xmm4
; AVX1-NEXT:    vextractf128 $1, %ymm2, %xmm2
; AVX1-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX1-NEXT:    vinsertf128 $1, %xmm2, %ymm4, %ymm2
; AVX1-NEXT:    vmaskmovps %ymm1, %ymm2, (%rdx)
; AVX1-NEXT:    vinsertf128 $1, %xmm5, %ymm6, %ymm1
; AVX1-NEXT:    vblendps {{.*#+}} ymm1 = ymm1[0],ymm3[1],ymm1[2],ymm3[3],ymm1[4],ymm3[5],ymm1[6],ymm3[7]
; AVX1-NEXT:    vpslld $31, %xmm1, %xmm2
; AVX1-NEXT:    vextractf128 $1, %ymm1, %xmm1
; AVX1-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX1-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX1-NEXT:    vmaskmovps %ymm0, %ymm1, 64(%rdx)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    vmovdqa (%rsi), %ymm0
; AVX2-NEXT:    vmovdqa 32(%rsi), %ymm1
; AVX2-NEXT:    vmovdqa 64(%rsi), %ymm2
; AVX2-NEXT:    vpxor %xmm3, %xmm3, %xmm3
; AVX2-NEXT:    vpcmpgtd 32(%rdi), %ymm3, %ymm4
; AVX2-NEXT:    vpcmpgtd (%rdi), %ymm3, %ymm5
; AVX2-NEXT:    vpackssdw %ymm4, %ymm5, %ymm4
; AVX2-NEXT:    vpermq {{.*#+}} ymm4 = ymm4[0,2,1,3]
; AVX2-NEXT:    vpshufd {{.*#+}} ymm5 = mem[0,2,2,3,4,6,6,7]
; AVX2-NEXT:    vpermq {{.*#+}} ymm5 = ymm5[0,2,2,3]
; AVX2-NEXT:    vpcmpgtd %ymm5, %ymm3, %ymm3
; AVX2-NEXT:    vpacksswb %ymm3, %ymm4, %ymm3
; AVX2-NEXT:    vpermq {{.*#+}} ymm4 = ymm3[0,2,1,3]
; AVX2-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm4, %ymm4
; AVX2-NEXT:    vpmovzxwq {{.*#+}} ymm3 = xmm3[0],zero,zero,zero,xmm3[1],zero,zero,zero,xmm3[2],zero,zero,zero,xmm3[3],zero,zero,zero
; AVX2-NEXT:    vpslld $31, %ymm3, %ymm3
; AVX2-NEXT:    vpmaskmovd %ymm0, %ymm3, (%rdx)
; AVX2-NEXT:    vextracti128 $1, %ymm4, %xmm0
; AVX2-NEXT:    vpmovzxbd {{.*#+}} ymm0 = xmm0[0],zero,zero,zero,xmm0[1],zero,zero,zero,xmm0[2],zero,zero,zero,xmm0[3],zero,zero,zero,xmm0[4],zero,zero,zero,xmm0[5],zero,zero,zero,xmm0[6],zero,zero,zero,xmm0[7],zero,zero,zero
; AVX2-NEXT:    vpslld $31, %ymm0, %ymm0
; AVX2-NEXT:    vpmaskmovd %ymm2, %ymm0, 64(%rdx)
; AVX2-NEXT:    vpunpckhbw {{.*#+}} xmm0 = xmm4[8,8,9,9,10,10,11,11,12,12,13,13,14,14,15,15]
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero,xmm0[4],zero,xmm0[5],zero,xmm0[6],zero,xmm0[7],zero
; AVX2-NEXT:    vpslld $31, %ymm0, %ymm0
; AVX2-NEXT:    vpmaskmovd %ymm1, %ymm0, 32(%rdx)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vmovdqa64 (%rsi), %zmm0
; AVX512F-NEXT:    vmovdqa64 64(%rsi), %zmm1
; AVX512F-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512F-NEXT:    movw $21845, %ax ## imm = 0x5555
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vpcmpgtd (%rdi), %zmm2, %k1 {%k1}
; AVX512F-NEXT:    movw $85, %ax
; AVX512F-NEXT:    kmovw %eax, %k2
; AVX512F-NEXT:    vpcmpgtd 64(%rdi), %zmm2, %k2 {%k2}
; AVX512F-NEXT:    vmovdqu32 %zmm1, 64(%rdx) {%k2}
; AVX512F-NEXT:    vmovdqu32 %zmm0, (%rdx) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vmovdqa64 (%rsi), %zmm0
; AVX512VLDQ-NEXT:    vmovdqa64 64(%rsi), %zmm1
; AVX512VLDQ-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLDQ-NEXT:    movw $21845, %ax ## imm = 0x5555
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vpcmpgtd (%rdi), %zmm2, %k1 {%k1}
; AVX512VLDQ-NEXT:    movw $85, %ax
; AVX512VLDQ-NEXT:    kmovw %eax, %k2
; AVX512VLDQ-NEXT:    vpcmpgtd 64(%rdi), %zmm2, %k2 {%k2}
; AVX512VLDQ-NEXT:    vmovdqu32 %zmm1, 64(%rdx) {%k2}
; AVX512VLDQ-NEXT:    vmovdqu32 %zmm0, (%rdx) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vmovdqa64 (%rsi), %zmm0
; AVX512VLBW-NEXT:    vmovdqa64 64(%rsi), %zmm1
; AVX512VLBW-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; AVX512VLBW-NEXT:    movw $21845, %ax ## imm = 0x5555
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vpcmpgtd (%rdi), %zmm2, %k1 {%k1}
; AVX512VLBW-NEXT:    movw $85, %ax
; AVX512VLBW-NEXT:    kmovd %eax, %k2
; AVX512VLBW-NEXT:    vpcmpgtd 64(%rdi), %zmm2, %k2 {%k2}
; AVX512VLBW-NEXT:    vmovdqu32 %zmm1, 64(%rdx) {%k2}
; AVX512VLBW-NEXT:    vmovdqu32 %zmm0, (%rdx) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: store_v24i32_v24i32_stride6_vf4_only_even_numbered_elts:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %ecx
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %edx
; X86-AVX512-NEXT:    vmovdqa64 (%edx), %zmm0
; X86-AVX512-NEXT:    vmovdqa64 64(%edx), %zmm1
; X86-AVX512-NEXT:    vpxor %xmm2, %xmm2, %xmm2
; X86-AVX512-NEXT:    movw $21845, %dx ## imm = 0x5555
; X86-AVX512-NEXT:    kmovd %edx, %k1
; X86-AVX512-NEXT:    vpcmpgtd (%ecx), %zmm2, %k1 {%k1}
; X86-AVX512-NEXT:    movw $85, %dx
; X86-AVX512-NEXT:    kmovd %edx, %k2
; X86-AVX512-NEXT:    vpcmpgtd 64(%ecx), %zmm2, %k2 {%k2}
; X86-AVX512-NEXT:    vmovdqu32 %zmm1, 64(%eax) {%k2}
; X86-AVX512-NEXT:    vmovdqu32 %zmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %trigger = load <24 x i32>, ptr %trigger.ptr
  %val = load <24 x i32>, ptr %val.ptr
  %mask.src = icmp slt <24 x i32> %trigger, zeroinitializer
  %mask = and <24 x i1> %mask.src, <i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false, i1 true, i1 false>
  call void @llvm.masked.store.v24i32.p0(<24 x i32> %val, ptr %dst, i32 1, <24 x i1> %mask)
  ret void
}

; From https://reviews.llvm.org/rGf8d9097168b7#1165311
define void @undefshuffle(<8 x i1> %i0, ptr %src, ptr %dst) nounwind {
; SSE2-LABEL: undefshuffle:
; SSE2:       ## %bb.0: ## %else
; SSE2-NEXT:    movaps %xmm0, -{{[0-9]+}}(%rsp)
; SSE2-NEXT:    movzbl -{{[0-9]+}}(%rsp), %eax
; SSE2-NEXT:    movd %eax, %xmm0
; SSE2-NEXT:    movzbl -{{[0-9]+}}(%rsp), %eax
; SSE2-NEXT:    pinsrw $1, %eax, %xmm0
; SSE2-NEXT:    pinsrw $2, -{{[0-9]+}}(%rsp), %xmm0
; SSE2-NEXT:    movzbl -{{[0-9]+}}(%rsp), %eax
; SSE2-NEXT:    pinsrw $3, %eax, %xmm0
; SSE2-NEXT:    psllw $15, %xmm0
; SSE2-NEXT:    packsswb %xmm0, %xmm0
; SSE2-NEXT:    pmovmskb %xmm0, %eax
; SSE2-NEXT:    testb $1, %al
; SSE2-NEXT:    jne LBB39_1
; SSE2-NEXT:  ## %bb.2: ## %else23
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    jne LBB39_3
; SSE2-NEXT:  LBB39_4: ## %else25
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    jne LBB39_5
; SSE2-NEXT:  LBB39_6: ## %else27
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    jne LBB39_7
; SSE2-NEXT:  LBB39_8: ## %else29
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    jne LBB39_9
; SSE2-NEXT:  LBB39_10: ## %else31
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    jne LBB39_11
; SSE2-NEXT:  LBB39_12: ## %else33
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    jne LBB39_13
; SSE2-NEXT:  LBB39_14: ## %else35
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    jne LBB39_15
; SSE2-NEXT:  LBB39_16: ## %else37
; SSE2-NEXT:    retq
; SSE2-NEXT:  LBB39_1: ## %cond.store
; SSE2-NEXT:    movl $0, (%rsi)
; SSE2-NEXT:    testb $2, %al
; SSE2-NEXT:    je LBB39_4
; SSE2-NEXT:  LBB39_3: ## %cond.store24
; SSE2-NEXT:    movl $0, 4(%rsi)
; SSE2-NEXT:    testb $4, %al
; SSE2-NEXT:    je LBB39_6
; SSE2-NEXT:  LBB39_5: ## %cond.store26
; SSE2-NEXT:    movl $0, 8(%rsi)
; SSE2-NEXT:    testb $8, %al
; SSE2-NEXT:    je LBB39_8
; SSE2-NEXT:  LBB39_7: ## %cond.store28
; SSE2-NEXT:    movl $0, 12(%rsi)
; SSE2-NEXT:    testb $16, %al
; SSE2-NEXT:    je LBB39_10
; SSE2-NEXT:  LBB39_9: ## %cond.store30
; SSE2-NEXT:    movl $0, 16(%rsi)
; SSE2-NEXT:    testb $32, %al
; SSE2-NEXT:    je LBB39_12
; SSE2-NEXT:  LBB39_11: ## %cond.store32
; SSE2-NEXT:    movl $0, 20(%rsi)
; SSE2-NEXT:    testb $64, %al
; SSE2-NEXT:    je LBB39_14
; SSE2-NEXT:  LBB39_13: ## %cond.store34
; SSE2-NEXT:    movl $0, 24(%rsi)
; SSE2-NEXT:    testb $-128, %al
; SSE2-NEXT:    je LBB39_16
; SSE2-NEXT:  LBB39_15: ## %cond.store36
; SSE2-NEXT:    movl $0, 28(%rsi)
; SSE2-NEXT:    retq
;
; SSE4-LABEL: undefshuffle:
; SSE4:       ## %bb.0: ## %else
; SSE4-NEXT:    psllw $15, %xmm0
; SSE4-NEXT:    pand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; SSE4-NEXT:    packsswb %xmm0, %xmm0
; SSE4-NEXT:    pmovmskb %xmm0, %eax
; SSE4-NEXT:    testb $1, %al
; SSE4-NEXT:    jne LBB39_1
; SSE4-NEXT:  ## %bb.2: ## %else23
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    jne LBB39_3
; SSE4-NEXT:  LBB39_4: ## %else25
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    jne LBB39_5
; SSE4-NEXT:  LBB39_6: ## %else27
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    jne LBB39_7
; SSE4-NEXT:  LBB39_8: ## %else29
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    jne LBB39_9
; SSE4-NEXT:  LBB39_10: ## %else31
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    jne LBB39_11
; SSE4-NEXT:  LBB39_12: ## %else33
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    jne LBB39_13
; SSE4-NEXT:  LBB39_14: ## %else35
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    jne LBB39_15
; SSE4-NEXT:  LBB39_16: ## %else37
; SSE4-NEXT:    retq
; SSE4-NEXT:  LBB39_1: ## %cond.store
; SSE4-NEXT:    movl $0, (%rsi)
; SSE4-NEXT:    testb $2, %al
; SSE4-NEXT:    je LBB39_4
; SSE4-NEXT:  LBB39_3: ## %cond.store24
; SSE4-NEXT:    movl $0, 4(%rsi)
; SSE4-NEXT:    testb $4, %al
; SSE4-NEXT:    je LBB39_6
; SSE4-NEXT:  LBB39_5: ## %cond.store26
; SSE4-NEXT:    movl $0, 8(%rsi)
; SSE4-NEXT:    testb $8, %al
; SSE4-NEXT:    je LBB39_8
; SSE4-NEXT:  LBB39_7: ## %cond.store28
; SSE4-NEXT:    movl $0, 12(%rsi)
; SSE4-NEXT:    testb $16, %al
; SSE4-NEXT:    je LBB39_10
; SSE4-NEXT:  LBB39_9: ## %cond.store30
; SSE4-NEXT:    movl $0, 16(%rsi)
; SSE4-NEXT:    testb $32, %al
; SSE4-NEXT:    je LBB39_12
; SSE4-NEXT:  LBB39_11: ## %cond.store32
; SSE4-NEXT:    movl $0, 20(%rsi)
; SSE4-NEXT:    testb $64, %al
; SSE4-NEXT:    je LBB39_14
; SSE4-NEXT:  LBB39_13: ## %cond.store34
; SSE4-NEXT:    movl $0, 24(%rsi)
; SSE4-NEXT:    testb $-128, %al
; SSE4-NEXT:    je LBB39_16
; SSE4-NEXT:  LBB39_15: ## %cond.store36
; SSE4-NEXT:    movl $0, 28(%rsi)
; SSE4-NEXT:    retq
;
; AVX1-LABEL: undefshuffle:
; AVX1:       ## %bb.0:
; AVX1-NEXT:    vpmovzxwd {{.*#+}} xmm0 = xmm0[0],zero,xmm0[1],zero,xmm0[2],zero,xmm0[3],zero
; AVX1-NEXT:    vpslld $31, %xmm0, %xmm0
; AVX1-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1-NEXT:    vmaskmovps %ymm1, %ymm0, (%rsi)
; AVX1-NEXT:    vzeroupper
; AVX1-NEXT:    retq
;
; AVX2-LABEL: undefshuffle:
; AVX2:       ## %bb.0:
; AVX2-NEXT:    ## kill: def $xmm0 killed $xmm0 def $ymm0
; AVX2-NEXT:    vpshufb {{.*#+}} ymm0 = ymm0[0,u,u,u,2,u,u,u,4,u,u,u,6,u,u,u],zero,ymm0[u,u,u],zero,ymm0[u,u,u],zero,ymm0[u,u,u],zero,ymm0[u,u,u]
; AVX2-NEXT:    vpslld $31, %ymm0, %ymm0
; AVX2-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX2-NEXT:    vpmaskmovd %ymm1, %ymm0, (%rsi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512F-LABEL: undefshuffle:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    vpmovsxwq %xmm0, %zmm0
; AVX512F-NEXT:    vpsllq $63, %zmm0, %zmm0
; AVX512F-NEXT:    movb $15, %al
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vptestmq %zmm0, %zmm0, %k1 {%k1}
; AVX512F-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512F-NEXT:    vmovdqu32 %zmm0, (%rsi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: undefshuffle:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    vpmovsxwd %xmm0, %ymm0
; AVX512VLDQ-NEXT:    vpslld $31, %ymm0, %ymm0
; AVX512VLDQ-NEXT:    vpxor %xmm1, %xmm1, %xmm1
; AVX512VLDQ-NEXT:    movb $15, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vpcmpgtd %ymm0, %ymm1, %k1 {%k1}
; AVX512VLDQ-NEXT:    vmovdqu32 %ymm1, (%rsi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: undefshuffle:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    vpsllw $15, %xmm0, %xmm0
; AVX512VLBW-NEXT:    vpmovw2m %xmm0, %k0
; AVX512VLBW-NEXT:    movl $15, %eax
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    kandd %k1, %k0, %k1
; AVX512VLBW-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; AVX512VLBW-NEXT:    vmovdqu32 %ymm0, (%rsi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: undefshuffle:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    vpsllw $15, %xmm0, %xmm0
; X86-AVX512-NEXT:    vpmovw2m %xmm0, %k0
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movl $15, %ecx
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    kandd %k1, %k0, %k1
; X86-AVX512-NEXT:    vpxor %xmm0, %xmm0, %xmm0
; X86-AVX512-NEXT:    vmovdqu32 %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %i1 = shufflevector <8 x i1> %i0, <8 x i1> zeroinitializer, <16 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %i2 = shufflevector <16 x i1> %i1, <16 x i1> zeroinitializer, <32 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7, i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15, i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23, i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %it51 = and <32 x i1> %i2, <i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>
  %i3 = shufflevector <32 x i1> %it51, <32 x i1> zeroinitializer, <8 x i32> <i32 0, i32 1, i32 2, i32 3, i32 4, i32 5, i32 6, i32 7>
  %i4 = shufflevector <32 x i1> %it51, <32 x i1> zeroinitializer, <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %i5 = tail call <8 x i32> @llvm.masked.load.v8i32.p0(ptr %src, i32 1, <8 x i1> %i4, <8 x i32> zeroinitializer)
  tail call void @llvm.masked.store.v8i32.p0(<8 x i32> zeroinitializer, ptr %dst, i32 1, <8 x i1> %i3)
  ret void
}
declare <8 x i32> @llvm.masked.load.v8i32.p0(ptr nocapture, i32 immarg, <8 x i1>, <8 x i32>)

declare void @llvm.masked.store.v8f64.p0(<8 x double>, ptr, i32, <8 x i1>)
declare void @llvm.masked.store.v4f64.p0(<4 x double>, ptr, i32, <4 x i1>)
declare void @llvm.masked.store.v2f64.p0(<2 x double>, ptr, i32, <2 x i1>)
declare void @llvm.masked.store.v1f64.p0(<1 x double>, ptr, i32, <1 x i1>)

declare void @llvm.masked.store.v16f32.p0(<16 x float>, ptr, i32, <16 x i1>)
declare void @llvm.masked.store.v8f32.p0(<8 x float>, ptr, i32, <8 x i1>)
declare void @llvm.masked.store.v4f32.p0(<4 x float>, ptr, i32, <4 x i1>)
declare void @llvm.masked.store.v2f32.p0(<2 x float>, ptr, i32, <2 x i1>)

declare void @llvm.masked.store.v16i64.p0(<16 x i64>, ptr, i32, <16 x i1>)
declare void @llvm.masked.store.v8i64.p0(<8 x i64>, ptr, i32, <8 x i1>)
declare void @llvm.masked.store.v4i64.p0(<4 x i64>, ptr, i32, <4 x i1>)
declare void @llvm.masked.store.v2i64.p0(<2 x i64>, ptr, i32, <2 x i1>)
declare void @llvm.masked.store.v1i64.p0(<1 x i64>, ptr, i32, <1 x i1>)

declare void @llvm.masked.store.v24i32.p0(<24 x i32>, ptr, i32, <24 x i1>)
declare void @llvm.masked.store.v16i32.p0(<16 x i32>, ptr, i32, <16 x i1>)
declare void @llvm.masked.store.v8i32.p0(<8 x i32>, ptr, i32, <8 x i1>)
declare void @llvm.masked.store.v4i32.p0(<4 x i32>, ptr, i32, <4 x i1>)
declare void @llvm.masked.store.v3i32.p0(<3 x i32>, ptr, i32, <3 x i1>)
declare void @llvm.masked.store.v2i32.p0(<2 x i32>, ptr, i32, <2 x i1>)
declare void @llvm.masked.store.v1i32.p0(<1 x i32>, ptr, i32, <1 x i1>)

declare void @llvm.masked.store.v32i16.p0(<32 x i16>, ptr, i32, <32 x i1>)
declare void @llvm.masked.store.v16i16.p0(<16 x i16>, ptr, i32, <16 x i1>)
declare void @llvm.masked.store.v8i16.p0(<8 x i16>, ptr, i32, <8 x i1>)
declare void @llvm.masked.store.v4i16.p0(<4 x i16>, ptr, i32, <4 x i1>)

declare void @llvm.masked.store.v64i8.p0(<64 x i8>, ptr, i32, <64 x i1>)
declare void @llvm.masked.store.v32i8.p0(<32 x i8>, ptr, i32, <32 x i1>)
declare void @llvm.masked.store.v16i8.p0(<16 x i8>, ptr, i32, <16 x i1>)
declare void @llvm.masked.store.v8i8.p0(<8 x i8>, ptr, i32, <8 x i1>)
