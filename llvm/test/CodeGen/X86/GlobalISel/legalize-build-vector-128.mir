# NOTE: Assertions have been autogenerated by utils/update_mir_test_checks.py UTC_ARGS: --version 6
# RUN: llc -mtriple=x86_64-linux-gnu -mattr=sse2    -run-pass=legalizer -global-isel-abort=1 -pass-remarks-missed='gisel*' %s -o -  | FileCheck %s --check-prefixes=SSE2
# RUN: llc -mtriple=x86_64-linux-gnu -mattr=sse4.1  -run-pass=legalizer -global-isel-abort=1 -pass-remarks-missed='gisel*' %s -o -  | FileCheck %s --check-prefixes=SSE41
# RUN: llc -mtriple=x86_64-linux-gnu -mattr=avx     -run-pass=legalizer -global-isel-abort=1 -pass-remarks-missed='gisel*' %s -o -  | FileCheck %s --check-prefixes=AVX
# RUN: llc -mtriple=x86_64-linux-gnu -mattr=avx512f -run-pass=legalizer -global-isel-abort=1 -pass-remarks-missed='gisel*' %s -o -  | FileCheck %s --check-prefixes=AVX512F

---
name: test_basic_build_v2i64
body: |
  bb.0:
    liveins: $rdi, $rsi
    ; SSE2-LABEL: name: test_basic_build_v2i64
    ; SSE2: liveins: $rdi, $rsi
    ; SSE2-NEXT: {{  $}}
    ; SSE2-NEXT: [[COPY:%[0-9]+]]:_(s64) = COPY $rdi
    ; SSE2-NEXT: [[COPY1:%[0-9]+]]:_(s64) = COPY $rsi
    ; SSE2-NEXT: [[COPY2:%[0-9]+]]:vr128(s64) = COPY [[COPY]](s64)
    ; SSE2-NEXT: [[COPY3:%[0-9]+]]:vr128(s64) = COPY [[COPY1]](s64)
    ; SSE2-NEXT: [[PUNPCKLQDQrr:%[0-9]+]]:_(<2 x s64>) = PUNPCKLQDQrr [[COPY2]](s64), [[COPY3]](s64)
    ; SSE2-NEXT: RET 0, implicit [[PUNPCKLQDQrr]](<2 x s64>)
    ;
    ; SSE41-LABEL: name: test_basic_build_v2i64
    ; SSE41: liveins: $rdi, $rsi
    ; SSE41-NEXT: {{  $}}
    ; SSE41-NEXT: [[COPY:%[0-9]+]]:_(s64) = COPY $rdi
    ; SSE41-NEXT: [[COPY1:%[0-9]+]]:_(s64) = COPY $rsi
    ; SSE41-NEXT: [[DEF:%[0-9]+]]:vr128(<2 x s64>) = G_IMPLICIT_DEF
    ; SSE41-NEXT: [[PINSRQrri:%[0-9]+]]:vr128(<2 x s64>) = PINSRQrri [[DEF]](<2 x s64>), [[COPY]](s64), 0
    ; SSE41-NEXT: [[PINSRQrri1:%[0-9]+]]:_(<2 x s64>) = PINSRQrri [[PINSRQrri]](<2 x s64>), [[COPY1]](s64), 1
    ; SSE41-NEXT: RET 0, implicit [[PINSRQrri1]](<2 x s64>)
    ;
    ; AVX-LABEL: name: test_basic_build_v2i64
    ; AVX: liveins: $rdi, $rsi
    ; AVX-NEXT: {{  $}}
    ; AVX-NEXT: [[COPY:%[0-9]+]]:_(s64) = COPY $rdi
    ; AVX-NEXT: [[COPY1:%[0-9]+]]:_(s64) = COPY $rsi
    ; AVX-NEXT: [[DEF:%[0-9]+]]:vr128(<2 x s64>) = G_IMPLICIT_DEF
    ; AVX-NEXT: [[PINSRQrri:%[0-9]+]]:vr128(<2 x s64>) = PINSRQrri [[DEF]](<2 x s64>), [[COPY]](s64), 0
    ; AVX-NEXT: [[PINSRQrri1:%[0-9]+]]:_(<2 x s64>) = PINSRQrri [[PINSRQrri]](<2 x s64>), [[COPY1]](s64), 1
    ; AVX-NEXT: RET 0, implicit [[PINSRQrri1]](<2 x s64>)
    ;
    ; AVX512F-LABEL: name: test_basic_build_v2i64
    ; AVX512F: liveins: $rdi, $rsi
    ; AVX512F-NEXT: {{  $}}
    ; AVX512F-NEXT: [[COPY:%[0-9]+]]:_(s64) = COPY $rdi
    ; AVX512F-NEXT: [[COPY1:%[0-9]+]]:_(s64) = COPY $rsi
    ; AVX512F-NEXT: [[DEF:%[0-9]+]]:vr128(<2 x s64>) = G_IMPLICIT_DEF
    ; AVX512F-NEXT: [[PINSRQrri:%[0-9]+]]:vr128(<2 x s64>) = PINSRQrri [[DEF]](<2 x s64>), [[COPY]](s64), 0
    ; AVX512F-NEXT: [[PINSRQrri1:%[0-9]+]]:_(<2 x s64>) = PINSRQrri [[PINSRQrri]](<2 x s64>), [[COPY1]](s64), 1
    ; AVX512F-NEXT: RET 0, implicit [[PINSRQrri1]](<2 x s64>)
    %1:_(s64) = COPY $rdi
    %2:_(s64) = COPY $rsi
    %0:_(<2 x s64>) = G_BUILD_VECTOR %1:_(s64), %2:_(s64)
    RET 0, implicit %0
...
---
name: test_build_v8i16
body: |
  bb.0:
    liveins: $di, $si, $dx, $cx
    ; SSE2-LABEL: name: test_build_v8i16
    ; SSE2: liveins: $di, $si, $dx, $cx
    ; SSE2-NEXT: {{  $}}
    ; SSE2-NEXT: [[COPY:%[0-9]+]]:_(s16) = COPY $di
    ; SSE2-NEXT: [[COPY1:%[0-9]+]]:_(s16) = COPY $si
    ; SSE2-NEXT: [[C:%[0-9]+]]:_(s16) = G_CONSTANT i16 3
    ; SSE2-NEXT: [[C1:%[0-9]+]]:_(s16) = G_CONSTANT i16 4
    ; SSE2-NEXT: [[C2:%[0-9]+]]:_(s16) = G_CONSTANT i16 5
    ; SSE2-NEXT: [[C3:%[0-9]+]]:_(s16) = G_CONSTANT i16 6
    ; SSE2-NEXT: [[COPY2:%[0-9]+]]:_(s16) = COPY $dx
    ; SSE2-NEXT: [[COPY3:%[0-9]+]]:_(s16) = COPY $cx
    ; SSE2-NEXT: [[COPY4:%[0-9]+]]:vr128(s16) = COPY [[COPY]](s16)
    ; SSE2-NEXT: [[COPY5:%[0-9]+]]:vr128(s16) = COPY [[COPY1]](s16)
    ; SSE2-NEXT: [[PUNPCKLWDrr:%[0-9]+]]:vr128(<2 x s16>) = PUNPCKLWDrr [[COPY4]](s16), [[COPY5]](s16)
    ; SSE2-NEXT: [[COPY6:%[0-9]+]]:vr128(s16) = COPY [[C]](s16)
    ; SSE2-NEXT: [[COPY7:%[0-9]+]]:vr128(s16) = COPY [[C1]](s16)
    ; SSE2-NEXT: [[PUNPCKLWDrr1:%[0-9]+]]:vr128(<2 x s16>) = PUNPCKLWDrr [[COPY6]](s16), [[COPY7]](s16)
    ; SSE2-NEXT: [[PUNPCKLDQrr:%[0-9]+]]:vr128(<4 x s16>) = PUNPCKLDQrr [[PUNPCKLWDrr]](<2 x s16>), [[PUNPCKLWDrr1]](<2 x s16>)
    ; SSE2-NEXT: [[COPY8:%[0-9]+]]:vr128(s16) = COPY [[C2]](s16)
    ; SSE2-NEXT: [[COPY9:%[0-9]+]]:vr128(s16) = COPY [[C3]](s16)
    ; SSE2-NEXT: [[PUNPCKLWDrr2:%[0-9]+]]:vr128(<2 x s16>) = PUNPCKLWDrr [[COPY8]](s16), [[COPY9]](s16)
    ; SSE2-NEXT: [[COPY10:%[0-9]+]]:vr128(s16) = COPY [[COPY2]](s16)
    ; SSE2-NEXT: [[COPY11:%[0-9]+]]:vr128(s16) = COPY [[COPY3]](s16)
    ; SSE2-NEXT: [[PUNPCKLWDrr3:%[0-9]+]]:vr128(<2 x s16>) = PUNPCKLWDrr [[COPY10]](s16), [[COPY11]](s16)
    ; SSE2-NEXT: [[PUNPCKLDQrr1:%[0-9]+]]:vr128(<4 x s16>) = PUNPCKLDQrr [[PUNPCKLWDrr2]](<2 x s16>), [[PUNPCKLWDrr3]](<2 x s16>)
    ; SSE2-NEXT: [[PUNPCKLQDQrr:%[0-9]+]]:_(<8 x s16>) = PUNPCKLQDQrr [[PUNPCKLDQrr]](<4 x s16>), [[PUNPCKLDQrr1]](<4 x s16>)
    ; SSE2-NEXT: RET 0, implicit [[PUNPCKLQDQrr]](<8 x s16>)
    ;
    ; SSE41-LABEL: name: test_build_v8i16
    ; SSE41: liveins: $di, $si, $dx, $cx
    ; SSE41-NEXT: {{  $}}
    ; SSE41-NEXT: [[COPY:%[0-9]+]]:_(s16) = COPY $di
    ; SSE41-NEXT: [[COPY1:%[0-9]+]]:_(s16) = COPY $si
    ; SSE41-NEXT: [[C:%[0-9]+]]:_(s16) = G_CONSTANT i16 3
    ; SSE41-NEXT: [[C1:%[0-9]+]]:_(s16) = G_CONSTANT i16 4
    ; SSE41-NEXT: [[C2:%[0-9]+]]:_(s16) = G_CONSTANT i16 5
    ; SSE41-NEXT: [[C3:%[0-9]+]]:_(s16) = G_CONSTANT i16 6
    ; SSE41-NEXT: [[COPY2:%[0-9]+]]:_(s16) = COPY $dx
    ; SSE41-NEXT: [[COPY3:%[0-9]+]]:_(s16) = COPY $cx
    ; SSE41-NEXT: [[DEF:%[0-9]+]]:vr128(<8 x s16>) = G_IMPLICIT_DEF
    ; SSE41-NEXT: [[PINSRWrri:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[DEF]](<8 x s16>), [[COPY]](s16), 0
    ; SSE41-NEXT: [[PINSRWrri1:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri]](<8 x s16>), [[COPY1]](s16), 1
    ; SSE41-NEXT: [[PINSRWrri2:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri1]](<8 x s16>), [[C]](s16), 2
    ; SSE41-NEXT: [[PINSRWrri3:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri2]](<8 x s16>), [[C1]](s16), 3
    ; SSE41-NEXT: [[PINSRWrri4:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri3]](<8 x s16>), [[C2]](s16), 4
    ; SSE41-NEXT: [[PINSRWrri5:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri4]](<8 x s16>), [[C3]](s16), 5
    ; SSE41-NEXT: [[PINSRWrri6:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri5]](<8 x s16>), [[COPY2]](s16), 6
    ; SSE41-NEXT: [[PINSRWrri7:%[0-9]+]]:_(<8 x s16>) = PINSRWrri [[PINSRWrri6]](<8 x s16>), [[COPY3]](s16), 7
    ; SSE41-NEXT: RET 0, implicit [[PINSRWrri7]](<8 x s16>)
    ;
    ; AVX-LABEL: name: test_build_v8i16
    ; AVX: liveins: $di, $si, $dx, $cx
    ; AVX-NEXT: {{  $}}
    ; AVX-NEXT: [[COPY:%[0-9]+]]:_(s16) = COPY $di
    ; AVX-NEXT: [[COPY1:%[0-9]+]]:_(s16) = COPY $si
    ; AVX-NEXT: [[C:%[0-9]+]]:_(s16) = G_CONSTANT i16 3
    ; AVX-NEXT: [[C1:%[0-9]+]]:_(s16) = G_CONSTANT i16 4
    ; AVX-NEXT: [[C2:%[0-9]+]]:_(s16) = G_CONSTANT i16 5
    ; AVX-NEXT: [[C3:%[0-9]+]]:_(s16) = G_CONSTANT i16 6
    ; AVX-NEXT: [[COPY2:%[0-9]+]]:_(s16) = COPY $dx
    ; AVX-NEXT: [[COPY3:%[0-9]+]]:_(s16) = COPY $cx
    ; AVX-NEXT: [[DEF:%[0-9]+]]:vr128(<8 x s16>) = G_IMPLICIT_DEF
    ; AVX-NEXT: [[PINSRWrri:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[DEF]](<8 x s16>), [[COPY]](s16), 0
    ; AVX-NEXT: [[PINSRWrri1:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri]](<8 x s16>), [[COPY1]](s16), 1
    ; AVX-NEXT: [[PINSRWrri2:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri1]](<8 x s16>), [[C]](s16), 2
    ; AVX-NEXT: [[PINSRWrri3:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri2]](<8 x s16>), [[C1]](s16), 3
    ; AVX-NEXT: [[PINSRWrri4:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri3]](<8 x s16>), [[C2]](s16), 4
    ; AVX-NEXT: [[PINSRWrri5:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri4]](<8 x s16>), [[C3]](s16), 5
    ; AVX-NEXT: [[PINSRWrri6:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri5]](<8 x s16>), [[COPY2]](s16), 6
    ; AVX-NEXT: [[PINSRWrri7:%[0-9]+]]:_(<8 x s16>) = PINSRWrri [[PINSRWrri6]](<8 x s16>), [[COPY3]](s16), 7
    ; AVX-NEXT: RET 0, implicit [[PINSRWrri7]](<8 x s16>)
    ;
    ; AVX512F-LABEL: name: test_build_v8i16
    ; AVX512F: liveins: $di, $si, $dx, $cx
    ; AVX512F-NEXT: {{  $}}
    ; AVX512F-NEXT: [[COPY:%[0-9]+]]:_(s16) = COPY $di
    ; AVX512F-NEXT: [[COPY1:%[0-9]+]]:_(s16) = COPY $si
    ; AVX512F-NEXT: [[C:%[0-9]+]]:_(s16) = G_CONSTANT i16 3
    ; AVX512F-NEXT: [[C1:%[0-9]+]]:_(s16) = G_CONSTANT i16 4
    ; AVX512F-NEXT: [[C2:%[0-9]+]]:_(s16) = G_CONSTANT i16 5
    ; AVX512F-NEXT: [[C3:%[0-9]+]]:_(s16) = G_CONSTANT i16 6
    ; AVX512F-NEXT: [[COPY2:%[0-9]+]]:_(s16) = COPY $dx
    ; AVX512F-NEXT: [[COPY3:%[0-9]+]]:_(s16) = COPY $cx
    ; AVX512F-NEXT: [[DEF:%[0-9]+]]:vr128(<8 x s16>) = G_IMPLICIT_DEF
    ; AVX512F-NEXT: [[PINSRWrri:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[DEF]](<8 x s16>), [[COPY]](s16), 0
    ; AVX512F-NEXT: [[PINSRWrri1:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri]](<8 x s16>), [[COPY1]](s16), 1
    ; AVX512F-NEXT: [[PINSRWrri2:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri1]](<8 x s16>), [[C]](s16), 2
    ; AVX512F-NEXT: [[PINSRWrri3:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri2]](<8 x s16>), [[C1]](s16), 3
    ; AVX512F-NEXT: [[PINSRWrri4:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri3]](<8 x s16>), [[C2]](s16), 4
    ; AVX512F-NEXT: [[PINSRWrri5:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri4]](<8 x s16>), [[C3]](s16), 5
    ; AVX512F-NEXT: [[PINSRWrri6:%[0-9]+]]:vr128(<8 x s16>) = PINSRWrri [[PINSRWrri5]](<8 x s16>), [[COPY2]](s16), 6
    ; AVX512F-NEXT: [[PINSRWrri7:%[0-9]+]]:_(<8 x s16>) = PINSRWrri [[PINSRWrri6]](<8 x s16>), [[COPY3]](s16), 7
    ; AVX512F-NEXT: RET 0, implicit [[PINSRWrri7]](<8 x s16>)
    %1:_(s16) = COPY $di
    %2:_(s16) = COPY $si
    %3:_(s16) = G_CONSTANT i16 3
    %4:_(s16) = G_CONSTANT i16 4
    %5:_(s16) = G_CONSTANT i16 5
    %6:_(s16) = G_CONSTANT i16 6
    %7:_(s16) = COPY $dx
    %8:_(s16) = COPY $cx
    %0:_(<8 x s16>) = G_BUILD_VECTOR %1:_(s16), %2:_(s16), %3:_(s16), %4:_(s16), %5:_(s16), %6:_(s16), %7:_(s16), %8:_(s16)
    RET 0, implicit %0
...
