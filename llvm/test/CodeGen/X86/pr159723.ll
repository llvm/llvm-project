; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512fp16,+avx512vl| FileCheck %s

declare <8 x half> @test_call_8()

declare <16 x half> @test_call_16()

declare <32 x half> @test_call_32()

define <8 x i1> @test_cmp_v8half_ogt(<8 x half> %rhs, <8 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v8half_ogt:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $40, %rsp
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vpsllw $15, %xmm1, %xmm0
; CHECK-NEXT:    vpmovw2m %xmm0, %k1
; CHECK-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    callq test_call_8@PLT
; CHECK-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; CHECK-NEXT:    vcmpgtph {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %k0 {%k1} # 16-byte Folded Reload
; CHECK-NEXT:    vpmovm2w %k0, %xmm0
; CHECK-NEXT:    addq $40, %rsp
; CHECK-NEXT:    retq
    %lhs = call <8 x half> @test_call_8()
    %comp = fcmp ogt <8 x half> %lhs, %rhs
    %res = and <8 x i1> %comp, %mask
    ret <8 x i1> %res
}

define <8 x i1> @test_cmp_v8half_ogt_commute(<8 x half> %rhs, <8 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v8half_ogt_commute:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $40, %rsp
; CHECK-NEXT:    vmovaps %xmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 16-byte Spill
; CHECK-NEXT:    vpsllw $15, %xmm1, %xmm0
; CHECK-NEXT:    vpmovw2m %xmm0, %k1
; CHECK-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    callq test_call_8@PLT
; CHECK-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; CHECK-NEXT:    vcmpltph {{[-0-9]+}}(%r{{[sb]}}p), %xmm0, %k0 {%k1} # 16-byte Folded Reload
; CHECK-NEXT:    vpmovm2w %k0, %xmm0
; CHECK-NEXT:    addq $40, %rsp
; CHECK-NEXT:    retq
    %lhs = call <8 x half> @test_call_8()
    %comp = fcmp ogt <8 x half> %rhs, %lhs
    %res = and <8 x i1> %comp, %mask
    ret <8 x i1> %res
}


define <16 x i1> @test_cmp_v16half_olt(<16 x half> %rhs, <16 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v16half_olt:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $56, %rsp
; CHECK-NEXT:    vmovups %ymm0, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    vpsllw $7, %xmm1, %xmm0
; CHECK-NEXT:    vpmovb2m %xmm0, %k1
; CHECK-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    callq test_call_16@PLT
; CHECK-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; CHECK-NEXT:    vcmpltph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %k0 {%k1} # 32-byte Folded Reload
; CHECK-NEXT:    vpmovm2b %k0, %xmm0
; CHECK-NEXT:    addq $56, %rsp
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
    %lhs = call <16 x half> @test_call_16()
    %comp = fcmp olt <16 x half> %lhs, %rhs
    %res = and <16 x i1> %comp, %mask
    ret <16 x i1> %res
}

define <16 x i1> @test_cmp_v16half_olt_commute(<16 x half> %rhs, <16 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v16half_olt_commute:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $56, %rsp
; CHECK-NEXT:    vmovups %ymm0, {{[-0-9]+}}(%r{{[sb]}}p) # 32-byte Spill
; CHECK-NEXT:    vpsllw $7, %xmm1, %xmm0
; CHECK-NEXT:    vpmovb2m %xmm0, %k1
; CHECK-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; CHECK-NEXT:    callq test_call_16@PLT
; CHECK-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; CHECK-NEXT:    vcmpgtph {{[-0-9]+}}(%r{{[sb]}}p), %ymm0, %k0 {%k1} # 32-byte Folded Reload
; CHECK-NEXT:    vpmovm2b %k0, %xmm0
; CHECK-NEXT:    addq $56, %rsp
; CHECK-NEXT:    vzeroupper
; CHECK-NEXT:    retq
    %lhs = call <16 x half> @test_call_16()
    %comp = fcmp olt <16 x half> %rhs, %lhs
    %res = and <16 x i1> %comp, %mask
    ret <16 x i1> %res
}

define <32 x i1> @test_cmp_v32half_oge(<32 x half> %rhs, <32 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v32half_oge:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $88, %rsp
; CHECK-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vpsllw $7, %ymm1, %ymm0
; CHECK-NEXT:    vpmovb2m %ymm0, %k1
; CHECK-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    callq test_call_32@PLT
; CHECK-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; CHECK-NEXT:    vcmpgeph {{[-0-9]+}}(%r{{[sb]}}p), %zmm0, %k0 {%k1} # 64-byte Folded Reload
; CHECK-NEXT:    vpmovm2b %k0, %ymm0
; CHECK-NEXT:    addq $88, %rsp
; CHECK-NEXT:    retq
    %lhs = call <32 x half> @test_call_32()
    %comp = fcmp oge <32 x half> %lhs, %rhs
    %res = and <32 x i1> %comp, %mask
    ret <32 x i1> %res
}

define <32 x i1> @test_cmp_v32half_oge_commute(<32 x half> %rhs, <32 x i1> %mask) nounwind {
; CHECK-LABEL: test_cmp_v32half_oge_commute:
; CHECK:       # %bb.0:
; CHECK-NEXT:    subq $88, %rsp
; CHECK-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; CHECK-NEXT:    vpsllw $7, %ymm1, %ymm0
; CHECK-NEXT:    vpmovb2m %ymm0, %k1
; CHECK-NEXT:    kmovd %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 4-byte Spill
; CHECK-NEXT:    callq test_call_32@PLT
; CHECK-NEXT:    kmovd {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 4-byte Reload
; CHECK-NEXT:    vcmpleph {{[-0-9]+}}(%r{{[sb]}}p), %zmm0, %k0 {%k1} # 64-byte Folded Reload
; CHECK-NEXT:    vpmovm2b %k0, %ymm0
; CHECK-NEXT:    addq $88, %rsp
; CHECK-NEXT:    retq
    %lhs = call <32 x half> @test_call_32()
    %comp = fcmp oge <32 x half> %rhs, %lhs
    %res = and <32 x i1> %comp, %mask
    ret <32 x i1> %res
}
