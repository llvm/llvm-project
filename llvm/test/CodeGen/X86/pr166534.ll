; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64    | FileCheck %s --check-prefixes=SSE2
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v2 | FileCheck %s --check-prefixes=SSE4
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v3 | FileCheck %s --check-prefixes=AVX2
; RUN: llc < %s -mtriple=x86_64-- -mcpu=x86-64-v4 | FileCheck %s --check-prefixes=AVX512

define void @pr166534(ptr %pa, ptr %pb, ptr %pc, ptr %pd) {
; SSE2-LABEL: pr166534:
; SSE2:       # %bb.0: # %entry
; SSE2-NEXT:    movdqu (%rdi), %xmm0
; SSE2-NEXT:    movdqu (%rsi), %xmm1
; SSE2-NEXT:    pcmpeqb %xmm0, %xmm1
; SSE2-NEXT:    pmovmskb %xmm1, %esi
; SSE2-NEXT:    xorl %eax, %eax
; SSE2-NEXT:    cmpl $65535, %esi # imm = 0xFFFF
; SSE2-NEXT:    sete %al
; SSE2-NEXT:    orq %rax, (%rdx)
; SSE2-NEXT:    cmpl $65535, %esi # imm = 0xFFFF
; SSE2-NEXT:    jne .LBB0_2
; SSE2-NEXT:  # %bb.1: # %if.then
; SSE2-NEXT:    orq %rax, (%rcx)
; SSE2-NEXT:  .LBB0_2: # %if.end
; SSE2-NEXT:    retq
;
; SSE4-LABEL: pr166534:
; SSE4:       # %bb.0: # %entry
; SSE4-NEXT:    movdqu (%rdi), %xmm0
; SSE4-NEXT:    movdqu (%rsi), %xmm1
; SSE4-NEXT:    pxor %xmm0, %xmm1
; SSE4-NEXT:    xorl %eax, %eax
; SSE4-NEXT:    ptest %xmm1, %xmm1
; SSE4-NEXT:    sete %al
; SSE4-NEXT:    orq %rax, (%rdx)
; SSE4-NEXT:    ptest %xmm1, %xmm1
; SSE4-NEXT:    jne .LBB0_2
; SSE4-NEXT:  # %bb.1: # %if.then
; SSE4-NEXT:    orq %rax, (%rcx)
; SSE4-NEXT:  .LBB0_2: # %if.end
; SSE4-NEXT:    retq
;
; AVX2-LABEL: pr166534:
; AVX2:       # %bb.0: # %entry
; AVX2-NEXT:    vmovdqu (%rdi), %xmm0
; AVX2-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; AVX2-NEXT:    xorl %eax, %eax
; AVX2-NEXT:    vptest %xmm0, %xmm0
; AVX2-NEXT:    sete %al
; AVX2-NEXT:    orq %rax, (%rdx)
; AVX2-NEXT:    vptest %xmm0, %xmm0
; AVX2-NEXT:    jne .LBB0_2
; AVX2-NEXT:  # %bb.1: # %if.then
; AVX2-NEXT:    orq %rax, (%rcx)
; AVX2-NEXT:  .LBB0_2: # %if.end
; AVX2-NEXT:    retq
;
; AVX512-LABEL: pr166534:
; AVX512:       # %bb.0: # %entry
; AVX512-NEXT:    vmovdqu (%rdi), %xmm0
; AVX512-NEXT:    vpxor (%rsi), %xmm0, %xmm0
; AVX512-NEXT:    xorl %eax, %eax
; AVX512-NEXT:    vptest %xmm0, %xmm0
; AVX512-NEXT:    sete %al
; AVX512-NEXT:    orq %rax, (%rdx)
; AVX512-NEXT:    vptest %xmm0, %xmm0
; AVX512-NEXT:    jne .LBB0_2
; AVX512-NEXT:  # %bb.1: # %if.then
; AVX512-NEXT:    orq %rax, (%rcx)
; AVX512-NEXT:  .LBB0_2: # %if.end
; AVX512-NEXT:    retq
entry:
  %a = load i128, ptr %pa, align 8
  %b = load i128, ptr %pb, align 8
  %cmp = icmp eq i128 %a, %b
  %conv1 = zext i1 %cmp to i128
  %c = load i128, ptr %pc, align 8
  %or = or i128 %c, %conv1
  store i128 %or, ptr %pc, align 8
  br i1 %cmp, label %if.then, label %if.end

if.then:
  %d = load i128, ptr %pd, align 8
  %or7 = or i128 %d, %conv1
  store i128 %or7, ptr %pd, align 8
  br label %if.end

if.end:
  ret void
}
