; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=sse2    | FileCheck %s --check-prefixes=SSE,SSE2
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=sse4.2  | FileCheck %s --check-prefixes=SSE,SSE42
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=avx     | FileCheck %s --check-prefixes=AVX,AVX1OR2,AVX1
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=avx2    | FileCheck %s --check-prefixes=AVX,AVX1OR2,AVX2
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=avx512f | FileCheck %s --check-prefixes=AVX,AVX512,AVX512F
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=avx512f,avx512dq,avx512vl | FileCheck %s --check-prefixes=AVX,AVX512,AVX512VL,AVX512VLDQ
; RUN: llc < %s -disable-peephole -mtriple=x86_64-apple-darwin -mattr=avx512f,avx512bw,avx512vl | FileCheck %s --check-prefixes=AVX,AVX512,AVX512VL,AVX512VLBW
; RUN: llc < %s -mtriple=i686-apple-darwin -mattr=avx512f,avx512bw,avx512dq,avx512vl -verify-machineinstrs | FileCheck %s --check-prefixes=X86-AVX512

define <4 x i64> @mload256_to_load128(ptr %p) nounwind {
; SSE-LABEL: mload256_to_load128:
; SSE:       ## %bb.0:
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    xorps %xmm1, %xmm1
; SSE-NEXT:    retq
;
; AVX-LABEL: mload256_to_load128:
; AVX:       ## %bb.0:
; AVX-NEXT:    vmovaps (%rdi), %xmm0
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: mload256_to_load128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovaps (%eax), %xmm0
; X86-AVX512-NEXT:    retl
  %tmp = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %p, i32 32, <8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false>, <8 x float> <float poison, float poison, float poison, float poison, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <8 x float> %tmp to <4 x i64>
  ret <4 x i64> %r
}

define <8 x i64> @mload512_to_load256(ptr %p) nounwind {
; SSE-LABEL: mload512_to_load256:
; SSE:       ## %bb.0:
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    movups 16(%rdi), %xmm1
; SSE-NEXT:    xorps %xmm2, %xmm2
; SSE-NEXT:    xorps %xmm3, %xmm3
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload512_to_load256:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovups (%rdi), %ymm0
; AVX1OR2-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1OR2-NEXT:    retq
;
; AVX512-LABEL: mload512_to_load256:
; AVX512:       ## %bb.0:
; AVX512-NEXT:    vmovups (%rdi), %ymm0
; AVX512-NEXT:    retq
;
; X86-AVX512-LABEL: mload512_to_load256:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovups (%eax), %ymm0
; X86-AVX512-NEXT:    retl
  %tmp = tail call <32 x i16> @llvm.masked.load.v32i16.p0(ptr %p, i32 1, <32 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>, <32 x i16> <i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 poison, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0, i16 0>)
  %r = bitcast <32 x i16> %tmp to <8 x i64>
  ret <8 x i64> %r
}

define <8 x i64> @mload512_to_mload128(ptr %p) nounwind {
; SSE-LABEL: mload512_to_mload128:
; SSE:       ## %bb.0:
; SSE-NEXT:    movsd {{.*#+}} xmm0 = mem[0],zero
; SSE-NEXT:    xorps %xmm1, %xmm1
; SSE-NEXT:    xorps %xmm2, %xmm2
; SSE-NEXT:    xorps %xmm3, %xmm3
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload512_to_mload128:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovsd {{.*#+}} xmm0 = [4294967295,4294967295,0,0]
; AVX1OR2-NEXT:    vmaskmovps (%rdi), %xmm0, %xmm0
; AVX1OR2-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload512_to_mload128:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $3, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps (%rdi), %zmm0 {%k1} {z}
; AVX512F-NEXT:    vmovaps %xmm0, %xmm0
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload512_to_mload128:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $3, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload512_to_mload128:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $3, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload512_to_mload128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $3, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %xmm0 {%k1} {z}
; X86-AVX512-NEXT:    retl
  %tmp = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %p, i32 64, <16 x i1> <i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>, <16 x float> <float poison, float poison, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <16 x float> %tmp to <8 x i64>
  ret <8 x i64> %r
}

define <4 x i64> @mload256_to_mload128(ptr %p) nounwind {
; SSE2-LABEL: mload256_to_mload128:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; SSE2-NEXT:    movss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; SSE2-NEXT:    movlhps {{.*#+}} xmm0 = xmm0[0],xmm1[0]
; SSE2-NEXT:    xorps %xmm1, %xmm1
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mload256_to_mload128:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movss {{.*#+}} xmm0 = mem[0],zero,zero,zero
; SSE42-NEXT:    insertps {{.*#+}} xmm0 = xmm0[0],zero,mem[0],zero
; SSE42-NEXT:    xorps %xmm1, %xmm1
; SSE42-NEXT:    retq
;
; AVX1OR2-LABEL: mload256_to_mload128:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovddup {{.*#+}} xmm0 = [4294967295,0,4294967295,0]
; AVX1OR2-NEXT:    ## xmm0 = mem[0,0]
; AVX1OR2-NEXT:    vmaskmovps (%rdi), %xmm0, %xmm0
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload256_to_mload128:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $5, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovups (%rdi), %zmm0 {%k1} {z}
; AVX512F-NEXT:    vmovaps %xmm0, %xmm0
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload256_to_mload128:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $5, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload256_to_mload128:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $5, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload256_to_mload128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $5, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %xmm0 {%k1} {z}
; X86-AVX512-NEXT:    retl
  %tmp = tail call <8 x float> @llvm.masked.load.v8f32.p0(ptr %p, i32 32, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false>, <8 x float> <float poison, float 0.000000e+00, float poison, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <8 x float> %tmp to <4 x i64>
  ret <4 x i64> %r
}

define <8 x i64> @mload512_to_mload256(ptr %p) nounwind {
; SSE-LABEL: mload512_to_mload256:
; SSE:       ## %bb.0:
; SSE-NEXT:    xorps %xmm0, %xmm0
; SSE-NEXT:    movhps {{.*#+}} xmm0 = xmm0[0,1],mem[0,1]
; SSE-NEXT:    movss {{.*#+}} xmm1 = mem[0],zero,zero,zero
; SSE-NEXT:    xorps %xmm2, %xmm2
; SSE-NEXT:    xorps %xmm3, %xmm3
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload512_to_mload256:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovaps {{.*#+}} ymm0 = [0,0,4294967295,4294967295,4294967295,0,0,0]
; AVX1OR2-NEXT:    vmaskmovps (%rdi), %ymm0, %ymm0
; AVX1OR2-NEXT:    vxorps %xmm1, %xmm1, %xmm1
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload512_to_mload256:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $28, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps (%rdi), %zmm0 {%k1} {z}
; AVX512F-NEXT:    vmovaps %ymm0, %ymm0
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload512_to_mload256:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $28, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %ymm0 {%k1} {z}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload512_to_mload256:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $28, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %ymm0 {%k1} {z}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload512_to_mload256:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $28, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %ymm0 {%k1} {z}
; X86-AVX512-NEXT:    retl
  %tmp = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %p, i32 64, <16 x i1> <i1 false, i1 false, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>, <16 x float> <float 0.000000e+00, float 0.000000e+00, float poison, float poison, float poison, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <16 x float> %tmp to <8 x i64>
  ret <8 x i64> %r
}

define <8 x i64> @mload512_fail_no_possible_shrink(ptr %p) nounwind {
; SSE-LABEL: mload512_fail_no_possible_shrink:
; SSE:       ## %bb.0:
; SSE-NEXT:    movss {{.*#+}} xmm2 = mem[0],zero,zero,zero
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    movups 16(%rdi), %xmm1
; SSE-NEXT:    xorps %xmm3, %xmm3
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload512_fail_no_possible_shrink:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovss {{.*#+}} xmm0 = [4294967295,0,0,0]
; AVX1OR2-NEXT:    vmaskmovps 32(%rdi), %xmm0, %xmm1
; AVX1OR2-NEXT:    vmovaps (%rdi), %ymm0
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload512_fail_no_possible_shrink:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $511, %ax ## imm = 0x1FF
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps (%rdi), %zmm0 {%k1} {z}
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload512_fail_no_possible_shrink:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movw $511, %ax ## imm = 0x1FF
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %zmm0 {%k1} {z}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload512_fail_no_possible_shrink:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movw $511, %ax ## imm = 0x1FF
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %zmm0 {%k1} {z}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload512_fail_no_possible_shrink:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movw $511, %cx ## imm = 0x1FF
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %zmm0 {%k1} {z}
; X86-AVX512-NEXT:    retl
  %tmp = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %p, i32 64, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>, <16 x float> <float poison, float poison, float poison, float poison, float poison, float poison, float poison, float poison, float poison, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <16 x float> %tmp to <8 x i64>
  ret <8 x i64> %r
}

define <8 x i64> @mload512_fail_non_zero_passthru(ptr %p, <8 x i64> %v) nounwind {
; SSE-LABEL: mload512_fail_non_zero_passthru:
; SSE:       ## %bb.0:
; SSE-NEXT:    movups (%rdi), %xmm0
; SSE-NEXT:    movups 16(%rdi), %xmm1
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload512_fail_non_zero_passthru:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovaps (%rdi), %ymm0
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload512_fail_non_zero_passthru:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $255, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps (%rdi), %zmm0 {%k1}
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload512_fail_non_zero_passthru:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movw $255, %ax
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %zmm0 {%k1}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload512_fail_non_zero_passthru:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movw $255, %ax
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %zmm0 {%k1}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload512_fail_non_zero_passthru:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movw $255, %cx
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %zmm0 {%k1}
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <8 x i64> %v to <16 x float>
  %r = tail call <16 x float> @llvm.masked.load.v16f32.p0(ptr %p, i32 64, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>, <16 x float> %tmp)
  %2 = bitcast <16 x float> %r to <8 x i64>
  ret <8 x i64> %2
}

define <2 x i64> @mload128_fail_no_possible_shrink(ptr %p) nounwind {
; SSE-LABEL: mload128_fail_no_possible_shrink:
; SSE:       ## %bb.0:
; SSE-NEXT:    movsd {{.*#+}} xmm0 = mem[0],zero
; SSE-NEXT:    retq
;
; AVX1OR2-LABEL: mload128_fail_no_possible_shrink:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovsd {{.*#+}} xmm0 = [4294967295,4294967295,0,0]
; AVX1OR2-NEXT:    vmaskmovps (%rdi), %xmm0, %xmm0
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mload128_fail_no_possible_shrink:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $3, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovups (%rdi), %zmm0 {%k1} {z}
; AVX512F-NEXT:    ## kill: def $xmm0 killed $xmm0 killed $zmm0
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mload128_fail_no_possible_shrink:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $3, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mload128_fail_no_possible_shrink:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $3, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps (%rdi), %xmm0 {%k1} {z}
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mload128_fail_no_possible_shrink:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $3, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps (%eax), %xmm0 {%k1} {z}
; X86-AVX512-NEXT:    retl
  %tmp = tail call <4 x float> @llvm.masked.load.v4f32.p0(ptr %p, i32 16, <4 x i1> <i1 true, i1 true, i1 false, i1 false>, <4 x float> <float poison, float poison, float 0.000000e+00, float 0.000000e+00>)
  %r = bitcast <4 x float> %tmp to <2 x i64>
  ret <2 x i64> %r
}

define void @mstore256_to_store128(ptr %p, <4 x i64> %v) nounwind {
; SSE2-LABEL: mstore256_to_store128:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm1 = xmm0[1,1,1,1]
; SSE2-NEXT:    movd %xmm1, 4(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm1 = xmm0[2,3,2,3]
; SSE2-NEXT:    movd %xmm1, 8(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 12(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore256_to_store128:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movups %xmm0, (%rdi)
; SSE42-NEXT:    retq
;
; AVX-LABEL: mstore256_to_store128:
; AVX:       ## %bb.0:
; AVX-NEXT:    vmovaps %xmm0, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: mstore256_to_store128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovaps %xmm0, (%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <4 x i64> %v to <8 x float>
  tail call void @llvm.masked.store.v8f32.p0(<8 x float> %tmp, ptr %p, i32 32, <8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @mstore512_to_store256(ptr %p, <8 x i64> %v) nounwind {
; SSE2-LABEL: mstore512_to_store256:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movd %xmm0, %eax
; SSE2-NEXT:    movw %ax, (%rdi)
; SSE2-NEXT:    pextrw $1, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 2(%rdi)
; SSE2-NEXT:    pextrw $2, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 4(%rdi)
; SSE2-NEXT:    pextrw $3, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 6(%rdi)
; SSE2-NEXT:    pextrw $4, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 8(%rdi)
; SSE2-NEXT:    pextrw $5, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 10(%rdi)
; SSE2-NEXT:    pextrw $6, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 12(%rdi)
; SSE2-NEXT:    pextrw $7, %xmm0, %eax
; SSE2-NEXT:    movw %ax, 14(%rdi)
; SSE2-NEXT:    movd %xmm1, %eax
; SSE2-NEXT:    movw %ax, 16(%rdi)
; SSE2-NEXT:    pextrw $1, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 18(%rdi)
; SSE2-NEXT:    pextrw $2, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 20(%rdi)
; SSE2-NEXT:    pextrw $3, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 22(%rdi)
; SSE2-NEXT:    pextrw $4, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 24(%rdi)
; SSE2-NEXT:    pextrw $5, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 26(%rdi)
; SSE2-NEXT:    pextrw $6, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 28(%rdi)
; SSE2-NEXT:    pextrw $7, %xmm1, %eax
; SSE2-NEXT:    movw %ax, 30(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore512_to_store256:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movups %xmm0, (%rdi)
; SSE42-NEXT:    movups %xmm1, 16(%rdi)
; SSE42-NEXT:    retq
;
; AVX-LABEL: mstore512_to_store256:
; AVX:       ## %bb.0:
; AVX-NEXT:    vmovups %ymm0, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; X86-AVX512-LABEL: mstore512_to_store256:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    vmovups %ymm0, (%eax)
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <8 x i64> %v to <32 x i16>
  tail call void @llvm.masked.store.v32i16.p0(<32 x i16> %tmp, ptr %p, i32 1, <32 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @mstore512_to_mstore128(ptr %p, <8 x i64> %v) nounwind {
; SSE2-LABEL: mstore512_to_mstore128:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[1,1,1,1]
; SSE2-NEXT:    movd %xmm0, 4(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore512_to_mstore128:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movss %xmm0, (%rdi)
; SSE42-NEXT:    extractps $1, %xmm0, 4(%rdi)
; SSE42-NEXT:    retq
;
; AVX1OR2-LABEL: mstore512_to_mstore128:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovsd {{.*#+}} xmm1 = [4294967295,4294967295,0,0]
; AVX1OR2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mstore512_to_mstore128:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $3, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mstore512_to_mstore128:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $3, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps %xmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mstore512_to_mstore128:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $3, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps %xmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mstore512_to_mstore128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $3, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <8 x i64> %v to <16 x float>
  tail call void @llvm.masked.store.v16f32.p0(<16 x float> %tmp, ptr %p, i32 64, <16 x i1> <i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @mstore256_to_mstore128(ptr %p, <4 x i64> %v) nounwind {
; SSE2-LABEL: mstore256_to_mstore128:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[2,3,2,3]
; SSE2-NEXT:    movd %xmm0, 8(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore256_to_mstore128:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movss %xmm0, (%rdi)
; SSE42-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE42-NEXT:    retq
;
; AVX1OR2-LABEL: mstore256_to_mstore128:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovddup {{.*#+}} xmm1 = [4294967295,0,4294967295,0]
; AVX1OR2-NEXT:    ## xmm1 = mem[0,0]
; AVX1OR2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mstore256_to_mstore128:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    movw $5, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mstore256_to_mstore128:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $5, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps %xmm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mstore256_to_mstore128:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $5, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps %xmm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mstore256_to_mstore128:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $5, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps %xmm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <4 x i64> %v to <8 x float>
  tail call void @llvm.masked.store.v8f32.p0(<8 x float> %tmp, ptr %p, i32 32, <8 x i1> <i1 true, i1 false, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @mstore512_to_mstore256(ptr %p, <8 x i64> %v) nounwind {
; SSE2-LABEL: mstore512_to_mstore256:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    pshufd {{.*#+}} xmm2 = xmm0[2,3,2,3]
; SSE2-NEXT:    movd %xmm2, 8(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 12(%rdi)
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore512_to_mstore256:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    extractps $2, %xmm0, 8(%rdi)
; SSE42-NEXT:    extractps $3, %xmm0, 12(%rdi)
; SSE42-NEXT:    movss %xmm1, 16(%rdi)
; SSE42-NEXT:    retq
;
; AVX1OR2-LABEL: mstore512_to_mstore256:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovaps {{.*#+}} ymm1 = [0,0,4294967295,4294967295,4294967295,0,0,0]
; AVX1OR2-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mstore512_to_mstore256:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    movw $28, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovaps %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mstore512_to_mstore256:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $28, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps %ymm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mstore512_to_mstore256:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $28, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps %ymm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mstore512_to_mstore256:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $28, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <8 x i64> %v to <16 x float>
  tail call void @llvm.masked.store.v16f32.p0(<16 x float> %tmp, ptr %p, i32 64, <16 x i1> <i1 false, i1 false, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false, i1 false>)
  ret void
}

define void @mstore256_fail_no_possible_shrink(ptr %p, <4 x i64> %v) nounwind {
; SSE2-LABEL: mstore256_fail_no_possible_shrink:
; SSE2:       ## %bb.0:
; SSE2-NEXT:    movd %xmm0, (%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm2 = xmm0[1,1,1,1]
; SSE2-NEXT:    movd %xmm2, 4(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm2 = xmm0[2,3,2,3]
; SSE2-NEXT:    movd %xmm2, 8(%rdi)
; SSE2-NEXT:    pshufd {{.*#+}} xmm0 = xmm0[3,3,3,3]
; SSE2-NEXT:    movd %xmm0, 12(%rdi)
; SSE2-NEXT:    movss %xmm1, 16(%rdi)
; SSE2-NEXT:    retq
;
; SSE42-LABEL: mstore256_fail_no_possible_shrink:
; SSE42:       ## %bb.0:
; SSE42-NEXT:    movups %xmm0, (%rdi)
; SSE42-NEXT:    movss %xmm1, 16(%rdi)
; SSE42-NEXT:    retq
;
; AVX1OR2-LABEL: mstore256_fail_no_possible_shrink:
; AVX1OR2:       ## %bb.0:
; AVX1OR2-NEXT:    vmovaps {{.*#+}} ymm1 = [4294967295,4294967295,4294967295,4294967295,4294967295,0,0,0]
; AVX1OR2-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX1OR2-NEXT:    vzeroupper
; AVX1OR2-NEXT:    retq
;
; AVX512F-LABEL: mstore256_fail_no_possible_shrink:
; AVX512F:       ## %bb.0:
; AVX512F-NEXT:    ## kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512F-NEXT:    movw $31, %ax
; AVX512F-NEXT:    kmovw %eax, %k1
; AVX512F-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512F-NEXT:    vzeroupper
; AVX512F-NEXT:    retq
;
; AVX512VLDQ-LABEL: mstore256_fail_no_possible_shrink:
; AVX512VLDQ:       ## %bb.0:
; AVX512VLDQ-NEXT:    movb $31, %al
; AVX512VLDQ-NEXT:    kmovw %eax, %k1
; AVX512VLDQ-NEXT:    vmovaps %ymm0, (%rdi) {%k1}
; AVX512VLDQ-NEXT:    vzeroupper
; AVX512VLDQ-NEXT:    retq
;
; AVX512VLBW-LABEL: mstore256_fail_no_possible_shrink:
; AVX512VLBW:       ## %bb.0:
; AVX512VLBW-NEXT:    movb $31, %al
; AVX512VLBW-NEXT:    kmovd %eax, %k1
; AVX512VLBW-NEXT:    vmovaps %ymm0, (%rdi) {%k1}
; AVX512VLBW-NEXT:    vzeroupper
; AVX512VLBW-NEXT:    retq
;
; X86-AVX512-LABEL: mstore256_fail_no_possible_shrink:
; X86-AVX512:       ## %bb.0:
; X86-AVX512-NEXT:    movl {{[0-9]+}}(%esp), %eax
; X86-AVX512-NEXT:    movb $31, %cl
; X86-AVX512-NEXT:    kmovd %ecx, %k1
; X86-AVX512-NEXT:    vmovaps %ymm0, (%eax) {%k1}
; X86-AVX512-NEXT:    vzeroupper
; X86-AVX512-NEXT:    retl
  %tmp = bitcast <4 x i64> %v to <8 x float>
  tail call void @llvm.masked.store.v8f32.p0(<8 x float> %tmp, ptr %p, i32 32, <8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 false, i1 false, i1 false>)
  ret void
}

declare <8 x float> @llvm.masked.load.v8f32.p0(ptr, i32 immarg, <8 x i1>, <8 x float>)

declare <32 x i16> @llvm.masked.load.v32i16.p0(ptr, i32 immarg, <32 x i1>, <32 x i16>)

declare <16 x float> @llvm.masked.load.v16f32.p0(ptr, i32 immarg, <16 x i1>, <16 x float>)

declare <4 x float> @llvm.masked.load.v4f32.p0(ptr, i32 immarg, <4 x i1>, <4 x float>)

declare void @llvm.masked.store.v8f32.p0(<8 x float>, ptr, i32 immarg, <8 x i1>)

declare void @llvm.masked.store.v32i16.p0(<32 x i16>, ptr, i32 immarg, <32 x i1>)

declare void @llvm.masked.store.v16f32.p0(<16 x float>, ptr, i32 immarg, <16 x i1>)
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; AVX1: {{.*}}
; AVX2: {{.*}}
; AVX512VL: {{.*}}
