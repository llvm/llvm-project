; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx     | FileCheck %s -check-prefix=AVX
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx2    | FileCheck %s -check-prefix=AVX2
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512f | FileCheck %s -check-prefix=AVX512

define void @test_masked_store_success_v4i32(<4 x i32> %x, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v4i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v4i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmaskmovd %xmm0, %xmm1, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v4i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovdqu32 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <4 x i32>, ptr %ptr, align 32
  %sel = select <4 x i1> %mask, <4 x i32> %x, <4 x i32> %load
  store <4 x i32> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v4i64(<4 x i64> %x, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v4i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm2
; AVX-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v4i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxdq %xmm1, %ymm1
; AVX2-NEXT:    vpmaskmovq %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v4i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovdqu64 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <4 x i64>, ptr %ptr, align 32
  %sel = select <4 x i1> %mask, <4 x i64> %x, <4 x i64> %load
  store <4 x i64> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v4f32(<4 x float> %x, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v4f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v4f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vmaskmovps %xmm0, %xmm1, (%rdi)
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v4f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <4 x float>, ptr %ptr, align 32
  %sel = select <4 x i1> %mask, <4 x float> %x, <4 x float> %load
  store <4 x float> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v4f64(<4 x double> %x, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v4f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm2
; AVX-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v4f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxdq %xmm1, %ymm1
; AVX2-NEXT:    vmaskmovpd %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v4f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovupd %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <4 x double>, ptr %ptr, align 32
  %sel = select <4 x i1> %mask, <4 x double> %x, <4 x double> %load
  store <4 x double> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v8i32(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v8i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vpmaskmovd %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v8i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqu32 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v8i64(<8 x i64> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v8i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm3
; AVX-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm2 = xmm2[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm2
; AVX-NEXT:    vinsertf128 $1, %xmm2, %ymm4, %ymm2
; AVX-NEXT:    vmaskmovpd %ymm1, %ymm3, 32(%rdi)
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm2, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v8i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX2-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX2-NEXT:    vpmovsxdq %xmm3, %ymm3
; AVX2-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX2-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX2-NEXT:    vpmovsxdq %xmm2, %ymm2
; AVX2-NEXT:    vpmaskmovq %ymm1, %ymm3, 32(%rdi)
; AVX2-NEXT:    vpmaskmovq %ymm0, %ymm2, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqu64 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i64>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i64> %x, <8 x i64> %load
  store <8 x i64> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v8f32(<8 x float> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v8f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v8f32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vmaskmovps %ymm0, %ymm1, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v8f32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovups %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x float>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x float> %x, <8 x float> %load
  store <8 x float> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_success_v8f64(<8 x double> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_success_v8f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm3
; AVX-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm2 = xmm2[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm2
; AVX-NEXT:    vinsertf128 $1, %xmm2, %ymm4, %ymm2
; AVX-NEXT:    vmaskmovpd %ymm1, %ymm3, 32(%rdi)
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm2, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_success_v8f64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX2-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX2-NEXT:    vpmovsxdq %xmm3, %ymm3
; AVX2-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX2-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX2-NEXT:    vpmovsxdq %xmm2, %ymm2
; AVX2-NEXT:    vmaskmovpd %ymm1, %ymm3, 32(%rdi)
; AVX2-NEXT:    vmaskmovpd %ymm0, %ymm2, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_success_v8f64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovupd %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x double>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x double> %x, <8 x double> %load
  store <8 x double> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_volatile_load(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_volatile_load:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmovaps (%rdi), %ymm2
; AVX-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovaps %ymm0, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_volatile_load:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vmovaps (%rdi), %ymm2
; AVX2-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovaps %ymm0, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_volatile_load:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqa (%rdi), %ymm1
; AVX512-NEXT:    vmovdqa32 %zmm0, %zmm1 {%k1}
; AVX512-NEXT:    vmovdqa %ymm1, (%rdi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load volatile <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

define void @test_masked_store_volatile_store(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_volatile_store:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmovaps (%rdi), %ymm2
; AVX-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovaps %ymm0, (%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_volatile_store:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vmovaps (%rdi), %ymm2
; AVX2-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovaps %ymm0, (%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_volatile_store:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqa (%rdi), %ymm1
; AVX512-NEXT:    vmovdqa32 %zmm0, %zmm1 {%k1}
; AVX512-NEXT:    vmovdqa %ymm1, (%rdi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i32>, ptr %ptr, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store volatile <8 x i32> %sel, ptr %ptr, align 32
  ret void
}

declare void @use_vec(<8 x i32>)

define void @test_masked_store_intervening(<8 x i32> %x, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_intervening:
; AVX:       # %bb.0:
; AVX-NEXT:    pushq %rbx
; AVX-NEXT:    .cfi_def_cfa_offset 16
; AVX-NEXT:    subq $32, %rsp
; AVX-NEXT:    .cfi_def_cfa_offset 48
; AVX-NEXT:    .cfi_offset %rbx, -16
; AVX-NEXT:    movq %rdi, %rbx
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmovaps (%rdi), %ymm2
; AVX-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX-NEXT:    vmovups %ymm0, (%rsp) # 32-byte Spill
; AVX-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; AVX-NEXT:    vmovaps %ymm0, (%rdi)
; AVX-NEXT:    callq use_vec@PLT
; AVX-NEXT:    vmovups (%rsp), %ymm0 # 32-byte Reload
; AVX-NEXT:    vmovaps %ymm0, (%rbx)
; AVX-NEXT:    addq $32, %rsp
; AVX-NEXT:    .cfi_def_cfa_offset 16
; AVX-NEXT:    popq %rbx
; AVX-NEXT:    .cfi_def_cfa_offset 8
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_intervening:
; AVX2:       # %bb.0:
; AVX2-NEXT:    pushq %rbx
; AVX2-NEXT:    .cfi_def_cfa_offset 16
; AVX2-NEXT:    subq $32, %rsp
; AVX2-NEXT:    .cfi_def_cfa_offset 48
; AVX2-NEXT:    .cfi_offset %rbx, -16
; AVX2-NEXT:    movq %rdi, %rbx
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vmovaps (%rdi), %ymm2
; AVX2-NEXT:    vblendvps %ymm1, %ymm0, %ymm2, %ymm0
; AVX2-NEXT:    vmovups %ymm0, (%rsp) # 32-byte Spill
; AVX2-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; AVX2-NEXT:    vmovaps %ymm0, (%rdi)
; AVX2-NEXT:    callq use_vec@PLT
; AVX2-NEXT:    vmovups (%rsp), %ymm0 # 32-byte Reload
; AVX2-NEXT:    vmovaps %ymm0, (%rbx)
; AVX2-NEXT:    addq $32, %rsp
; AVX2-NEXT:    .cfi_def_cfa_offset 16
; AVX2-NEXT:    popq %rbx
; AVX2-NEXT:    .cfi_def_cfa_offset 8
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_intervening:
; AVX512:       # %bb.0:
; AVX512-NEXT:    pushq %rbx
; AVX512-NEXT:    .cfi_def_cfa_offset 16
; AVX512-NEXT:    subq $144, %rsp
; AVX512-NEXT:    .cfi_def_cfa_offset 160
; AVX512-NEXT:    .cfi_offset %rbx, -16
; AVX512-NEXT:    movq %rdi, %rbx
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm0
; AVX512-NEXT:    vpsllq $63, %zmm0, %zmm0
; AVX512-NEXT:    vptestmq %zmm0, %zmm0, %k1
; AVX512-NEXT:    kmovw %k1, {{[-0-9]+}}(%r{{[sb]}}p) # 2-byte Spill
; AVX512-NEXT:    vmovaps (%rdi), %ymm0
; AVX512-NEXT:    vmovups %zmm0, {{[-0-9]+}}(%r{{[sb]}}p) # 64-byte Spill
; AVX512-NEXT:    vxorps %xmm0, %xmm0, %xmm0
; AVX512-NEXT:    vmovaps %ymm0, (%rdi)
; AVX512-NEXT:    callq use_vec@PLT
; AVX512-NEXT:    vmovdqu64 {{[-0-9]+}}(%r{{[sb]}}p), %zmm0 # 64-byte Reload
; AVX512-NEXT:    vmovdqu64 {{[-0-9]+}}(%r{{[sb]}}p), %zmm1 # 64-byte Reload
; AVX512-NEXT:    kmovw {{[-0-9]+}}(%r{{[sb]}}p), %k1 # 2-byte Reload
; AVX512-NEXT:    vmovdqa32 %zmm0, %zmm1 {%k1}
; AVX512-NEXT:    vmovdqa %ymm1, (%rbx)
; AVX512-NEXT:    addq $144, %rsp
; AVX512-NEXT:    .cfi_def_cfa_offset 16
; AVX512-NEXT:    popq %rbx
; AVX512-NEXT:    .cfi_def_cfa_offset 8
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i32>, ptr %ptr, align 32
  store <8 x i32> zeroinitializer, ptr %ptr, align 32
  %tmp = load <8 x i32>, ptr %ptr
  call void @use_vec(<8 x i32> %tmp)
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr, align 32
  ret void
}


define void @test_masked_store_multiple_v8i32(<8 x i32> %x, <8 x i32> %y, ptr %ptr1, ptr %ptr2, <8 x i1> %mask, <8 x i1> %mask2) {
; AVX-LABEL: test_masked_store_multiple_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm4 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm2 = xmm2[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vinsertf128 $1, %xmm2, %ymm4, %ymm2
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm4 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero
; AVX-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm3[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX-NEXT:    vmovaps (%rsi), %ymm4
; AVX-NEXT:    vblendvps %ymm3, %ymm1, %ymm4, %ymm1
; AVX-NEXT:    vmaskmovps %ymm0, %ymm2, (%rdi)
; AVX-NEXT:    vmovaps %ymm1, (%rsi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_multiple_v8i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero,xmm2[4],zero,xmm2[5],zero,xmm2[6],zero,xmm2[7],zero
; AVX2-NEXT:    vpslld $31, %ymm2, %ymm2
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm3 = xmm3[0],zero,xmm3[1],zero,xmm3[2],zero,xmm3[3],zero,xmm3[4],zero,xmm3[5],zero,xmm3[6],zero,xmm3[7],zero
; AVX2-NEXT:    vpslld $31, %ymm3, %ymm3
; AVX2-NEXT:    vmovaps (%rsi), %ymm4
; AVX2-NEXT:    vblendvps %ymm3, %ymm1, %ymm4, %ymm1
; AVX2-NEXT:    vpmaskmovd %ymm0, %ymm2, (%rdi)
; AVX2-NEXT:    vmovaps %ymm1, (%rsi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_multiple_v8i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm1 killed $ymm1 def $zmm1
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm2, %zmm2
; AVX512-NEXT:    vpsllq $63, %zmm2, %zmm2
; AVX512-NEXT:    vptestmq %zmm2, %zmm2, %k1
; AVX512-NEXT:    vpmovsxwq %xmm3, %zmm2
; AVX512-NEXT:    vpsllq $63, %zmm2, %zmm2
; AVX512-NEXT:    vptestmq %zmm2, %zmm2, %k2
; AVX512-NEXT:    vmovdqa (%rsi), %ymm2
; AVX512-NEXT:    vmovdqa32 %zmm1, %zmm2 {%k2}
; AVX512-NEXT:    vmovdqu32 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vmovdqa %ymm2, (%rsi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i32>, ptr %ptr1, align 32
  %load2 = load <8 x i32>, ptr %ptr2, align 32
  %sel = select <8 x i1> %mask, <8 x i32> %x, <8 x i32> %load
  %sel2 = select <8 x i1> %mask2, <8 x i32> %y, <8 x i32> %load2
  store <8 x i32> %sel, ptr %ptr1, align 32
  store <8 x i32> %sel2, ptr %ptr2, align 32
  ret void
}

define void @test_masked_store_multiple_v8i64(<8 x i64> %x, <8 x i64> %y, ptr %ptr1, ptr %ptr2, <8 x i1> %mask, <8 x i1> %mask2) {
; AVX-LABEL: test_masked_store_multiple_v8i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vmovapd (%rsi), %ymm6
; AVX-NEXT:    vmovapd 32(%rsi), %ymm7
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm8 = xmm4[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm8, %xmm8
; AVX-NEXT:    vpmovsxdq %xmm8, %xmm9
; AVX-NEXT:    vpshufd {{.*#+}} xmm8 = xmm8[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm8, %xmm8
; AVX-NEXT:    vinsertf128 $1, %xmm8, %ymm9, %ymm8
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
; AVX-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX-NEXT:    vpmovsxdq %xmm4, %xmm9
; AVX-NEXT:    vpshufd {{.*#+}} xmm4 = xmm4[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm4, %xmm4
; AVX-NEXT:    vinsertf128 $1, %xmm4, %ymm9, %ymm4
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm9 = xmm5[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm9, %xmm9
; AVX-NEXT:    vpmovsxdq %xmm9, %xmm10
; AVX-NEXT:    vpshufd {{.*#+}} xmm9 = xmm9[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm9, %xmm9
; AVX-NEXT:    vinsertf128 $1, %xmm9, %ymm10, %ymm9
; AVX-NEXT:    vblendvpd %ymm9, %ymm3, %ymm7, %ymm3
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm5 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
; AVX-NEXT:    vpslld $31, %xmm5, %xmm5
; AVX-NEXT:    vpmovsxdq %xmm5, %xmm7
; AVX-NEXT:    vpshufd {{.*#+}} xmm5 = xmm5[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm5, %xmm5
; AVX-NEXT:    vinsertf128 $1, %xmm5, %ymm7, %ymm5
; AVX-NEXT:    vblendvpd %ymm5, %ymm2, %ymm6, %ymm2
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm4, (%rdi)
; AVX-NEXT:    vmaskmovpd %ymm1, %ymm8, 32(%rdi)
; AVX-NEXT:    vmovapd %ymm3, 32(%rsi)
; AVX-NEXT:    vmovapd %ymm2, (%rsi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_multiple_v8i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vmovapd (%rsi), %ymm6
; AVX2-NEXT:    vmovapd 32(%rsi), %ymm7
; AVX2-NEXT:    vpunpckhwd {{.*#+}} xmm8 = xmm4[4,4,5,5,6,6,7,7]
; AVX2-NEXT:    vpslld $31, %xmm8, %xmm8
; AVX2-NEXT:    vpmovsxdq %xmm8, %ymm8
; AVX2-NEXT:    vpmovzxwd {{.*#+}} xmm4 = xmm4[0],zero,xmm4[1],zero,xmm4[2],zero,xmm4[3],zero
; AVX2-NEXT:    vpslld $31, %xmm4, %xmm4
; AVX2-NEXT:    vpmovsxdq %xmm4, %ymm4
; AVX2-NEXT:    vpmovzxwd {{.*#+}} xmm9 = xmm5[0],zero,xmm5[1],zero,xmm5[2],zero,xmm5[3],zero
; AVX2-NEXT:    vpslld $31, %xmm9, %xmm9
; AVX2-NEXT:    vpmovsxdq %xmm9, %ymm9
; AVX2-NEXT:    vblendvpd %ymm9, %ymm2, %ymm6, %ymm2
; AVX2-NEXT:    vpunpckhwd {{.*#+}} xmm5 = xmm5[4,4,5,5,6,6,7,7]
; AVX2-NEXT:    vpslld $31, %xmm5, %xmm5
; AVX2-NEXT:    vpmovsxdq %xmm5, %ymm5
; AVX2-NEXT:    vblendvpd %ymm5, %ymm3, %ymm7, %ymm3
; AVX2-NEXT:    vpmaskmovq %ymm0, %ymm4, (%rdi)
; AVX2-NEXT:    vpmaskmovq %ymm1, %ymm8, 32(%rdi)
; AVX2-NEXT:    vmovapd %ymm3, 32(%rsi)
; AVX2-NEXT:    vmovapd %ymm2, (%rsi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_multiple_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmovsxwq %xmm2, %zmm2
; AVX512-NEXT:    vpsllq $63, %zmm2, %zmm2
; AVX512-NEXT:    vptestmq %zmm2, %zmm2, %k1
; AVX512-NEXT:    vpmovsxwq %xmm3, %zmm2
; AVX512-NEXT:    vpsllq $63, %zmm2, %zmm2
; AVX512-NEXT:    vptestmq %zmm2, %zmm2, %k2
; AVX512-NEXT:    vmovdqu64 (%rsi), %zmm2
; AVX512-NEXT:    vmovdqa64 %zmm1, %zmm2 {%k2}
; AVX512-NEXT:    vmovdqu64 %zmm0, (%rdi) {%k1}
; AVX512-NEXT:    vmovdqu64 %zmm2, (%rsi)
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %load = load <8 x i64>, ptr %ptr1, align 32
  %load2 = load <8 x i64>, ptr %ptr2, align 32
  %sel = select <8 x i1> %mask, <8 x i64> %x, <8 x i64> %load
  %sel2 = select <8 x i1> %mask2, <8 x i64> %y, <8 x i64> %load2
  store <8 x i64> %sel, ptr %ptr1, align 32
  store <8 x i64> %sel2, ptr %ptr2, align 32
  ret void
}

define void @test_masked_store_unaligned_v4i32(<4 x i32> %data, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_unaligned_v4i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vmaskmovps %xmm0, %xmm1, 1(%rdi)
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_unaligned_v4i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmaskmovd %xmm0, %xmm1, 1(%rdi)
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_unaligned_v4i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $xmm0 killed $xmm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovdqu32 %zmm0, 1(%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ptr_i8 = getelementptr i8, ptr %ptr, i32 1
  %ptr_vec = bitcast ptr %ptr_i8 to ptr
  %load = load <4 x i32>, ptr %ptr_vec, align 1
  %sel = select <4 x i1> %mask, <4 x i32> %data, <4 x i32> %load
  store <4 x i32> %sel, ptr %ptr_vec, align 1
  ret void
}

define void @test_masked_store_unaligned_v4i64(<4 x i64> %data, ptr %ptr, <4 x i1> %mask) {
; AVX-LABEL: test_masked_store_unaligned_v4i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm2
; AVX-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm1, 1(%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_unaligned_v4i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX2-NEXT:    vpmovsxdq %xmm1, %ymm1
; AVX2-NEXT:    vpmaskmovq %ymm0, %ymm1, 1(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_unaligned_v4i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX512-NEXT:    vptestmd %zmm1, %zmm1, %k0
; AVX512-NEXT:    kshiftlw $12, %k0, %k0
; AVX512-NEXT:    kshiftrw $12, %k0, %k1
; AVX512-NEXT:    vmovdqu64 %zmm0, 1(%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ptr_i8 = getelementptr i8, ptr %ptr, i64 1
  %ptr_vec = bitcast ptr %ptr_i8 to ptr
  %load = load <4 x i64>, ptr %ptr_vec, align 1
  %sel = select <4 x i1> %mask, <4 x i64> %data, <4 x i64> %load
  store <4 x i64> %sel, ptr %ptr_vec, align 1
  ret void
}

define void @test_masked_store_unaligned_v8i32(<8 x i32> %data, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_unaligned_v8i32:
; AVX:       # %bb.0:
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm1 = xmm1[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm1, %xmm1
; AVX-NEXT:    vinsertf128 $1, %xmm1, %ymm2, %ymm1
; AVX-NEXT:    vmaskmovps %ymm0, %ymm1, 1(%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_unaligned_v8i32:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpmovzxwd {{.*#+}} ymm1 = xmm1[0],zero,xmm1[1],zero,xmm1[2],zero,xmm1[3],zero,xmm1[4],zero,xmm1[5],zero,xmm1[6],zero,xmm1[7],zero
; AVX2-NEXT:    vpslld $31, %ymm1, %ymm1
; AVX2-NEXT:    vpmaskmovd %ymm0, %ymm1, 1(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_unaligned_v8i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    # kill: def $ymm0 killed $ymm0 def $zmm0
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqu32 %zmm0, 1(%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ptr_i8 = getelementptr i8, ptr %ptr, i32 1
  %ptr_vec = bitcast ptr %ptr_i8 to ptr
  %load = load <8 x i32>, ptr %ptr_vec, align 1
  %sel = select <8 x i1> %mask, <8 x i32> %data, <8 x i32> %load
  store <8 x i32> %sel, ptr %ptr_vec, align 1
  ret void
}

define void @test_masked_store_unaligned_v8i64(<8 x i64> %data, ptr %ptr, <8 x i1> %mask) {
; AVX-LABEL: test_masked_store_unaligned_v8i64:
; AVX:       # %bb.0:
; AVX-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm3, %xmm3
; AVX-NEXT:    vinsertf128 $1, %xmm3, %ymm4, %ymm3
; AVX-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm4
; AVX-NEXT:    vpshufd {{.*#+}} xmm2 = xmm2[2,3,2,3]
; AVX-NEXT:    vpmovsxdq %xmm2, %xmm2
; AVX-NEXT:    vinsertf128 $1, %xmm2, %ymm4, %ymm2
; AVX-NEXT:    vmaskmovpd %ymm1, %ymm3, 33(%rdi)
; AVX-NEXT:    vmaskmovpd %ymm0, %ymm2, 1(%rdi)
; AVX-NEXT:    vzeroupper
; AVX-NEXT:    retq
;
; AVX2-LABEL: test_masked_store_unaligned_v8i64:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vpunpckhwd {{.*#+}} xmm3 = xmm2[4,4,5,5,6,6,7,7]
; AVX2-NEXT:    vpslld $31, %xmm3, %xmm3
; AVX2-NEXT:    vpmovsxdq %xmm3, %ymm3
; AVX2-NEXT:    vpmovzxwd {{.*#+}} xmm2 = xmm2[0],zero,xmm2[1],zero,xmm2[2],zero,xmm2[3],zero
; AVX2-NEXT:    vpslld $31, %xmm2, %xmm2
; AVX2-NEXT:    vpmovsxdq %xmm2, %ymm2
; AVX2-NEXT:    vpmaskmovq %ymm1, %ymm3, 33(%rdi)
; AVX2-NEXT:    vpmaskmovq %ymm0, %ymm2, 1(%rdi)
; AVX2-NEXT:    vzeroupper
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_masked_store_unaligned_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmovsxwq %xmm1, %zmm1
; AVX512-NEXT:    vpsllq $63, %zmm1, %zmm1
; AVX512-NEXT:    vptestmq %zmm1, %zmm1, %k1
; AVX512-NEXT:    vmovdqu64 %zmm0, 1(%rdi) {%k1}
; AVX512-NEXT:    vzeroupper
; AVX512-NEXT:    retq
  %ptr_i8 = getelementptr i8, ptr %ptr, i64 1
  %ptr_vec = bitcast ptr %ptr_i8 to ptr
  %load = load <8 x i64>, ptr %ptr_vec, align 1
  %sel = select <8 x i1> %mask, <8 x i64> %data, <8 x i64> %load
  store <8 x i64> %sel, ptr %ptr_vec, align 1
  ret void
}
