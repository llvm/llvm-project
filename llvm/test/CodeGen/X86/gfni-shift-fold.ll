; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=+gfni | FileCheck %s --check-prefixes=GFNI
; RUN: llc < %s -mtriple=x86_64-- -mattr=+gfni,+avx2 | FileCheck %s --check-prefixes=AVX2
; RUN: llc < %s -mtriple=x86_64-- -mattr=+gfni,+avx512bw | FileCheck %s --check-prefixes=AVX512

; Test that shift operations on gf2p8affineqb results are folded
; into the matrix transformation.

;
; 128-bit tests
;

define <16 x i8> @test_shl1_v16i8(<16 x i8> %src) {
; GFNI-LABEL: test_shl1_v16i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    gf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_v16i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_v16i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX512-NEXT:    retq
  %1 = call <16 x i8> @llvm.x86.vgf2p8affineqb.128(<16 x i8> %src,
       <16 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 0)
  %2 = shl <16 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <16 x i8> %2
}

define <16 x i8> @test_shl2_v16i8(<16 x i8> %src) {
; GFNI-LABEL: test_shl2_v16i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    gf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl2_v16i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl2_v16i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX512-NEXT:    retq
  %1 = call <16 x i8> @llvm.x86.vgf2p8affineqb.128(<16 x i8> %src,
       <16 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 0)
  %2 = shl <16 x i8> %1, <i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2,
                          i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2, i8 2>
  ret <16 x i8> %2
}

define <16 x i8> @test_shl1_nonzero_imm_v16i8(<16 x i8> %src) {
; GFNI-LABEL: test_shl1_nonzero_imm_v16i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    gf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_nonzero_imm_v16i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_nonzero_imm_v16i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm0, %xmm0
; AVX512-NEXT:    retq
  %1 = call <16 x i8> @llvm.x86.vgf2p8affineqb.128(<16 x i8> %src,
       <16 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 1)
  %2 = shl <16 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <16 x i8> %2
}

;
; 256-bit tests (require avx2)
;

define <32 x i8> @test_shl1_v32i8(<32 x i8> %src) #1 {
; GFNI-LABEL: test_shl1_v32i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_v32i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_v32i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; AVX512-NEXT:    retq
  %1 = call <32 x i8> @llvm.x86.vgf2p8affineqb.256(<32 x i8> %src,
       <32 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 0)
  %2 = shl <32 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <32 x i8> %2
}

define <32 x i8> @test_shl1_nonzero_imm_v32i8(<32 x i8> %src) #1 {
; GFNI-LABEL: test_shl1_nonzero_imm_v32i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_nonzero_imm_v32i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_nonzero_imm_v32i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %ymm0, %ymm0
; AVX512-NEXT:    retq
  %1 = call <32 x i8> @llvm.x86.vgf2p8affineqb.256(<32 x i8> %src,
       <32 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 1)
  %2 = shl <32 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <32 x i8> %2
}

;
; 512-bit tests (require avx512bw)
;

define <64 x i8> @test_shl1_v64i8(<64 x i8> %src) #0 {
; GFNI-LABEL: test_shl1_v64i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_v64i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_v64i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $0, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %1 = call <64 x i8> @llvm.x86.vgf2p8affineqb.512(<64 x i8> %src,
       <64 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 0)
  %2 = shl <64 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <64 x i8> %2
}

define <64 x i8> @test_shl1_nonzero_imm_v64i8(<64 x i8> %src) #0 {
; GFNI-LABEL: test_shl1_nonzero_imm_v64i8:
; GFNI:       # %bb.0:
; GFNI-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; GFNI-NEXT:    retq
;
; AVX2-LABEL: test_shl1_nonzero_imm_v64i8:
; AVX2:       # %bb.0:
; AVX2-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX2-NEXT:    retq
;
; AVX512-LABEL: test_shl1_nonzero_imm_v64i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vgf2p8affineqb $2, {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to8}, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %1 = call <64 x i8> @llvm.x86.vgf2p8affineqb.512(<64 x i8> %src,
       <64 x i8> <i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1,
                  i8 -128, i8 64, i8 32, i8 16, i8 8, i8 4, i8 2, i8 1>, i8 1)
  %2 = shl <64 x i8> %1, <i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1,
                          i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1, i8 1>
  ret <64 x i8> %2
}

attributes #0 = { "target-features"="+avx512bw" }
attributes #1 = { "target-features"="+avx2" }

declare <16 x i8> @llvm.x86.vgf2p8affineqb.128(<16 x i8>, <16 x i8>, i8)
declare <32 x i8> @llvm.x86.vgf2p8affineqb.256(<32 x i8>, <32 x i8>, i8)
declare <64 x i8> @llvm.x86.vgf2p8affineqb.512(<64 x i8>, <64 x i8>, i8)
