; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=sse2 | FileCheck %s --check-prefixes=SSE
; RUN: llc < %s -mtriple=x86_64-- -mattr=avx  | FileCheck %s --check-prefixes=AVX

define i1 @olt_ole_and_f32(float %w, float %x, float %y, float %z) {
; SSE-LABEL: olt_ole_and_f32:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomiss %xmm0, %xmm1
; SSE-NEXT:    seta %cl
; SSE-NEXT:    ucomiss %xmm2, %xmm3
; SSE-NEXT:    setae %al
; SSE-NEXT:    andb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: olt_ole_and_f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomiss %xmm0, %xmm1
; AVX-NEXT:    seta %cl
; AVX-NEXT:    vucomiss %xmm2, %xmm3
; AVX-NEXT:    setae %al
; AVX-NEXT:    andb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp olt float %w, %x
  %f2 = fcmp ole float %y, %z
  %r = and i1 %f1, %f2
  ret i1 %r
}

define i1 @oge_oeq_or_f32(float %w, float %x, float %y, float %z) {
; SSE-LABEL: oge_oeq_or_f32:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomiss %xmm1, %xmm0
; SSE-NEXT:    setae %cl
; SSE-NEXT:    ucomiss %xmm3, %xmm2
; SSE-NEXT:    setnp %dl
; SSE-NEXT:    sete %al
; SSE-NEXT:    andb %dl, %al
; SSE-NEXT:    orb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: oge_oeq_or_f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomiss %xmm1, %xmm0
; AVX-NEXT:    setae %cl
; AVX-NEXT:    vucomiss %xmm3, %xmm2
; AVX-NEXT:    setnp %dl
; AVX-NEXT:    sete %al
; AVX-NEXT:    andb %dl, %al
; AVX-NEXT:    orb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp oge float %w, %x
  %f2 = fcmp oeq float %y, %z
  %r = or i1 %f1, %f2
  ret i1 %r
}

define i1 @ord_one_xor_f32(float %w, float %x, float %y, float %z) {
; SSE-LABEL: ord_one_xor_f32:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomiss %xmm1, %xmm0
; SSE-NEXT:    setnp %cl
; SSE-NEXT:    ucomiss %xmm3, %xmm2
; SSE-NEXT:    setne %al
; SSE-NEXT:    xorb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: ord_one_xor_f32:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomiss %xmm1, %xmm0
; AVX-NEXT:    setnp %cl
; AVX-NEXT:    vucomiss %xmm3, %xmm2
; AVX-NEXT:    setne %al
; AVX-NEXT:    xorb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp ord float %w, %x
  %f2 = fcmp one float %y, %z
  %r = xor i1 %f1, %f2
  ret i1 %r
}

define i1 @une_ugt_and_f64(double %w, double %x, double %y, double %z) {
; SSE-LABEL: une_ugt_and_f64:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomisd %xmm1, %xmm0
; SSE-NEXT:    setp %al
; SSE-NEXT:    setne %cl
; SSE-NEXT:    orb %al, %cl
; SSE-NEXT:    ucomisd %xmm2, %xmm3
; SSE-NEXT:    setb %al
; SSE-NEXT:    andb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: une_ugt_and_f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomisd %xmm1, %xmm0
; AVX-NEXT:    setp %al
; AVX-NEXT:    setne %cl
; AVX-NEXT:    orb %al, %cl
; AVX-NEXT:    vucomisd %xmm2, %xmm3
; AVX-NEXT:    setb %al
; AVX-NEXT:    andb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp une double %w, %x
  %f2 = fcmp ugt double %y, %z
  %r = and i1 %f1, %f2
  ret i1 %r
}

define i1 @ult_uge_or_f64(double %w, double %x, double %y, double %z) {
; SSE-LABEL: ult_uge_or_f64:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomisd %xmm1, %xmm0
; SSE-NEXT:    setb %cl
; SSE-NEXT:    ucomisd %xmm2, %xmm3
; SSE-NEXT:    setbe %al
; SSE-NEXT:    orb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: ult_uge_or_f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomisd %xmm1, %xmm0
; AVX-NEXT:    setb %cl
; AVX-NEXT:    vucomisd %xmm2, %xmm3
; AVX-NEXT:    setbe %al
; AVX-NEXT:    orb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp ult double %w, %x
  %f2 = fcmp uge double %y, %z
  %r = or i1 %f1, %f2
  ret i1 %r
}

define i1 @une_uno_xor_f64(double %w, double %x, double %y, double %z) {
; SSE-LABEL: une_uno_xor_f64:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomisd %xmm1, %xmm0
; SSE-NEXT:    setp %al
; SSE-NEXT:    setne %cl
; SSE-NEXT:    orb %al, %cl
; SSE-NEXT:    ucomisd %xmm3, %xmm2
; SSE-NEXT:    setp %al
; SSE-NEXT:    xorb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: une_uno_xor_f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomisd %xmm1, %xmm0
; AVX-NEXT:    setp %al
; AVX-NEXT:    setne %cl
; AVX-NEXT:    orb %al, %cl
; AVX-NEXT:    vucomisd %xmm3, %xmm2
; AVX-NEXT:    setp %al
; AVX-NEXT:    xorb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp une double %w, %x
  %f2 = fcmp uno double %y, %z
  %r = xor i1 %f1, %f2
  ret i1 %r
}

define i1 @olt_olt_and_f32_f64(float %w, float %x, double %y, double %z) {
; SSE-LABEL: olt_olt_and_f32_f64:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomiss %xmm0, %xmm1
; SSE-NEXT:    seta %cl
; SSE-NEXT:    ucomisd %xmm2, %xmm3
; SSE-NEXT:    seta %al
; SSE-NEXT:    andb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: olt_olt_and_f32_f64:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomiss %xmm0, %xmm1
; AVX-NEXT:    seta %cl
; AVX-NEXT:    vucomisd %xmm2, %xmm3
; AVX-NEXT:    seta %al
; AVX-NEXT:    andb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp olt float %w, %x
  %f2 = fcmp olt double %y, %z
  %r = and i1 %f1, %f2
  ret i1 %r
}

define i1 @une_uno_xor_f64_use1(double %w, double %x, double %y, double %z, i1* %p) {
; SSE-LABEL: une_uno_xor_f64_use1:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomisd %xmm1, %xmm0
; SSE-NEXT:    setp %al
; SSE-NEXT:    setne %cl
; SSE-NEXT:    orb %al, %cl
; SSE-NEXT:    movb %cl, (%rdi)
; SSE-NEXT:    ucomisd %xmm3, %xmm2
; SSE-NEXT:    setp %al
; SSE-NEXT:    xorb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: une_uno_xor_f64_use1:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomisd %xmm1, %xmm0
; AVX-NEXT:    setp %al
; AVX-NEXT:    setne %cl
; AVX-NEXT:    orb %al, %cl
; AVX-NEXT:    movb %cl, (%rdi)
; AVX-NEXT:    vucomisd %xmm3, %xmm2
; AVX-NEXT:    setp %al
; AVX-NEXT:    xorb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp une double %w, %x
  store i1 %f1, i1* %p
  %f2 = fcmp uno double %y, %z
  %r = xor i1 %f1, %f2
  ret i1 %r
}

define i1 @une_uno_xor_f64_use2(double %w, double %x, double %y, double %z, i1* %p) {
; SSE-LABEL: une_uno_xor_f64_use2:
; SSE:       # %bb.0:
; SSE-NEXT:    ucomisd %xmm1, %xmm0
; SSE-NEXT:    setp %al
; SSE-NEXT:    setne %cl
; SSE-NEXT:    orb %al, %cl
; SSE-NEXT:    ucomisd %xmm3, %xmm2
; SSE-NEXT:    setp %al
; SSE-NEXT:    setp (%rdi)
; SSE-NEXT:    xorb %cl, %al
; SSE-NEXT:    retq
;
; AVX-LABEL: une_uno_xor_f64_use2:
; AVX:       # %bb.0:
; AVX-NEXT:    vucomisd %xmm1, %xmm0
; AVX-NEXT:    setp %al
; AVX-NEXT:    setne %cl
; AVX-NEXT:    orb %al, %cl
; AVX-NEXT:    vucomisd %xmm3, %xmm2
; AVX-NEXT:    setp %al
; AVX-NEXT:    setp (%rdi)
; AVX-NEXT:    xorb %cl, %al
; AVX-NEXT:    retq
  %f1 = fcmp une double %w, %x
  %f2 = fcmp uno double %y, %z
  store i1 %f2, i1* %p
  %r = xor i1 %f1, %f2
  ret i1 %r
}
