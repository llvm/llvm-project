; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc -mtriple=i686-windows -mattr=+sse2 < %s | FileCheck %s

target datalayout = "e-m:w-p:32:32-i64:64-f80:32-n8:16:32-S32"
target triple = "i686-pc-windows-msvc"

; There is a conflict between lowering the X86 memory intrinsics and the "base"
; register used to address stack locals.  See X86RegisterInfo::hasBaseRegister
; for when this is necessary. Typically, we chose ESI for the base register,
; which all of the X86 string instructions use.

declare void @escape_vla_and_icmp(ptr, i1 zeroext)
declare void @llvm.memcpy.p0.p0.i32(ptr nocapture, ptr nocapture readonly, i32, i1)
declare void @llvm.memset.p0.i32(ptr nocapture, i8, i32, i1)

define i32 @memcpy_novla_vector(ptr %vp0, ptr %a, ptr %b, i32 %n, i1 zeroext %cond) {
; CHECK-LABEL: memcpy_novla_vector:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushl %ebp
; CHECK-NEXT:    movl %esp, %ebp
; CHECK-NEXT:    pushl %esi
; CHECK-NEXT:    andl $-16, %esp
; CHECK-NEXT:    subl $32, %esp
; CHECK-NEXT:    movzbl 24(%ebp), %eax
; CHECK-NEXT:    movl 12(%ebp), %ecx
; CHECK-NEXT:    movl 16(%ebp), %edx
; CHECK-NEXT:    movups 112(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 112(%ecx)
; CHECK-NEXT:    movups 96(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 96(%ecx)
; CHECK-NEXT:    movups 80(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 80(%ecx)
; CHECK-NEXT:    movups 64(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 64(%ecx)
; CHECK-NEXT:    movups 48(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 48(%ecx)
; CHECK-NEXT:    movups 32(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 32(%ecx)
; CHECK-NEXT:    movdqu (%edx), %xmm0
; CHECK-NEXT:    movups 16(%edx), %xmm1
; CHECK-NEXT:    movups %xmm1, 16(%ecx)
; CHECK-NEXT:    movdqu %xmm0, (%ecx)
; CHECK-NEXT:    testb %al, %al
; CHECK-NEXT:    je LBB0_1
; CHECK-NEXT:  # %bb.3: # %spill_vectors
; CHECK-NEXT:    movl 8(%ebp), %eax
; CHECK-NEXT:    movl (%eax), %esi
; CHECK-NEXT:    movdqa 16(%eax), %xmm0
; CHECK-NEXT:    pcmpgtd (%eax), %xmm0
; CHECK-NEXT:    movd %xmm0, %eax
; CHECK-NEXT:    andl $1, %eax
; CHECK-NEXT:    pushl %eax
; CHECK-NEXT:    pushl $0
; CHECK-NEXT:    calll _escape_vla_and_icmp
; CHECK-NEXT:    addl $8, %esp
; CHECK-NEXT:    movl %esi, %eax
; CHECK-NEXT:    jmp LBB0_2
; CHECK-NEXT:  LBB0_1: # %no_vectors
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:  LBB0_2: # %no_vectors
; CHECK-NEXT:    leal -4(%ebp), %esp
; CHECK-NEXT:    popl %esi
; CHECK-NEXT:    popl %ebp
; CHECK-NEXT:    retl
  %foo = alloca <4 x i32>, align 16
  call void @llvm.memcpy.p0.p0.i32(ptr align 4 %a, ptr align 4 %b, i32 128, i1 false)
  br i1 %cond, label %spill_vectors, label %no_vectors

no_vectors:
  ret i32 0

spill_vectors:
  %vp1 = getelementptr <4 x i32>, ptr %vp0, i32 1
  %v0 = load <4 x i32>, ptr %vp0
  %v1 = load <4 x i32>, ptr %vp1
  %vicmp = icmp slt <4 x i32> %v0, %v1
  %icmp = extractelement <4 x i1> %vicmp, i32 0
  call void @escape_vla_and_icmp(ptr null, i1 zeroext %icmp)
  %r = extractelement <4 x i32> %v0, i32 0
  ret i32 %r
}

define i32 @memcpy_vla_vector(ptr %vp0, ptr %a, ptr %b, i32 %n, i1 zeroext %cond) {
; CHECK-LABEL: memcpy_vla_vector:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushl %ebp
; CHECK-NEXT:    movl %esp, %ebp
; CHECK-NEXT:    pushl %edi
; CHECK-NEXT:    pushl %esi
; CHECK-NEXT:    andl $-16, %esp
; CHECK-NEXT:    subl $16, %esp
; CHECK-NEXT:    movl %esp, %esi
; CHECK-NEXT:    movzbl 24(%ebp), %eax
; CHECK-NEXT:    movl 12(%ebp), %ecx
; CHECK-NEXT:    movl 16(%ebp), %edx
; CHECK-NEXT:    movups 112(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 112(%ecx)
; CHECK-NEXT:    movups 96(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 96(%ecx)
; CHECK-NEXT:    movups 80(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 80(%ecx)
; CHECK-NEXT:    movups 64(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 64(%ecx)
; CHECK-NEXT:    movups 48(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 48(%ecx)
; CHECK-NEXT:    movups 32(%edx), %xmm0
; CHECK-NEXT:    movups %xmm0, 32(%ecx)
; CHECK-NEXT:    movdqu (%edx), %xmm0
; CHECK-NEXT:    movups 16(%edx), %xmm1
; CHECK-NEXT:    movups %xmm1, 16(%ecx)
; CHECK-NEXT:    movdqu %xmm0, (%ecx)
; CHECK-NEXT:    testb %al, %al
; CHECK-NEXT:    je LBB1_1
; CHECK-NEXT:  # %bb.3: # %spill_vectors
; CHECK-NEXT:    movl 20(%ebp), %eax
; CHECK-NEXT:    movl 8(%ebp), %ecx
; CHECK-NEXT:    movl (%ecx), %edi
; CHECK-NEXT:    movdqa 16(%ecx), %xmm0
; CHECK-NEXT:    pcmpgtd (%ecx), %xmm0
; CHECK-NEXT:    movd %xmm0, %ecx
; CHECK-NEXT:    addl $3, %eax
; CHECK-NEXT:    andl $-4, %eax
; CHECK-NEXT:    calll __chkstk
; CHECK-NEXT:    movl %esp, %eax
; CHECK-NEXT:    andl $1, %ecx
; CHECK-NEXT:    pushl %ecx
; CHECK-NEXT:    pushl %eax
; CHECK-NEXT:    calll _escape_vla_and_icmp
; CHECK-NEXT:    addl $8, %esp
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    jmp LBB1_2
; CHECK-NEXT:  LBB1_1: # %no_vectors
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:  LBB1_2: # %no_vectors
; CHECK-NEXT:    leal -8(%ebp), %esp
; CHECK-NEXT:    popl %esi
; CHECK-NEXT:    popl %edi
; CHECK-NEXT:    popl %ebp
; CHECK-NEXT:    retl
  %foo = alloca <4 x i32>, align 16
  call void @llvm.memcpy.p0.p0.i32(ptr align 4 %a, ptr align 4 %b, i32 128, i1 false)
  br i1 %cond, label %spill_vectors, label %no_vectors

no_vectors:
  ret i32 0

spill_vectors:
  %vp1 = getelementptr <4 x i32>, ptr %vp0, i32 1
  %v0 = load <4 x i32>, ptr %vp0
  %v1 = load <4 x i32>, ptr %vp1
  %vicmp = icmp slt <4 x i32> %v0, %v1
  %icmp = extractelement <4 x i1> %vicmp, i32 0
  %vla = alloca i8, i32 %n
  call void @escape_vla_and_icmp(ptr %vla, i1 zeroext %icmp)
  %r = extractelement <4 x i32> %v0, i32 0
  ret i32 %r
}


define i32 @memset_vla_vector(ptr %vp0, ptr %a, i32 %n, i1 zeroext %cond) {
; CHECK-LABEL: memset_vla_vector:
; CHECK:       # %bb.0:
; CHECK-NEXT:    pushl %ebp
; CHECK-NEXT:    movl %esp, %ebp
; CHECK-NEXT:    pushl %edi
; CHECK-NEXT:    pushl %esi
; CHECK-NEXT:    andl $-16, %esp
; CHECK-NEXT:    subl $16, %esp
; CHECK-NEXT:    movl %esp, %esi
; CHECK-NEXT:    movzbl 20(%ebp), %eax
; CHECK-NEXT:    movl 12(%ebp), %ecx
; CHECK-NEXT:    movdqa {{.*#+}} xmm0 = [42,42,42,42,42,42,42,42,42,42,42,42,42,42,42,42]
; CHECK-NEXT:    movdqu %xmm0, 112(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 96(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 80(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 64(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 48(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 32(%ecx)
; CHECK-NEXT:    movdqu %xmm0, 16(%ecx)
; CHECK-NEXT:    movdqu %xmm0, (%ecx)
; CHECK-NEXT:    testb %al, %al
; CHECK-NEXT:    je LBB2_1
; CHECK-NEXT:  # %bb.3: # %spill_vectors
; CHECK-NEXT:    movl 16(%ebp), %eax
; CHECK-NEXT:    movl 8(%ebp), %ecx
; CHECK-NEXT:    movl (%ecx), %edi
; CHECK-NEXT:    movdqa 16(%ecx), %xmm0
; CHECK-NEXT:    pcmpgtd (%ecx), %xmm0
; CHECK-NEXT:    movd %xmm0, %ecx
; CHECK-NEXT:    addl $3, %eax
; CHECK-NEXT:    andl $-4, %eax
; CHECK-NEXT:    calll __chkstk
; CHECK-NEXT:    movl %esp, %eax
; CHECK-NEXT:    andl $1, %ecx
; CHECK-NEXT:    pushl %ecx
; CHECK-NEXT:    pushl %eax
; CHECK-NEXT:    calll _escape_vla_and_icmp
; CHECK-NEXT:    addl $8, %esp
; CHECK-NEXT:    movl %edi, %eax
; CHECK-NEXT:    jmp LBB2_2
; CHECK-NEXT:  LBB2_1: # %no_vectors
; CHECK-NEXT:    xorl %eax, %eax
; CHECK-NEXT:  LBB2_2: # %no_vectors
; CHECK-NEXT:    leal -8(%ebp), %esp
; CHECK-NEXT:    popl %esi
; CHECK-NEXT:    popl %edi
; CHECK-NEXT:    popl %ebp
; CHECK-NEXT:    retl
  %foo = alloca <4 x i32>, align 16
  call void @llvm.memset.p0.i32(ptr align 4 %a, i8 42, i32 128, i1 false)
  br i1 %cond, label %spill_vectors, label %no_vectors

no_vectors:
  ret i32 0

spill_vectors:
  %vp1 = getelementptr <4 x i32>, ptr %vp0, i32 1
  %v0 = load <4 x i32>, ptr %vp0
  %v1 = load <4 x i32>, ptr %vp1
  %vicmp = icmp slt <4 x i32> %v0, %v1
  %icmp = extractelement <4 x i1> %vicmp, i32 0
  %vla = alloca i8, i32 %n
  call void @escape_vla_and_icmp(ptr %vla, i1 zeroext %icmp)
  %r = extractelement <4 x i32> %v0, i32 0
  ret i32 %r
}

; Add a test for memcmp if we ever add a special lowering for it.
