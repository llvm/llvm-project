; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512f,+vpclmulqdq | FileCheck %s --check-prefixes=AVX512,AVX512F
; RUN: llc < %s -mtriple=x86_64-- -mattr=+avx512vl,+vpclmulqdq | FileCheck %s --check-prefixes=AVX512,AVX512VL

define <64 x i8> @clmul_v64i8(<64 x i8> %a, <64 x i8> %b) nounwind {
; AVX512-LABEL: clmul_v64i8:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm2
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm3
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm3, %zmm6, %zmm6
; AVX512-NEXT:    vpbroadcastd {{.*#+}} zmm3 = [255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0,255,0]
; AVX512-NEXT:    vpandn %ymm4, %ymm3, %ymm4
; AVX512-NEXT:    vpmaddubsw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vpsllw $8, %ymm4, %ymm4
; AVX512-NEXT:    vpandn %ymm5, %ymm3, %ymm5
; AVX512-NEXT:    vpmaddubsw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpsllw $8, %ymm5, %ymm5
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm4 | (zmm6 & zmm3)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm5, %ymm3, %ymm5
; AVX512-NEXT:    vpmaddubsw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vpsllw $8, %ymm5, %ymm5
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm4 ^ (zmm5 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm4, %ymm3, %ymm4
; AVX512-NEXT:    vpmaddubsw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vpsllw $8, %ymm4, %ymm4
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm5 ^ (zmm4 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm5, %ymm3, %ymm5
; AVX512-NEXT:    vpmaddubsw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vpsllw $8, %ymm5, %ymm5
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm4 ^ (zmm5 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm4, %ymm3, %ymm4
; AVX512-NEXT:    vpmaddubsw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vpsllw $8, %ymm4, %ymm4
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm5 ^ (zmm4 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm5, %ymm3, %ymm5
; AVX512-NEXT:    vpmaddubsw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vpsllw $8, %ymm5, %ymm5
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm4 ^ (zmm5 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm7
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm8
; AVX512-NEXT:    vinserti64x4 $1, %ymm7, %zmm8, %zmm7
; AVX512-NEXT:    vpandq %zmm3, %zmm7, %zmm7
; AVX512-NEXT:    vpandn %ymm4, %ymm3, %ymm4
; AVX512-NEXT:    vpmaddubsw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vpsllw $8, %ymm4, %ymm4
; AVX512-NEXT:    vpandn %ymm6, %ymm3, %ymm6
; AVX512-NEXT:    vpmaddubsw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpsllw $8, %ymm6, %ymm6
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm5 ^ (zmm4 | zmm7)
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm1
; AVX512-NEXT:    vextracti64x4 $1, %zmm1, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm1, %ymm0, %ymm7
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm7, %zmm6
; AVX512-NEXT:    vpandq %zmm3, %zmm6, %zmm6
; AVX512-NEXT:    vpandn %ymm1, %ymm3, %ymm1
; AVX512-NEXT:    vpmaddubsw %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vpsllw $8, %ymm0, %ymm0
; AVX512-NEXT:    vpandn %ymm5, %ymm3, %ymm1
; AVX512-NEXT:    vpmaddubsw %ymm1, %ymm2, %ymm1
; AVX512-NEXT:    vpsllw $8, %ymm1, %ymm1
; AVX512-NEXT:    vinserti64x4 $1, %ymm1, %zmm0, %zmm0
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm0 = zmm4 ^ (zmm0 | zmm6)
; AVX512-NEXT:    retq
  %res = call <64 x i8> @llvm.clmul.v64i8(<64 x i8> %a, <64 x i8> %b)
  ret <64 x i8> %res
}

define <32 x i16> @clmul_v32i16(<32 x i16> %a, <32 x i16> %b) nounwind {
; AVX512-LABEL: clmul_v32i16:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextracti64x4 $1, %zmm0, %ymm2
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm4
; AVX512-NEXT:    vpmullw %ymm4, %ymm2, %ymm4
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm4, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm4, %zmm4
; AVX512-NEXT:    vpxorq %zmm3, %zmm4, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm4, %zmm4
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm5 ^ zmm3 ^ zmm4
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm4
; AVX512-NEXT:    vpmullw %ymm4, %ymm2, %ymm4
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm4, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm4 ^ zmm5 ^ zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm5 ^ zmm4 ^ zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm4
; AVX512-NEXT:    vpmullw %ymm4, %ymm2, %ymm4
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm4, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm4 ^ zmm5 ^ zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm5
; AVX512-NEXT:    vextracti64x4 $1, %zmm5, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm5, %ymm0, %ymm5
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm5, %zmm5
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm5 = zmm5 ^ zmm4 ^ zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm4
; AVX512-NEXT:    vpmullw %ymm4, %ymm2, %ymm4
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm4, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm4
; AVX512-NEXT:    vextracti64x4 $1, %zmm4, %ymm6
; AVX512-NEXT:    vpmullw %ymm6, %ymm2, %ymm6
; AVX512-NEXT:    vpmullw %ymm4, %ymm0, %ymm4
; AVX512-NEXT:    vinserti64x4 $1, %ymm6, %zmm4, %zmm4
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm4 = zmm4 ^ zmm5 ^ zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm3
; AVX512-NEXT:    vextracti64x4 $1, %zmm3, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm5
; AVX512-NEXT:    vpmullw %ymm3, %ymm0, %ymm3
; AVX512-NEXT:    vinserti64x4 $1, %ymm5, %zmm3, %zmm3
; AVX512-NEXT:    vpandd {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to16}, %zmm1, %zmm1
; AVX512-NEXT:    vextracti64x4 $1, %zmm1, %ymm5
; AVX512-NEXT:    vpmullw %ymm5, %ymm2, %ymm2
; AVX512-NEXT:    vpmullw %ymm1, %ymm0, %ymm0
; AVX512-NEXT:    vinserti64x4 $1, %ymm2, %zmm0, %zmm0
; AVX512-NEXT:    vpternlogq {{.*#+}} zmm0 = zmm0 ^ zmm4 ^ zmm3
; AVX512-NEXT:    retq
  %res = call <32 x i16> @llvm.clmul.v32i16(<32 x i16> %a, <32 x i16> %b)
  ret <32 x i16> %res
}

define <16 x i32> @clmul_v16i32(<16 x i32> %a, <16 x i32> %b) nounwind {
; AVX512-LABEL: clmul_v16i32:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vextracti32x4 $3, %zmm1, %xmm2
; AVX512-NEXT:    vextracti32x4 $3, %zmm0, %xmm3
; AVX512-NEXT:    vpclmulqdq $0, %xmm2, %xmm3, %xmm4
; AVX512-NEXT:    vpshufd {{.*#+}} xmm5 = xmm2[1,1,1,1]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm6 = xmm3[1,1,1,1]
; AVX512-NEXT:    vpclmulqdq $0, %xmm5, %xmm6, %xmm5
; AVX512-NEXT:    vpunpckldq {{.*#+}} xmm4 = xmm4[0],xmm5[0],xmm4[1],xmm5[1]
; AVX512-NEXT:    vpclmulqdq $17, %xmm2, %xmm3, %xmm5
; AVX512-NEXT:    vmovq %xmm5, %rax
; AVX512-NEXT:    vpinsrd $2, %eax, %xmm4, %xmm4
; AVX512-NEXT:    vpshufd {{.*#+}} xmm2 = xmm2[3,3,3,3]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vpclmulqdq $0, %xmm2, %xmm3, %xmm2
; AVX512-NEXT:    vmovq %xmm2, %rax
; AVX512-NEXT:    vpinsrd $3, %eax, %xmm4, %xmm2
; AVX512-NEXT:    vextracti32x4 $2, %zmm1, %xmm3
; AVX512-NEXT:    vextracti32x4 $2, %zmm0, %xmm4
; AVX512-NEXT:    vpclmulqdq $0, %xmm3, %xmm4, %xmm5
; AVX512-NEXT:    vpshufd {{.*#+}} xmm6 = xmm3[1,1,1,1]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm7 = xmm4[1,1,1,1]
; AVX512-NEXT:    vpclmulqdq $0, %xmm6, %xmm7, %xmm6
; AVX512-NEXT:    vpunpckldq {{.*#+}} xmm5 = xmm5[0],xmm6[0],xmm5[1],xmm6[1]
; AVX512-NEXT:    vpclmulqdq $17, %xmm3, %xmm4, %xmm6
; AVX512-NEXT:    vmovq %xmm6, %rax
; AVX512-NEXT:    vpinsrd $2, %eax, %xmm5, %xmm5
; AVX512-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm4 = xmm4[3,3,3,3]
; AVX512-NEXT:    vpclmulqdq $0, %xmm3, %xmm4, %xmm3
; AVX512-NEXT:    vmovq %xmm3, %rax
; AVX512-NEXT:    vpinsrd $3, %eax, %xmm5, %xmm3
; AVX512-NEXT:    vinserti128 $1, %xmm2, %ymm3, %ymm2
; AVX512-NEXT:    vextracti128 $1, %ymm1, %xmm3
; AVX512-NEXT:    vextracti128 $1, %ymm0, %xmm4
; AVX512-NEXT:    vpclmulqdq $0, %xmm3, %xmm4, %xmm5
; AVX512-NEXT:    vpshufd {{.*#+}} xmm6 = xmm3[1,1,1,1]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm7 = xmm4[1,1,1,1]
; AVX512-NEXT:    vpclmulqdq $0, %xmm6, %xmm7, %xmm6
; AVX512-NEXT:    vpunpckldq {{.*#+}} xmm5 = xmm5[0],xmm6[0],xmm5[1],xmm6[1]
; AVX512-NEXT:    vpclmulqdq $17, %xmm3, %xmm4, %xmm6
; AVX512-NEXT:    vmovq %xmm6, %rax
; AVX512-NEXT:    vpinsrd $2, %eax, %xmm5, %xmm5
; AVX512-NEXT:    vpshufd {{.*#+}} xmm3 = xmm3[3,3,3,3]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm4 = xmm4[3,3,3,3]
; AVX512-NEXT:    vpclmulqdq $0, %xmm3, %xmm4, %xmm3
; AVX512-NEXT:    vmovq %xmm3, %rax
; AVX512-NEXT:    vpinsrd $3, %eax, %xmm5, %xmm3
; AVX512-NEXT:    vpclmulqdq $0, %xmm1, %xmm0, %xmm4
; AVX512-NEXT:    vpshufd {{.*#+}} xmm5 = xmm1[1,1,1,1]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm6 = xmm0[1,1,1,1]
; AVX512-NEXT:    vpclmulqdq $0, %xmm5, %xmm6, %xmm5
; AVX512-NEXT:    vpunpckldq {{.*#+}} xmm4 = xmm4[0],xmm5[0],xmm4[1],xmm5[1]
; AVX512-NEXT:    vpclmulqdq $17, %xmm1, %xmm0, %xmm5
; AVX512-NEXT:    vmovq %xmm5, %rax
; AVX512-NEXT:    vpinsrd $2, %eax, %xmm4, %xmm4
; AVX512-NEXT:    vpshufd {{.*#+}} xmm1 = xmm1[3,3,3,3]
; AVX512-NEXT:    vpshufd {{.*#+}} xmm0 = xmm0[3,3,3,3]
; AVX512-NEXT:    vpclmulqdq $0, %xmm1, %xmm0, %xmm0
; AVX512-NEXT:    vmovq %xmm0, %rax
; AVX512-NEXT:    vpinsrd $3, %eax, %xmm4, %xmm0
; AVX512-NEXT:    vinserti128 $1, %xmm3, %ymm0, %ymm0
; AVX512-NEXT:    vinserti64x4 $1, %ymm2, %zmm0, %zmm0
; AVX512-NEXT:    retq
  %res = call <16 x i32> @llvm.clmul.v16i32(<16 x i32> %a, <16 x i32> %b)
  ret <16 x i32> %res
}

define <8 x i64> @clmul_v8i64(<8 x i64> %a, <8 x i64> %b) nounwind {
; AVX512-LABEL: clmul_v8i64:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpclmulqdq $17, %zmm1, %zmm0, %zmm2
; AVX512-NEXT:    vpclmulqdq $0, %zmm1, %zmm0, %zmm0
; AVX512-NEXT:    vpunpcklqdq {{.*#+}} zmm0 = zmm0[0],zmm2[0],zmm0[2],zmm2[2],zmm0[4],zmm2[4],zmm0[6],zmm2[6]
; AVX512-NEXT:    retq
  %res = call <8 x i64> @llvm.clmul.v8i64(<8 x i64> %a, <8 x i64> %b)
  ret <8 x i64> %res
}

;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; AVX512F: {{.*}}
; AVX512VL: {{.*}}
