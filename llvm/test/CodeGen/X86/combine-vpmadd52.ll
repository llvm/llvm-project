; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avx512ifma,+avx512vl | FileCheck %s --check-prefixes=CHECK,AVX512
; RUN: llc < %s -mtriple=x86_64-unknown-unknown -mattr=+avxifma | FileCheck %s --check-prefixes=CHECK,AVX

define <2 x i64> @test1_vpmadd52l(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test1_vpmadd52l:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test1_vpmadd52l:
; AVX:       # %bb.0:
; AVX-NEXT:    {vex} vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq

  %and = and <2 x i64> %x1, splat (i64 4503599627370495) ; (1LL << 52) - 1
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52l.uq.128(<2 x i64> %x0, <2 x i64> %and, <2 x i64> %x2)
  ret <2 x i64> %1
}

define <2 x i64> @test2_vpmadd52l(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test2_vpmadd52l:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test2_vpmadd52l:
; AVX:       # %bb.0:
; AVX-NEXT:    {vex} vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq
  %and = and <2 x i64> %x2, splat (i64 4503599627370495) ; (1LL << 52) - 1
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52l.uq.128(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %and)
  ret <2 x i64> %1
}

define <2 x i64> @test3_vpmadd52l(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test3_vpmadd52l:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test3_vpmadd52l:
; AVX:       # %bb.0:
; AVX-NEXT:    {vex} vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq
  %and = and <2 x i64> %x1, splat (i64 4503599627370495) ; (1LL << 52) - 1
  %or = or <2 x i64> %x2, splat (i64 4503599627370496) ; 1LL << 52
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52l.uq.128(<2 x i64> %x0, <2 x i64> %and, <2 x i64> %or)
  ret <2 x i64> %1
}

define <2 x i64> @test_vpmadd52l_wrong_bits(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test_vpmadd52l_wrong_bits:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpandq {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to2}, %xmm1, %xmm1
; AVX512-NEXT:    vporq {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to2}, %xmm2, %xmm2
; AVX512-NEXT:    vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test_vpmadd52l_wrong_bits:
; AVX:       # %bb.0:
; AVX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm1
; AVX-NEXT:    vpor {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm2, %xmm2
; AVX-NEXT:    {vex} vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq
  %and = and <2 x i64> %x1, splat (i64 2251799813685247) ; (1LL << 51) - 1
  %or = or <2 x i64> %x2, splat (i64 2251799813685248) ; 1LL << 51
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52l.uq.128(<2 x i64> %x0, <2 x i64> %and, <2 x i64> %or)
  ret <2 x i64> %1
}

define <2 x i64> @test_vpmadd52l_wrong_op(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test_vpmadd52l_wrong_op:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpandq {{\.?LCPI[0-9]+_[0-9]+}}(%rip){1to2}, %xmm1, %xmm0
; AVX512-NEXT:    vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test_vpmadd52l_wrong_op:
; AVX:       # %bb.0:
; AVX-NEXT:    vpand {{\.?LCPI[0-9]+_[0-9]+}}(%rip), %xmm1, %xmm0
; AVX-NEXT:    {vex} vpmadd52luq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq
  %and = and <2 x i64> %x1, splat (i64 4503599627370495) ; (1LL << 52) - 1
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52l.uq.128(<2 x i64> %and, <2 x i64> %x1, <2 x i64> %x2)
  ret <2 x i64> %1
}

define <2 x i64> @test_vpmadd52h(<2 x i64> %x0, <2 x i64> %x1, <2 x i64> %x2) {
; AVX512-LABEL: test_vpmadd52h:
; AVX512:       # %bb.0:
; AVX512-NEXT:    vpmadd52huq %xmm2, %xmm1, %xmm0
; AVX512-NEXT:    retq
;
; AVX-LABEL: test_vpmadd52h:
; AVX:       # %bb.0:
; AVX-NEXT:    {vex} vpmadd52huq %xmm2, %xmm1, %xmm0
; AVX-NEXT:    retq

  %and = and <2 x i64> %x1, splat (i64 4503599627370495) ; (1LL << 52) - 1
  %or = or <2 x i64> %x2, splat (i64 4503599627370496) ; 1LL << 52
  %1 = call <2 x i64> @llvm.x86.avx512.vpmadd52h.uq.128(<2 x i64> %x0, <2 x i64> %and, <2 x i64> %or)
  ret <2 x i64> %1
}
;; NOTE: These prefixes are unused and the list is autogenerated. Do not add tests below this line:
; CHECK: {{.*}}
