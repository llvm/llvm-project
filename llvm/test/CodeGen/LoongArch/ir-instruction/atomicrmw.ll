; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 < %s | FileCheck %s --check-prefix=LA32
; RUN: llc --mtriple=loongarch64 < %s | FileCheck %s --check-prefix=LA64

define i8 @atomicrmw_xchg_i8_acquire(ptr %a, i8 %b) nounwind {
; LA32-LABEL: atomicrmw_xchg_i8_acquire:
; LA32:       # %bb.0:
; LA32-NEXT:    addi.w $a2, $zero, -4
; LA32-NEXT:    and $a2, $a0, $a2
; LA32-NEXT:    slli.w $a0, $a0, 3
; LA32-NEXT:    ori $a3, $zero, 255
; LA32-NEXT:    sll.w $a3, $a3, $a0
; LA32-NEXT:    andi $a1, $a1, 255
; LA32-NEXT:    sll.w $a1, $a1, $a0
; LA32-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; LA32-NEXT:    dbar 0
; LA32-NEXT:    ll.w $a4, $a2, 0
; LA32-NEXT:    addi.w $a5, $a1, 0
; LA32-NEXT:    xor $a5, $a4, $a5
; LA32-NEXT:    and $a5, $a5, $a3
; LA32-NEXT:    xor $a5, $a4, $a5
; LA32-NEXT:    sc.w $a5, $a2, 0
; LA32-NEXT:    beq $a5, $zero, .LBB0_1
; LA32-NEXT:  # %bb.2:
; LA32-NEXT:    srl.w $a0, $a4, $a0
; LA32-NEXT:    ret
;
; LA64-LABEL: atomicrmw_xchg_i8_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    addi.w $a2, $zero, -4
; LA64-NEXT:    and $a2, $a0, $a2
; LA64-NEXT:    slli.d $a0, $a0, 3
; LA64-NEXT:    ori $a3, $zero, 255
; LA64-NEXT:    sll.w $a3, $a3, $a0
; LA64-NEXT:    addi.w $a3, $a3, 0
; LA64-NEXT:    andi $a1, $a1, 255
; LA64-NEXT:    sll.w $a1, $a1, $a0
; LA64-NEXT:    addi.w $a1, $a1, 0
; LA64-NEXT:  .LBB0_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    dbar 0
; LA64-NEXT:    ll.w $a4, $a2, 0
; LA64-NEXT:    addi.w $a5, $a1, 0
; LA64-NEXT:    xor $a5, $a4, $a5
; LA64-NEXT:    and $a5, $a5, $a3
; LA64-NEXT:    xor $a5, $a4, $a5
; LA64-NEXT:    sc.w $a5, $a2, 0
; LA64-NEXT:    beq $a5, $zero, .LBB0_1
; LA64-NEXT:  # %bb.2:
; LA64-NEXT:    srl.w $a0, $a4, $a0
; LA64-NEXT:    ret
  %1 = atomicrmw xchg ptr %a, i8 %b acquire
  ret i8 %1
}

define i16 @atomicrmw_xchg_i16_acquire(ptr %a, i16 %b) nounwind {
; LA32-LABEL: atomicrmw_xchg_i16_acquire:
; LA32:       # %bb.0:
; LA32-NEXT:    addi.w $a2, $zero, -4
; LA32-NEXT:    and $a2, $a0, $a2
; LA32-NEXT:    slli.w $a0, $a0, 3
; LA32-NEXT:    lu12i.w $a3, 15
; LA32-NEXT:    ori $a3, $a3, 4095
; LA32-NEXT:    sll.w $a3, $a3, $a0
; LA32-NEXT:    bstrpick.w $a1, $a1, 15, 0
; LA32-NEXT:    sll.w $a1, $a1, $a0
; LA32-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; LA32-NEXT:    dbar 0
; LA32-NEXT:    ll.w $a4, $a2, 0
; LA32-NEXT:    addi.w $a5, $a1, 0
; LA32-NEXT:    xor $a5, $a4, $a5
; LA32-NEXT:    and $a5, $a5, $a3
; LA32-NEXT:    xor $a5, $a4, $a5
; LA32-NEXT:    sc.w $a5, $a2, 0
; LA32-NEXT:    beq $a5, $zero, .LBB1_1
; LA32-NEXT:  # %bb.2:
; LA32-NEXT:    srl.w $a0, $a4, $a0
; LA32-NEXT:    ret
;
; LA64-LABEL: atomicrmw_xchg_i16_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    addi.w $a2, $zero, -4
; LA64-NEXT:    and $a2, $a0, $a2
; LA64-NEXT:    slli.d $a0, $a0, 3
; LA64-NEXT:    lu12i.w $a3, 15
; LA64-NEXT:    ori $a3, $a3, 4095
; LA64-NEXT:    sll.w $a3, $a3, $a0
; LA64-NEXT:    addi.w $a3, $a3, 0
; LA64-NEXT:    bstrpick.d $a1, $a1, 15, 0
; LA64-NEXT:    sll.w $a1, $a1, $a0
; LA64-NEXT:    addi.w $a1, $a1, 0
; LA64-NEXT:  .LBB1_1: # =>This Inner Loop Header: Depth=1
; LA64-NEXT:    dbar 0
; LA64-NEXT:    ll.w $a4, $a2, 0
; LA64-NEXT:    addi.w $a5, $a1, 0
; LA64-NEXT:    xor $a5, $a4, $a5
; LA64-NEXT:    and $a5, $a5, $a3
; LA64-NEXT:    xor $a5, $a4, $a5
; LA64-NEXT:    sc.w $a5, $a2, 0
; LA64-NEXT:    beq $a5, $zero, .LBB1_1
; LA64-NEXT:  # %bb.2:
; LA64-NEXT:    srl.w $a0, $a4, $a0
; LA64-NEXT:    ret
  %1 = atomicrmw xchg ptr %a, i16 %b acquire
  ret i16 %1
}

define i32 @atomicrmw_xchg_i32_acquire(ptr %a, i32 %b) nounwind {
; LA32-LABEL: atomicrmw_xchg_i32_acquire:
; LA32:       # %bb.0:
; LA32-NEXT:  .LBB2_1: # =>This Inner Loop Header: Depth=1
; LA32-NEXT:    dbar 0
; LA32-NEXT:    ll.w $a2, $a1, 0
; LA32-NEXT:    move $a3, $a0
; LA32-NEXT:    sc.w $a3, $a1, 0
; LA32-NEXT:    beq $a3, $zero, .LBB2_1
; LA32-NEXT:  # %bb.2:
; LA32-NEXT:    move $a0, $a2
; LA32-NEXT:    ret
;
; LA64-LABEL: atomicrmw_xchg_i32_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    amswap_db.w $a0, $a1, $a0
; LA64-NEXT:    ret
  %1 = atomicrmw xchg ptr %a, i32 %b acquire
  ret i32 %1
}

define i64 @atomicrmw_xchg_i64_acquire(ptr %a, i64 %b) nounwind {
; LA32-LABEL: atomicrmw_xchg_i64_acquire:
; LA32:       # %bb.0:
; LA32-NEXT:    addi.w $sp, $sp, -16
; LA32-NEXT:    st.w $ra, $sp, 12 # 4-byte Folded Spill
; LA32-NEXT:    ori $a3, $zero, 2
; LA32-NEXT:    bl %plt(__atomic_exchange_8)
; LA32-NEXT:    ld.w $ra, $sp, 12 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 16
; LA32-NEXT:    ret
;
; LA64-LABEL: atomicrmw_xchg_i64_acquire:
; LA64:       # %bb.0:
; LA64-NEXT:    amswap_db.d $a0, $a1, $a0
; LA64-NEXT:    ret
  %1 = atomicrmw xchg ptr %a, i64 %b acquire
  ret i64 %1
}
