; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 3
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx --verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx --verify-machineinstrs < %s | FileCheck %s --check-prefixes=CHECK,LA64

define void @isnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 3) strictfp  ; "nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isnot_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isnot_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1020) strictfp  ; 0x3fc = "zero|subnormal|normal|inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issignaling_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issignaling_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvslt.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1) strictfp  ; "snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issignaling_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issignaling_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvslt.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1022) strictfp  ; ~"snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isquiet_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isquiet_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 2) strictfp  ; "qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isquiet_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isquiet_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1021) strictfp  ; ~"qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isinf_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isinf_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 516) strictfp  ; 0x204 = "inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isinf_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isinf_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 507) strictfp  ; ~0x204 = "~inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_inf_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_inf_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 512) strictfp  ; 0x200 = "+inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_inf_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_inf_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 4) strictfp  ; "-inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_inf_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1019) strictfp  ; ~"-inf"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isfinite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isfinite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 504) strictfp  ; 0x1f8 = "finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isfinite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isfinite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 519) strictfp  ; ~0x1f8 = "~finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_finite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_finite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 448) strictfp  ; 0x1c0 = "+finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_finite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_finite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvsle.wu $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 575) strictfp  ; ~0x1c0 = ~"+finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_finite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_finite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    xvslti.w $xr0, $xr0, 0
; CHECK-NEXT:    xvand.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 56) strictfp  ; 0x38 = "-finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_finite_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_finite_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    xvslti.w $xr0, $xr0, 0
; CHECK-NEXT:    xvand.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 967) strictfp  ; ~0x38 = ~"-finite"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvldi $xr1, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -3201
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 264) strictfp  ; 0x108 = "normal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvldi $xr1, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -3201
; CHECK-NEXT:    xvsle.wu $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 759) strictfp  ; ~0x108 = "~normal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_normal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_normal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr1, $xr0, 31
; CHECK-NEXT:    xvldi $xr2, -3456
; CHECK-NEXT:    xvsub.w $xr1, $xr1, $xr2
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr1, $xr1, $xr2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 0
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 1
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 2
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 3
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 4
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 5
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 6
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr1, 7
; CHECK-NEXT:    vinsgr2vr.h $vr2, $a1, 7
; CHECK-NEXT:    xvslti.w $xr0, $xr0, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vand.v $vr0, $vr2, $vr0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 256) strictfp  ; 0x100 = "+normal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsubi.wu $xr0, $xr0, 1
; CHECK-NEXT:    xvldi $xr1, -2177
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 144) strictfp  ; 0x90 = "subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsubi.wu $xr0, $xr0, 1
; CHECK-NEXT:    xvldi $xr1, -2177
; CHECK-NEXT:    xvsle.wu $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 879) strictfp  ; ~0x90 = "~subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_subnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_subnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvsubi.wu $xr0, $xr0, 1
; CHECK-NEXT:    xvldi $xr1, -2177
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 128) strictfp  ; 0x80 = "+subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_subnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_subnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvsubi.wu $xr0, $xr0, 1
; CHECK-NEXT:    xvldi $xr1, -2177
; CHECK-NEXT:    xvsle.wu $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 895) strictfp  ; ~0x80 = ~"+subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_subnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_subnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr1, $xr0, 31
; CHECK-NEXT:    xvsubi.wu $xr1, $xr1, 1
; CHECK-NEXT:    xvldi $xr2, -2177
; CHECK-NEXT:    xvslt.wu $xr1, $xr1, $xr2
; CHECK-NEXT:    xvslti.w $xr0, $xr0, 0
; CHECK-NEXT:    xvand.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 16) strictfp  ; 0x10 = "-subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_subnormal_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_subnormal_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr1, $xr0, 31
; CHECK-NEXT:    xvsubi.wu $xr1, $xr1, 1
; CHECK-NEXT:    xvldi $xr2, -2177
; CHECK-NEXT:    xvslt.wu $xr1, $xr1, $xr2
; CHECK-NEXT:    xvslti.w $xr0, $xr0, 0
; CHECK-NEXT:    xvand.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1007) strictfp  ; ~0x10 = ~"-subnormal"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @iszero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: iszero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 96) strictfp  ; 0x60 = "zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_iszero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_iszero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 927) strictfp  ; ~0x60 = "~zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_or_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 240) strictfp  ; 0xf0 = "subnormal|zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_or_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 783) strictfp  ; ~0xf0 = "~(subnormal|zero)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 64) strictfp  ; 0x40 = "+zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 959) strictfp  ; ~0x40 = ~"+zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvldi $xr1, -3200
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 32) strictfp  ; 0x20 = "-zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_zero_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_zero_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvldi $xr1, -3200
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 991) strictfp  ; ~0x20 = ~"-zero"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isnone_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isnone_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvrepli.b $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 0) strictfp
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isany_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isany_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvrepli.b $xr0, -1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1023) strictfp
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @iszero_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: iszero_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr1, $xr0
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 99) strictfp  ; 0x60|0x3 = "zero|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_iszero_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_iszero_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr1, $xr0
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 924) strictfp  ; ~0x60 = "~(zero|nan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @iszero_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: iszero_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr1, $xr1, $xr0
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 98) strictfp  ; 0x60|0x2 = "zero|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @iszero_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: iszero_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvslt.w $xr2, $xr2, $xr0
; CHECK-NEXT:    xvand.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 97) strictfp  ; 0x60|0x1 = "zero|snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_iszero_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_iszero_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvslt.w $xr3, $xr2, $xr0
; CHECK-NEXT:    xvand.v $xr1, $xr3, $xr1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr2
; CHECK-NEXT:    xvsubi.wu $xr3, $xr0, 1
; CHECK-NEXT:    xvldi $xr4, -2177
; CHECK-NEXT:    xvslt.wu $xr3, $xr3, $xr4
; CHECK-NEXT:    xvor.v $xr2, $xr3, $xr2
; CHECK-NEXT:    xvor.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvldi $xr2, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 925) strictfp  ; ~(0x60|0x2) = "~(zero|qnan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_iszero_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_iszero_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    xvsubi.wu $xr2, $xr0, 1
; CHECK-NEXT:    xvldi $xr3, -2177
; CHECK-NEXT:    xvslt.wu $xr2, $xr2, $xr3
; CHECK-NEXT:    xvor.v $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvsle.w $xr2, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr1, $xr1, $xr2
; CHECK-NEXT:    xvldi $xr2, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 926) strictfp  ; ~(0x60|0x1) = "~(zero|snan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isinf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isinf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 519) strictfp  ; 0x204|0x3 = "inf|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isinf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isinf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 504) strictfp  ; ~(0x204|0x3) = "~(inf|nan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isfinite_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isfinite_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 507) strictfp  ; 0x1f8|0x3 = "finite|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_isfinite_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_isfinite_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 516) strictfp  ; ~(0x1f8|0x3) = "~(finite|nan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_inf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 515) strictfp  ; 0x200|0x3 = "+inf|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_inf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 7) strictfp  ; "-inf|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_inf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 508) strictfp  ; ~(0x200|0x3) = "~(+inf|nan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_inf_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1016) strictfp  ; "~(-inf|nan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_inf_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr3, $a1
; CHECK-NEXT:    xvslt.w $xr2, $xr3, $xr2
; CHECK-NEXT:    xvand.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr3
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 513) strictfp  ; 0x200|0x1 = "+inf|snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_inf_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 514) strictfp  ; 0x200|0x1 = "+inf|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_inf_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr2, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvsle.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 510) strictfp  ; ~(+inf|snan)
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_inf_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr3, $a1
; CHECK-NEXT:    xvslt.w $xr4, $xr3, $xr2
; CHECK-NEXT:    xvand.v $xr1, $xr4, $xr1
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr4, $a1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr4
; CHECK-NEXT:    xvslt.w $xr2, $xr2, $xr3
; CHECK-NEXT:    xvor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 509) strictfp  ; ~(+inf|qnan)
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_inf_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr3, $a1
; CHECK-NEXT:    xvslt.w $xr2, $xr3, $xr2
; CHECK-NEXT:    xvand.v $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 5) strictfp  ; "-inf|snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_minus_inf_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, -2048
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 6) strictfp  ; "-inf|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_inf_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    xvor.v $xr1, $xr1, $xr2
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvsle.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1018) strictfp  ; "~(-inf|snan)"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_minus_inf_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr3, $a1
; CHECK-NEXT:    xvslt.w $xr4, $xr3, $xr2
; CHECK-NEXT:    xvand.v $xr1, $xr4, $xr1
; CHECK-NEXT:    xvseq.w $xr0, $xr0, $xr3
; CHECK-NEXT:    xvslt.w $xr2, $xr2, $xr3
; CHECK-NEXT:    xvor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 1017) strictfp  ; "-inf|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr1, $xr0
; CHECK-NEXT:    xvsubi.wu $xr0, $xr0, 1
; CHECK-NEXT:    xvldi $xr2, -2177
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 147) strictfp  ; 0x90|0x3 = "subnormal|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_or_zero_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr2, $xr1, $xr2
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 243) strictfp  ; 0xf0|0x3 = "subnormal|zero|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_or_zero_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr2, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr3, $a1
; CHECK-NEXT:    xvslt.w $xr2, $xr3, $xr2
; CHECK-NEXT:    xvand.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr3
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 241) strictfp  ; 0x90|0x1 = "subnormal|snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: issubnormal_or_zero_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr1, $xr1, $xr2
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 242) strictfp  ; 0x90|0x2 = "subnormal|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvldi $xr1, -3456
; CHECK-NEXT:    xvsub.w $xr1, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr1, $xr1, $xr2
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr2
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 876) strictfp  ; ~(0x90|0x3) = ~"subnormal|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_nan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_or_zero_or_nan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr2, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr2, $xr1, $xr2
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvseqi.w $xr0, $xr0, 0
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 0
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 1
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 1
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 2
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 3
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 3
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 4
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 5
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 5
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 6
; CHECK-NEXT:    xvpickve2gr.w $a1, $xr0, 7
; CHECK-NEXT:    vinsgr2vr.h $vr1, $a1, 7
; CHECK-NEXT:    vbitrevi.h $vr0, $vr1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 4
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 5
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 6
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 7
; CHECK-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 0
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 1
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 2
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; CHECK-NEXT:    vpickve2gr.h $a1, $vr0, 3
; CHECK-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; CHECK-NEXT:    xvpermi.q $xr2, $xr1, 2
; CHECK-NEXT:    xvslli.w $xr0, $xr2, 31
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 31
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 780) strictfp  ; ~(0xf0|0x3) = ~"subnormal|zero|nan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_snan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_or_zero_or_snan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvsle.w $xr1, $xr1, $xr0
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvldi $xr2, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 782) strictfp  ; ~(0x90|0x1) = ~"subnormal|snan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_qnan_v8f32(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_issubnormal_or_zero_or_qnan_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    lu12i.w $a1, 523264
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvbitclri.w $xr0, $xr0, 31
; CHECK-NEXT:    xvslt.w $xr1, $xr0, $xr1
; CHECK-NEXT:    lu12i.w $a1, 522240
; CHECK-NEXT:    xvreplgr2vr.w $xr2, $a1
; CHECK-NEXT:    xvslt.w $xr3, $xr2, $xr0
; CHECK-NEXT:    xvand.v $xr1, $xr3, $xr1
; CHECK-NEXT:    xvseq.w $xr2, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr1, $xr2, $xr1
; CHECK-NEXT:    xvldi $xr2, -3456
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr2
; CHECK-NEXT:    xvldi $xr2, -3201
; CHECK-NEXT:    xvslt.wu $xr0, $xr0, $xr2
; CHECK-NEXT:    xvor.v $xr0, $xr1, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <8 x float>, ptr %a0
  %class = tail call <8 x i1> @llvm.is.fpclass.v8f32(<8 x float> %x, i32 781) strictfp  ; ~(0x90|0x2) = ~"subnormal|qnan"
  %ext = sext <8 x i1> %class to <8 x i32>
  store <8 x i32> %ext, ptr %res
  ret void
}

define void @isnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI66_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI66_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI66_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI66_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr0, $xr2, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 3) strictfp  ; "nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isnot_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isnot_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI67_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI67_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI67_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI67_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isnot_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsle.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1020) strictfp  ; 0x3fc = "zero|subnormal|normal|inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issignaling_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issignaling_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI68_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI68_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI68_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI68_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI68_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI68_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr3, $xr0
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issignaling_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1) strictfp  ; "snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issignaling_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issignaling_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI69_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI69_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI69_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI69_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI69_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI69_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr3, $xr0
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vrepli.b $vr0, -1
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issignaling_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1022) strictfp  ; ~"snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isquiet_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isquiet_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI70_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI70_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI70_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI70_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isquiet_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsle.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 2) strictfp  ; "qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isquiet_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isquiet_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI71_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI71_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI71_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI71_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isquiet_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1021) strictfp  ; ~"qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isinf_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isinf_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI72_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI72_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI72_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI72_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isinf_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 516) strictfp  ; 0x204 = "inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isinf_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isinf_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI73_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI73_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI73_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI73_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isinf_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 507) strictfp  ; ~0x204 = "~inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_inf_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_inf_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI74_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI74_0)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_inf_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 512) strictfp  ; 0x200 = "+inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_inf_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_inf_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI75_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI75_0)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_inf_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 4) strictfp  ; "-inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_inf_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI76_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI76_0)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_inf_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1019) strictfp  ; ~"-inf"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isfinite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isfinite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI77_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI77_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI77_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI77_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isfinite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 504) strictfp  ; 0x1f8 = "finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isfinite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isfinite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI78_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI78_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI78_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI78_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isfinite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsle.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 519) strictfp  ; ~0x1f8 = "~finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_finite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_finite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI79_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI79_0)
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_finite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 448) strictfp  ; 0x1c0 = "+finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_finite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_plus_finite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI80_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI80_0)
; LA32-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_plus_finite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 575) strictfp  ; ~0x1c0 = ~"+finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_finite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_finite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI81_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI81_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI81_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI81_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr1, $xr2
; LA32-NEXT:    xvslti.d $xr0, $xr0, 0
; LA32-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_finite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr2, $xr1
; LA64-NEXT:    xvslti.d $xr0, $xr0, 0
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 56) strictfp  ; 0x38 = "-finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_finite_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_finite_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI82_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI82_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI82_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI82_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr1, $xr2
; LA32-NEXT:    xvslti.d $xr0, $xr0, 0
; LA32-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vrepli.b $vr0, -1
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_finite_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr2, $xr1
; LA64-NEXT:    xvslti.d $xr0, $xr0, 0
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 967) strictfp  ; ~0x38 = ~"-finite"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI83_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI83_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI83_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI83_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI83_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI83_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr3
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 264) strictfp  ; 0x108 = "normal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI84_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI84_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI84_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI84_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI84_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI84_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvsle.du $xr0, $xr3, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 759) strictfp  ; ~0x108 = "~normal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_normal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_normal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI85_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI85_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI85_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI85_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI85_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI85_2)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvsub.d $xr1, $xr1, $xr2
; LA32-NEXT:    xvslt.du $xr1, $xr1, $xr3
; LA32-NEXT:    xvpickve2gr.w $a1, $xr1, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr1, 2
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr1, 4
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr1, 6
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvslti.d $xr0, $xr0, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vandn.v $vr0, $vr1, $vr2
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_normal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvsub.d $xr1, $xr2, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr1, $xr1, $xr2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr1, 0
; LA64-NEXT:    vinsgr2vr.w $vr2, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr1, 1
; LA64-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr1, 2
; LA64-NEXT:    vinsgr2vr.w $vr2, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr1, 3
; LA64-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA64-NEXT:    xvslti.d $xr0, $xr0, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vand.v $vr0, $vr2, $vr0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 256) strictfp  ; 0x100 = "+normal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI86_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI86_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI86_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI86_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 144) strictfp  ; 0x90 = "subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI87_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI87_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI87_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI87_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA32-NEXT:    xvsle.du $xr0, $xr2, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA64-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 879) strictfp  ; ~0x90 = "~subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_subnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_subnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI88_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI88_0)
; LA32-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_subnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 128) strictfp  ; 0x80 = "+subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_subnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_plus_subnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI89_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI89_0)
; LA32-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA32-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_plus_subnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsle.du $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 895) strictfp  ; ~0x80 = ~"+subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_subnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_subnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI90_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI90_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI90_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI90_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvsubi.du $xr1, $xr1, 1
; LA32-NEXT:    xvslt.du $xr1, $xr1, $xr2
; LA32-NEXT:    xvslti.d $xr0, $xr0, 0
; LA32-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_subnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvsubi.du $xr2, $xr2, 1
; LA64-NEXT:    xvslt.du $xr1, $xr2, $xr1
; LA64-NEXT:    xvslti.d $xr0, $xr0, 0
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 16) strictfp  ; 0x10 = "-subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_subnormal_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_subnormal_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI91_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI91_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI91_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI91_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvsubi.du $xr1, $xr1, 1
; LA32-NEXT:    xvslt.du $xr1, $xr1, $xr2
; LA32-NEXT:    xvslti.d $xr0, $xr0, 0
; LA32-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vrepli.b $vr0, -1
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_subnormal_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvsubi.du $xr2, $xr2, 1
; LA64-NEXT:    xvslt.du $xr1, $xr2, $xr1
; LA64-NEXT:    xvslti.d $xr0, $xr0, 0
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1007) strictfp  ; ~0x10 = ~"-subnormal"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @iszero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: iszero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI92_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI92_0)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: iszero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 96) strictfp  ; 0x60 = "zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_iszero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_iszero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI93_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI93_0)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_iszero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 927) strictfp  ; ~0x60 = "~zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_or_zero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI94_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI94_0)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_or_zero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 240) strictfp  ; 0xf0 = "subnormal|zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_or_zero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI95_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI95_0)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_or_zero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 783) strictfp  ; ~0xf0 = "~(subnormal|zero)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_zero_v4f64(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: is_plus_zero_v4f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvseqi.d $xr0, $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 64) strictfp  ; 0x40 = "+zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_zero_v4f64(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: not_is_plus_zero_v4f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvseqi.d $xr0, $xr0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 959) strictfp  ; ~0x40 = ~"+zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_zero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_zero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI98_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI98_0)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_zero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -2048
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 32) strictfp  ; 0x20 = "-zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_zero_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_zero_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI99_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI99_0)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_zero_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -2048
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 991) strictfp  ; ~0x20 = ~"-zero"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isnone_v4f64(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isnone_v4f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvrepli.b $xr0, 0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 0) strictfp
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isany_v4f64(ptr %res, ptr %a0) strictfp {
; CHECK-LABEL: isany_v4f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvrepli.b $xr0, -1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1023) strictfp
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @iszero_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: iszero_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI102_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI102_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI102_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI102_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr2, $xr0
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: iszero_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 99) strictfp  ; 0x60|0x3 = "zero|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_iszero_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_iszero_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI103_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI103_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI103_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI103_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr2, $xr0
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vrepli.b $vr0, -1
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_iszero_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 924) strictfp  ; ~0x60 = "~(zero|nan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @iszero_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: iszero_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI104_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI104_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI104_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI104_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr1, $xr2, $xr0
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: iszero_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvsle.d $xr1, $xr1, $xr0
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 98) strictfp  ; 0x60|0x2 = "zero|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @iszero_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: iszero_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI105_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI105_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI105_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI105_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI105_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI105_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr2, $xr3, $xr0
; LA32-NEXT:    xvand.v $xr1, $xr2, $xr1
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: iszero_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.d $xr2, $xr0, $xr2
; LA64-NEXT:    xvand.v $xr1, $xr1, $xr2
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 97) strictfp  ; 0x60|0x1 = "zero|snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_iszero_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_iszero_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI106_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI106_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI106_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr2, $xr3, $xr0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_3)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI106_3)
; LA32-NEXT:    xvand.v $xr1, $xr2, $xr1
; LA32-NEXT:    xvseq.d $xr2, $xr0, $xr3
; LA32-NEXT:    xvsubi.du $xr3, $xr0, 1
; LA32-NEXT:    xvslt.du $xr3, $xr3, $xr4
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_4)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI106_4)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI106_5)
; LA32-NEXT:    xvld $xr5, $a1, %pc_lo12(.LCPI106_5)
; LA32-NEXT:    xvor.v $xr2, $xr3, $xr2
; LA32-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr4
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr5
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_iszero_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr2, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvslt.d $xr3, $xr0, $xr3
; LA64-NEXT:    xvand.v $xr2, $xr2, $xr3
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvsubi.du $xr4, $xr0, 1
; LA64-NEXT:    xvslt.du $xr3, $xr4, $xr3
; LA64-NEXT:    xvor.v $xr1, $xr3, $xr1
; LA64-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 925) strictfp  ; ~(0x60|0x2) = "~(zero|qnan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_iszero_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_iszero_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI107_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI107_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI107_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvsubi.du $xr2, $xr0, 1
; LA32-NEXT:    xvslt.du $xr2, $xr2, $xr3
; LA32-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_3)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI107_3)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_4)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI107_4)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI107_5)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI107_5)
; LA32-NEXT:    xvsle.d $xr2, $xr2, $xr0
; LA32-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr3
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr4
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_iszero_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsubi.du $xr3, $xr0, 1
; LA64-NEXT:    xvslt.du $xr2, $xr3, $xr2
; LA64-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr2, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 926) strictfp  ; ~(0x60|0x1) = "~(zero|snan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isinf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isinf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI108_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI108_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI108_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI108_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isinf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsle.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 519) strictfp  ; 0x204|0x3 = "inf|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isinf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isinf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI109_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI109_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI109_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI109_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isinf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 504) strictfp  ; ~(0x204|0x3) = "~(inf|nan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @isfinite_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: isfinite_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI110_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI110_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI110_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI110_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvrepli.b $xr1, -1
; LA32-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: isfinite_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvrepli.b $xr1, -1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 507) strictfp  ; 0x1f8|0x3 = "finite|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_isfinite_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_isfinite_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI111_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI111_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI111_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI111_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_isfinite_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 516) strictfp  ; ~(0x1f8|0x3) = "~(finite|nan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_inf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI112_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI112_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI112_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI112_1)
; LA32-NEXT:    xvseq.d $xr3, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr1, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr3, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_inf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr2, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr2, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 515) strictfp  ; 0x200|0x3 = "+inf|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_inf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI113_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI113_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI113_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI113_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI113_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI113_2)
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr3, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_inf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 7) strictfp  ; "-inf|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_plus_inf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI114_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI114_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI114_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI114_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI114_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI114_2)
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_plus_inf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 508) strictfp  ; ~(0x200|0x3) = "~(+inf|nan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_inf_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI115_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI115_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI115_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI115_1)
; LA32-NEXT:    xvseq.d $xr3, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr3
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_inf_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr2, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1016) strictfp  ; "~(-inf|nan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_inf_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI116_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI116_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI116_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI116_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI116_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI116_2)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvslt.d $xr1, $xr3, $xr1
; LA32-NEXT:    xvand.v $xr1, $xr1, $xr2
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_inf_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr3, $xr1, $xr2
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr4, $a1
; LA64-NEXT:    xvslt.d $xr2, $xr2, $xr4
; LA64-NEXT:    xvand.v $xr2, $xr3, $xr2
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 513) strictfp  ; 0x200|0x1 = "+inf|snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_plus_inf_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_plus_inf_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI117_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI117_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI117_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI117_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI117_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI117_2)
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvsle.d $xr0, $xr3, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_plus_inf_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 514) strictfp  ; 0x200|0x1 = "+inf|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_plus_inf_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI118_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI118_0)
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI118_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI118_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI118_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI118_2)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI118_3)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI118_3)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr2, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA32-NEXT:    xvsle.d $xr0, $xr4, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_plus_inf_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr2, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 510) strictfp  ; ~(+inf|snan)
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_plus_inf_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_plus_inf_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI119_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI119_0)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI119_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI119_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI119_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI119_2)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI119_3)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI119_3)
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvslt.d $xr5, $xr3, $xr1
; LA32-NEXT:    xvand.v $xr2, $xr5, $xr2
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr4
; LA32-NEXT:    xvslt.d $xr1, $xr1, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_plus_inf_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr3, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr1, $xr3, $xr1
; LA64-NEXT:    xvslt.d $xr2, $xr2, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr3
; LA64-NEXT:    xvand.v $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 509) strictfp  ; ~(+inf|qnan)
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_inf_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI120_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI120_0)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI120_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI120_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI120_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI120_2)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI120_3)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI120_3)
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvslt.d $xr1, $xr3, $xr1
; LA32-NEXT:    xvand.v $xr1, $xr1, $xr2
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr4
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_inf_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr2
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvslt.d $xr2, $xr2, $xr3
; LA64-NEXT:    xvand.v $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 5) strictfp  ; "-inf|snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @is_minus_inf_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: is_minus_inf_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI121_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI121_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI121_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI121_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI121_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI121_2)
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvsle.d $xr0, $xr3, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: is_minus_inf_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, -1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 6) strictfp  ; "-inf|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_inf_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI122_1)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI122_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI122_0)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI122_0)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI122_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI122_2)
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvor.v $xr0, $xr2, $xr0
; LA32-NEXT:    xvsle.d $xr1, $xr3, $xr1
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_inf_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr2, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr0, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1018) strictfp  ; "~(-inf|snan)"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_is_minus_inf_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_is_minus_inf_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI123_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI123_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI123_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI123_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI123_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI123_2)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvslt.d $xr4, $xr3, $xr1
; LA32-NEXT:    xvand.v $xr2, $xr4, $xr2
; LA32-NEXT:    xvseq.d $xr0, $xr0, $xr3
; LA32-NEXT:    xvslt.d $xr1, $xr1, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_is_minus_inf_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvseq.d $xr2, $xr0, $xr1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr3, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr2, $xr3, $xr2
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvslt.d $xr0, $xr0, $xr3
; LA64-NEXT:    xvand.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvor.v $xr0, $xr2, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 1017) strictfp  ; "-inf|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI124_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI124_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI124_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI124_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI124_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI124_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr2, $xr0
; LA32-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr1, $xr1, $xr0
; LA64-NEXT:    addi.w $a1, $zero, -1
; LA64-NEXT:    lu52i.d $a1, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsubi.du $xr0, $xr0, 1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 147) strictfp  ; 0x90|0x3 = "subnormal|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_or_zero_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI125_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI125_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI125_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI125_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr2, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_or_zero_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 243) strictfp  ; 0xf0|0x3 = "subnormal|zero|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_or_zero_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI126_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI126_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI126_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI126_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI126_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI126_2)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA32-NEXT:    xvslt.d $xr1, $xr3, $xr1
; LA32-NEXT:    xvand.v $xr1, $xr1, $xr2
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr3
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_or_zero_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr3, $xr1, $xr2
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr4, $a1
; LA64-NEXT:    xvslt.d $xr2, $xr2, $xr4
; LA64-NEXT:    xvand.v $xr2, $xr3, $xr2
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 241) strictfp  ; 0x90|0x1 = "subnormal|snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @issubnormal_or_zero_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: issubnormal_or_zero_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI127_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI127_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI127_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI127_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI127_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI127_2)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr1, $xr2, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr3
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: issubnormal_or_zero_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvbitclri.d $xr1, $xr0, 63
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr1, $xr2, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 242) strictfp  ; 0x90|0x2 = "subnormal|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI128_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI128_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI128_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI128_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvseq.d $xr1, $xr0, $xr2
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI128_2)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI128_2)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI128_3)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI128_3)
; LA32-NEXT:    xvseqi.d $xr4, $xr0, 0
; LA32-NEXT:    xvor.v $xr1, $xr4, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvsub.d $xr1, $xr0, $xr1
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvseq.d $xr2, $xr0, $xr2
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 876) strictfp  ; ~(0x90|0x3) = ~"subnormal|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_nan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_or_zero_or_nan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI129_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI129_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI129_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI129_1)
; LA32-NEXT:    xvand.v $xr1, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr2, $xr1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA32-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vrepli.b $vr0, -1
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vori.b $vr2, $vr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 1
; LA32-NEXT:    vextrins.w $vr2, $vr0, 33
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    srai.w $a1, $a1, 31
; LA32-NEXT:    vinsgr2vr.w $vr2, $a1, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvst $xr2, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_or_zero_or_nan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr2, $xr0, 63
; LA64-NEXT:    xvslt.d $xr2, $xr1, $xr2
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvseqi.d $xr0, $xr0, 0
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 0
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 1
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 2
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA64-NEXT:    xvpickve2gr.d $a1, $xr0, 3
; LA64-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA64-NEXT:    vbitrevi.w $vr0, $vr1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA64-NEXT:    vinsgr2vr.d $vr1, $a1, 1
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 0
; LA64-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA64-NEXT:    vinsgr2vr.d $vr2, $a1, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvslli.d $xr0, $xr2, 63
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 63
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 780) strictfp  ; ~(0xf0|0x3) = ~"subnormal|zero|nan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_snan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_or_zero_or_snan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI130_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI130_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI130_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI130_1)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvsle.d $xr1, $xr2, $xr0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI130_2)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI130_2)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI130_3)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI130_3)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI130_4)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI130_4)
; LA32-NEXT:    xvseq.d $xr2, $xr0, $xr2
; LA32-NEXT:    xvor.v $xr1, $xr2, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr3
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr4
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_or_zero_or_snan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsle.d $xr2, $xr2, $xr0
; LA64-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 782) strictfp  ; ~(0x90|0x1) = ~"subnormal|snan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

define void @not_issubnormal_or_zero_or_qnan_v4f64(ptr %res, ptr %a0) strictfp {
; LA32-LABEL: not_issubnormal_or_zero_or_qnan_v4f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI131_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI131_0)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI131_1)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI131_1)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI131_2)
; LA32-NEXT:    xvld $xr3, $a1, %pc_lo12(.LCPI131_2)
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvslt.d $xr1, $xr0, $xr2
; LA32-NEXT:    xvslt.d $xr2, $xr3, $xr0
; LA32-NEXT:    xvand.v $xr1, $xr2, $xr1
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI131_3)
; LA32-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI131_3)
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI131_4)
; LA32-NEXT:    xvld $xr4, $a1, %pc_lo12(.LCPI131_4)
; LA32-NEXT:    xvseq.d $xr3, $xr0, $xr3
; LA32-NEXT:    xvor.v $xr1, $xr3, $xr1
; LA32-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA32-NEXT:    xvslt.du $xr0, $xr0, $xr4
; LA32-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: not_issubnormal_or_zero_or_qnan_v4f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    lu52i.d $a1, $zero, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvbitclri.d $xr0, $xr0, 63
; LA64-NEXT:    xvslt.d $xr2, $xr1, $xr0
; LA64-NEXT:    ori $a1, $zero, 0
; LA64-NEXT:    lu32i.d $a1, -524288
; LA64-NEXT:    lu52i.d $a1, $a1, 2047
; LA64-NEXT:    xvreplgr2vr.d $xr3, $a1
; LA64-NEXT:    xvslt.d $xr3, $xr0, $xr3
; LA64-NEXT:    xvand.v $xr2, $xr2, $xr3
; LA64-NEXT:    xvseq.d $xr1, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr1, $xr1, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 1
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvsub.d $xr0, $xr0, $xr2
; LA64-NEXT:    lu52i.d $a1, $zero, 2046
; LA64-NEXT:    xvreplgr2vr.d $xr2, $a1
; LA64-NEXT:    xvslt.du $xr0, $xr0, $xr2
; LA64-NEXT:    xvor.v $xr0, $xr1, $xr0
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %x = load <4 x double>, ptr %a0
  %class = tail call <4 x i1> @llvm.is.fpclass.v4f64(<4 x double> %x, i32 781) strictfp  ; ~(0x90|0x2) = ~"subnormal|qnan"
  %ext = sext <4 x i1> %class to <4 x i64>
  store <4 x i64> %ext, ptr %res
  ret void
}

declare <8 x i1> @llvm.is.fpclass.v8f32(<8 x float>, i32)
declare <4 x i1> @llvm.is.fpclass.v4f64(<4 x double>, i32)
