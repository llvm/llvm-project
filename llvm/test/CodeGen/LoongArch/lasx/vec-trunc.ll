; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx %s -o - | FileCheck %s --check-prefix=LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx %s -o - | FileCheck %s --check-prefix=LA64

define void @trunc_v4i64_to_v4i32(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v4i64_to_v4i32:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI0_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI0_0)
; LA32-NEXT:    xvperm.w $xr0, $xr0, $xr1
; LA32-NEXT:    vst $vr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v4i64_to_v4i32:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI0_0)
; LA64-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI0_0)
; LA64-NEXT:    xvperm.w $xr0, $xr0, $xr1
; LA64-NEXT:    vst $vr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %v = load <4 x i64>, ptr %a
  %vtrunc = trunc <4 x i64> %v to <4 x i32>
  store <4 x i32> %vtrunc, ptr %res
  ret void
}

define void @trunc_v4i64_to_v4i16(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v4i64_to_v4i16:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI1_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI1_0)
; LA32-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA32-NEXT:    xvshuf.h $xr1, $xr2, $xr0
; LA32-NEXT:    vpickve2gr.w $a1, $vr1, 1
; LA32-NEXT:    st.w $a1, $a0, 4
; LA32-NEXT:    vpickve2gr.w $a1, $vr1, 0
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v4i64_to_v4i16:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI1_0)
; LA64-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI1_0)
; LA64-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA64-NEXT:    xvshuf.h $xr1, $xr2, $xr0
; LA64-NEXT:    vstelm.d $vr1, $a0, 0, 0
; LA64-NEXT:    ret
entry:
  %v = load <4 x i64>, ptr %a
  %vtrunc = trunc <4 x i64> %v to <4 x i16>
  store <4 x i16> %vtrunc, ptr %res
  ret void
}

define void @trunc_v4i64_to_v4i8(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v4i64_to_v4i8:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI2_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI2_0)
; LA32-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA32-NEXT:    xvshuf.b $xr0, $xr2, $xr0, $xr1
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v4i64_to_v4i8:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI2_0)
; LA64-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI2_0)
; LA64-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA64-NEXT:    xvshuf.b $xr0, $xr2, $xr0, $xr1
; LA64-NEXT:    vstelm.w $vr0, $a0, 0, 0
; LA64-NEXT:    ret
entry:
  %v = load <4 x i64>, ptr %a
  %vtrunc = trunc <4 x i64> %v to <4 x i8>
  store <4 x i8> %vtrunc, ptr %res
  ret void
}

define void @trunc_v8i32_to_v8i16(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v8i32_to_v8i16:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpermi.d $xr1, $xr0, 78
; LA32-NEXT:    xvpickev.h $xr0, $xr1, $xr0
; LA32-NEXT:    vst $vr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v8i32_to_v8i16:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvpermi.d $xr1, $xr0, 78
; LA64-NEXT:    xvpickev.h $xr0, $xr1, $xr0
; LA64-NEXT:    vst $vr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %v = load <8 x i32>, ptr %a
  %vtrunc = trunc <8 x i32> %v to <8 x i16>
  store <8 x i16> %vtrunc, ptr %res
  ret void
}

define void @trunc_v8i32_to_v8i8(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v8i32_to_v8i8:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI4_0)
; LA32-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI4_0)
; LA32-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA32-NEXT:    xvshuf.b $xr0, $xr2, $xr0, $xr1
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    st.w $a1, $a0, 4
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v8i32_to_v8i8:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI4_0)
; LA64-NEXT:    xvld $xr1, $a1, %pc_lo12(.LCPI4_0)
; LA64-NEXT:    xvpermi.d $xr2, $xr0, 78
; LA64-NEXT:    xvshuf.b $xr0, $xr2, $xr0, $xr1
; LA64-NEXT:    vstelm.d $vr0, $a0, 0, 0
; LA64-NEXT:    ret
entry:
  %v = load <8 x i32>, ptr %a
  %vtrunc = trunc <8 x i32> %v to <8 x i8>
  store <8 x i8> %vtrunc, ptr %res
  ret void
}

define void @trunc_v16i16_to_v16i8(ptr %res, ptr %a) nounwind {
; LA32-LABEL: trunc_v16i16_to_v16i8:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvpermi.d $xr1, $xr0, 78
; LA32-NEXT:    xvpickev.b $xr0, $xr1, $xr0
; LA32-NEXT:    vst $vr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: trunc_v16i16_to_v16i8:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvpermi.d $xr1, $xr0, 78
; LA64-NEXT:    xvpickev.b $xr0, $xr1, $xr0
; LA64-NEXT:    vst $vr0, $a0, 0
; LA64-NEXT:    ret
entry:
  %v = load <16 x i16>, ptr %a
  %vtrunc = trunc <16 x i16> %v to <16 x i8>
  store <16 x i8> %vtrunc, ptr %res
  ret void
}
