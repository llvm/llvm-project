; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s

define void @and_not_combine_v32i8(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v32i8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <32 x i8>, ptr %a0
  %v1 = load <32 x i8>, ptr %a1
  %v2 = load <32 x i8>, ptr %a2
  %not = xor <32 x i8> %v1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %add = add <32 x i8> %not, %v2
  %and = and <32 x i8> %v0, %add
  store <32 x i8> %and, ptr %res
  ret void
}

define void @and_not_combine_v16i16(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v16i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <16 x i16>, ptr %a0
  %v1 = load <16 x i16>, ptr %a1
  %v2 = load <16 x i16>, ptr %a2
  %not = xor <16 x i16> %v1, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %add = add <16 x i16> %not, %v2
  %and = and <16 x i16> %v0, %add
  store <16 x i16> %and, ptr %res
  ret void
}

define void @and_not_combine_v8i32(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v8i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <8 x i32>, ptr %a0
  %v1 = load <8 x i32>, ptr %a1
  %v2 = load <8 x i32>, ptr %a2
  %not = xor <8 x i32> %v1, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %add = add <8 x i32> %not, %v2
  %and = and <8 x i32> %v0, %add
  store <8 x i32> %and, ptr %res
  ret void
}

define void @and_not_combine_v4i64(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v4i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.d $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <4 x i64>, ptr %a0
  %v1 = load <4 x i64>, ptr %a1
  %v2 = load <4 x i64>, ptr %a2
  %not = xor <4 x i64> %v1, <i64 -1, i64 -1, i64 -1, i64 -1>
  %add = add <4 x i64> %not, %v2
  %and = and <4 x i64> %v0, %add
  store <4 x i64> %and, ptr %res
  ret void
}
