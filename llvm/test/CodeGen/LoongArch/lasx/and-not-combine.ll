; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define void @and_not_combine_v32i8(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v32i8:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <32 x i8>, ptr %a0
  %v1 = load <32 x i8>, ptr %a1
  %v2 = load <32 x i8>, ptr %a2
  %not = xor <32 x i8> %v1, <i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1, i8 -1>
  %add = add <32 x i8> %not, %v2
  %and = and <32 x i8> %v0, %add
  store <32 x i8> %and, ptr %res
  ret void
}

define void @and_not_combine_v16i16(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v16i16:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <16 x i16>, ptr %a0
  %v1 = load <16 x i16>, ptr %a1
  %v2 = load <16 x i16>, ptr %a2
  %not = xor <16 x i16> %v1, <i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1, i16 -1>
  %add = add <16 x i16> %not, %v2
  %and = and <16 x i16> %v0, %add
  store <16 x i16> %and, ptr %res
  ret void
}

define void @and_not_combine_v8i32(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v8i32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <8 x i32>, ptr %a0
  %v1 = load <8 x i32>, ptr %a1
  %v2 = load <8 x i32>, ptr %a2
  %not = xor <8 x i32> %v1, <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
  %add = add <8 x i32> %not, %v2
  %and = and <8 x i32> %v0, %add
  store <8 x i32> %and, ptr %res
  ret void
}

define void @and_not_combine_v4i64(ptr %res, ptr %a0, ptr %a1, ptr %a2) nounwind {
; CHECK-LABEL: and_not_combine_v4i64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a2, 0
; CHECK-NEXT:    xvld $xr1, $a3, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvsub.d $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr2
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %v0 = load <4 x i64>, ptr %a0
  %v1 = load <4 x i64>, ptr %a1
  %v2 = load <4 x i64>, ptr %a2
  %not = xor <4 x i64> %v1, <i64 -1, i64 -1, i64 -1, i64 -1>
  %add = add <4 x i64> %not, %v2
  %and = and <4 x i64> %v0, %add
  store <4 x i64> %and, ptr %res
  ret void
}

define void @pre_not_and_not_combine_v32i8(ptr %res, ptr %a, i8 %b) nounwind {
; CHECK-LABEL: pre_not_and_not_combine_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    nor $a1, $a2, $zero
; CHECK-NEXT:    xvreplgr2vr.b $xr1, $a1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <32 x i8>, ptr %a
  %b.not = xor i8 %b, -1
  %b.not.ele = insertelement <32 x i8> poison, i8 %b.not, i64 0
  %v1.not = shufflevector <32 x i8> %b.not.ele, <32 x i8> poison, <32 x i32> zeroinitializer
  %v0.not = xor <32 x i8> %v0, splat (i8 -1)
  %and = and <32 x i8> %v0.not, %v1.not
  store <32 x i8> %and, ptr %res
  ret void
}

define void @post_not_and_not_combine_v32i8(ptr %res, ptr %a, i8 %b) nounwind {
; CHECK-LABEL: post_not_and_not_combine_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvreplgr2vr.b $xr1, $a2
; CHECK-NEXT:    xvxori.b $xr1, $xr1, 255
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <32 x i8>, ptr %a
  %b.ele = insertelement <32 x i8> poison, i8 %b, i64 0
  %v1 = shufflevector <32 x i8> %b.ele, <32 x i8> poison, <32 x i32> zeroinitializer
  %v0.not = xor <32 x i8> %v0, splat (i8 -1)
  %v1.not = xor <32 x i8> %v1, splat (i8 -1)
  %and = and <32 x i8> %v0.not, %v1.not
  store <32 x i8> %and, ptr %res
  ret void
}

define void @pre_not_and_not_combine_v16i16(ptr %res, ptr %a, i16 %b) nounwind {
; CHECK-LABEL: pre_not_and_not_combine_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    nor $a1, $a2, $zero
; CHECK-NEXT:    xvreplgr2vr.h $xr1, $a1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %b.not = xor i16 %b, -1
  %b.not.ele = insertelement <16 x i16> poison, i16 %b.not, i64 0
  %v1.not = shufflevector <16 x i16> %b.not.ele, <16 x i16> poison, <16 x i32> zeroinitializer
  %v0.not = xor <16 x i16> %v0, splat (i16 -1)
  %and = and <16 x i16> %v0.not, %v1.not
  store <16 x i16> %and, ptr %res
  ret void
}

define void @post_not_and_not_combine_v16i16(ptr %res, ptr %a, i16 %b) nounwind {
; CHECK-LABEL: post_not_and_not_combine_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvreplgr2vr.h $xr1, $a2
; CHECK-NEXT:    xvrepli.b $xr2, -1
; CHECK-NEXT:    xvxor.v $xr1, $xr1, $xr2
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %b.ele = insertelement <16 x i16> poison, i16 %b, i64 0
  %v1 = shufflevector <16 x i16> %b.ele, <16 x i16> poison, <16 x i32> zeroinitializer
  %v0.not = xor <16 x i16> %v0, splat (i16 -1)
  %v1.not = xor <16 x i16> %v1, splat (i16 -1)
  %and = and <16 x i16> %v0.not, %v1.not
  store <16 x i16> %and, ptr %res
  ret void
}

define void @pre_not_and_not_combine_v8i32(ptr %res, ptr %a, i32 %b) nounwind {
; CHECK-LABEL: pre_not_and_not_combine_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    nor $a1, $a2, $zero
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a1
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x i32>, ptr %a
  %b.not = xor i32 %b, -1
  %b.not.ele = insertelement <8 x i32> poison, i32 %b.not, i64 0
  %v1.not = shufflevector <8 x i32> %b.not.ele, <8 x i32> poison, <8 x i32> zeroinitializer
  %v0.not = xor <8 x i32> %v0, splat (i32 -1)
  %and = and <8 x i32> %v0.not, %v1.not
  store <8 x i32> %and, ptr %res
  ret void
}

define void @post_not_and_not_combine_v8i32(ptr %res, ptr %a, i32 %b) nounwind {
; CHECK-LABEL: post_not_and_not_combine_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvreplgr2vr.w $xr1, $a2
; CHECK-NEXT:    xvrepli.b $xr2, -1
; CHECK-NEXT:    xvxor.v $xr1, $xr1, $xr2
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x i32>, ptr %a
  %b.ele = insertelement <8 x i32> poison, i32 %b, i64 0
  %v1 = shufflevector <8 x i32> %b.ele, <8 x i32> poison, <8 x i32> zeroinitializer
  %v0.not = xor <8 x i32> %v0, splat (i32 -1)
  %v1.not = xor <8 x i32> %v1, splat (i32 -1)
  %and = and <8 x i32> %v0.not, %v1.not
  store <8 x i32> %and, ptr %res
  ret void
}

define void @pre_not_and_not_combine_v4i64(ptr %res, ptr %a, i64 %b) nounwind {
; LA32-LABEL: pre_not_and_not_combine_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    nor $a1, $a3, $zero
; LA32-NEXT:    nor $a2, $a2, $zero
; LA32-NEXT:    vinsgr2vr.w $vr1, $a2, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvreplve0.d $xr1, $xr1
; LA32-NEXT:    xvandn.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: pre_not_and_not_combine_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    nor $a1, $a2, $zero
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a1
; LA64-NEXT:    xvandn.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x i64>, ptr %a
  %b.not = xor i64 %b, -1
  %b.not.ele = insertelement <4 x i64> poison, i64 %b.not, i64 0
  %v1.not = shufflevector <4 x i64> %b.not.ele, <4 x i64> poison, <4 x i32> zeroinitializer
  %v0.not = xor <4 x i64> %v0, splat (i64 -1)
  %and = and <4 x i64> %v0.not, %v1.not
  store <4 x i64> %and, ptr %res
  ret void
}

define void @post_not_and_not_combine_v4i64(ptr %res, ptr %a, i64 %b) nounwind {
; LA32-LABEL: post_not_and_not_combine_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a2, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a3, 1
; LA32-NEXT:    xvreplve0.d $xr1, $xr1
; LA32-NEXT:    xvrepli.b $xr2, -1
; LA32-NEXT:    xvxor.v $xr1, $xr1, $xr2
; LA32-NEXT:    xvandn.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: post_not_and_not_combine_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvreplgr2vr.d $xr1, $a2
; LA64-NEXT:    xvrepli.b $xr2, -1
; LA64-NEXT:    xvxor.v $xr1, $xr1, $xr2
; LA64-NEXT:    xvandn.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x i64>, ptr %a
  %b.ele = insertelement <4 x i64> poison, i64 %b, i64 0
  %v1 = shufflevector <4 x i64> %b.ele, <4 x i64> poison, <4 x i32> zeroinitializer
  %v0.not = xor <4 x i64> %v0, splat (i64 -1)
  %v1.not = xor <4 x i64> %v1, splat (i64 -1)
  %and = and <4 x i64> %v0.not, %v1.not
  store <4 x i64> %and, ptr %res
  ret void
}

define void @and_not_combine_splatimm_v32i8(ptr %res, ptr %a0) nounwind {
; CHECK-LABEL: and_not_combine_splatimm_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvrepli.b $xr1, -4
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <32 x i8>, ptr %a0
  %and = and <32 x i8> %v0, splat (i8 -4)
  %xor = xor <32 x i8> %and, splat (i8 -4)
  store <32 x i8> %xor, ptr %res
  ret void
}

define void @and_not_combine_splatimm_v16i16(ptr %res, ptr %a0) nounwind {
; CHECK-LABEL: and_not_combine_splatimm_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvrepli.h $xr1, -4
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a0
  %and = and <16 x i16> %v0, splat (i16 -4)
  %xor = xor <16 x i16> %and, splat (i16 -4)
  store <16 x i16> %xor, ptr %res
  ret void
}

define void @and_not_combine_splatimm_v8i32(ptr %res, ptr %a0) nounwind {
; CHECK-LABEL: and_not_combine_splatimm_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvrepli.w $xr1, -4
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x i32>, ptr %a0
  %and = and <8 x i32> %v0, splat (i32 -4)
  %xor = xor <8 x i32> %and, splat (i32 -4)
  store <8 x i32> %xor, ptr %res
  ret void
}

define void @and_not_combine_splatimm_v4i64(ptr %res, ptr %a0) nounwind {
; CHECK-LABEL: and_not_combine_splatimm_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvrepli.d $xr1, -4
; CHECK-NEXT:    xvandn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <4 x i64>, ptr %a0
  %and = and <4 x i64> %v0, splat (i64 -4)
  %xor = xor <4 x i64> %and, splat (i64 -4)
  store <4 x i64> %xor, ptr %res
  ret void
}

define void @and_or_not_combine_v32i8(ptr %pa, ptr %pb, ptr %pv, ptr %dst) nounwind {
; CHECK-LABEL: and_or_not_combine_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvseq.b $xr0, $xr1, $xr0
; CHECK-NEXT:    xvxori.b $xr0, $xr0, 255
; CHECK-NEXT:    xvseq.b $xr1, $xr1, $xr2
; CHECK-NEXT:    xvorn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvandi.b $xr0, $xr0, 4
; CHECK-NEXT:    xvst $xr0, $a3, 0
; CHECK-NEXT:    ret
  %a = load <32 x i8>, ptr %pa
  %b = load <32 x i8>, ptr %pb
  %v = load <32 x i8>, ptr %pv
  %ca = icmp ne <32 x i8> %v, %a
  %cb = icmp ne <32 x i8> %v, %b
  %or = or <32 x i1> %ca, %cb
  %ext = sext <32 x i1> %or to <32 x i8>
  %and = and <32 x i8> %ext, splat (i8 4)
  store <32 x i8> %and, ptr %dst
  ret void
}

define void @and_or_not_combine_v16i16(ptr %pa, ptr %pb, ptr %pv, ptr %dst) nounwind {
; CHECK-LABEL: and_or_not_combine_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr1, $xr0
; CHECK-NEXT:    xvrepli.b $xr3, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr3
; CHECK-NEXT:    xvseq.h $xr1, $xr1, $xr2
; CHECK-NEXT:    xvorn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.h $xr1, 4
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a3, 0
; CHECK-NEXT:    ret
  %a = load <16 x i16>, ptr %pa
  %b = load <16 x i16>, ptr %pb
  %v = load <16 x i16>, ptr %pv
  %ca = icmp ne <16 x i16> %v, %a
  %cb = icmp ne <16 x i16> %v, %b
  %or = or <16 x i1> %ca, %cb
  %ext = sext <16 x i1> %or to <16 x i16>
  %and = and <16 x i16> %ext, splat (i16 4)
  store <16 x i16> %and, ptr %dst
  ret void
}

define void @and_or_not_combine_v8i32(ptr %pa, ptr %pb, ptr %pv, ptr %dst) nounwind {
; CHECK-LABEL: and_or_not_combine_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvseq.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvrepli.b $xr3, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr3
; CHECK-NEXT:    xvseq.w $xr1, $xr1, $xr2
; CHECK-NEXT:    xvorn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 4
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a3, 0
; CHECK-NEXT:    ret
  %a = load <8 x i32>, ptr %pa
  %b = load <8 x i32>, ptr %pb
  %v = load <8 x i32>, ptr %pv
  %ca = icmp ne <8 x i32> %v, %a
  %cb = icmp ne <8 x i32> %v, %b
  %or = or <8 x i1> %ca, %cb
  %ext = sext <8 x i1> %or to <8 x i32>
  %and = and <8 x i32> %ext, splat (i32 4)
  store <8 x i32> %and, ptr %dst
  ret void
}

define void @and_or_not_combine_v4i64(ptr %pa, ptr %pb, ptr %pv, ptr %dst) nounwind {
; CHECK-LABEL: and_or_not_combine_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvld $xr2, $a1, 0
; CHECK-NEXT:    xvseq.d $xr0, $xr1, $xr0
; CHECK-NEXT:    xvrepli.b $xr3, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr3
; CHECK-NEXT:    xvseq.d $xr1, $xr1, $xr2
; CHECK-NEXT:    xvorn.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.d $xr1, 4
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a3, 0
; CHECK-NEXT:    ret
  %a = load <4 x i64>, ptr %pa
  %b = load <4 x i64>, ptr %pb
  %v = load <4 x i64>, ptr %pv
  %ca = icmp ne <4 x i64> %v, %a
  %cb = icmp ne <4 x i64> %v, %b
  %or = or <4 x i1> %ca, %cb
  %ext = sext <4 x i1> %or to <4 x i64>
  %and = and <4 x i64> %ext, splat (i64 4)
  store <4 x i64> %and, ptr %dst
  ret void
}

define void @and_extract_subvector_not_combine_v32i8(ptr %pa, ptr %dst) nounwind {
; CHECK-LABEL: and_extract_subvector_not_combine_v32i8:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvxori.b $xr0, $xr0, 255
; CHECK-NEXT:    xvpermi.q $xr0, $xr0, 1
; CHECK-NEXT:    vandi.b $vr0, $vr0, 4
; CHECK-NEXT:    vst $vr0, $a1, 0
; CHECK-NEXT:    ret
  %a = load volatile <32 x i8>, ptr %pa
  %a.not = xor <32 x i8> %a, splat (i8 -1)
  %subv = shufflevector <32 x i8> %a.not, <32 x i8> poison,
                        <16 x i32> <i32 16, i32 17, i32 18, i32 19, i32 20, i32 21, i32 22, i32 23,
                                    i32 24, i32 25, i32 26, i32 27, i32 28, i32 29, i32 30, i32 31>
  %and = and <16 x i8> %subv, splat (i8 4)
  store <16 x i8> %and, ptr %dst
  ret void
}

define void @and_extract_subvector_not_combine_v16i16(ptr %pa, ptr %dst) nounwind {
; CHECK-LABEL: and_extract_subvector_not_combine_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvpermi.q $xr0, $xr0, 1
; CHECK-NEXT:    vrepli.h $vr1, 4
; CHECK-NEXT:    vand.v $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a1, 0
; CHECK-NEXT:    ret
  %a = load volatile <16 x i16>, ptr %pa
  %a.not = xor <16 x i16> %a, splat (i16 -1)
  %subv = shufflevector <16 x i16> %a.not, <16 x i16> poison,
                        <8 x i32> <i32 8, i32 9, i32 10, i32 11, i32 12, i32 13, i32 14, i32 15>
  %and = and <8 x i16> %subv, splat (i16 4)
  store <8 x i16> %and, ptr %dst
  ret void
}

define void @and_extract_subvector_not_combine_v8i32(ptr %pa, ptr %dst) nounwind {
; CHECK-LABEL: and_extract_subvector_not_combine_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvpermi.q $xr0, $xr0, 1
; CHECK-NEXT:    vrepli.w $vr1, 4
; CHECK-NEXT:    vand.v $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a1, 0
; CHECK-NEXT:    ret
  %a = load volatile <8 x i32>, ptr %pa
  %a.not = xor <8 x i32> %a, splat (i32 -1)
  %subv = shufflevector <8 x i32> %a.not, <8 x i32> poison, <4 x i32> <i32 4, i32 5, i32 6, i32 7>
  %and = and <4 x i32> %subv, splat (i32 4)
  store <4 x i32> %and, ptr %dst
  ret void
}

define void @and_extract_subvector_not_combine_v4i64(ptr %pa, ptr %dst) nounwind {
; CHECK-LABEL: and_extract_subvector_not_combine_v4i64:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a0, 0
; CHECK-NEXT:    xvrepli.b $xr1, -1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvpermi.q $xr0, $xr0, 1
; CHECK-NEXT:    vrepli.d $vr1, 4
; CHECK-NEXT:    vand.v $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a1, 0
; CHECK-NEXT:    ret
  %a = load volatile <4 x i64>, ptr %pa
  %a.not = xor <4 x i64> %a, splat (i64 -1)
  %subv = shufflevector <4 x i64> %a.not, <4 x i64> poison, <2 x i32> <i32 2, i32 3>
  %and = and <2 x i64> %subv, splat (i64 4)
  store <2 x i64> %and, ptr %dst
  ret void
}
