; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s

define void @xvavg_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.b $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.b $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %ea = sext <32 x i8> %va to <32 x i16>
  %eb = sext <32 x i8> %vb to <32 x i16>
  %add = add <32 x i16> %ea, %eb
  %shr = lshr <32 x i16> %add, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %r = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %r, ptr %res
  ret void
}

define void @xvavg_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.h $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.h $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %ea = sext <16 x i16> %va to <16 x i32>
  %eb = sext <16 x i16> %vb to <16 x i32>
  %add = add <16 x i32> %ea, %eb
  %shr = lshr <16 x i32> %add, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %r = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @xvavg_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %ea = sext <8 x i32> %va to <8 x i64>
  %eb = sext <8 x i32> %vb to <8 x i64>
  %add = add <8 x i64> %ea, %eb
  %shr = lshr <8 x i64> %add, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %r = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xvavg_d(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_d:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.d $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.d $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %ea = sext <4 x i64> %va to <4 x i128>
  %eb = sext <4 x i64> %vb to <4 x i128>
  %add = add <4 x i128> %ea, %eb
  %shr = lshr <4 x i128> %add, <i128 1, i128 1, i128 1, i128 1>
  %r = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @xvavg_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.b $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.b $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %ea = zext <32 x i8> %va to <32 x i16>
  %eb = zext <32 x i8> %vb to <32 x i16>
  %add = add <32 x i16> %ea, %eb
  %shr = lshr <32 x i16> %add, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %r = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %r, ptr %res
  ret void
}

define void @xvavg_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.h $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.h $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %ea = zext <16 x i16> %va to <16 x i32>
  %eb = zext <16 x i16> %vb to <16 x i32>
  %add = add <16 x i32> %ea, %eb
  %shr = lshr <16 x i32> %add, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %r = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @xvavg_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.w $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %ea = zext <8 x i32> %va to <8 x i64>
  %eb = zext <8 x i32> %vb to <8 x i64>
  %add = add <8 x i64> %ea, %eb
  %shr = lshr <8 x i64> %add, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %r = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xvavg_du(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavg_du:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvand.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.d $xr0, $xr0, 1
; CHECK-NEXT:    xvadd.d $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %ea = zext <4 x i64> %va to <4 x i128>
  %eb = zext <4 x i64> %vb to <4 x i128>
  %add = add <4 x i128> %ea, %eb
  %shr = lshr <4 x i128> %add, <i128 1, i128 1, i128 1, i128 1>
  %r = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @xvavgr_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.b $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.b $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %ea = sext <32 x i8> %va to <32 x i16>
  %eb = sext <32 x i8> %vb to <32 x i16>
  %add = add <32 x i16> %ea, %eb
  %add1 = add <32 x i16> %add, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %shr = lshr <32 x i16> %add1, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %r = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %r, ptr %res
  ret void
}

define void @xvavgr_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.h $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.h $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %ea = sext <16 x i16> %va to <16 x i32>
  %eb = sext <16 x i16> %vb to <16 x i32>
  %add = add <16 x i32> %ea, %eb
  %add1 = add <16 x i32> %add, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %shr = lshr <16 x i32> %add1, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %r = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @xvavgr_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %ea = sext <8 x i32> %va to <8 x i64>
  %eb = sext <8 x i32> %vb to <8 x i64>
  %add = add <8 x i64> %ea, %eb
  %add1 = add <8 x i64> %add, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %shr = lshr <8 x i64> %add1, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %r = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xvavgr_d(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_d:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.d $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.d $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %ea = sext <4 x i64> %va to <4 x i128>
  %eb = sext <4 x i64> %vb to <4 x i128>
  %add = add <4 x i128> %ea, %eb
  %add1 = add <4 x i128> %add, <i128 1, i128 1, i128 1, i128 1>
  %shr = lshr <4 x i128> %add1, <i128 1, i128 1, i128 1, i128 1>
  %r = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @xvavgr_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.b $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.b $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <32 x i8>, ptr %a
  %vb = load <32 x i8>, ptr %b
  %ea = zext <32 x i8> %va to <32 x i16>
  %eb = zext <32 x i8> %vb to <32 x i16>
  %add = add <32 x i16> %ea, %eb
  %add1 = add <32 x i16> %add, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %shr = lshr <32 x i16> %add1, <i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1, i16 1>
  %r = trunc <32 x i16> %shr to <32 x i8>
  store <32 x i8> %r, ptr %res
  ret void
}

define void @xvavgr_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.h $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.h $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i16>, ptr %a
  %vb = load <16 x i16>, ptr %b
  %ea = zext <16 x i16> %va to <16 x i32>
  %eb = zext <16 x i16> %vb to <16 x i32>
  %add = add <16 x i32> %ea, %eb
  %add1 = add <16 x i32> %add, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %shr = lshr <16 x i32> %add1, <i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1, i32 1>
  %r = trunc <16 x i32> %shr to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @xvavgr_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.w $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.w $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i32>, ptr %a
  %vb = load <8 x i32>, ptr %b
  %ea = zext <8 x i32> %va to <8 x i64>
  %eb = zext <8 x i32> %vb to <8 x i64>
  %add = add <8 x i64> %ea, %eb
  %add1 = add <8 x i64> %add, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %shr = lshr <8 x i64> %add1, <i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1, i64 1>
  %r = trunc <8 x i64> %shr to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xvavgr_du(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xvavgr_du:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvor.v $xr2, $xr0, $xr1
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrli.d $xr0, $xr0, 1
; CHECK-NEXT:    xvsub.d $xr0, $xr2, $xr0
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i64>, ptr %a
  %vb = load <4 x i64>, ptr %b
  %ea = zext <4 x i64> %va to <4 x i128>
  %eb = zext <4 x i64> %vb to <4 x i128>
  %add = add <4 x i128> %ea, %eb
  %add1 = add <4 x i128> %add, <i128 1, i128 1, i128 1, i128 1>
  %shr = lshr <4 x i128> %add1, <i128 1, i128 1, i128 1, i128 1>
  %r = trunc <4 x i128> %shr to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}
