; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc -mtriple=loongarch64 -mattr=+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define <32 x i8> @xvssub_b(<32 x i8> %a, <32 x i8> %b) {
; CHECK-LABEL: xvssub_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvrepli.b $xr2, 0
; CHECK-NEXT:    xvslt.b $xr2, $xr2, $xr1
; CHECK-NEXT:    xvsub.b $xr1, $xr0, $xr1
; CHECK-NEXT:    xvslt.b $xr0, $xr1, $xr0
; CHECK-NEXT:    xvxor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvsrai.b $xr2, $xr1, 7
; CHECK-NEXT:    xvbitrevi.b $xr2, $xr2, 7
; CHECK-NEXT:    xvbitsel.v $xr0, $xr1, $xr2, $xr0
; CHECK-NEXT:    ret
  %ret = call <32 x i8> @llvm.ssub.sat.v32i8(<32 x i8> %a, <32 x i8> %b)
  ret <32 x i8> %ret
}

define <16 x i16> @xvssub_h(<16 x i16> %a, <16 x i16> %b) {
; CHECK-LABEL: xvssub_h:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvrepli.b $xr2, 0
; CHECK-NEXT:    xvslt.h $xr2, $xr2, $xr1
; CHECK-NEXT:    xvsub.h $xr1, $xr0, $xr1
; CHECK-NEXT:    xvslt.h $xr0, $xr1, $xr0
; CHECK-NEXT:    xvxor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvsrai.h $xr2, $xr1, 15
; CHECK-NEXT:    xvbitrevi.h $xr2, $xr2, 15
; CHECK-NEXT:    xvbitsel.v $xr0, $xr1, $xr2, $xr0
; CHECK-NEXT:    ret
  %ret = call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %a, <16 x i16> %b)
  ret <16 x i16> %ret
}

define <8 x i32> @xvssub_w(<8 x i32> %a, <8 x i32> %b) {
; CHECK-LABEL: xvssub_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvrepli.b $xr2, 0
; CHECK-NEXT:    xvslt.w $xr2, $xr2, $xr1
; CHECK-NEXT:    xvsub.w $xr1, $xr0, $xr1
; CHECK-NEXT:    xvslt.w $xr0, $xr1, $xr0
; CHECK-NEXT:    xvxor.v $xr0, $xr2, $xr0
; CHECK-NEXT:    xvsrai.w $xr2, $xr1, 31
; CHECK-NEXT:    xvbitrevi.w $xr2, $xr2, 31
; CHECK-NEXT:    xvbitsel.v $xr0, $xr1, $xr2, $xr0
; CHECK-NEXT:    ret
  %ret = call <8 x i32> @llvm.ssub.sat.v8i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %ret
}

define <4 x i64> @xvssub_d(<4 x i64> %a, <4 x i64> %b) {
; LA32-LABEL: xvssub_d:
; LA32:       # %bb.0:
; LA32-NEXT:    xvrepli.b $xr2, 0
; LA32-NEXT:    xvslt.d $xr2, $xr2, $xr1
; LA32-NEXT:    xvsub.d $xr1, $xr0, $xr1
; LA32-NEXT:    pcalau12i $a0, %pc_hi20(.LCPI3_0)
; LA32-NEXT:    xvld $xr3, $a0, %pc_lo12(.LCPI3_0)
; LA32-NEXT:    xvslt.d $xr0, $xr1, $xr0
; LA32-NEXT:    xvxor.v $xr0, $xr2, $xr0
; LA32-NEXT:    xvsrai.d $xr2, $xr1, 63
; LA32-NEXT:    xvxor.v $xr2, $xr2, $xr3
; LA32-NEXT:    xvbitsel.v $xr0, $xr1, $xr2, $xr0
; LA32-NEXT:    ret
;
; LA64-LABEL: xvssub_d:
; LA64:       # %bb.0:
; LA64-NEXT:    xvrepli.b $xr2, 0
; LA64-NEXT:    xvslt.d $xr2, $xr2, $xr1
; LA64-NEXT:    xvsub.d $xr1, $xr0, $xr1
; LA64-NEXT:    xvslt.d $xr0, $xr1, $xr0
; LA64-NEXT:    xvxor.v $xr0, $xr2, $xr0
; LA64-NEXT:    xvsrai.d $xr2, $xr1, 63
; LA64-NEXT:    xvbitrevi.d $xr2, $xr2, 63
; LA64-NEXT:    xvbitsel.v $xr0, $xr1, $xr2, $xr0
; LA64-NEXT:    ret
  %ret = call <4 x i64> @llvm.ssub.sat.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %ret
}

declare <32 x i8> @llvm.ssub.sat.v32i8(<32 x i8>, <32 x i8>)
declare <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16>, <16 x i16>)
declare <8 x i32> @llvm.ssub.sat.v8i32(<8 x i32>, <8 x i32>)
declare <4 x i64> @llvm.ssub.sat.v4i64(<4 x i64>, <4 x i64>)
