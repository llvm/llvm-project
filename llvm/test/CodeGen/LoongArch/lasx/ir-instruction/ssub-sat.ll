; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s
; RUN: llc -mtriple=loongarch64 -mattr=+lasx < %s | FileCheck %s

define <32 x i8> @xvssub_b(<32 x i8> %a, <32 x i8> %b) {
; CHECK-LABEL: xvssub_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvssub.b $xr0, $xr0, $xr1
; CHECK-NEXT:    ret
  %ret = call <32 x i8> @llvm.ssub.sat.v32i8(<32 x i8> %a, <32 x i8> %b)
  ret <32 x i8> %ret
}

define <16 x i16> @xvssub_h(<16 x i16> %a, <16 x i16> %b) {
; CHECK-LABEL: xvssub_h:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvssub.h $xr0, $xr0, $xr1
; CHECK-NEXT:    ret
  %ret = call <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16> %a, <16 x i16> %b)
  ret <16 x i16> %ret
}

define <8 x i32> @xvssub_w(<8 x i32> %a, <8 x i32> %b) {
; CHECK-LABEL: xvssub_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvssub.w $xr0, $xr0, $xr1
; CHECK-NEXT:    ret
  %ret = call <8 x i32> @llvm.ssub.sat.v8i32(<8 x i32> %a, <8 x i32> %b)
  ret <8 x i32> %ret
}

define <4 x i64> @xvssub_d(<4 x i64> %a, <4 x i64> %b) {
; CHECK-LABEL: xvssub_d:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvssub.d $xr0, $xr0, $xr1
; CHECK-NEXT:    ret
  %ret = call <4 x i64> @llvm.ssub.sat.v4i64(<4 x i64> %a, <4 x i64> %b)
  ret <4 x i64> %ret
}

declare <32 x i8> @llvm.ssub.sat.v32i8(<32 x i8>, <32 x i8>)
declare <16 x i16> @llvm.ssub.sat.v16i16(<16 x i16>, <16 x i16>)
declare <8 x i32> @llvm.ssub.sat.v8i32(<8 x i32>, <8 x i32>)
declare <4 x i64> @llvm.ssub.sat.v4i64(<4 x i64>, <4 x i64>)
