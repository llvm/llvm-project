; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s --check-prefix=LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s --check-prefix=LA64

define <64 x i64> @test1(<64 x i64> %0) nounwind {
; LA32-LABEL: test1:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -32
; LA32-NEXT:    st.w $ra, $sp, 28 # 4-byte Folded Spill
; LA32-NEXT:    st.w $fp, $sp, 24 # 4-byte Folded Spill
; LA32-NEXT:    addi.w $fp, $sp, 32
; LA32-NEXT:    bstrins.w $sp, $zero, 4, 0
; LA32-NEXT:    xvld $xr8, $fp, 0
; LA32-NEXT:    xvld $xr9, $fp, 32
; LA32-NEXT:    xvld $xr10, $fp, 64
; LA32-NEXT:    xvld $xr11, $fp, 96
; LA32-NEXT:    xvld $xr12, $fp, 224
; LA32-NEXT:    xvld $xr13, $fp, 192
; LA32-NEXT:    xvld $xr14, $fp, 160
; LA32-NEXT:    xvld $xr15, $fp, 128
; LA32-NEXT:    xvst $xr12, $a0, 480
; LA32-NEXT:    xvst $xr13, $a0, 448
; LA32-NEXT:    xvst $xr14, $a0, 416
; LA32-NEXT:    xvst $xr15, $a0, 384
; LA32-NEXT:    xvst $xr11, $a0, 352
; LA32-NEXT:    xvst $xr10, $a0, 320
; LA32-NEXT:    xvst $xr9, $a0, 288
; LA32-NEXT:    xvst $xr8, $a0, 256
; LA32-NEXT:    xvst $xr7, $a0, 224
; LA32-NEXT:    xvst $xr6, $a0, 192
; LA32-NEXT:    xvst $xr5, $a0, 160
; LA32-NEXT:    xvst $xr4, $a0, 128
; LA32-NEXT:    xvst $xr3, $a0, 96
; LA32-NEXT:    xvst $xr2, $a0, 64
; LA32-NEXT:    xvst $xr1, $a0, 32
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    addi.w $sp, $fp, -32
; LA32-NEXT:    ld.w $fp, $sp, 24 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $ra, $sp, 28 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 32
; LA32-NEXT:    ret
;
; LA64-LABEL: test1:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    addi.d $sp, $sp, -32
; LA64-NEXT:    st.d $ra, $sp, 24 # 8-byte Folded Spill
; LA64-NEXT:    st.d $fp, $sp, 16 # 8-byte Folded Spill
; LA64-NEXT:    addi.d $fp, $sp, 32
; LA64-NEXT:    bstrins.d $sp, $zero, 4, 0
; LA64-NEXT:    xvld $xr8, $fp, 0
; LA64-NEXT:    xvld $xr9, $fp, 32
; LA64-NEXT:    xvld $xr10, $fp, 64
; LA64-NEXT:    xvld $xr11, $fp, 96
; LA64-NEXT:    xvld $xr12, $fp, 224
; LA64-NEXT:    xvld $xr13, $fp, 192
; LA64-NEXT:    xvld $xr14, $fp, 160
; LA64-NEXT:    xvld $xr15, $fp, 128
; LA64-NEXT:    xvst $xr12, $a0, 480
; LA64-NEXT:    xvst $xr13, $a0, 448
; LA64-NEXT:    xvst $xr14, $a0, 416
; LA64-NEXT:    xvst $xr15, $a0, 384
; LA64-NEXT:    xvst $xr11, $a0, 352
; LA64-NEXT:    xvst $xr10, $a0, 320
; LA64-NEXT:    xvst $xr9, $a0, 288
; LA64-NEXT:    xvst $xr8, $a0, 256
; LA64-NEXT:    xvst $xr7, $a0, 224
; LA64-NEXT:    xvst $xr6, $a0, 192
; LA64-NEXT:    xvst $xr5, $a0, 160
; LA64-NEXT:    xvst $xr4, $a0, 128
; LA64-NEXT:    xvst $xr3, $a0, 96
; LA64-NEXT:    xvst $xr2, $a0, 64
; LA64-NEXT:    xvst $xr1, $a0, 32
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    addi.d $sp, $fp, -32
; LA64-NEXT:    ld.d $fp, $sp, 16 # 8-byte Folded Reload
; LA64-NEXT:    ld.d $ra, $sp, 24 # 8-byte Folded Reload
; LA64-NEXT:    addi.d $sp, $sp, 32
; LA64-NEXT:    ret
entry:
  ret <64 x i64> %0
}

define <32 x double> @test2(<32 x double> %0, <32 x double> %1) nounwind {
; LA32-LABEL: test2:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -32
; LA32-NEXT:    st.w $ra, $sp, 28 # 4-byte Folded Spill
; LA32-NEXT:    st.w $fp, $sp, 24 # 4-byte Folded Spill
; LA32-NEXT:    addi.w $fp, $sp, 32
; LA32-NEXT:    bstrins.w $sp, $zero, 4, 0
; LA32-NEXT:    xvst $xr7, $a0, 224
; LA32-NEXT:    xvst $xr6, $a0, 192
; LA32-NEXT:    xvst $xr5, $a0, 160
; LA32-NEXT:    xvst $xr4, $a0, 128
; LA32-NEXT:    xvst $xr3, $a0, 96
; LA32-NEXT:    xvst $xr2, $a0, 64
; LA32-NEXT:    xvst $xr1, $a0, 32
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    addi.w $sp, $fp, -32
; LA32-NEXT:    ld.w $fp, $sp, 24 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $ra, $sp, 28 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 32
; LA32-NEXT:    ret
;
; LA64-LABEL: test2:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    addi.d $sp, $sp, -32
; LA64-NEXT:    st.d $ra, $sp, 24 # 8-byte Folded Spill
; LA64-NEXT:    st.d $fp, $sp, 16 # 8-byte Folded Spill
; LA64-NEXT:    addi.d $fp, $sp, 32
; LA64-NEXT:    bstrins.d $sp, $zero, 4, 0
; LA64-NEXT:    xvst $xr7, $a0, 224
; LA64-NEXT:    xvst $xr6, $a0, 192
; LA64-NEXT:    xvst $xr5, $a0, 160
; LA64-NEXT:    xvst $xr4, $a0, 128
; LA64-NEXT:    xvst $xr3, $a0, 96
; LA64-NEXT:    xvst $xr2, $a0, 64
; LA64-NEXT:    xvst $xr1, $a0, 32
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    addi.d $sp, $fp, -32
; LA64-NEXT:    ld.d $fp, $sp, 16 # 8-byte Folded Reload
; LA64-NEXT:    ld.d $ra, $sp, 24 # 8-byte Folded Reload
; LA64-NEXT:    addi.d $sp, $sp, 32
; LA64-NEXT:    ret
entry:
  ret <32 x double> %0
}
