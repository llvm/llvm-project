; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s

define void @vabs_b(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.b $xr1, $xr0
; CHECK-NEXT:    xvmax.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <32 x i8>, ptr %src
  %b = tail call <32 x i8> @llvm.abs.v32i8(<32 x i8> %a, i1 true)
  store <32 x i8> %b, ptr %dst
  ret void
}

define void @vabs_b_1(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_b_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.b $xr1, $xr0
; CHECK-NEXT:    xvmax.b $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <32 x i8>, ptr %src
  %b = tail call <32 x i8> @llvm.abs.v32i8(<32 x i8> %a, i1 false)
  store <32 x i8> %b, ptr %dst
  ret void
}

define void @vabs_h(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.h $xr1, $xr0
; CHECK-NEXT:    xvmax.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <16 x i16>, ptr %src
  %b = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> %a, i1 true)
  store <16 x i16> %b, ptr %dst
  ret void
}

define void @vabs_h_1(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_h_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.h $xr1, $xr0
; CHECK-NEXT:    xvmax.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <16 x i16>, ptr %src
  %b = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> %a, i1 false)
  store <16 x i16> %b, ptr %dst
  ret void
}

define void @vabs_w(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.w $xr1, $xr0
; CHECK-NEXT:    xvmax.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <8 x i32>, ptr %src
  %b = tail call <8 x i32> @llvm.abs.v8i32(<8 x i32> %a, i1 true)
  store <8 x i32> %b, ptr %dst
  ret void
}

define void @vabs_w_1(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_w_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.w $xr1, $xr0
; CHECK-NEXT:    xvmax.w $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <8 x i32>, ptr %src
  %b = tail call <8 x i32> @llvm.abs.v8i32(<8 x i32> %a, i1 false)
  store <8 x i32> %b, ptr %dst
  ret void
}

define void @vabs_d(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_d:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.d $xr1, $xr0
; CHECK-NEXT:    xvmax.d $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <4 x i64>, ptr %src
  %b = tail call <4 x i64> @llvm.abs.v4i64(<4 x i64> %a, i1 true)
  store <4 x i64> %b, ptr %dst
  ret void
}

define void @vabs_d_1(ptr %dst, ptr %src) {
; CHECK-LABEL: vabs_d_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvneg.d $xr1, $xr0
; CHECK-NEXT:    xvmax.d $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %a = load <4 x i64>, ptr %src
  %b = tail call <4 x i64> @llvm.abs.v4i64(<4 x i64> %a, i1 false)
  store <4 x i64> %b, ptr %dst
  ret void
}

declare <32 x i8> @llvm.abs.v32i8(<32 x i8>, i1)
declare <16 x i16> @llvm.abs.v16i16(<16 x i16>, i1)
declare <8 x i32> @llvm.abs.v8i32(<8 x i32>, i1)
declare <4 x i64> @llvm.abs.v4i64(<4 x i64>, i1)
