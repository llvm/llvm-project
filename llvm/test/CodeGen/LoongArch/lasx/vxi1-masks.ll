; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define void @xor_zext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: xor_zext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vextrins.w $vr0, $vr0, 33
; LA32-NEXT:    xvpermi.q $xr0, $xr1, 2
; LA32-NEXT:    xvrepli.d $xr1, 1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: xor_zext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI0_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI0_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvrepli.d $xr1, 1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mxor = xor <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = zext <4 x i1> %mxor to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @xor_zext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xor_zext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -1789
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mxor = xor <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <8 x i1> %mxor to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xor_zext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xor_zext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 255
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.h $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mxor = xor <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <16 x i1> %mxor to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @xor_sext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: xor_sext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vxor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vori.b $vr1, $vr0, 0
; LA32-NEXT:    vextrins.w $vr1, $vr1, 16
; LA32-NEXT:    vextrins.w $vr1, $vr0, 33
; LA32-NEXT:    vextrins.w $vr1, $vr0, 49
; LA32-NEXT:    vextrins.w $vr2, $vr0, 2
; LA32-NEXT:    vextrins.w $vr2, $vr0, 18
; LA32-NEXT:    vextrins.w $vr2, $vr0, 35
; LA32-NEXT:    vextrins.w $vr2, $vr0, 51
; LA32-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA32-NEXT:    xvst $xr1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: xor_sext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI3_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI3_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvxor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvslli.d $xr0, $xr0, 32
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 32
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mxor = xor <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = sext <4 x i1> %mxor to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @xor_sext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xor_sext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -1789
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvslli.w $xr0, $xr0, 16
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 16
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mxor = xor <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <8 x i1> %mxor to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @xor_sext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: xor_sext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 255
; CHECK-NEXT:    xvxor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvslli.h $xr0, $xr0, 8
; CHECK-NEXT:    xvsrai.h $xr0, $xr0, 8
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mxor = xor <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <16 x i1> %mxor to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @or_zext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: or_zext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vextrins.w $vr0, $vr0, 33
; LA32-NEXT:    xvpermi.q $xr0, $xr1, 2
; LA32-NEXT:    xvrepli.d $xr1, 1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: or_zext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI6_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI6_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvrepli.d $xr1, 1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mor = or <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = zext <4 x i1> %mor to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @or_zext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: or_zext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -1789
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mor = or <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <8 x i1> %mor to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @or_zext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: or_zext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 255
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.h $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mor = or <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <16 x i1> %mor to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @or_sext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: or_sext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 2
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 6
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 3
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vor.v $vr0, $vr1, $vr0
; LA32-NEXT:    vori.b $vr1, $vr0, 0
; LA32-NEXT:    vextrins.w $vr1, $vr1, 16
; LA32-NEXT:    vextrins.w $vr1, $vr0, 33
; LA32-NEXT:    vextrins.w $vr1, $vr0, 49
; LA32-NEXT:    vextrins.w $vr2, $vr0, 2
; LA32-NEXT:    vextrins.w $vr2, $vr0, 18
; LA32-NEXT:    vextrins.w $vr2, $vr0, 35
; LA32-NEXT:    vextrins.w $vr2, $vr0, 51
; LA32-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA32-NEXT:    xvst $xr1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: or_sext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI9_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI9_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvor.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 32
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mor = or <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = sext <4 x i1> %mor to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @or_sext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: or_sext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -1780
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 16
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mor = or <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <8 x i1> %mor to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @or_sext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: or_sext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -3585
; CHECK-NEXT:    xvor.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.h $xr0, $xr0, 8
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mor = or <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <16 x i1> %mor to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @and_zext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: and_zext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vand.v $vr0, $vr1, $vr0
; LA32-NEXT:    vextrins.w $vr1, $vr0, 2
; LA32-NEXT:    vextrins.w $vr1, $vr0, 35
; LA32-NEXT:    vextrins.w $vr0, $vr0, 33
; LA32-NEXT:    xvpermi.q $xr0, $xr1, 2
; LA32-NEXT:    xvrepli.d $xr1, 1
; LA32-NEXT:    xvand.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvst $xr0, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: and_zext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI12_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI12_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mand = and <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = zext <4 x i1> %mand to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @and_zext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: and_zext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.d $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mand = and <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <8 x i1> %mand to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @and_zext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: and_zext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvrepli.w $xr1, 1
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mand = and <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = zext <16 x i1> %mand to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}

define void @and_sext_masks_v4i64(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: and_sext_masks_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    xvld $xr0, $a1, 0
; LA32-NEXT:    xvld $xr1, $a2, 0
; LA32-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 0
; LA32-NEXT:    xvpickve2gr.w $a1, $xr0, 4
; LA32-NEXT:    vinsgr2vr.w $vr1, $a1, 2
; LA32-NEXT:    vldi $vr0, -1777
; LA32-NEXT:    vand.v $vr0, $vr1, $vr0
; LA32-NEXT:    vori.b $vr1, $vr0, 0
; LA32-NEXT:    vextrins.w $vr1, $vr1, 16
; LA32-NEXT:    vextrins.w $vr1, $vr0, 33
; LA32-NEXT:    vextrins.w $vr1, $vr0, 49
; LA32-NEXT:    vextrins.w $vr2, $vr0, 2
; LA32-NEXT:    vextrins.w $vr2, $vr0, 18
; LA32-NEXT:    vextrins.w $vr2, $vr0, 35
; LA32-NEXT:    vextrins.w $vr2, $vr0, 51
; LA32-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA32-NEXT:    xvst $xr1, $a0, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: and_sext_masks_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvld $xr0, $a1, 0
; LA64-NEXT:    xvld $xr1, $a2, 0
; LA64-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI15_0)
; LA64-NEXT:    xvld $xr2, $a1, %pc_lo12(.LCPI15_0)
; LA64-NEXT:    xvfcmp.clt.d $xr0, $xr0, $xr1
; LA64-NEXT:    xvand.v $xr0, $xr0, $xr2
; LA64-NEXT:    xvsrai.d $xr0, $xr0, 32
; LA64-NEXT:    xvst $xr0, $a0, 0
; LA64-NEXT:    ret
  %v0 = load <4 x double>, ptr %a
  %v1 = load <4 x double>, ptr %b
  %m0 = fcmp olt <4 x double> %v0, %v1
  %mand = and <4 x i1> %m0, <i1 1, i1 0, i1 1, i1 0>
  %r = sext <4 x i1> %mand to <4 x i64>
  store <4 x i64> %r, ptr %res
  ret void
}

define void @and_sext_masks_v8i32(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: and_sext_masks_v8i32:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvfcmp.clt.s $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -1780
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.w $xr0, $xr0, 16
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <8 x float>, ptr %a
  %v1 = load <8 x float>, ptr %b
  %m0 = fcmp olt <8 x float> %v0, %v1
  %mand = and <8 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <8 x i1> %mand to <8 x i32>
  store <8 x i32> %r, ptr %res
  ret void
}

define void @and_sext_masks_v16i16(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: and_sext_masks_v16i16:
; CHECK:       # %bb.0:
; CHECK-NEXT:    xvld $xr0, $a1, 0
; CHECK-NEXT:    xvld $xr1, $a2, 0
; CHECK-NEXT:    xvseq.h $xr0, $xr0, $xr1
; CHECK-NEXT:    xvldi $xr1, -3585
; CHECK-NEXT:    xvand.v $xr0, $xr0, $xr1
; CHECK-NEXT:    xvsrai.h $xr0, $xr0, 8
; CHECK-NEXT:    xvst $xr0, $a0, 0
; CHECK-NEXT:    ret
  %v0 = load <16 x i16>, ptr %a
  %v1 = load <16 x i16>, ptr %b
  %m0 = icmp eq <16 x i16> %v0, %v1
  %mand = and <16 x i1> %m0, <i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0,
                              i1 1, i1 0, i1 1, i1 0, i1 1, i1 0, i1 1, i1 0>
  %r = sext <16 x i1> %mand to <16 x i16>
  store <16 x i16> %r, ptr %res
  ret void
}
