; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc --mtriple=loongarch64 --mattr=+lasx < %s | FileCheck %s

declare <8 x float> @llvm.powi.v8f32.i32(<8 x float>, i32)

define <8 x float> @powi_v8f32(<8 x float> %va, i32 %b) nounwind {
; CHECK-LABEL: powi_v8f32:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -80
; CHECK-NEXT:    st.d $ra, $sp, 72 # 8-byte Folded Spill
; CHECK-NEXT:    st.d $fp, $sp, 64 # 8-byte Folded Spill
; CHECK-NEXT:    xvst $xr0, $sp, 0 # 32-byte Folded Spill
; CHECK-NEXT:    addi.w $fp, $a0, 0
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 0
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 0
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 1
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 1
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 2
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 2
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 3
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 3
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 4
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 4
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 5
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 5
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 6
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 6
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.w $a0, $xr0, 7
; CHECK-NEXT:    movgr2fr.w $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powisf2)
; CHECK-NEXT:    movfr2gr.s $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.w $xr0, $a0, 7
; CHECK-NEXT:    ld.d $fp, $sp, 64 # 8-byte Folded Reload
; CHECK-NEXT:    ld.d $ra, $sp, 72 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 80
; CHECK-NEXT:    ret
entry:
  %res = call <8 x float> @llvm.powi.v8f32.i32(<8 x float> %va, i32 %b)
  ret <8 x float> %res
}

declare <4 x double> @llvm.powi.v4f64.i32(<4 x double>, i32)

define <4 x double> @powi_v4f64(<4 x double> %va, i32 %b) nounwind {
; CHECK-LABEL: powi_v4f64:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    addi.d $sp, $sp, -80
; CHECK-NEXT:    st.d $ra, $sp, 72 # 8-byte Folded Spill
; CHECK-NEXT:    st.d $fp, $sp, 64 # 8-byte Folded Spill
; CHECK-NEXT:    xvst $xr0, $sp, 0 # 32-byte Folded Spill
; CHECK-NEXT:    addi.w $fp, $a0, 0
; CHECK-NEXT:    xvpickve2gr.d $a0, $xr0, 0
; CHECK-NEXT:    movgr2fr.d $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powidf2)
; CHECK-NEXT:    movfr2gr.d $a0, $fa0
; CHECK-NEXT:    xvinsgr2vr.d $xr0, $a0, 0
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.d $a0, $xr0, 1
; CHECK-NEXT:    movgr2fr.d $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powidf2)
; CHECK-NEXT:    movfr2gr.d $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.d $xr0, $a0, 1
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.d $a0, $xr0, 2
; CHECK-NEXT:    movgr2fr.d $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powidf2)
; CHECK-NEXT:    movfr2gr.d $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.d $xr0, $a0, 2
; CHECK-NEXT:    xvst $xr0, $sp, 32 # 32-byte Folded Spill
; CHECK-NEXT:    xvld $xr0, $sp, 0 # 32-byte Folded Reload
; CHECK-NEXT:    xvpickve2gr.d $a0, $xr0, 3
; CHECK-NEXT:    movgr2fr.d $fa0, $a0
; CHECK-NEXT:    move $a0, $fp
; CHECK-NEXT:    bl %plt(__powidf2)
; CHECK-NEXT:    movfr2gr.d $a0, $fa0
; CHECK-NEXT:    xvld $xr0, $sp, 32 # 32-byte Folded Reload
; CHECK-NEXT:    xvinsgr2vr.d $xr0, $a0, 3
; CHECK-NEXT:    ld.d $fp, $sp, 64 # 8-byte Folded Reload
; CHECK-NEXT:    ld.d $ra, $sp, 72 # 8-byte Folded Reload
; CHECK-NEXT:    addi.d $sp, $sp, 80
; CHECK-NEXT:    ret
entry:
  %res = call <4 x double> @llvm.powi.v4f64.i32(<4 x double> %va, i32 %b)
  ret <4 x double> %res
}
