; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 5
; RUN: llc --mtriple=loongarch32 -mattr=+32s,+lasx --verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=LA32
; RUN: llc --mtriple=loongarch64 -mattr=+lasx --verify-machineinstrs < %s \
; RUN:   | FileCheck %s --check-prefix=LA64

declare <32 x i8> @llvm.bitreverse.v32i8(<32 x i8>)

define <32 x i8> @test_bitreverse_v32i8(<32 x i8> %a) nounwind {
; LA32-LABEL: test_bitreverse_v32i8:
; LA32:       # %bb.0:
; LA32-NEXT:    xvslli.b $xr1, $xr0, 4
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 4
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvandi.b $xr1, $xr0, 51
; LA32-NEXT:    xvslli.b $xr1, $xr1, 2
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 2
; LA32-NEXT:    xvandi.b $xr0, $xr0, 51
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvandi.b $xr1, $xr0, 85
; LA32-NEXT:    xvslli.b $xr1, $xr1, 1
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 1
; LA32-NEXT:    xvandi.b $xr0, $xr0, 85
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    ret
;
; LA64-LABEL: test_bitreverse_v32i8:
; LA64:       # %bb.0:
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 2
; LA64-NEXT:    bitrev.8b $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 3
; LA64-NEXT:    bitrev.8b $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 1
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 0
; LA64-NEXT:    bitrev.8b $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 1
; LA64-NEXT:    bitrev.8b $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 1
; LA64-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA64-NEXT:    xvori.b $xr0, $xr1, 0
; LA64-NEXT:    ret
  %b = call <32 x i8> @llvm.bitreverse.v32i8(<32 x i8> %a)
  ret <32 x i8> %b
}

declare <16 x i16> @llvm.bitreverse.v16i16(<16 x i16>)

define <16 x i16> @test_bitreverse_v16i16(<16 x i16> %a) nounwind {
; LA32-LABEL: test_bitreverse_v16i16:
; LA32:       # %bb.0:
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 5
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 0
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 4
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 1
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 7
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 2
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 6
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 3
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 1
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 0
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 0
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 1
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 3
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 2
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 2
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 3
; LA32-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA32-NEXT:    xvshuf4i.h $xr0, $xr2, 27
; LA32-NEXT:    ret
;
; LA64-LABEL: test_bitreverse_v16i16:
; LA64:       # %bb.0:
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 2
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 3
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 1
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 0
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 1
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvshuf4i.h $xr0, $xr2, 27
; LA64-NEXT:    ret
  %b = call <16 x i16> @llvm.bitreverse.v16i16(<16 x i16> %a)
  ret <16 x i16> %b
}

declare <8 x i32> @llvm.bitreverse.v8i32(<8 x i32>)

define <8 x i32> @test_bitreverse_v8i32(<8 x i32> %a) nounwind {
; LA32-LABEL: test_bitreverse_v8i32:
; LA32:       # %bb.0:
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 4
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 0
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 5
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 1
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 6
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 2
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 7
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr2, $a0, 3
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 0
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 0
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 1
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 1
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 2
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 2
; LA32-NEXT:    xvpickve2gr.w $a0, $xr0, 3
; LA32-NEXT:    bitrev.w $a0, $a0
; LA32-NEXT:    vinsgr2vr.w $vr1, $a0, 3
; LA32-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA32-NEXT:    xvori.b $xr0, $xr1, 0
; LA32-NEXT:    ret
;
; LA64-LABEL: test_bitreverse_v8i32:
; LA64:       # %bb.0:
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 2
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 3
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 1
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 0
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 1
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 1
; LA64-NEXT:    xvpermi.q $xr2, $xr1, 2
; LA64-NEXT:    xvshuf4i.w $xr0, $xr2, 177
; LA64-NEXT:    ret
  %b = call <8 x i32> @llvm.bitreverse.v8i32(<8 x i32> %a)
  ret <8 x i32> %b
}

declare <4 x i64> @llvm.bitreverse.v4i64(<4 x i64>)

define <4 x i64> @test_bitreverse_v4i64(<4 x i64> %a) nounwind {
; LA32-LABEL: test_bitreverse_v4i64:
; LA32:       # %bb.0:
; LA32-NEXT:    pcalau12i $a0, %pc_hi20(.LCPI3_0)
; LA32-NEXT:    xvld $xr1, $a0, %pc_lo12(.LCPI3_0)
; LA32-NEXT:    xvshuf.b $xr0, $xr0, $xr0, $xr1
; LA32-NEXT:    xvslli.b $xr1, $xr0, 4
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 4
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvandi.b $xr1, $xr0, 51
; LA32-NEXT:    xvslli.b $xr1, $xr1, 2
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 2
; LA32-NEXT:    xvandi.b $xr0, $xr0, 51
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    xvandi.b $xr1, $xr0, 85
; LA32-NEXT:    xvslli.b $xr1, $xr1, 1
; LA32-NEXT:    xvsrli.b $xr0, $xr0, 1
; LA32-NEXT:    xvandi.b $xr0, $xr0, 85
; LA32-NEXT:    xvor.v $xr0, $xr0, $xr1
; LA32-NEXT:    ret
;
; LA64-LABEL: test_bitreverse_v4i64:
; LA64:       # %bb.0:
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 2
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 3
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr2, $a0, 1
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 0
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 0
; LA64-NEXT:    xvpickve2gr.d $a0, $xr0, 1
; LA64-NEXT:    bitrev.d $a0, $a0
; LA64-NEXT:    vinsgr2vr.d $vr1, $a0, 1
; LA64-NEXT:    xvpermi.q $xr1, $xr2, 2
; LA64-NEXT:    xvori.b $xr0, $xr1, 0
; LA64-NEXT:    ret
  %b = call <4 x i64> @llvm.bitreverse.v4i64(<4 x i64> %a)
  ret <4 x i64> %b
}
