; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lsx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc -mtriple=loongarch64 -mattr=+lsx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define <16 x i8> @vssub_b(<16 x i8> %a, <16 x i8> %b) {
; CHECK-LABEL: vssub_b:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vslt.b $vr2, $vr2, $vr1
; CHECK-NEXT:    vsub.b $vr1, $vr0, $vr1
; CHECK-NEXT:    vslt.b $vr0, $vr1, $vr0
; CHECK-NEXT:    vxor.v $vr0, $vr2, $vr0
; CHECK-NEXT:    vsrai.b $vr2, $vr1, 7
; CHECK-NEXT:    vbitrevi.b $vr2, $vr2, 7
; CHECK-NEXT:    vbitsel.v $vr0, $vr1, $vr2, $vr0
; CHECK-NEXT:    ret
  %ret = call <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8> %a, <16 x i8> %b)
  ret <16 x i8> %ret
}

define <8 x i16> @vssub_h(<8 x i16> %a, <8 x i16> %b) {
; CHECK-LABEL: vssub_h:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vslt.h $vr2, $vr2, $vr1
; CHECK-NEXT:    vsub.h $vr1, $vr0, $vr1
; CHECK-NEXT:    vslt.h $vr0, $vr1, $vr0
; CHECK-NEXT:    vxor.v $vr0, $vr2, $vr0
; CHECK-NEXT:    vsrai.h $vr2, $vr1, 15
; CHECK-NEXT:    vbitrevi.h $vr2, $vr2, 15
; CHECK-NEXT:    vbitsel.v $vr0, $vr1, $vr2, $vr0
; CHECK-NEXT:    ret
  %ret = call <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16> %a, <8 x i16> %b)
  ret <8 x i16> %ret
}

define <4 x i32> @vssub_w(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: vssub_w:
; CHECK:       # %bb.0:
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vslt.w $vr2, $vr2, $vr1
; CHECK-NEXT:    vsub.w $vr1, $vr0, $vr1
; CHECK-NEXT:    vslt.w $vr0, $vr1, $vr0
; CHECK-NEXT:    vxor.v $vr0, $vr2, $vr0
; CHECK-NEXT:    vsrai.w $vr2, $vr1, 31
; CHECK-NEXT:    vbitrevi.w $vr2, $vr2, 31
; CHECK-NEXT:    vbitsel.v $vr0, $vr1, $vr2, $vr0
; CHECK-NEXT:    ret
  %ret = call <4 x i32> @llvm.ssub.sat.v4i32(<4 x i32> %a, <4 x i32> %b)
  ret <4 x i32> %ret
}

define <2 x i64> @vssub_d(<2 x i64> %a, <2 x i64> %b) {
; LA32-LABEL: vssub_d:
; LA32:       # %bb.0:
; LA32-NEXT:    vrepli.b $vr2, 0
; LA32-NEXT:    vslt.d $vr2, $vr2, $vr1
; LA32-NEXT:    vsub.d $vr1, $vr0, $vr1
; LA32-NEXT:    pcalau12i $a0, %pc_hi20(.LCPI3_0)
; LA32-NEXT:    vld $vr3, $a0, %pc_lo12(.LCPI3_0)
; LA32-NEXT:    vslt.d $vr0, $vr1, $vr0
; LA32-NEXT:    vxor.v $vr0, $vr2, $vr0
; LA32-NEXT:    vsrai.d $vr2, $vr1, 63
; LA32-NEXT:    vxor.v $vr2, $vr2, $vr3
; LA32-NEXT:    vbitsel.v $vr0, $vr1, $vr2, $vr0
; LA32-NEXT:    ret
;
; LA64-LABEL: vssub_d:
; LA64:       # %bb.0:
; LA64-NEXT:    vrepli.b $vr2, 0
; LA64-NEXT:    vslt.d $vr2, $vr2, $vr1
; LA64-NEXT:    vsub.d $vr1, $vr0, $vr1
; LA64-NEXT:    vslt.d $vr0, $vr1, $vr0
; LA64-NEXT:    vxor.v $vr0, $vr2, $vr0
; LA64-NEXT:    vsrai.d $vr2, $vr1, 63
; LA64-NEXT:    vbitrevi.d $vr2, $vr2, 63
; LA64-NEXT:    vbitsel.v $vr0, $vr1, $vr2, $vr0
; LA64-NEXT:    ret
  %ret = call <2 x i64> @llvm.ssub.sat.v2i64(<2 x i64> %a, <2 x i64> %b)
  ret <2 x i64> %ret
}

declare <16 x i8> @llvm.ssub.sat.v16i8(<16 x i8>, <16 x i8>)
declare <8 x i16> @llvm.ssub.sat.v8i16(<8 x i16>, <8 x i16>)
declare <4 x i32> @llvm.ssub.sat.v4i32(<4 x i32>, <4 x i32>)
declare <2 x i64> @llvm.ssub.sat.v2i64(<2 x i64>, <2 x i64>)
