; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lsx < %s | FileCheck %s --check-prefix=LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lsx < %s | FileCheck %s --check-prefix=LA64

declare <4 x float> @llvm.log2.v4f32(<4 x float>)
declare <2 x double> @llvm.log2.v2f64(<2 x double>)

define void @flog2_v4f32(ptr %res, ptr %a) nounwind {
; LA32-LABEL: flog2_v4f32:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -48
; LA32-NEXT:    st.w $ra, $sp, 44 # 4-byte Folded Spill
; LA32-NEXT:    st.w $fp, $sp, 40 # 4-byte Folded Spill
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vst $vr0, $sp, 16 # 16-byte Folded Spill
; LA32-NEXT:    move $fp, $a0
; LA32-NEXT:    vreplvei.w $vr0, $vr0, 1
; LA32-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA32-NEXT:    bl log2f
; LA32-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA32-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA32-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA32-NEXT:    vreplvei.w $vr0, $vr0, 0
; LA32-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA32-NEXT:    bl log2f
; LA32-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA32-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA32-NEXT:    vextrins.w $vr0, $vr1, 16
; LA32-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA32-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA32-NEXT:    vreplvei.w $vr0, $vr0, 2
; LA32-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA32-NEXT:    bl log2f
; LA32-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA32-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA32-NEXT:    vextrins.w $vr1, $vr0, 32
; LA32-NEXT:    vst $vr1, $sp, 0 # 16-byte Folded Spill
; LA32-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA32-NEXT:    vreplvei.w $vr0, $vr0, 3
; LA32-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA32-NEXT:    bl log2f
; LA32-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA32-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA32-NEXT:    vextrins.w $vr1, $vr0, 48
; LA32-NEXT:    vst $vr1, $fp, 0
; LA32-NEXT:    ld.w $fp, $sp, 40 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $ra, $sp, 44 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 48
; LA32-NEXT:    ret
;
; LA64-LABEL: flog2_v4f32:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    addi.d $sp, $sp, -48
; LA64-NEXT:    st.d $ra, $sp, 40 # 8-byte Folded Spill
; LA64-NEXT:    st.d $fp, $sp, 32 # 8-byte Folded Spill
; LA64-NEXT:    vld $vr0, $a1, 0
; LA64-NEXT:    vst $vr0, $sp, 16 # 16-byte Folded Spill
; LA64-NEXT:    move $fp, $a0
; LA64-NEXT:    vreplvei.w $vr0, $vr0, 1
; LA64-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2f)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA64-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA64-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA64-NEXT:    vreplvei.w $vr0, $vr0, 0
; LA64-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2f)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA64-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA64-NEXT:    vextrins.w $vr0, $vr1, 16
; LA64-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA64-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA64-NEXT:    vreplvei.w $vr0, $vr0, 2
; LA64-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2f)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA64-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA64-NEXT:    vextrins.w $vr1, $vr0, 32
; LA64-NEXT:    vst $vr1, $sp, 0 # 16-byte Folded Spill
; LA64-NEXT:    vld $vr0, $sp, 16 # 16-byte Folded Reload
; LA64-NEXT:    vreplvei.w $vr0, $vr0, 3
; LA64-NEXT:    # kill: def $f0 killed $f0 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2f)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0 killed $f0 def $vr0
; LA64-NEXT:    vld $vr1, $sp, 0 # 16-byte Folded Reload
; LA64-NEXT:    vextrins.w $vr1, $vr0, 48
; LA64-NEXT:    vst $vr1, $fp, 0
; LA64-NEXT:    ld.d $fp, $sp, 32 # 8-byte Folded Reload
; LA64-NEXT:    ld.d $ra, $sp, 40 # 8-byte Folded Reload
; LA64-NEXT:    addi.d $sp, $sp, 48
; LA64-NEXT:    ret
entry:
  %v = load <4 x float>, ptr %a
  %r = call <4 x float> @llvm.log2.v4f32(<4 x float> %v)
  store <4 x float> %r, ptr %res
  ret void
}

define void @flog2_v2f64(ptr %res, ptr %a) nounwind {
; LA32-LABEL: flog2_v2f64:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    addi.w $sp, $sp, -48
; LA32-NEXT:    st.w $ra, $sp, 44 # 4-byte Folded Spill
; LA32-NEXT:    st.w $fp, $sp, 40 # 4-byte Folded Spill
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA32-NEXT:    move $fp, $a0
; LA32-NEXT:    vreplvei.d $vr0, $vr0, 1
; LA32-NEXT:    # kill: def $f0_64 killed $f0_64 killed $vr0
; LA32-NEXT:    bl log2
; LA32-NEXT:    # kill: def $f0_64 killed $f0_64 def $vr0
; LA32-NEXT:    vst $vr0, $sp, 16 # 16-byte Folded Spill
; LA32-NEXT:    vld $vr0, $sp, 0 # 16-byte Folded Reload
; LA32-NEXT:    vreplvei.d $vr0, $vr0, 0
; LA32-NEXT:    # kill: def $f0_64 killed $f0_64 killed $vr0
; LA32-NEXT:    bl log2
; LA32-NEXT:    # kill: def $f0_64 killed $f0_64 def $vr0
; LA32-NEXT:    vld $vr1, $sp, 16 # 16-byte Folded Reload
; LA32-NEXT:    vextrins.d $vr0, $vr1, 16
; LA32-NEXT:    vst $vr0, $fp, 0
; LA32-NEXT:    ld.w $fp, $sp, 40 # 4-byte Folded Reload
; LA32-NEXT:    ld.w $ra, $sp, 44 # 4-byte Folded Reload
; LA32-NEXT:    addi.w $sp, $sp, 48
; LA32-NEXT:    ret
;
; LA64-LABEL: flog2_v2f64:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    addi.d $sp, $sp, -48
; LA64-NEXT:    st.d $ra, $sp, 40 # 8-byte Folded Spill
; LA64-NEXT:    st.d $fp, $sp, 32 # 8-byte Folded Spill
; LA64-NEXT:    vld $vr0, $a1, 0
; LA64-NEXT:    vst $vr0, $sp, 0 # 16-byte Folded Spill
; LA64-NEXT:    move $fp, $a0
; LA64-NEXT:    vreplvei.d $vr0, $vr0, 1
; LA64-NEXT:    # kill: def $f0_64 killed $f0_64 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0_64 killed $f0_64 def $vr0
; LA64-NEXT:    vst $vr0, $sp, 16 # 16-byte Folded Spill
; LA64-NEXT:    vld $vr0, $sp, 0 # 16-byte Folded Reload
; LA64-NEXT:    vreplvei.d $vr0, $vr0, 0
; LA64-NEXT:    # kill: def $f0_64 killed $f0_64 killed $vr0
; LA64-NEXT:    pcaddu18i $ra, %call36(log2)
; LA64-NEXT:    jirl $ra, $ra, 0
; LA64-NEXT:    # kill: def $f0_64 killed $f0_64 def $vr0
; LA64-NEXT:    vld $vr1, $sp, 16 # 16-byte Folded Reload
; LA64-NEXT:    vextrins.d $vr0, $vr1, 16
; LA64-NEXT:    vst $vr0, $fp, 0
; LA64-NEXT:    ld.d $fp, $sp, 32 # 8-byte Folded Reload
; LA64-NEXT:    ld.d $ra, $sp, 40 # 8-byte Folded Reload
; LA64-NEXT:    addi.d $sp, $sp, 48
; LA64-NEXT:    ret
entry:
  %v = load <2 x double>, ptr %a
  %r = call <2 x double> @llvm.log2.v2f64(<2 x double> %v)
  store <2 x double> %r, ptr %res
  ret void
}
