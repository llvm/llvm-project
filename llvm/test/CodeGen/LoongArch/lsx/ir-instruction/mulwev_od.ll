; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py UTC_ARGS: --version 6
; RUN: llc --mtriple=loongarch32 --mattr=+32s,+lsx < %s | FileCheck %s --check-prefixes=CHECK,LA32
; RUN: llc --mtriple=loongarch64 --mattr=+lsx < %s | FileCheck %s --check-prefixes=CHECK,LA64

define void @vmulwev_h_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vslli.h $vr0, $vr0, 8
; CHECK-NEXT:    vsrai.h $vr0, $vr0, 8
; CHECK-NEXT:    vslli.h $vr1, $vr1, 8
; CHECK-NEXT:    vsrai.h $vr1, $vr1, 8
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = sext <8 x i8> %vas to <8 x i16>
  %vbe = sext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vslli.w $vr0, $vr0, 16
; CHECK-NEXT:    vsrai.w $vr0, $vr0, 16
; CHECK-NEXT:    vslli.w $vr1, $vr1, 16
; CHECK-NEXT:    vsrai.w $vr1, $vr1, 16
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = sext <4 x i16> %vas to <4 x i32>
  %vbe = sext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vslli.d $vr0, $vr0, 32
; CHECK-NEXT:    vsrai.d $vr0, $vr0, 32
; CHECK-NEXT:    vslli.d $vr1, $vr1, 32
; CHECK-NEXT:    vsrai.d $vr1, $vr1, 32
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vae = sext <2 x i32> %vas to <2 x i64>
  %vbe = sext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 1
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 0
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 1
; LA32-NEXT:    srai.w $a5, $a2, 31
; LA32-NEXT:    srai.w $a6, $a4, 31
; LA32-NEXT:    mulh.wu $a7, $a1, $a3
; LA32-NEXT:    mul.w $t0, $a2, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    sltu $t0, $a7, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a3
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    mul.w $t1, $a1, $a4
; LA32-NEXT:    add.w $a7, $t1, $a7
; LA32-NEXT:    sltu $t1, $a7, $t1
; LA32-NEXT:    mulh.wu $t2, $a1, $a4
; LA32-NEXT:    add.w $t1, $t2, $t1
; LA32-NEXT:    add.w $t1, $t0, $t1
; LA32-NEXT:    mul.w $t2, $a2, $a4
; LA32-NEXT:    add.w $t3, $t2, $t1
; LA32-NEXT:    mul.w $t4, $a3, $a5
; LA32-NEXT:    mul.w $t5, $a6, $a1
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    add.w $t7, $t3, $t6
; LA32-NEXT:    sltu $t8, $t7, $t3
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    sltu $t0, $t1, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a4
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $t0, $t2
; LA32-NEXT:    mulh.wu $t1, $a3, $a5
; LA32-NEXT:    add.w $t1, $t1, $t4
; LA32-NEXT:    mul.w $a4, $a4, $a5
; LA32-NEXT:    add.w $a4, $t1, $a4
; LA32-NEXT:    mul.w $a2, $a6, $a2
; LA32-NEXT:    mulh.wu $a5, $a6, $a1
; LA32-NEXT:    add.w $a2, $a5, $a2
; LA32-NEXT:    add.w $a2, $a2, $t5
; LA32-NEXT:    add.w $a2, $a2, $a4
; LA32-NEXT:    sltu $a4, $t6, $t5
; LA32-NEXT:    add.w $a2, $a2, $a4
; LA32-NEXT:    add.w $a2, $t0, $a2
; LA32-NEXT:    add.w $a2, $a2, $t8
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 4
; LA32-NEXT:    st.w $t7, $a0, 8
; LA32-NEXT:    st.w $a2, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 0
; LA64-NEXT:    ld.d $a2, $a2, 0
; LA64-NEXT:    mul.d $a3, $a1, $a2
; LA64-NEXT:    mulh.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 8
; LA64-NEXT:    st.d $a3, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 0
  %be = extractelement <2 x i64> %vb, i32 0
  %ax = sext i64 %ae to i128
  %bx = sext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwod_h_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.b $vr0, $vr0, 49
; CHECK-NEXT:    vslli.h $vr0, $vr0, 8
; CHECK-NEXT:    vsrai.h $vr0, $vr0, 8
; CHECK-NEXT:    vshuf4i.b $vr1, $vr1, 49
; CHECK-NEXT:    vslli.h $vr1, $vr1, 8
; CHECK-NEXT:    vsrai.h $vr1, $vr1, 8
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = sext <8 x i8> %vas to <8 x i16>
  %vbe = sext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.h $vr0, $vr0, 49
; CHECK-NEXT:    vslli.w $vr0, $vr0, 16
; CHECK-NEXT:    vsrai.w $vr0, $vr0, 16
; CHECK-NEXT:    vshuf4i.h $vr1, $vr1, 49
; CHECK-NEXT:    vslli.w $vr1, $vr1, 16
; CHECK-NEXT:    vsrai.w $vr1, $vr1, 16
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = sext <4 x i16> %vas to <4 x i32>
  %vbe = sext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.w $vr0, $vr0, 49
; CHECK-NEXT:    vslli.d $vr0, $vr0, 32
; CHECK-NEXT:    vsrai.d $vr0, $vr0, 32
; CHECK-NEXT:    vshuf4i.w $vr1, $vr1, 49
; CHECK-NEXT:    vslli.d $vr1, $vr1, 32
; CHECK-NEXT:    vsrai.d $vr1, $vr1, 32
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vae = sext <2 x i32> %vas to <2 x i64>
  %vbe = sext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 3
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 2
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 3
; LA32-NEXT:    srai.w $a5, $a2, 31
; LA32-NEXT:    srai.w $a6, $a4, 31
; LA32-NEXT:    mulh.wu $a7, $a1, $a3
; LA32-NEXT:    mul.w $t0, $a2, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    sltu $t0, $a7, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a3
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    mul.w $t1, $a1, $a4
; LA32-NEXT:    add.w $a7, $t1, $a7
; LA32-NEXT:    sltu $t1, $a7, $t1
; LA32-NEXT:    mulh.wu $t2, $a1, $a4
; LA32-NEXT:    add.w $t1, $t2, $t1
; LA32-NEXT:    add.w $t1, $t0, $t1
; LA32-NEXT:    mul.w $t2, $a2, $a4
; LA32-NEXT:    add.w $t3, $t2, $t1
; LA32-NEXT:    mul.w $t4, $a3, $a5
; LA32-NEXT:    mul.w $t5, $a6, $a1
; LA32-NEXT:    add.w $t6, $t5, $t4
; LA32-NEXT:    add.w $t7, $t3, $t6
; LA32-NEXT:    sltu $t8, $t7, $t3
; LA32-NEXT:    sltu $t2, $t3, $t2
; LA32-NEXT:    sltu $t0, $t1, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a4
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $t0, $t2
; LA32-NEXT:    mulh.wu $t1, $a3, $a5
; LA32-NEXT:    add.w $t1, $t1, $t4
; LA32-NEXT:    mul.w $a4, $a4, $a5
; LA32-NEXT:    add.w $a4, $t1, $a4
; LA32-NEXT:    mul.w $a2, $a6, $a2
; LA32-NEXT:    mulh.wu $a5, $a6, $a1
; LA32-NEXT:    add.w $a2, $a5, $a2
; LA32-NEXT:    add.w $a2, $a2, $t5
; LA32-NEXT:    add.w $a2, $a2, $a4
; LA32-NEXT:    sltu $a4, $t6, $t5
; LA32-NEXT:    add.w $a2, $a2, $a4
; LA32-NEXT:    add.w $a2, $t0, $a2
; LA32-NEXT:    add.w $a2, $a2, $t8
; LA32-NEXT:    mul.w $a1, $a1, $a3
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    st.w $a7, $a0, 4
; LA32-NEXT:    st.w $t7, $a0, 8
; LA32-NEXT:    st.w $a2, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 8
; LA64-NEXT:    ld.d $a2, $a2, 8
; LA64-NEXT:    mul.d $a3, $a1, $a2
; LA64-NEXT:    mulh.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 8
; LA64-NEXT:    st.d $a3, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 1
  %be = extractelement <2 x i64> %vb, i32 1
  %ax = sext i64 %ae to i128
  %bx = sext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI8_0)
; CHECK-NEXT:    vld $vr1, $a1, %pc_lo12(.LCPI8_0)
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.b $vr0, $vr3, $vr0, $vr1
; CHECK-NEXT:    vshuf.b $vr1, $vr3, $vr2, $vr1
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = zext <8 x i8> %vas to <8 x i16>
  %vbe = zext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pcalau12i $a3, %pc_hi20(.LCPI9_0)
; CHECK-NEXT:    vld $vr0, $a3, %pc_lo12(.LCPI9_0)
; CHECK-NEXT:    vld $vr1, $a1, 0
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vori.b $vr4, $vr0, 0
; CHECK-NEXT:    vshuf.h $vr4, $vr3, $vr1
; CHECK-NEXT:    vshuf.h $vr0, $vr3, $vr2
; CHECK-NEXT:    vmul.w $vr0, $vr4, $vr0
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = zext <4 x i16> %vas to <4 x i32>
  %vbe = zext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    pcalau12i $a3, %pc_hi20(.LCPI10_0)
; CHECK-NEXT:    vld $vr0, $a3, %pc_lo12(.LCPI10_0)
; CHECK-NEXT:    vld $vr1, $a1, 0
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vori.b $vr4, $vr0, 0
; CHECK-NEXT:    vshuf.w $vr4, $vr3, $vr1
; CHECK-NEXT:    vshuf.w $vr0, $vr3, $vr2
; CHECK-NEXT:    vmul.d $vr0, $vr4, $vr0
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vae = zext <2 x i32> %vas to <2 x i64>
  %vbe = zext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 0
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 1
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 0
; LA32-NEXT:    mulh.wu $a5, $a2, $a4
; LA32-NEXT:    mul.w $a6, $a1, $a4
; LA32-NEXT:    add.w $a5, $a6, $a5
; LA32-NEXT:    sltu $a6, $a5, $a6
; LA32-NEXT:    mulh.wu $a7, $a1, $a4
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    mul.w $a7, $a2, $a3
; LA32-NEXT:    add.w $a5, $a7, $a5
; LA32-NEXT:    sltu $a7, $a5, $a7
; LA32-NEXT:    mulh.wu $t0, $a2, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    add.w $a7, $a6, $a7
; LA32-NEXT:    mul.w $t0, $a1, $a3
; LA32-NEXT:    add.w $t1, $t0, $a7
; LA32-NEXT:    sltu $t0, $t1, $t0
; LA32-NEXT:    sltu $a6, $a7, $a6
; LA32-NEXT:    mulh.wu $a1, $a1, $a3
; LA32-NEXT:    add.w $a1, $a1, $a6
; LA32-NEXT:    add.w $a1, $a1, $t0
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a5, $a0, 4
; LA32-NEXT:    st.w $t1, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 0
; LA64-NEXT:    ld.d $a2, $a2, 0
; LA64-NEXT:    mul.d $a3, $a1, $a2
; LA64-NEXT:    mulh.du $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 8
; LA64-NEXT:    st.d $a3, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 0
  %be = extractelement <2 x i64> %vb, i32 0
  %ax = zext i64 %ae to i128
  %bx = zext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.b $vr0, $vr2, $vr0
; CHECK-NEXT:    vpackod.b $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = zext <8 x i8> %vas to <8 x i16>
  %vbe = zext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.h $vr0, $vr2, $vr0
; CHECK-NEXT:    vpackod.h $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = zext <4 x i16> %vas to <4 x i32>
  %vbe = zext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.w $vr0, $vr2, $vr0
; CHECK-NEXT:    vpackod.w $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vae = zext <2 x i32> %vas to <2 x i64>
  %vbe = zext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 3
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 2
; LA32-NEXT:    mulh.wu $a5, $a2, $a4
; LA32-NEXT:    mul.w $a6, $a1, $a4
; LA32-NEXT:    add.w $a5, $a6, $a5
; LA32-NEXT:    sltu $a6, $a5, $a6
; LA32-NEXT:    mulh.wu $a7, $a1, $a4
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    mul.w $a7, $a2, $a3
; LA32-NEXT:    add.w $a5, $a7, $a5
; LA32-NEXT:    sltu $a7, $a5, $a7
; LA32-NEXT:    mulh.wu $t0, $a2, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    add.w $a7, $a6, $a7
; LA32-NEXT:    mul.w $t0, $a1, $a3
; LA32-NEXT:    add.w $t1, $t0, $a7
; LA32-NEXT:    sltu $t0, $t1, $t0
; LA32-NEXT:    sltu $a6, $a7, $a6
; LA32-NEXT:    mulh.wu $a1, $a1, $a3
; LA32-NEXT:    add.w $a1, $a1, $a6
; LA32-NEXT:    add.w $a1, $a1, $t0
; LA32-NEXT:    mul.w $a2, $a2, $a4
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a5, $a0, 4
; LA32-NEXT:    st.w $t1, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 8
; LA64-NEXT:    ld.d $a2, $a2, 8
; LA64-NEXT:    mul.d $a3, $a1, $a2
; LA64-NEXT:    mulh.du $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 8
; LA64-NEXT:    st.d $a3, $a0, 0
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 1
  %be = extractelement <2 x i64> %vb, i32 1
  %ax = zext i64 %ae to i128
  %bx = zext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI16_0)
; CHECK-NEXT:    vld $vr1, $a1, %pc_lo12(.LCPI16_0)
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.b $vr0, $vr3, $vr0, $vr1
; CHECK-NEXT:    vslli.h $vr1, $vr2, 8
; CHECK-NEXT:    vsrai.h $vr1, $vr1, 8
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = zext <8 x i8> %vas to <8 x i16>
  %vbe = sext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI17_0)
; CHECK-NEXT:    vld $vr1, $a1, %pc_lo12(.LCPI17_0)
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.h $vr1, $vr3, $vr0
; CHECK-NEXT:    vslli.w $vr0, $vr2, 16
; CHECK-NEXT:    vsrai.w $vr0, $vr0, 16
; CHECK-NEXT:    vmul.w $vr0, $vr1, $vr0
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = zext <4 x i16> %vas to <4 x i32>
  %vbe = sext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI18_0)
; CHECK-NEXT:    vld $vr1, $a1, %pc_lo12(.LCPI18_0)
; CHECK-NEXT:    vld $vr2, $a2, 0
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.w $vr1, $vr3, $vr0
; CHECK-NEXT:    vslli.d $vr0, $vr2, 32
; CHECK-NEXT:    vsrai.d $vr0, $vr0, 32
; CHECK-NEXT:    vmul.d $vr0, $vr1, $vr0
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vae = zext <2 x i32> %vas to <2 x i64>
  %vbe = sext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 1
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 0
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 0
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 1
; LA32-NEXT:    srai.w $a5, $a4, 31
; LA32-NEXT:    mulh.wu $a6, $a2, $a3
; LA32-NEXT:    mul.w $a7, $a1, $a3
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    sltu $a7, $a6, $a7
; LA32-NEXT:    mulh.wu $t0, $a1, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    mul.w $t0, $a2, $a4
; LA32-NEXT:    add.w $a6, $t0, $a6
; LA32-NEXT:    sltu $t0, $a6, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a4
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $a7, $t0
; LA32-NEXT:    mul.w $t1, $a1, $a4
; LA32-NEXT:    add.w $t2, $t1, $t0
; LA32-NEXT:    mul.w $t3, $a5, $a2
; LA32-NEXT:    add.w $t4, $t2, $t3
; LA32-NEXT:    sltu $t5, $t4, $t2
; LA32-NEXT:    sltu $t1, $t2, $t1
; LA32-NEXT:    sltu $a7, $t0, $a7
; LA32-NEXT:    mulh.wu $a4, $a1, $a4
; LA32-NEXT:    add.w $a4, $a4, $a7
; LA32-NEXT:    add.w $a4, $a4, $t1
; LA32-NEXT:    mul.w $a1, $a5, $a1
; LA32-NEXT:    mulh.wu $a5, $a5, $a2
; LA32-NEXT:    add.w $a1, $a5, $a1
; LA32-NEXT:    add.w $a1, $a1, $t3
; LA32-NEXT:    add.w $a1, $a4, $a1
; LA32-NEXT:    add.w $a1, $a1, $t5
; LA32-NEXT:    mul.w $a2, $a2, $a3
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $t4, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a2, $a2, 0
; LA64-NEXT:    ld.d $a1, $a1, 0
; LA64-NEXT:    srai.d $a3, $a2, 63
; LA64-NEXT:    mulh.du $a4, $a1, $a2
; LA64-NEXT:    mul.d $a3, $a1, $a3
; LA64-NEXT:    add.d $a3, $a4, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a3, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 0
  %be = extractelement <2 x i64> %vb, i32 0
  %ax = zext i64 %ae to i128
  %bx = sext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu_b(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu_b:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.b $vr0, $vr2, $vr0
; CHECK-NEXT:    vshuf4i.b $vr1, $vr1, 49
; CHECK-NEXT:    vslli.h $vr1, $vr1, 8
; CHECK-NEXT:    vsrai.h $vr1, $vr1, 8
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = zext <8 x i8> %vas to <8 x i16>
  %vbe = sext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu_h(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu_h:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.h $vr0, $vr2, $vr0
; CHECK-NEXT:    vshuf4i.h $vr1, $vr1, 49
; CHECK-NEXT:    vslli.w $vr1, $vr1, 16
; CHECK-NEXT:    vsrai.w $vr1, $vr1, 16
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = zext <4 x i16> %vas to <4 x i32>
  %vbe = sext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu_w(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu_w:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.w $vr0, $vr2, $vr0
; CHECK-NEXT:    vshuf4i.w $vr1, $vr1, 49
; CHECK-NEXT:    vslli.d $vr1, $vr1, 32
; CHECK-NEXT:    vsrai.d $vr1, $vr1, 32
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vae = zext <2 x i32> %vas to <2 x i64>
  %vbe = sext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du_d(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du_d:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 3
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 2
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 3
; LA32-NEXT:    srai.w $a5, $a4, 31
; LA32-NEXT:    mulh.wu $a6, $a2, $a3
; LA32-NEXT:    mul.w $a7, $a1, $a3
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    sltu $a7, $a6, $a7
; LA32-NEXT:    mulh.wu $t0, $a1, $a3
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    mul.w $t0, $a2, $a4
; LA32-NEXT:    add.w $a6, $t0, $a6
; LA32-NEXT:    sltu $t0, $a6, $t0
; LA32-NEXT:    mulh.wu $t1, $a2, $a4
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $a7, $t0
; LA32-NEXT:    mul.w $t1, $a1, $a4
; LA32-NEXT:    add.w $t2, $t1, $t0
; LA32-NEXT:    mul.w $t3, $a5, $a2
; LA32-NEXT:    add.w $t4, $t2, $t3
; LA32-NEXT:    sltu $t5, $t4, $t2
; LA32-NEXT:    sltu $t1, $t2, $t1
; LA32-NEXT:    sltu $a7, $t0, $a7
; LA32-NEXT:    mulh.wu $a4, $a1, $a4
; LA32-NEXT:    add.w $a4, $a4, $a7
; LA32-NEXT:    add.w $a4, $a4, $t1
; LA32-NEXT:    mul.w $a1, $a5, $a1
; LA32-NEXT:    mulh.wu $a5, $a5, $a2
; LA32-NEXT:    add.w $a1, $a5, $a1
; LA32-NEXT:    add.w $a1, $a1, $t3
; LA32-NEXT:    add.w $a1, $a4, $a1
; LA32-NEXT:    add.w $a1, $a1, $t5
; LA32-NEXT:    mul.w $a2, $a2, $a3
; LA32-NEXT:    st.w $a2, $a0, 0
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $t4, $a0, 8
; LA32-NEXT:    st.w $a1, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du_d:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a2, $a2, 8
; LA64-NEXT:    ld.d $a1, $a1, 8
; LA64-NEXT:    srai.d $a3, $a2, 63
; LA64-NEXT:    mulh.du $a4, $a1, $a2
; LA64-NEXT:    mul.d $a3, $a1, $a3
; LA64-NEXT:    add.d $a3, $a4, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a3, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 1
  %be = extractelement <2 x i64> %vb, i32 1
  %ax = zext i64 %ae to i128
  %bx = sext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwev_h_bu_b_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_h_bu_b_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI24_0)
; CHECK-NEXT:    vld $vr2, $a1, %pc_lo12(.LCPI24_0)
; CHECK-NEXT:    vslli.h $vr0, $vr0, 8
; CHECK-NEXT:    vsrai.h $vr0, $vr0, 8
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.b $vr1, $vr3, $vr1, $vr2
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
  %vae = sext <8 x i8> %vas to <8 x i16>
  %vbe = zext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwev_w_hu_h_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_w_hu_h_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI25_0)
; CHECK-NEXT:    vld $vr2, $a1, %pc_lo12(.LCPI25_0)
; CHECK-NEXT:    vslli.w $vr0, $vr0, 16
; CHECK-NEXT:    vsrai.w $vr0, $vr0, 16
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.h $vr2, $vr3, $vr1
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr2
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 0, i32 2, i32 4, i32 6>
  %vae = sext <4 x i16> %vas to <4 x i32>
  %vbe = zext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwev_d_wu_w_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwev_d_wu_w_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    pcalau12i $a1, %pc_hi20(.LCPI26_0)
; CHECK-NEXT:    vld $vr2, $a1, %pc_lo12(.LCPI26_0)
; CHECK-NEXT:    vslli.d $vr0, $vr0, 32
; CHECK-NEXT:    vsrai.d $vr0, $vr0, 32
; CHECK-NEXT:    vrepli.b $vr3, 0
; CHECK-NEXT:    vshuf.w $vr2, $vr3, $vr1
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr2
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 0, i32 2>
  %vae = sext <2 x i32> %vas to <2 x i64>
  %vbe = zext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwev_q_du_d_1(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwev_q_du_d_1:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 0
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 1
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 1
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 0
; LA32-NEXT:    srai.w $a5, $a2, 31
; LA32-NEXT:    mulh.wu $a6, $a1, $a4
; LA32-NEXT:    mul.w $a7, $a2, $a4
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    sltu $a7, $a6, $a7
; LA32-NEXT:    mulh.wu $t0, $a2, $a4
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    mul.w $t0, $a1, $a3
; LA32-NEXT:    add.w $a6, $t0, $a6
; LA32-NEXT:    sltu $t0, $a6, $t0
; LA32-NEXT:    mulh.wu $t1, $a1, $a3
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $a7, $t0
; LA32-NEXT:    mul.w $t1, $a2, $a3
; LA32-NEXT:    add.w $t2, $t1, $t0
; LA32-NEXT:    mul.w $t3, $a4, $a5
; LA32-NEXT:    add.w $t4, $t2, $t3
; LA32-NEXT:    sltu $t5, $t4, $t2
; LA32-NEXT:    sltu $t1, $t2, $t1
; LA32-NEXT:    sltu $a7, $t0, $a7
; LA32-NEXT:    mulh.wu $a2, $a2, $a3
; LA32-NEXT:    add.w $a2, $a2, $a7
; LA32-NEXT:    add.w $a2, $a2, $t1
; LA32-NEXT:    mulh.wu $a7, $a4, $a5
; LA32-NEXT:    add.w $a7, $a7, $t3
; LA32-NEXT:    mul.w $a3, $a3, $a5
; LA32-NEXT:    add.w $a3, $a7, $a3
; LA32-NEXT:    add.w $a2, $a2, $a3
; LA32-NEXT:    add.w $a2, $a2, $t5
; LA32-NEXT:    mul.w $a1, $a1, $a4
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $t4, $a0, 8
; LA32-NEXT:    st.w $a2, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwev_q_du_d_1:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 0
; LA64-NEXT:    ld.d $a2, $a2, 0
; LA64-NEXT:    srai.d $a3, $a1, 63
; LA64-NEXT:    mulh.du $a4, $a1, $a2
; LA64-NEXT:    mul.d $a3, $a3, $a2
; LA64-NEXT:    add.d $a3, $a4, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a3, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 0
  %be = extractelement <2 x i64> %vb, i32 0
  %ax = sext i64 %ae to i128
  %bx = zext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}

define void @vmulwod_h_bu_b_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_h_bu_b_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.b $vr0, $vr0, 49
; CHECK-NEXT:    vslli.h $vr0, $vr0, 8
; CHECK-NEXT:    vsrai.h $vr0, $vr0, 8
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.b $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.h $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <16 x i8>, ptr %a
  %vb = load <16 x i8>, ptr %b
  %vas = shufflevector <16 x i8> %va, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vbs = shufflevector <16 x i8> %vb, <16 x i8> undef, <8 x i32> <i32 1, i32 3, i32 5, i32 7, i32 9, i32 11, i32 13, i32 15>
  %vae = sext <8 x i8> %vas to <8 x i16>
  %vbe = zext <8 x i8> %vbs to <8 x i16>
  %mul = mul <8 x i16> %vae, %vbe
  store <8 x i16> %mul, ptr %res
  ret void
}

define void @vmulwod_w_hu_h_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_w_hu_h_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.h $vr0, $vr0, 49
; CHECK-NEXT:    vslli.w $vr0, $vr0, 16
; CHECK-NEXT:    vsrai.w $vr0, $vr0, 16
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.h $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.w $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <8 x i16>, ptr %a
  %vb = load <8 x i16>, ptr %b
  %vas = shufflevector <8 x i16> %va, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vbs = shufflevector <8 x i16> %vb, <8 x i16> undef, <4 x i32> <i32 1, i32 3, i32 5, i32 7>
  %vae = sext <4 x i16> %vas to <4 x i32>
  %vbe = zext <4 x i16> %vbs to <4 x i32>
  %mul = mul <4 x i32> %vae, %vbe
  store <4 x i32> %mul, ptr %res
  ret void
}

define void @vmulwod_d_wu_w_1(ptr %res, ptr %a, ptr %b) nounwind {
; CHECK-LABEL: vmulwod_d_wu_w_1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vld $vr0, $a1, 0
; CHECK-NEXT:    vld $vr1, $a2, 0
; CHECK-NEXT:    vshuf4i.w $vr0, $vr0, 49
; CHECK-NEXT:    vslli.d $vr0, $vr0, 32
; CHECK-NEXT:    vsrai.d $vr0, $vr0, 32
; CHECK-NEXT:    vrepli.b $vr2, 0
; CHECK-NEXT:    vpackod.w $vr1, $vr2, $vr1
; CHECK-NEXT:    vmul.d $vr0, $vr0, $vr1
; CHECK-NEXT:    vst $vr0, $a0, 0
; CHECK-NEXT:    ret
entry:
  %va = load <4 x i32>, ptr %a
  %vb = load <4 x i32>, ptr %b
  %vas = shufflevector <4 x i32> %va, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vbs = shufflevector <4 x i32> %vb, <4 x i32> undef, <2 x i32> <i32 1, i32 3>
  %vae = sext <2 x i32> %vas to <2 x i64>
  %vbe = zext <2 x i32> %vbs to <2 x i64>
  %mul = mul <2 x i64> %vae, %vbe
  store <2 x i64> %mul, ptr %res
  ret void
}

define void @vmulwod_q_du_d_1(ptr %res, ptr %a, ptr %b) nounwind {
; LA32-LABEL: vmulwod_q_du_d_1:
; LA32:       # %bb.0: # %entry
; LA32-NEXT:    vld $vr0, $a1, 0
; LA32-NEXT:    vld $vr1, $a2, 0
; LA32-NEXT:    vpickve2gr.w $a1, $vr0, 2
; LA32-NEXT:    vpickve2gr.w $a2, $vr0, 3
; LA32-NEXT:    vpickve2gr.w $a3, $vr1, 3
; LA32-NEXT:    vpickve2gr.w $a4, $vr1, 2
; LA32-NEXT:    srai.w $a5, $a2, 31
; LA32-NEXT:    mulh.wu $a6, $a1, $a4
; LA32-NEXT:    mul.w $a7, $a2, $a4
; LA32-NEXT:    add.w $a6, $a7, $a6
; LA32-NEXT:    sltu $a7, $a6, $a7
; LA32-NEXT:    mulh.wu $t0, $a2, $a4
; LA32-NEXT:    add.w $a7, $t0, $a7
; LA32-NEXT:    mul.w $t0, $a1, $a3
; LA32-NEXT:    add.w $a6, $t0, $a6
; LA32-NEXT:    sltu $t0, $a6, $t0
; LA32-NEXT:    mulh.wu $t1, $a1, $a3
; LA32-NEXT:    add.w $t0, $t1, $t0
; LA32-NEXT:    add.w $t0, $a7, $t0
; LA32-NEXT:    mul.w $t1, $a2, $a3
; LA32-NEXT:    add.w $t2, $t1, $t0
; LA32-NEXT:    mul.w $t3, $a4, $a5
; LA32-NEXT:    add.w $t4, $t2, $t3
; LA32-NEXT:    sltu $t5, $t4, $t2
; LA32-NEXT:    sltu $t1, $t2, $t1
; LA32-NEXT:    sltu $a7, $t0, $a7
; LA32-NEXT:    mulh.wu $a2, $a2, $a3
; LA32-NEXT:    add.w $a2, $a2, $a7
; LA32-NEXT:    add.w $a2, $a2, $t1
; LA32-NEXT:    mulh.wu $a7, $a4, $a5
; LA32-NEXT:    add.w $a7, $a7, $t3
; LA32-NEXT:    mul.w $a3, $a3, $a5
; LA32-NEXT:    add.w $a3, $a7, $a3
; LA32-NEXT:    add.w $a2, $a2, $a3
; LA32-NEXT:    add.w $a2, $a2, $t5
; LA32-NEXT:    mul.w $a1, $a1, $a4
; LA32-NEXT:    st.w $a1, $a0, 0
; LA32-NEXT:    st.w $a6, $a0, 4
; LA32-NEXT:    st.w $t4, $a0, 8
; LA32-NEXT:    st.w $a2, $a0, 12
; LA32-NEXT:    ret
;
; LA64-LABEL: vmulwod_q_du_d_1:
; LA64:       # %bb.0: # %entry
; LA64-NEXT:    ld.d $a1, $a1, 8
; LA64-NEXT:    ld.d $a2, $a2, 8
; LA64-NEXT:    srai.d $a3, $a1, 63
; LA64-NEXT:    mulh.du $a4, $a1, $a2
; LA64-NEXT:    mul.d $a3, $a3, $a2
; LA64-NEXT:    add.d $a3, $a4, $a3
; LA64-NEXT:    mul.d $a1, $a1, $a2
; LA64-NEXT:    st.d $a1, $a0, 0
; LA64-NEXT:    st.d $a3, $a0, 8
; LA64-NEXT:    ret
entry:
  %va = load <2 x i64>, ptr %a
  %vb = load <2 x i64>, ptr %b
  %ae = extractelement <2 x i64> %va, i32 1
  %be = extractelement <2 x i64> %vb, i32 1
  %ax = sext i64 %ae to i128
  %bx = zext i64 %be to i128
  %mul = mul i128 %ax, %bx
  store i128 %mul, ptr %res
  ret void
}
