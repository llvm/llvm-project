; NOTE: Assertions have been autogenerated by utils/update_llc_test_checks.py
; RUN: llc -verify-machineinstrs -mtriple=powerpc64le-unknown-linux-gnu -O3 \
; RUN:   -mcpu=pwr10 -ppc-asm-full-reg-names -ppc-vsr-nums-as-vr -mattr=-mma \
; RUN:   < %s | FileCheck %s

; This test is to check that the paired vector intrinsics are available even
; when MMA is disabled.

define <16 x i8> @test1(<256 x i1>* %ptr) {
; CHECK-LABEL: test1:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lxv v3, 0(r3)
; CHECK-NEXT:    lxv v2, 16(r3)
; CHECK-NEXT:    vaddubm v2, v3, v2
; CHECK-NEXT:    blr
entry:
  %0 = load <256 x i1>, <256 x i1>* %ptr, align 32
  %1 = tail call { <16 x i8>, <16 x i8> } @llvm.ppc.mma.disassemble.pair(<256 x i1> %0)
  %2 = extractvalue { <16 x i8>, <16 x i8> } %1, 0
  %3 = extractvalue { <16 x i8>, <16 x i8> } %1, 1
  %add = add <16 x i8> %2, %3
  ret <16 x i8> %add
}

declare { <16 x i8>, <16 x i8> } @llvm.ppc.mma.disassemble.pair(<256 x i1>)

define void @test2(<16 x i8> %v1, <16 x i8> %v2, <256 x i1>* %ptr) {
; CHECK-LABEL: test2:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    vmr v4, v3
; CHECK-NEXT:    vmr v5, v2
; CHECK-NEXT:    stxv v4, 16(r7)
; CHECK-NEXT:    stxv v5, 0(r7)
; CHECK-NEXT:    blr
entry:
  %0 = tail call <256 x i1> @llvm.ppc.mma.assemble.pair(<16 x i8> %v2, <16 x i8> %v1)
  store <256 x i1> %0, <256 x i1>* %ptr, align 32
  ret void
}

declare <256 x i1> @llvm.ppc.mma.assemble.pair(<16 x i8>, <16 x i8>)

define void @test3(<256 x i1>* %ptr) {
; CHECK-LABEL: test3:
; CHECK:       # %bb.0: # %entry
; CHECK-NEXT:    lxvp vsp0, 0(r3)
; CHECK-NEXT:    stxvp vsp0, 32(r3)
; CHECK-NEXT:    blr
entry:
  %0 = bitcast <256 x i1>* %ptr to i8*
  %1 = tail call <256 x i1> @llvm.ppc.mma.lxvp(i8* %0)
  %add.ptr1 = getelementptr inbounds <256 x i1>, <256 x i1>* %ptr, i64 1
  %2 = bitcast <256 x i1>* %add.ptr1 to i8*
  tail call void @llvm.ppc.mma.stxvp(<256 x i1> %1, i8* %2)
  ret void
}

declare <256 x i1> @llvm.ppc.mma.lxvp(i8*)
declare void @llvm.ppc.mma.stxvp(<256 x i1>, i8*)
