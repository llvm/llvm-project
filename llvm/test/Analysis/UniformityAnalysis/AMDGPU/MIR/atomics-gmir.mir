# RUN: llc -mtriple=amdgcn-- -run-pass=print-machine-uniformity -o - %s 2>&1 | FileCheck %s

---
name:            test1
tracksRegLiveness: true
body:             |
  bb.1:
    %2:_(s32) = IMPLICIT_DEF
    %3:_(s32) = IMPLICIT_DEF
    %0:_(p0) = G_MERGE_VALUES %2(s32), %3(s32)
    %1:_(s32) = IMPLICIT_DEF

    ; CHECK: DIVERGENT
    ; CHECK-SAME: G_ATOMICRMW_XCHG
    %4:_(s32) = G_ATOMICRMW_XCHG %0(p0), %1 :: (load store seq_cst (s32))

    ; CHECK: DIVERGENT
    ; CHECK-SAME: G_ATOMIC_CMPXCHG_WITH_SUCCESS
    %5:_(s32), %6:_(s1) = G_ATOMIC_CMPXCHG_WITH_SUCCESS %0(p0), %1, %2 :: (load store seq_cst seq_cst (s32) )
    $vgpr0 = COPY %4(s32)
    SI_RETURN implicit $vgpr0
...

---
name:            test_atomic_inc_dec
tracksRegLiveness: true
body:             |
  bb.1:

    %2:_(s32) = IMPLICIT_DEF
    %3:_(s32) = IMPLICIT_DEF
    %0:_(p1) = G_MERGE_VALUES %2(s32), %3(s32)
    %1:_(s32) = IMPLICIT_DEF
    %5:_(s64) = IMPLICIT_DEF

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.inc)
    %4:_(s32) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.inc), %0(p1), %1(s32), 0, 0, 0 :: (load store (s32) )

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s64) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.inc)
    %6:_(s64) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.inc), %0(p1), %5(s64), 0, 0, 0 :: (load store (s64) )

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.dec)
    %7:_(s32) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.dec), %0(p1), %1(s32), 0, 0, 0 :: (load store (s32) )

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s64) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.dec)
    %8:_(s64) = G_INTRINSIC_W_SIDE_EFFECTS intrinsic(@llvm.amdgcn.atomic.dec), %0(p1), %5(s64), 0, 0, 0 :: (load store (s64) )
    $vgpr0 = COPY %4(s32)
    SI_RETURN implicit $vgpr0

...

---
name:            test_atomics
tracksRegLiveness: true
body:             |
  bb.1:

    %2:_(s32) = IMPLICIT_DEF
    %3:_(s32) = IMPLICIT_DEF
    %0:_(p1) = G_MERGE_VALUES %2(s32), %3(s32)
    %1:_(s32) = IMPLICIT_DEF
    %5:_(s32) = IMPLICIT_DEF

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_ATOMICRMW_ADD
    %4:_(s32) = G_ATOMICRMW_ADD %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_ATOMICRMW_SUB
    %6:_(s32) = G_ATOMICRMW_SUB %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_ATOMICRMW_AND
    %7:_(s32) = G_ATOMICRMW_AND %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_ATOMICRMW_NAND
    %8:_(s32) = G_ATOMICRMW_NAND %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]}}: %{{[0-9]}}:_(s32) = G_ATOMICRMW_OR
    %9:_(s32) = G_ATOMICRMW_OR %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_XOR
    %10:_(s32) = G_ATOMICRMW_XOR %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_MAX
    %11:_(s32) = G_ATOMICRMW_MAX %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_MIN
    %12:_(s32) = G_ATOMICRMW_MIN %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_UMAX
    %13:_(s32) = G_ATOMICRMW_UMAX %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_UMIN
    %14:_(s32) = G_ATOMICRMW_UMIN %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_FADD
    %15:_(s32) = G_ATOMICRMW_FADD %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_FSUB
    %16:_(s32) = G_ATOMICRMW_FSUB %1, %5

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_FMAX
    %17:_(s32) = G_ATOMICRMW_FMAX %2, %3

    ; CHECK: DIVERGENT: %{{[0-9]*}}: %{{[0-9]*}}:_(s32) = G_ATOMICRMW_FMIN
    %18:_(s32) = G_ATOMICRMW_FMIN %1, %5

    $vgpr0 = COPY %4(s32)
    SI_RETURN implicit $vgpr0

...
