; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt < %s --mtriple=x86_64 --passes=atomic-expand -S -o - | FileCheck %s

define <1 x float> @load_atomic_vector1_float(ptr %src) {
; CHECK-LABEL: define <1 x float> @load_atomic_vector1_float(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = load atomic i32, ptr [[SRC]] acquire, align 4
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <1 x float>
; CHECK-NEXT:    ret <1 x float> [[TMP2]]
;
  %ret = load atomic <1 x float>, ptr %src acquire, align 4
  ret <1 x float> %ret
}

define <2 x float> @load_atomic_vector2_float(ptr %src) {
; CHECK-LABEL: define <2 x float> @load_atomic_vector2_float(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = alloca <2 x float>, align 8
; CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; CHECK-NEXT:    call void @__atomic_load(i64 8, ptr [[SRC]], ptr [[TMP1]], i32 2)
; CHECK-NEXT:    [[TMP2:%.*]] = load <2 x float>, ptr [[TMP1]], align 8
; CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; CHECK-NEXT:    ret <2 x float> [[TMP2]]
;
  %ret = load atomic <2 x float>, ptr %src acquire, align 4
  ret <2 x float> %ret
}

define <1 x double> @load_atomic_vector1_double(ptr %src) {
; CHECK-LABEL: define <1 x double> @load_atomic_vector1_double(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = alloca <1 x double>, align 8
; CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 8, ptr [[TMP1]])
; CHECK-NEXT:    call void @__atomic_load(i64 8, ptr [[SRC]], ptr [[TMP1]], i32 2)
; CHECK-NEXT:    [[TMP2:%.*]] = load <1 x double>, ptr [[TMP1]], align 8
; CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 8, ptr [[TMP1]])
; CHECK-NEXT:    ret <1 x double> [[TMP2]]
;
  %ret = load atomic <1 x double>, ptr %src acquire, align 4
  ret <1 x double> %ret
}

define <2 x double> @load_atomic_vector2_double(ptr %src) {
; CHECK-LABEL: define <2 x double> @load_atomic_vector2_double(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = alloca <2 x double>, align 16
; CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 16, ptr [[TMP1]])
; CHECK-NEXT:    call void @__atomic_load(i64 16, ptr [[SRC]], ptr [[TMP1]], i32 2)
; CHECK-NEXT:    [[TMP2:%.*]] = load <2 x double>, ptr [[TMP1]], align 16
; CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 16, ptr [[TMP1]])
; CHECK-NEXT:    ret <2 x double> [[TMP2]]
;
  %ret = load atomic <2 x double>, ptr %src acquire, align 4
  ret <2 x double> %ret
}

define <2 x half> @load_atomic_vector_half(ptr %src) {
; CHECK-LABEL: define <2 x half> @load_atomic_vector_half(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = load atomic i32, ptr [[SRC]] acquire, align 4
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <2 x half>
; CHECK-NEXT:    ret <2 x half> [[TMP2]]
;
  %ret = load atomic <2 x half>, ptr %src acquire, align 4
  ret <2 x half> %ret
}

define <2 x bfloat> @load_atomic_vector_bfloat(ptr %src) {
; CHECK-LABEL: define <2 x bfloat> @load_atomic_vector_bfloat(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = load atomic i32, ptr [[SRC]] acquire, align 4
; CHECK-NEXT:    [[TMP2:%.*]] = bitcast i32 [[TMP1]] to <2 x bfloat>
; CHECK-NEXT:    ret <2 x bfloat> [[TMP2]]
;
  %ret = load atomic <2 x bfloat>, ptr %src acquire, align 4
  ret <2 x bfloat> %ret
}

define <2 x fp128> @load_atomic_vector_fp128(ptr %src) {
; CHECK-LABEL: define <2 x fp128> @load_atomic_vector_fp128(
; CHECK-SAME: ptr [[SRC:%.*]]) {
; CHECK-NEXT:    [[TMP1:%.*]] = alloca <2 x fp128>, align 16
; CHECK-NEXT:    call void @llvm.lifetime.start.p0(i64 32, ptr [[TMP1]])
; CHECK-NEXT:    call void @__atomic_load(i64 32, ptr [[SRC]], ptr [[TMP1]], i32 2)
; CHECK-NEXT:    [[TMP2:%.*]] = load <2 x fp128>, ptr [[TMP1]], align 16
; CHECK-NEXT:    call void @llvm.lifetime.end.p0(i64 32, ptr [[TMP1]])
; CHECK-NEXT:    ret <2 x fp128> [[TMP2]]
;
  %ret = load atomic <2 x fp128>, ptr %src acquire, align 4
  ret <2 x fp128> %ret
}
