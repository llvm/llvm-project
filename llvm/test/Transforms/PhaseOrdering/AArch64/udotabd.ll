; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S -O3 < %s | FileCheck %s --check-prefixes=CHECK-O3
; RUN: opt -S -passes="default<O3>,default<O3>" < %s | FileCheck %s --check-prefixes=CHECK-LTO

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i8:8:32-i16:16:32-i64:64-i128:128-n32:64-S128-Fn32"
target triple = "aarch64"

define dso_local i32 @test(ptr noundef %p1, i32 noundef %s_p1, ptr noundef %p2, i32 noundef %s_p2) #0 {
; CHECK-O3-LABEL: define dso_local i32 @test(
; CHECK-O3-SAME: ptr noundef readonly captures(none) [[P1:%.*]], i32 noundef [[S_P1:%.*]], ptr noundef readonly captures(none) [[P2:%.*]], i32 noundef [[S_P2:%.*]]) local_unnamed_addr #[[ATTR0:[0-9]+]] {
; CHECK-O3-NEXT:  [[ENTRY:.*]]:
; CHECK-O3-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[S_P1]] to i64
; CHECK-O3-NEXT:    [[IDX_EXT8:%.*]] = sext i32 [[S_P2]] to i64
; CHECK-O3-NEXT:    [[TMP0:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp samesign ugt i64 [[TMP0]], 2
; CHECK-O3-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[FOR_COND1_PREHEADER_PREHEADER:.*]], label %[[VECTOR_SCEVCHECK:.*]]
; CHECK-O3:       [[VECTOR_SCEVCHECK]]:
; CHECK-O3-NEXT:    [[IDENT_CHECK:%.*]] = icmp ne i32 [[S_P1]], 1
; CHECK-O3-NEXT:    [[IDENT_CHECK15:%.*]] = icmp ne i32 [[S_P2]], 1
; CHECK-O3-NEXT:    [[TMP1:%.*]] = or i1 [[IDENT_CHECK]], [[IDENT_CHECK15]]
; CHECK-O3-NEXT:    br i1 [[TMP1]], label %[[FOR_COND1_PREHEADER_PREHEADER]], label %[[VECTOR_PH:.*]]
; CHECK-O3:       [[VECTOR_PH]]:
; CHECK-O3-NEXT:    [[TMP2:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP3:%.*]] = shl nuw nsw i64 [[TMP2]], 3
; CHECK-O3-NEXT:    [[DOTNOT:%.*]] = sub nsw i64 0, [[TMP3]]
; CHECK-O3-NEXT:    [[N_VEC:%.*]] = and i64 [[DOTNOT]], 16
; CHECK-O3-NEXT:    [[TMP4:%.*]] = trunc nuw nsw i64 [[N_VEC]] to i32
; CHECK-O3-NEXT:    [[TMP5:%.*]] = mul nuw nsw i64 [[N_VEC]], [[IDX_EXT]]
; CHECK-O3-NEXT:    [[TMP6:%.*]] = getelementptr i8, ptr [[P1]], i64 [[TMP5]]
; CHECK-O3-NEXT:    [[TMP7:%.*]] = mul nuw nsw i64 [[N_VEC]], [[IDX_EXT8]]
; CHECK-O3-NEXT:    [[TMP8:%.*]] = getelementptr i8, ptr [[P2]], i64 [[TMP7]]
; CHECK-O3-NEXT:    [[TMP9:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP10:%.*]] = shl nuw nsw i64 [[TMP9]], 2
; CHECK-O3-NEXT:    [[TMP11:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP12:%.*]] = shl nuw nsw i64 [[TMP11]], 2
; CHECK-O3-NEXT:    [[TMP13:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP14:%.*]] = shl nuw nsw i64 [[TMP13]], 2
; CHECK-O3-NEXT:    [[TMP15:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP16:%.*]] = shl nuw nsw i64 [[TMP15]], 2
; CHECK-O3-NEXT:    [[TMP17:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP18:%.*]] = shl nuw nsw i64 [[TMP17]], 2
; CHECK-O3-NEXT:    [[TMP19:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP20:%.*]] = shl nuw nsw i64 [[TMP19]], 2
; CHECK-O3-NEXT:    [[TMP21:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP22:%.*]] = shl nuw nsw i64 [[TMP21]], 2
; CHECK-O3-NEXT:    [[TMP23:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP24:%.*]] = shl nuw nsw i64 [[TMP23]], 2
; CHECK-O3-NEXT:    [[TMP25:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP26:%.*]] = shl nuw nsw i64 [[TMP25]], 2
; CHECK-O3-NEXT:    [[TMP27:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP28:%.*]] = shl nuw nsw i64 [[TMP27]], 2
; CHECK-O3-NEXT:    [[TMP29:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP30:%.*]] = shl nuw nsw i64 [[TMP29]], 2
; CHECK-O3-NEXT:    [[TMP31:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP32:%.*]] = shl nuw nsw i64 [[TMP31]], 2
; CHECK-O3-NEXT:    [[TMP33:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP34:%.*]] = shl nuw nsw i64 [[TMP33]], 2
; CHECK-O3-NEXT:    [[TMP35:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP36:%.*]] = shl nuw nsw i64 [[TMP35]], 2
; CHECK-O3-NEXT:    [[TMP37:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP38:%.*]] = shl nuw nsw i64 [[TMP37]], 2
; CHECK-O3-NEXT:    [[TMP39:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP40:%.*]] = shl nuw nsw i64 [[TMP39]], 2
; CHECK-O3-NEXT:    [[TMP41:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP42:%.*]] = shl nuw nsw i64 [[TMP41]], 2
; CHECK-O3-NEXT:    [[TMP43:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP44:%.*]] = shl nuw nsw i64 [[TMP43]], 2
; CHECK-O3-NEXT:    [[TMP45:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP46:%.*]] = shl nuw nsw i64 [[TMP45]], 2
; CHECK-O3-NEXT:    [[TMP47:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP48:%.*]] = shl nuw nsw i64 [[TMP47]], 2
; CHECK-O3-NEXT:    [[TMP49:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP50:%.*]] = shl nuw nsw i64 [[TMP49]], 2
; CHECK-O3-NEXT:    [[TMP51:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP52:%.*]] = shl nuw nsw i64 [[TMP51]], 2
; CHECK-O3-NEXT:    [[TMP53:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP54:%.*]] = shl nuw nsw i64 [[TMP53]], 2
; CHECK-O3-NEXT:    [[TMP55:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP56:%.*]] = shl nuw nsw i64 [[TMP55]], 2
; CHECK-O3-NEXT:    [[TMP57:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP58:%.*]] = shl nuw nsw i64 [[TMP57]], 2
; CHECK-O3-NEXT:    [[TMP59:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP60:%.*]] = shl nuw nsw i64 [[TMP59]], 2
; CHECK-O3-NEXT:    [[TMP61:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP62:%.*]] = shl nuw nsw i64 [[TMP61]], 2
; CHECK-O3-NEXT:    [[TMP63:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP64:%.*]] = shl nuw nsw i64 [[TMP63]], 2
; CHECK-O3-NEXT:    [[TMP65:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP66:%.*]] = shl nuw nsw i64 [[TMP65]], 2
; CHECK-O3-NEXT:    [[TMP67:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP68:%.*]] = shl nuw nsw i64 [[TMP67]], 2
; CHECK-O3-NEXT:    [[TMP69:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP70:%.*]] = shl nuw nsw i64 [[TMP69]], 2
; CHECK-O3-NEXT:    [[TMP71:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-O3-NEXT:    [[TMP72:%.*]] = shl nuw nsw i64 [[TMP71]], 2
; CHECK-O3-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-O3:       [[VECTOR_BODY]]:
; CHECK-O3-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-O3-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP293:%.*]], %[[VECTOR_BODY]] ]
; CHECK-O3-NEXT:    [[VEC_PHI16:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP294:%.*]], %[[VECTOR_BODY]] ]
; CHECK-O3-NEXT:    [[OFFSET_IDX:%.*]] = mul nuw i64 [[INDEX]], [[IDX_EXT]]
; CHECK-O3-NEXT:    [[NEXT_GEP:%.*]] = getelementptr i8, ptr [[P1]], i64 [[OFFSET_IDX]]
; CHECK-O3-NEXT:    [[OFFSET_IDX17:%.*]] = mul nuw i64 [[INDEX]], [[IDX_EXT8]]
; CHECK-O3-NEXT:    [[NEXT_GEP18:%.*]] = getelementptr i8, ptr [[P2]], i64 [[OFFSET_IDX17]]
; CHECK-O3-NEXT:    [[TMP73:%.*]] = getelementptr i8, ptr [[NEXT_GEP]], i64 [[TMP10]]
; CHECK-O3-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 4 x i8>, ptr [[NEXT_GEP]], align 1, !tbaa [[TBAA0:![0-9]+]]
; CHECK-O3-NEXT:    [[WIDE_LOAD19:%.*]] = load <vscale x 4 x i8>, ptr [[TMP73]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP74:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP75:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD19]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP76:%.*]] = getelementptr i8, ptr [[NEXT_GEP18]], i64 [[TMP12]]
; CHECK-O3-NEXT:    [[WIDE_LOAD20:%.*]] = load <vscale x 4 x i8>, ptr [[NEXT_GEP18]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD21:%.*]] = load <vscale x 4 x i8>, ptr [[TMP76]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP77:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD20]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP78:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD21]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP79:%.*]] = sub nsw <vscale x 4 x i32> [[TMP74]], [[TMP77]]
; CHECK-O3-NEXT:    [[TMP80:%.*]] = sub nsw <vscale x 4 x i32> [[TMP75]], [[TMP78]]
; CHECK-O3-NEXT:    [[TMP81:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP79]], i1 true)
; CHECK-O3-NEXT:    [[TMP82:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP80]], i1 true)
; CHECK-O3-NEXT:    [[TMP83:%.*]] = add <vscale x 4 x i32> [[TMP81]], [[VEC_PHI]]
; CHECK-O3-NEXT:    [[TMP84:%.*]] = add <vscale x 4 x i32> [[TMP82]], [[VEC_PHI16]]
; CHECK-O3-NEXT:    [[TMP85:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 1
; CHECK-O3-NEXT:    [[TMP86:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP85]], i64 [[TMP14]]
; CHECK-O3-NEXT:    [[WIDE_LOAD22:%.*]] = load <vscale x 4 x i8>, ptr [[TMP85]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD23:%.*]] = load <vscale x 4 x i8>, ptr [[TMP86]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP87:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD22]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP88:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD23]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP89:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 1
; CHECK-O3-NEXT:    [[TMP90:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP89]], i64 [[TMP16]]
; CHECK-O3-NEXT:    [[WIDE_LOAD24:%.*]] = load <vscale x 4 x i8>, ptr [[TMP89]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD25:%.*]] = load <vscale x 4 x i8>, ptr [[TMP90]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP91:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD24]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP92:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD25]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP93:%.*]] = sub nsw <vscale x 4 x i32> [[TMP87]], [[TMP91]]
; CHECK-O3-NEXT:    [[TMP94:%.*]] = sub nsw <vscale x 4 x i32> [[TMP88]], [[TMP92]]
; CHECK-O3-NEXT:    [[TMP95:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP93]], i1 true)
; CHECK-O3-NEXT:    [[TMP96:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP94]], i1 true)
; CHECK-O3-NEXT:    [[TMP97:%.*]] = add <vscale x 4 x i32> [[TMP95]], [[TMP83]]
; CHECK-O3-NEXT:    [[TMP98:%.*]] = add <vscale x 4 x i32> [[TMP96]], [[TMP84]]
; CHECK-O3-NEXT:    [[TMP99:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 2
; CHECK-O3-NEXT:    [[TMP100:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP99]], i64 [[TMP18]]
; CHECK-O3-NEXT:    [[WIDE_LOAD26:%.*]] = load <vscale x 4 x i8>, ptr [[TMP99]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD27:%.*]] = load <vscale x 4 x i8>, ptr [[TMP100]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP101:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD26]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP102:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD27]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP103:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 2
; CHECK-O3-NEXT:    [[TMP104:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP103]], i64 [[TMP20]]
; CHECK-O3-NEXT:    [[WIDE_LOAD28:%.*]] = load <vscale x 4 x i8>, ptr [[TMP103]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD29:%.*]] = load <vscale x 4 x i8>, ptr [[TMP104]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP105:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD28]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP106:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD29]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP107:%.*]] = sub nsw <vscale x 4 x i32> [[TMP101]], [[TMP105]]
; CHECK-O3-NEXT:    [[TMP108:%.*]] = sub nsw <vscale x 4 x i32> [[TMP102]], [[TMP106]]
; CHECK-O3-NEXT:    [[TMP109:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP107]], i1 true)
; CHECK-O3-NEXT:    [[TMP110:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP108]], i1 true)
; CHECK-O3-NEXT:    [[TMP111:%.*]] = add <vscale x 4 x i32> [[TMP109]], [[TMP97]]
; CHECK-O3-NEXT:    [[TMP297:%.*]] = add <vscale x 4 x i32> [[TMP110]], [[TMP98]]
; CHECK-O3-NEXT:    [[TMP298:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 3
; CHECK-O3-NEXT:    [[TMP299:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP298]], i64 [[TMP22]]
; CHECK-O3-NEXT:    [[WIDE_LOAD30:%.*]] = load <vscale x 4 x i8>, ptr [[TMP298]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD31:%.*]] = load <vscale x 4 x i8>, ptr [[TMP299]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP300:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD30]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP301:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD31]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP302:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 3
; CHECK-O3-NEXT:    [[TMP303:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP302]], i64 [[TMP24]]
; CHECK-O3-NEXT:    [[WIDE_LOAD32:%.*]] = load <vscale x 4 x i8>, ptr [[TMP302]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD33:%.*]] = load <vscale x 4 x i8>, ptr [[TMP303]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP304:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD32]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP120:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD33]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP121:%.*]] = sub nsw <vscale x 4 x i32> [[TMP300]], [[TMP304]]
; CHECK-O3-NEXT:    [[TMP122:%.*]] = sub nsw <vscale x 4 x i32> [[TMP301]], [[TMP120]]
; CHECK-O3-NEXT:    [[TMP123:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP121]], i1 true)
; CHECK-O3-NEXT:    [[TMP124:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP122]], i1 true)
; CHECK-O3-NEXT:    [[TMP125:%.*]] = add <vscale x 4 x i32> [[TMP123]], [[TMP111]]
; CHECK-O3-NEXT:    [[TMP126:%.*]] = add <vscale x 4 x i32> [[TMP124]], [[TMP297]]
; CHECK-O3-NEXT:    [[TMP127:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 4
; CHECK-O3-NEXT:    [[TMP128:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP127]], i64 [[TMP26]]
; CHECK-O3-NEXT:    [[WIDE_LOAD34:%.*]] = load <vscale x 4 x i8>, ptr [[TMP127]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD35:%.*]] = load <vscale x 4 x i8>, ptr [[TMP128]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP129:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD34]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP130:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD35]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP131:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 4
; CHECK-O3-NEXT:    [[TMP132:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP131]], i64 [[TMP28]]
; CHECK-O3-NEXT:    [[WIDE_LOAD36:%.*]] = load <vscale x 4 x i8>, ptr [[TMP131]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD37:%.*]] = load <vscale x 4 x i8>, ptr [[TMP132]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP133:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD36]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP134:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD37]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP135:%.*]] = sub nsw <vscale x 4 x i32> [[TMP129]], [[TMP133]]
; CHECK-O3-NEXT:    [[TMP136:%.*]] = sub nsw <vscale x 4 x i32> [[TMP130]], [[TMP134]]
; CHECK-O3-NEXT:    [[TMP137:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP135]], i1 true)
; CHECK-O3-NEXT:    [[TMP138:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP136]], i1 true)
; CHECK-O3-NEXT:    [[TMP139:%.*]] = add <vscale x 4 x i32> [[TMP137]], [[TMP125]]
; CHECK-O3-NEXT:    [[TMP140:%.*]] = add <vscale x 4 x i32> [[TMP138]], [[TMP126]]
; CHECK-O3-NEXT:    [[TMP141:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 5
; CHECK-O3-NEXT:    [[TMP142:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP141]], i64 [[TMP30]]
; CHECK-O3-NEXT:    [[WIDE_LOAD38:%.*]] = load <vscale x 4 x i8>, ptr [[TMP141]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD39:%.*]] = load <vscale x 4 x i8>, ptr [[TMP142]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP143:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD38]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP144:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD39]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP145:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 5
; CHECK-O3-NEXT:    [[TMP146:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP145]], i64 [[TMP32]]
; CHECK-O3-NEXT:    [[WIDE_LOAD40:%.*]] = load <vscale x 4 x i8>, ptr [[TMP145]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD41:%.*]] = load <vscale x 4 x i8>, ptr [[TMP146]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP147:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD40]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP148:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD41]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP149:%.*]] = sub nsw <vscale x 4 x i32> [[TMP143]], [[TMP147]]
; CHECK-O3-NEXT:    [[TMP150:%.*]] = sub nsw <vscale x 4 x i32> [[TMP144]], [[TMP148]]
; CHECK-O3-NEXT:    [[TMP151:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP149]], i1 true)
; CHECK-O3-NEXT:    [[TMP152:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP150]], i1 true)
; CHECK-O3-NEXT:    [[TMP153:%.*]] = add <vscale x 4 x i32> [[TMP151]], [[TMP139]]
; CHECK-O3-NEXT:    [[TMP154:%.*]] = add <vscale x 4 x i32> [[TMP152]], [[TMP140]]
; CHECK-O3-NEXT:    [[TMP155:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 6
; CHECK-O3-NEXT:    [[TMP156:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP155]], i64 [[TMP34]]
; CHECK-O3-NEXT:    [[WIDE_LOAD42:%.*]] = load <vscale x 4 x i8>, ptr [[TMP155]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD43:%.*]] = load <vscale x 4 x i8>, ptr [[TMP156]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP157:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD42]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP158:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD43]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP159:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 6
; CHECK-O3-NEXT:    [[TMP160:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP159]], i64 [[TMP36]]
; CHECK-O3-NEXT:    [[WIDE_LOAD44:%.*]] = load <vscale x 4 x i8>, ptr [[TMP159]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD45:%.*]] = load <vscale x 4 x i8>, ptr [[TMP160]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP161:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD44]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP162:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD45]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP163:%.*]] = sub nsw <vscale x 4 x i32> [[TMP157]], [[TMP161]]
; CHECK-O3-NEXT:    [[TMP164:%.*]] = sub nsw <vscale x 4 x i32> [[TMP158]], [[TMP162]]
; CHECK-O3-NEXT:    [[TMP165:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP163]], i1 true)
; CHECK-O3-NEXT:    [[TMP166:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP164]], i1 true)
; CHECK-O3-NEXT:    [[TMP167:%.*]] = add <vscale x 4 x i32> [[TMP165]], [[TMP153]]
; CHECK-O3-NEXT:    [[TMP168:%.*]] = add <vscale x 4 x i32> [[TMP166]], [[TMP154]]
; CHECK-O3-NEXT:    [[TMP169:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 7
; CHECK-O3-NEXT:    [[TMP170:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP169]], i64 [[TMP38]]
; CHECK-O3-NEXT:    [[WIDE_LOAD46:%.*]] = load <vscale x 4 x i8>, ptr [[TMP169]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD47:%.*]] = load <vscale x 4 x i8>, ptr [[TMP170]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP171:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD46]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP172:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD47]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP173:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 7
; CHECK-O3-NEXT:    [[TMP174:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP173]], i64 [[TMP40]]
; CHECK-O3-NEXT:    [[WIDE_LOAD48:%.*]] = load <vscale x 4 x i8>, ptr [[TMP173]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD49:%.*]] = load <vscale x 4 x i8>, ptr [[TMP174]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP175:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD48]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP176:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD49]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP177:%.*]] = sub nsw <vscale x 4 x i32> [[TMP171]], [[TMP175]]
; CHECK-O3-NEXT:    [[TMP178:%.*]] = sub nsw <vscale x 4 x i32> [[TMP172]], [[TMP176]]
; CHECK-O3-NEXT:    [[TMP179:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP177]], i1 true)
; CHECK-O3-NEXT:    [[TMP180:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP178]], i1 true)
; CHECK-O3-NEXT:    [[TMP181:%.*]] = add <vscale x 4 x i32> [[TMP179]], [[TMP167]]
; CHECK-O3-NEXT:    [[TMP182:%.*]] = add <vscale x 4 x i32> [[TMP180]], [[TMP168]]
; CHECK-O3-NEXT:    [[TMP183:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 8
; CHECK-O3-NEXT:    [[TMP184:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP183]], i64 [[TMP42]]
; CHECK-O3-NEXT:    [[WIDE_LOAD50:%.*]] = load <vscale x 4 x i8>, ptr [[TMP183]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD51:%.*]] = load <vscale x 4 x i8>, ptr [[TMP184]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP185:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD50]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP186:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD51]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP187:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 8
; CHECK-O3-NEXT:    [[TMP188:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP187]], i64 [[TMP44]]
; CHECK-O3-NEXT:    [[WIDE_LOAD52:%.*]] = load <vscale x 4 x i8>, ptr [[TMP187]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD53:%.*]] = load <vscale x 4 x i8>, ptr [[TMP188]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP189:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD52]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP190:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD53]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP191:%.*]] = sub nsw <vscale x 4 x i32> [[TMP185]], [[TMP189]]
; CHECK-O3-NEXT:    [[TMP192:%.*]] = sub nsw <vscale x 4 x i32> [[TMP186]], [[TMP190]]
; CHECK-O3-NEXT:    [[TMP193:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP191]], i1 true)
; CHECK-O3-NEXT:    [[TMP194:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP192]], i1 true)
; CHECK-O3-NEXT:    [[TMP195:%.*]] = add <vscale x 4 x i32> [[TMP193]], [[TMP181]]
; CHECK-O3-NEXT:    [[TMP196:%.*]] = add <vscale x 4 x i32> [[TMP194]], [[TMP182]]
; CHECK-O3-NEXT:    [[TMP197:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 9
; CHECK-O3-NEXT:    [[TMP198:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP197]], i64 [[TMP46]]
; CHECK-O3-NEXT:    [[WIDE_LOAD54:%.*]] = load <vscale x 4 x i8>, ptr [[TMP197]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD55:%.*]] = load <vscale x 4 x i8>, ptr [[TMP198]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP199:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD54]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP200:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD55]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP201:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 9
; CHECK-O3-NEXT:    [[TMP202:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP201]], i64 [[TMP48]]
; CHECK-O3-NEXT:    [[WIDE_LOAD56:%.*]] = load <vscale x 4 x i8>, ptr [[TMP201]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD57:%.*]] = load <vscale x 4 x i8>, ptr [[TMP202]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP203:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD56]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP204:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD57]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP205:%.*]] = sub nsw <vscale x 4 x i32> [[TMP199]], [[TMP203]]
; CHECK-O3-NEXT:    [[TMP206:%.*]] = sub nsw <vscale x 4 x i32> [[TMP200]], [[TMP204]]
; CHECK-O3-NEXT:    [[TMP207:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP205]], i1 true)
; CHECK-O3-NEXT:    [[TMP208:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP206]], i1 true)
; CHECK-O3-NEXT:    [[TMP209:%.*]] = add <vscale x 4 x i32> [[TMP207]], [[TMP195]]
; CHECK-O3-NEXT:    [[TMP210:%.*]] = add <vscale x 4 x i32> [[TMP208]], [[TMP196]]
; CHECK-O3-NEXT:    [[TMP211:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 10
; CHECK-O3-NEXT:    [[TMP212:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP211]], i64 [[TMP50]]
; CHECK-O3-NEXT:    [[WIDE_LOAD58:%.*]] = load <vscale x 4 x i8>, ptr [[TMP211]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD59:%.*]] = load <vscale x 4 x i8>, ptr [[TMP212]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP213:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD58]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP214:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD59]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP215:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 10
; CHECK-O3-NEXT:    [[TMP216:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP215]], i64 [[TMP52]]
; CHECK-O3-NEXT:    [[WIDE_LOAD60:%.*]] = load <vscale x 4 x i8>, ptr [[TMP215]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD61:%.*]] = load <vscale x 4 x i8>, ptr [[TMP216]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP217:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD60]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP218:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD61]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP219:%.*]] = sub nsw <vscale x 4 x i32> [[TMP213]], [[TMP217]]
; CHECK-O3-NEXT:    [[TMP220:%.*]] = sub nsw <vscale x 4 x i32> [[TMP214]], [[TMP218]]
; CHECK-O3-NEXT:    [[TMP221:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP219]], i1 true)
; CHECK-O3-NEXT:    [[TMP222:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP220]], i1 true)
; CHECK-O3-NEXT:    [[TMP223:%.*]] = add <vscale x 4 x i32> [[TMP221]], [[TMP209]]
; CHECK-O3-NEXT:    [[TMP224:%.*]] = add <vscale x 4 x i32> [[TMP222]], [[TMP210]]
; CHECK-O3-NEXT:    [[TMP225:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 11
; CHECK-O3-NEXT:    [[TMP226:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP225]], i64 [[TMP54]]
; CHECK-O3-NEXT:    [[WIDE_LOAD62:%.*]] = load <vscale x 4 x i8>, ptr [[TMP225]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD63:%.*]] = load <vscale x 4 x i8>, ptr [[TMP226]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP227:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD62]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP228:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD63]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP229:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 11
; CHECK-O3-NEXT:    [[TMP230:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP229]], i64 [[TMP56]]
; CHECK-O3-NEXT:    [[WIDE_LOAD64:%.*]] = load <vscale x 4 x i8>, ptr [[TMP229]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD65:%.*]] = load <vscale x 4 x i8>, ptr [[TMP230]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP231:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD64]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP232:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD65]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP233:%.*]] = sub nsw <vscale x 4 x i32> [[TMP227]], [[TMP231]]
; CHECK-O3-NEXT:    [[TMP234:%.*]] = sub nsw <vscale x 4 x i32> [[TMP228]], [[TMP232]]
; CHECK-O3-NEXT:    [[TMP235:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP233]], i1 true)
; CHECK-O3-NEXT:    [[TMP236:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP234]], i1 true)
; CHECK-O3-NEXT:    [[TMP237:%.*]] = add <vscale x 4 x i32> [[TMP235]], [[TMP223]]
; CHECK-O3-NEXT:    [[TMP238:%.*]] = add <vscale x 4 x i32> [[TMP236]], [[TMP224]]
; CHECK-O3-NEXT:    [[TMP239:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 12
; CHECK-O3-NEXT:    [[TMP240:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP239]], i64 [[TMP58]]
; CHECK-O3-NEXT:    [[WIDE_LOAD66:%.*]] = load <vscale x 4 x i8>, ptr [[TMP239]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD67:%.*]] = load <vscale x 4 x i8>, ptr [[TMP240]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP241:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD66]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP242:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD67]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP243:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 12
; CHECK-O3-NEXT:    [[TMP244:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP243]], i64 [[TMP60]]
; CHECK-O3-NEXT:    [[WIDE_LOAD68:%.*]] = load <vscale x 4 x i8>, ptr [[TMP243]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD69:%.*]] = load <vscale x 4 x i8>, ptr [[TMP244]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP245:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD68]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP246:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD69]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP247:%.*]] = sub nsw <vscale x 4 x i32> [[TMP241]], [[TMP245]]
; CHECK-O3-NEXT:    [[TMP248:%.*]] = sub nsw <vscale x 4 x i32> [[TMP242]], [[TMP246]]
; CHECK-O3-NEXT:    [[TMP249:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP247]], i1 true)
; CHECK-O3-NEXT:    [[TMP250:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP248]], i1 true)
; CHECK-O3-NEXT:    [[TMP251:%.*]] = add <vscale x 4 x i32> [[TMP249]], [[TMP237]]
; CHECK-O3-NEXT:    [[TMP252:%.*]] = add <vscale x 4 x i32> [[TMP250]], [[TMP238]]
; CHECK-O3-NEXT:    [[TMP253:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 13
; CHECK-O3-NEXT:    [[TMP254:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP253]], i64 [[TMP62]]
; CHECK-O3-NEXT:    [[WIDE_LOAD70:%.*]] = load <vscale x 4 x i8>, ptr [[TMP253]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD71:%.*]] = load <vscale x 4 x i8>, ptr [[TMP254]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP255:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD70]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP256:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD71]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP257:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 13
; CHECK-O3-NEXT:    [[TMP258:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP257]], i64 [[TMP64]]
; CHECK-O3-NEXT:    [[WIDE_LOAD72:%.*]] = load <vscale x 4 x i8>, ptr [[TMP257]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD73:%.*]] = load <vscale x 4 x i8>, ptr [[TMP258]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP259:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD72]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP260:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD73]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP261:%.*]] = sub nsw <vscale x 4 x i32> [[TMP255]], [[TMP259]]
; CHECK-O3-NEXT:    [[TMP262:%.*]] = sub nsw <vscale x 4 x i32> [[TMP256]], [[TMP260]]
; CHECK-O3-NEXT:    [[TMP263:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP261]], i1 true)
; CHECK-O3-NEXT:    [[TMP264:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP262]], i1 true)
; CHECK-O3-NEXT:    [[TMP265:%.*]] = add <vscale x 4 x i32> [[TMP263]], [[TMP251]]
; CHECK-O3-NEXT:    [[TMP266:%.*]] = add <vscale x 4 x i32> [[TMP264]], [[TMP252]]
; CHECK-O3-NEXT:    [[TMP267:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 14
; CHECK-O3-NEXT:    [[TMP268:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP267]], i64 [[TMP66]]
; CHECK-O3-NEXT:    [[WIDE_LOAD74:%.*]] = load <vscale x 4 x i8>, ptr [[TMP267]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD75:%.*]] = load <vscale x 4 x i8>, ptr [[TMP268]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP269:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD74]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP270:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD75]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP271:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 14
; CHECK-O3-NEXT:    [[TMP272:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP271]], i64 [[TMP68]]
; CHECK-O3-NEXT:    [[WIDE_LOAD76:%.*]] = load <vscale x 4 x i8>, ptr [[TMP271]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD77:%.*]] = load <vscale x 4 x i8>, ptr [[TMP272]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP273:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD76]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP274:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD77]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP275:%.*]] = sub nsw <vscale x 4 x i32> [[TMP269]], [[TMP273]]
; CHECK-O3-NEXT:    [[TMP276:%.*]] = sub nsw <vscale x 4 x i32> [[TMP270]], [[TMP274]]
; CHECK-O3-NEXT:    [[TMP277:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP275]], i1 true)
; CHECK-O3-NEXT:    [[TMP278:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP276]], i1 true)
; CHECK-O3-NEXT:    [[TMP279:%.*]] = add <vscale x 4 x i32> [[TMP277]], [[TMP265]]
; CHECK-O3-NEXT:    [[TMP280:%.*]] = add <vscale x 4 x i32> [[TMP278]], [[TMP266]]
; CHECK-O3-NEXT:    [[TMP281:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 15
; CHECK-O3-NEXT:    [[TMP282:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP281]], i64 [[TMP70]]
; CHECK-O3-NEXT:    [[WIDE_LOAD78:%.*]] = load <vscale x 4 x i8>, ptr [[TMP281]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD79:%.*]] = load <vscale x 4 x i8>, ptr [[TMP282]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP283:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD78]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP284:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD79]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP285:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 15
; CHECK-O3-NEXT:    [[TMP286:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP285]], i64 [[TMP72]]
; CHECK-O3-NEXT:    [[WIDE_LOAD80:%.*]] = load <vscale x 4 x i8>, ptr [[TMP285]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[WIDE_LOAD81:%.*]] = load <vscale x 4 x i8>, ptr [[TMP286]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP287:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD80]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP288:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD81]] to <vscale x 4 x i32>
; CHECK-O3-NEXT:    [[TMP289:%.*]] = sub nsw <vscale x 4 x i32> [[TMP283]], [[TMP287]]
; CHECK-O3-NEXT:    [[TMP290:%.*]] = sub nsw <vscale x 4 x i32> [[TMP284]], [[TMP288]]
; CHECK-O3-NEXT:    [[TMP291:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP289]], i1 true)
; CHECK-O3-NEXT:    [[TMP292:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP290]], i1 true)
; CHECK-O3-NEXT:    [[TMP293]] = add <vscale x 4 x i32> [[TMP291]], [[TMP279]]
; CHECK-O3-NEXT:    [[TMP294]] = add <vscale x 4 x i32> [[TMP292]], [[TMP280]]
; CHECK-O3-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP3]]
; CHECK-O3-NEXT:    [[TMP295:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-O3-NEXT:    br i1 [[TMP295]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK-O3:       [[MIDDLE_BLOCK]]:
; CHECK-O3-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[TMP294]], [[TMP293]]
; CHECK-O3-NEXT:    [[TMP296:%.*]] = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
; CHECK-O3-NEXT:    [[CMP_N_NOT:%.*]] = icmp eq i64 [[N_VEC]], 0
; CHECK-O3-NEXT:    br i1 [[CMP_N_NOT]], label %[[FOR_COND1_PREHEADER_PREHEADER]], label %[[FOR_COND_CLEANUP:.*]]
; CHECK-O3:       [[FOR_COND1_PREHEADER_PREHEADER]]:
; CHECK-O3-NEXT:    [[Y_013_PH:%.*]] = phi i32 [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[ENTRY]] ], [ [[TMP4]], %[[MIDDLE_BLOCK]] ]
; CHECK-O3-NEXT:    [[I_SUM_012_PH:%.*]] = phi i32 [ 0, %[[VECTOR_SCEVCHECK]] ], [ 0, %[[ENTRY]] ], [ [[TMP296]], %[[MIDDLE_BLOCK]] ]
; CHECK-O3-NEXT:    [[P1_ADDR_011_PH:%.*]] = phi ptr [ [[P1]], %[[VECTOR_SCEVCHECK]] ], [ [[P1]], %[[ENTRY]] ], [ [[TMP6]], %[[MIDDLE_BLOCK]] ]
; CHECK-O3-NEXT:    [[P2_ADDR_010_PH:%.*]] = phi ptr [ [[P2]], %[[VECTOR_SCEVCHECK]] ], [ [[P2]], %[[ENTRY]] ], [ [[TMP8]], %[[MIDDLE_BLOCK]] ]
; CHECK-O3-NEXT:    br label %[[FOR_COND1_PREHEADER:.*]]
; CHECK-O3:       [[FOR_COND1_PREHEADER]]:
; CHECK-O3-NEXT:    [[Y_013:%.*]] = phi i32 [ [[INC11:%.*]], %[[FOR_COND1_PREHEADER]] ], [ [[Y_013_PH]], %[[FOR_COND1_PREHEADER_PREHEADER]] ]
; CHECK-O3-NEXT:    [[OP_RDX_13:%.*]] = phi i32 [ [[OP_RDX_14:%.*]], %[[FOR_COND1_PREHEADER]] ], [ [[I_SUM_012_PH]], %[[FOR_COND1_PREHEADER_PREHEADER]] ]
; CHECK-O3-NEXT:    [[ADD_PTR_13:%.*]] = phi ptr [ [[ADD_PTR_14:%.*]], %[[FOR_COND1_PREHEADER]] ], [ [[P1_ADDR_011_PH]], %[[FOR_COND1_PREHEADER_PREHEADER]] ]
; CHECK-O3-NEXT:    [[ADD_PTR9_13:%.*]] = phi ptr [ [[ADD_PTR9_14:%.*]], %[[FOR_COND1_PREHEADER]] ], [ [[P2_ADDR_010_PH]], %[[FOR_COND1_PREHEADER_PREHEADER]] ]
; CHECK-O3-NEXT:    [[TMP112:%.*]] = load <16 x i8>, ptr [[ADD_PTR_13]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP113:%.*]] = zext <16 x i8> [[TMP112]] to <16 x i16>
; CHECK-O3-NEXT:    [[TMP114:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_13]], align 1, !tbaa [[TBAA0]]
; CHECK-O3-NEXT:    [[TMP115:%.*]] = zext <16 x i8> [[TMP114]] to <16 x i16>
; CHECK-O3-NEXT:    [[TMP116:%.*]] = sub nsw <16 x i16> [[TMP113]], [[TMP115]]
; CHECK-O3-NEXT:    [[TMP117:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP116]], i1 false)
; CHECK-O3-NEXT:    [[TMP118:%.*]] = zext <16 x i16> [[TMP117]] to <16 x i32>
; CHECK-O3-NEXT:    [[TMP119:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP118]])
; CHECK-O3-NEXT:    [[OP_RDX_14]] = add i32 [[TMP119]], [[OP_RDX_13]]
; CHECK-O3-NEXT:    [[ADD_PTR_14]] = getelementptr inbounds i8, ptr [[ADD_PTR_13]], i64 [[IDX_EXT]]
; CHECK-O3-NEXT:    [[ADD_PTR9_14]] = getelementptr inbounds i8, ptr [[ADD_PTR9_13]], i64 [[IDX_EXT8]]
; CHECK-O3-NEXT:    [[INC11]] = add nuw nsw i32 [[Y_013]], 1
; CHECK-O3-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i32 [[INC11]], 16
; CHECK-O3-NEXT:    br i1 [[EXITCOND_NOT]], label %[[FOR_COND_CLEANUP]], label %[[FOR_COND1_PREHEADER]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK-O3:       [[FOR_COND_CLEANUP]]:
; CHECK-O3-NEXT:    [[OP_RDX_15:%.*]] = phi i32 [ [[TMP296]], %[[MIDDLE_BLOCK]] ], [ [[OP_RDX_14]], %[[FOR_COND1_PREHEADER]] ]
; CHECK-O3-NEXT:    ret i32 [[OP_RDX_15]]
;
; CHECK-LTO-LABEL: define dso_local i32 @test(
; CHECK-LTO-SAME: ptr noundef readonly captures(none) [[P1:%.*]], i32 noundef [[S_P1:%.*]], ptr noundef readonly captures(none) [[P2:%.*]], i32 noundef [[S_P2:%.*]]) local_unnamed_addr #[[ATTR0:[0-9]+]] {
; CHECK-LTO-NEXT:  [[ENTRY:.*:]]
; CHECK-LTO-NEXT:    [[IDX_EXT:%.*]] = sext i32 [[S_P1]] to i64
; CHECK-LTO-NEXT:    [[IDX_EXT8:%.*]] = sext i32 [[S_P2]] to i64
; CHECK-LTO-NEXT:    [[TMP228:%.*]] = tail call i64 @llvm.vscale.i64()
; CHECK-LTO-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp samesign ugt i64 [[TMP228]], 2
; CHECK-LTO-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[FOR_COND1_PREHEADER_PREHEADER:.*]], label %[[VECTOR_SCEVCHECK:.*]]
; CHECK-LTO:       [[VECTOR_SCEVCHECK]]:
; CHECK-LTO-NEXT:    [[IDENT_CHECK:%.*]] = icmp ne i32 [[S_P1]], 1
; CHECK-LTO-NEXT:    [[IDENT_CHECK15:%.*]] = icmp ne i32 [[S_P2]], 1
; CHECK-LTO-NEXT:    [[TMP229:%.*]] = or i1 [[IDENT_CHECK]], [[IDENT_CHECK15]]
; CHECK-LTO-NEXT:    br i1 [[TMP229]], label %[[FOR_COND1_PREHEADER_PREHEADER]], label %[[VECTOR_PH:.*]]
; CHECK-LTO:       [[VECTOR_PH]]:
; CHECK-LTO-NEXT:    [[TMP230:%.*]] = shl nuw nsw i64 [[TMP228]], 3
; CHECK-LTO-NEXT:    [[TMP231:%.*]] = shl nuw nsw i64 [[TMP228]], 2
; CHECK-LTO-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-LTO:       [[VECTOR_BODY]]:
; CHECK-LTO-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-LTO-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP224:%.*]], %[[VECTOR_BODY]] ]
; CHECK-LTO-NEXT:    [[VEC_PHI16:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP225:%.*]], %[[VECTOR_BODY]] ]
; CHECK-LTO-NEXT:    [[OFFSET_IDX:%.*]] = mul nuw nsw i64 [[INDEX]], [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[NEXT_GEP:%.*]] = getelementptr i8, ptr [[P1]], i64 [[OFFSET_IDX]]
; CHECK-LTO-NEXT:    [[OFFSET_IDX17:%.*]] = mul nuw nsw i64 [[INDEX]], [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[NEXT_GEP18:%.*]] = getelementptr i8, ptr [[P2]], i64 [[OFFSET_IDX17]]
; CHECK-LTO-NEXT:    [[TMP232:%.*]] = getelementptr i8, ptr [[NEXT_GEP]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 4 x i8>, ptr [[NEXT_GEP]], align 1, !tbaa [[TBAA0:![0-9]+]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD19:%.*]] = load <vscale x 4 x i8>, ptr [[TMP232]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP233:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP234:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD19]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP235:%.*]] = getelementptr i8, ptr [[NEXT_GEP18]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD20:%.*]] = load <vscale x 4 x i8>, ptr [[NEXT_GEP18]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD21:%.*]] = load <vscale x 4 x i8>, ptr [[TMP235]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP236:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD20]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP237:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD21]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP238:%.*]] = sub nsw <vscale x 4 x i32> [[TMP233]], [[TMP236]]
; CHECK-LTO-NEXT:    [[TMP239:%.*]] = sub nsw <vscale x 4 x i32> [[TMP234]], [[TMP237]]
; CHECK-LTO-NEXT:    [[TMP240:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP238]], i1 true)
; CHECK-LTO-NEXT:    [[TMP241:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP239]], i1 true)
; CHECK-LTO-NEXT:    [[TMP242:%.*]] = add <vscale x 4 x i32> [[TMP240]], [[VEC_PHI]]
; CHECK-LTO-NEXT:    [[TMP243:%.*]] = add <vscale x 4 x i32> [[TMP241]], [[VEC_PHI16]]
; CHECK-LTO-NEXT:    [[TMP244:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 1
; CHECK-LTO-NEXT:    [[TMP245:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP244]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD22:%.*]] = load <vscale x 4 x i8>, ptr [[TMP244]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD23:%.*]] = load <vscale x 4 x i8>, ptr [[TMP245]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP246:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD22]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP247:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD23]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP248:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 1
; CHECK-LTO-NEXT:    [[TMP249:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP248]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD24:%.*]] = load <vscale x 4 x i8>, ptr [[TMP248]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD25:%.*]] = load <vscale x 4 x i8>, ptr [[TMP249]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP250:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD24]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP251:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD25]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP252:%.*]] = sub nsw <vscale x 4 x i32> [[TMP246]], [[TMP250]]
; CHECK-LTO-NEXT:    [[TMP253:%.*]] = sub nsw <vscale x 4 x i32> [[TMP247]], [[TMP251]]
; CHECK-LTO-NEXT:    [[TMP254:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP252]], i1 true)
; CHECK-LTO-NEXT:    [[TMP255:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP253]], i1 true)
; CHECK-LTO-NEXT:    [[TMP256:%.*]] = add <vscale x 4 x i32> [[TMP242]], [[TMP254]]
; CHECK-LTO-NEXT:    [[TMP257:%.*]] = add <vscale x 4 x i32> [[TMP243]], [[TMP255]]
; CHECK-LTO-NEXT:    [[TMP258:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 2
; CHECK-LTO-NEXT:    [[TMP259:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP258]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD26:%.*]] = load <vscale x 4 x i8>, ptr [[TMP258]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD27:%.*]] = load <vscale x 4 x i8>, ptr [[TMP259]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP260:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD26]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP261:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD27]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP262:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 2
; CHECK-LTO-NEXT:    [[TMP263:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP262]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD28:%.*]] = load <vscale x 4 x i8>, ptr [[TMP262]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD29:%.*]] = load <vscale x 4 x i8>, ptr [[TMP263]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP264:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD28]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP265:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD29]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP266:%.*]] = sub nsw <vscale x 4 x i32> [[TMP260]], [[TMP264]]
; CHECK-LTO-NEXT:    [[TMP267:%.*]] = sub nsw <vscale x 4 x i32> [[TMP261]], [[TMP265]]
; CHECK-LTO-NEXT:    [[TMP268:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP266]], i1 true)
; CHECK-LTO-NEXT:    [[TMP269:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP267]], i1 true)
; CHECK-LTO-NEXT:    [[TMP270:%.*]] = add <vscale x 4 x i32> [[TMP256]], [[TMP268]]
; CHECK-LTO-NEXT:    [[TMP271:%.*]] = add <vscale x 4 x i32> [[TMP257]], [[TMP269]]
; CHECK-LTO-NEXT:    [[TMP272:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 3
; CHECK-LTO-NEXT:    [[TMP273:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP272]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD30:%.*]] = load <vscale x 4 x i8>, ptr [[TMP272]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD31:%.*]] = load <vscale x 4 x i8>, ptr [[TMP273]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP274:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD30]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP275:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD31]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP276:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 3
; CHECK-LTO-NEXT:    [[TMP277:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP276]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD32:%.*]] = load <vscale x 4 x i8>, ptr [[TMP276]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD33:%.*]] = load <vscale x 4 x i8>, ptr [[TMP277]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP278:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD32]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP279:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD33]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP280:%.*]] = sub nsw <vscale x 4 x i32> [[TMP274]], [[TMP278]]
; CHECK-LTO-NEXT:    [[TMP281:%.*]] = sub nsw <vscale x 4 x i32> [[TMP275]], [[TMP279]]
; CHECK-LTO-NEXT:    [[TMP282:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP280]], i1 true)
; CHECK-LTO-NEXT:    [[TMP283:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP281]], i1 true)
; CHECK-LTO-NEXT:    [[TMP284:%.*]] = add <vscale x 4 x i32> [[TMP270]], [[TMP282]]
; CHECK-LTO-NEXT:    [[TMP285:%.*]] = add <vscale x 4 x i32> [[TMP271]], [[TMP283]]
; CHECK-LTO-NEXT:    [[TMP286:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 4
; CHECK-LTO-NEXT:    [[TMP287:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP286]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD34:%.*]] = load <vscale x 4 x i8>, ptr [[TMP286]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD35:%.*]] = load <vscale x 4 x i8>, ptr [[TMP287]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP288:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD34]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP289:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD35]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP290:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 4
; CHECK-LTO-NEXT:    [[TMP291:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP290]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD36:%.*]] = load <vscale x 4 x i8>, ptr [[TMP290]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD37:%.*]] = load <vscale x 4 x i8>, ptr [[TMP291]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP292:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD36]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP293:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD37]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP294:%.*]] = sub nsw <vscale x 4 x i32> [[TMP288]], [[TMP292]]
; CHECK-LTO-NEXT:    [[TMP295:%.*]] = sub nsw <vscale x 4 x i32> [[TMP289]], [[TMP293]]
; CHECK-LTO-NEXT:    [[TMP296:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP294]], i1 true)
; CHECK-LTO-NEXT:    [[TMP297:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP295]], i1 true)
; CHECK-LTO-NEXT:    [[TMP298:%.*]] = add <vscale x 4 x i32> [[TMP284]], [[TMP296]]
; CHECK-LTO-NEXT:    [[TMP299:%.*]] = add <vscale x 4 x i32> [[TMP285]], [[TMP297]]
; CHECK-LTO-NEXT:    [[TMP300:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 5
; CHECK-LTO-NEXT:    [[TMP301:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP300]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD38:%.*]] = load <vscale x 4 x i8>, ptr [[TMP300]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD39:%.*]] = load <vscale x 4 x i8>, ptr [[TMP301]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP302:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD38]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP303:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD39]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP304:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 5
; CHECK-LTO-NEXT:    [[TMP305:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP304]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD40:%.*]] = load <vscale x 4 x i8>, ptr [[TMP304]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD41:%.*]] = load <vscale x 4 x i8>, ptr [[TMP305]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP306:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD40]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP307:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD41]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP308:%.*]] = sub nsw <vscale x 4 x i32> [[TMP302]], [[TMP306]]
; CHECK-LTO-NEXT:    [[TMP309:%.*]] = sub nsw <vscale x 4 x i32> [[TMP303]], [[TMP307]]
; CHECK-LTO-NEXT:    [[TMP310:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP308]], i1 true)
; CHECK-LTO-NEXT:    [[TMP311:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP309]], i1 true)
; CHECK-LTO-NEXT:    [[TMP312:%.*]] = add <vscale x 4 x i32> [[TMP298]], [[TMP310]]
; CHECK-LTO-NEXT:    [[TMP313:%.*]] = add <vscale x 4 x i32> [[TMP299]], [[TMP311]]
; CHECK-LTO-NEXT:    [[TMP314:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 6
; CHECK-LTO-NEXT:    [[TMP315:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP314]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD42:%.*]] = load <vscale x 4 x i8>, ptr [[TMP314]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD43:%.*]] = load <vscale x 4 x i8>, ptr [[TMP315]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP316:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD42]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP317:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD43]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP318:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 6
; CHECK-LTO-NEXT:    [[TMP319:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP318]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD44:%.*]] = load <vscale x 4 x i8>, ptr [[TMP318]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD45:%.*]] = load <vscale x 4 x i8>, ptr [[TMP319]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP320:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD44]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP321:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD45]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP322:%.*]] = sub nsw <vscale x 4 x i32> [[TMP316]], [[TMP320]]
; CHECK-LTO-NEXT:    [[TMP323:%.*]] = sub nsw <vscale x 4 x i32> [[TMP317]], [[TMP321]]
; CHECK-LTO-NEXT:    [[TMP324:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP322]], i1 true)
; CHECK-LTO-NEXT:    [[TMP325:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP323]], i1 true)
; CHECK-LTO-NEXT:    [[TMP326:%.*]] = add <vscale x 4 x i32> [[TMP312]], [[TMP324]]
; CHECK-LTO-NEXT:    [[TMP327:%.*]] = add <vscale x 4 x i32> [[TMP313]], [[TMP325]]
; CHECK-LTO-NEXT:    [[TMP328:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 7
; CHECK-LTO-NEXT:    [[TMP329:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP328]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD46:%.*]] = load <vscale x 4 x i8>, ptr [[TMP328]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD47:%.*]] = load <vscale x 4 x i8>, ptr [[TMP329]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP330:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD46]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP331:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD47]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP332:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 7
; CHECK-LTO-NEXT:    [[TMP333:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP332]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD48:%.*]] = load <vscale x 4 x i8>, ptr [[TMP332]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD49:%.*]] = load <vscale x 4 x i8>, ptr [[TMP333]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP334:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD48]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP335:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD49]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP336:%.*]] = sub nsw <vscale x 4 x i32> [[TMP330]], [[TMP334]]
; CHECK-LTO-NEXT:    [[TMP337:%.*]] = sub nsw <vscale x 4 x i32> [[TMP331]], [[TMP335]]
; CHECK-LTO-NEXT:    [[TMP338:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP336]], i1 true)
; CHECK-LTO-NEXT:    [[TMP339:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP337]], i1 true)
; CHECK-LTO-NEXT:    [[TMP340:%.*]] = add <vscale x 4 x i32> [[TMP326]], [[TMP338]]
; CHECK-LTO-NEXT:    [[TMP341:%.*]] = add <vscale x 4 x i32> [[TMP327]], [[TMP339]]
; CHECK-LTO-NEXT:    [[TMP342:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 8
; CHECK-LTO-NEXT:    [[TMP343:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP342]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD50:%.*]] = load <vscale x 4 x i8>, ptr [[TMP342]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD51:%.*]] = load <vscale x 4 x i8>, ptr [[TMP343]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP344:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD50]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP345:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD51]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP346:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 8
; CHECK-LTO-NEXT:    [[TMP347:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP346]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD52:%.*]] = load <vscale x 4 x i8>, ptr [[TMP346]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD53:%.*]] = load <vscale x 4 x i8>, ptr [[TMP347]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP348:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD52]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP349:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD53]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP350:%.*]] = sub nsw <vscale x 4 x i32> [[TMP344]], [[TMP348]]
; CHECK-LTO-NEXT:    [[TMP351:%.*]] = sub nsw <vscale x 4 x i32> [[TMP345]], [[TMP349]]
; CHECK-LTO-NEXT:    [[TMP352:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP350]], i1 true)
; CHECK-LTO-NEXT:    [[TMP353:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP351]], i1 true)
; CHECK-LTO-NEXT:    [[TMP354:%.*]] = add <vscale x 4 x i32> [[TMP340]], [[TMP352]]
; CHECK-LTO-NEXT:    [[TMP355:%.*]] = add <vscale x 4 x i32> [[TMP341]], [[TMP353]]
; CHECK-LTO-NEXT:    [[TMP128:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 9
; CHECK-LTO-NEXT:    [[TMP129:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP128]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD54:%.*]] = load <vscale x 4 x i8>, ptr [[TMP128]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD55:%.*]] = load <vscale x 4 x i8>, ptr [[TMP129]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP130:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD54]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP131:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD55]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP132:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 9
; CHECK-LTO-NEXT:    [[TMP133:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP132]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD56:%.*]] = load <vscale x 4 x i8>, ptr [[TMP132]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD57:%.*]] = load <vscale x 4 x i8>, ptr [[TMP133]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP134:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD56]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP135:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD57]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP136:%.*]] = sub nsw <vscale x 4 x i32> [[TMP130]], [[TMP134]]
; CHECK-LTO-NEXT:    [[TMP137:%.*]] = sub nsw <vscale x 4 x i32> [[TMP131]], [[TMP135]]
; CHECK-LTO-NEXT:    [[TMP138:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP136]], i1 true)
; CHECK-LTO-NEXT:    [[TMP139:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP137]], i1 true)
; CHECK-LTO-NEXT:    [[TMP140:%.*]] = add <vscale x 4 x i32> [[TMP354]], [[TMP138]]
; CHECK-LTO-NEXT:    [[TMP141:%.*]] = add <vscale x 4 x i32> [[TMP355]], [[TMP139]]
; CHECK-LTO-NEXT:    [[TMP142:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 10
; CHECK-LTO-NEXT:    [[TMP143:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP142]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD58:%.*]] = load <vscale x 4 x i8>, ptr [[TMP142]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD59:%.*]] = load <vscale x 4 x i8>, ptr [[TMP143]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP144:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD58]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP145:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD59]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP146:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 10
; CHECK-LTO-NEXT:    [[TMP147:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP146]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD60:%.*]] = load <vscale x 4 x i8>, ptr [[TMP146]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD61:%.*]] = load <vscale x 4 x i8>, ptr [[TMP147]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP148:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD60]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP149:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD61]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP150:%.*]] = sub nsw <vscale x 4 x i32> [[TMP144]], [[TMP148]]
; CHECK-LTO-NEXT:    [[TMP151:%.*]] = sub nsw <vscale x 4 x i32> [[TMP145]], [[TMP149]]
; CHECK-LTO-NEXT:    [[TMP152:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP150]], i1 true)
; CHECK-LTO-NEXT:    [[TMP153:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP151]], i1 true)
; CHECK-LTO-NEXT:    [[TMP154:%.*]] = add <vscale x 4 x i32> [[TMP140]], [[TMP152]]
; CHECK-LTO-NEXT:    [[TMP155:%.*]] = add <vscale x 4 x i32> [[TMP141]], [[TMP153]]
; CHECK-LTO-NEXT:    [[TMP156:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 11
; CHECK-LTO-NEXT:    [[TMP157:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP156]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD62:%.*]] = load <vscale x 4 x i8>, ptr [[TMP156]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD63:%.*]] = load <vscale x 4 x i8>, ptr [[TMP157]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP158:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD62]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP159:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD63]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP160:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 11
; CHECK-LTO-NEXT:    [[TMP161:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP160]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD64:%.*]] = load <vscale x 4 x i8>, ptr [[TMP160]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD65:%.*]] = load <vscale x 4 x i8>, ptr [[TMP161]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP162:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD64]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP163:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD65]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP164:%.*]] = sub nsw <vscale x 4 x i32> [[TMP158]], [[TMP162]]
; CHECK-LTO-NEXT:    [[TMP165:%.*]] = sub nsw <vscale x 4 x i32> [[TMP159]], [[TMP163]]
; CHECK-LTO-NEXT:    [[TMP166:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP164]], i1 true)
; CHECK-LTO-NEXT:    [[TMP167:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP165]], i1 true)
; CHECK-LTO-NEXT:    [[TMP168:%.*]] = add <vscale x 4 x i32> [[TMP154]], [[TMP166]]
; CHECK-LTO-NEXT:    [[TMP169:%.*]] = add <vscale x 4 x i32> [[TMP155]], [[TMP167]]
; CHECK-LTO-NEXT:    [[TMP170:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 12
; CHECK-LTO-NEXT:    [[TMP171:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP170]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD66:%.*]] = load <vscale x 4 x i8>, ptr [[TMP170]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD67:%.*]] = load <vscale x 4 x i8>, ptr [[TMP171]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP172:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD66]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP173:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD67]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP174:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 12
; CHECK-LTO-NEXT:    [[TMP175:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP174]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD68:%.*]] = load <vscale x 4 x i8>, ptr [[TMP174]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD69:%.*]] = load <vscale x 4 x i8>, ptr [[TMP175]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP176:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD68]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP177:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD69]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP178:%.*]] = sub nsw <vscale x 4 x i32> [[TMP172]], [[TMP176]]
; CHECK-LTO-NEXT:    [[TMP179:%.*]] = sub nsw <vscale x 4 x i32> [[TMP173]], [[TMP177]]
; CHECK-LTO-NEXT:    [[TMP180:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP178]], i1 true)
; CHECK-LTO-NEXT:    [[TMP181:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP179]], i1 true)
; CHECK-LTO-NEXT:    [[TMP182:%.*]] = add <vscale x 4 x i32> [[TMP168]], [[TMP180]]
; CHECK-LTO-NEXT:    [[TMP183:%.*]] = add <vscale x 4 x i32> [[TMP169]], [[TMP181]]
; CHECK-LTO-NEXT:    [[TMP184:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 13
; CHECK-LTO-NEXT:    [[TMP185:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP184]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD70:%.*]] = load <vscale x 4 x i8>, ptr [[TMP184]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD71:%.*]] = load <vscale x 4 x i8>, ptr [[TMP185]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP186:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD70]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP187:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD71]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP188:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 13
; CHECK-LTO-NEXT:    [[TMP189:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP188]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD72:%.*]] = load <vscale x 4 x i8>, ptr [[TMP188]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD73:%.*]] = load <vscale x 4 x i8>, ptr [[TMP189]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP190:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD72]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP191:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD73]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP192:%.*]] = sub nsw <vscale x 4 x i32> [[TMP186]], [[TMP190]]
; CHECK-LTO-NEXT:    [[TMP193:%.*]] = sub nsw <vscale x 4 x i32> [[TMP187]], [[TMP191]]
; CHECK-LTO-NEXT:    [[TMP194:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP192]], i1 true)
; CHECK-LTO-NEXT:    [[TMP195:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP193]], i1 true)
; CHECK-LTO-NEXT:    [[TMP196:%.*]] = add <vscale x 4 x i32> [[TMP182]], [[TMP194]]
; CHECK-LTO-NEXT:    [[TMP197:%.*]] = add <vscale x 4 x i32> [[TMP183]], [[TMP195]]
; CHECK-LTO-NEXT:    [[TMP198:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 14
; CHECK-LTO-NEXT:    [[TMP199:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP198]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD74:%.*]] = load <vscale x 4 x i8>, ptr [[TMP198]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD75:%.*]] = load <vscale x 4 x i8>, ptr [[TMP199]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP200:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD74]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP201:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD75]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP202:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 14
; CHECK-LTO-NEXT:    [[TMP203:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP202]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD76:%.*]] = load <vscale x 4 x i8>, ptr [[TMP202]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD77:%.*]] = load <vscale x 4 x i8>, ptr [[TMP203]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP204:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD76]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP205:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD77]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP206:%.*]] = sub nsw <vscale x 4 x i32> [[TMP200]], [[TMP204]]
; CHECK-LTO-NEXT:    [[TMP207:%.*]] = sub nsw <vscale x 4 x i32> [[TMP201]], [[TMP205]]
; CHECK-LTO-NEXT:    [[TMP208:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP206]], i1 true)
; CHECK-LTO-NEXT:    [[TMP209:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP207]], i1 true)
; CHECK-LTO-NEXT:    [[TMP210:%.*]] = add <vscale x 4 x i32> [[TMP196]], [[TMP208]]
; CHECK-LTO-NEXT:    [[TMP211:%.*]] = add <vscale x 4 x i32> [[TMP197]], [[TMP209]]
; CHECK-LTO-NEXT:    [[TMP212:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP]], i64 15
; CHECK-LTO-NEXT:    [[TMP213:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP212]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD78:%.*]] = load <vscale x 4 x i8>, ptr [[TMP212]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD79:%.*]] = load <vscale x 4 x i8>, ptr [[TMP213]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP214:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD78]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP215:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD79]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP216:%.*]] = getelementptr inbounds nuw i8, ptr [[NEXT_GEP18]], i64 15
; CHECK-LTO-NEXT:    [[TMP217:%.*]] = getelementptr inbounds nuw i8, ptr [[TMP216]], i64 [[TMP231]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD80:%.*]] = load <vscale x 4 x i8>, ptr [[TMP216]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[WIDE_LOAD81:%.*]] = load <vscale x 4 x i8>, ptr [[TMP217]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP218:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD80]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP219:%.*]] = zext <vscale x 4 x i8> [[WIDE_LOAD81]] to <vscale x 4 x i32>
; CHECK-LTO-NEXT:    [[TMP220:%.*]] = sub nsw <vscale x 4 x i32> [[TMP214]], [[TMP218]]
; CHECK-LTO-NEXT:    [[TMP221:%.*]] = sub nsw <vscale x 4 x i32> [[TMP215]], [[TMP219]]
; CHECK-LTO-NEXT:    [[TMP222:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP220]], i1 true)
; CHECK-LTO-NEXT:    [[TMP223:%.*]] = tail call <vscale x 4 x i32> @llvm.abs.nxv4i32(<vscale x 4 x i32> [[TMP221]], i1 true)
; CHECK-LTO-NEXT:    [[TMP224]] = add <vscale x 4 x i32> [[TMP210]], [[TMP222]]
; CHECK-LTO-NEXT:    [[TMP225]] = add <vscale x 4 x i32> [[TMP211]], [[TMP223]]
; CHECK-LTO-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP230]]
; CHECK-LTO-NEXT:    [[TMP226:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-LTO-NEXT:    br i1 [[TMP226]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK-LTO:       [[MIDDLE_BLOCK]]:
; CHECK-LTO-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[TMP225]], [[TMP224]]
; CHECK-LTO-NEXT:    [[TMP227:%.*]] = tail call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
; CHECK-LTO-NEXT:    br label %[[FOR_COND_CLEANUP:.*]]
; CHECK-LTO:       [[FOR_COND1_PREHEADER_PREHEADER]]:
; CHECK-LTO-NEXT:    [[TMP0:%.*]] = load <16 x i8>, ptr [[P1]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP1:%.*]] = zext <16 x i8> [[TMP0]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP2:%.*]] = load <16 x i8>, ptr [[P2]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP3:%.*]] = zext <16 x i8> [[TMP2]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP4:%.*]] = sub nsw <16 x i16> [[TMP1]], [[TMP3]]
; CHECK-LTO-NEXT:    [[TMP5:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP4]], i1 true)
; CHECK-LTO-NEXT:    [[TMP36:%.*]] = zext nneg <16 x i16> [[TMP5]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP44:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP36]])
; CHECK-LTO-NEXT:    [[ADD_PTR:%.*]] = getelementptr inbounds i8, ptr [[P1]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9:%.*]] = getelementptr inbounds i8, ptr [[P2]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP6:%.*]] = load <16 x i8>, ptr [[ADD_PTR]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP7:%.*]] = zext <16 x i8> [[TMP6]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP8:%.*]] = load <16 x i8>, ptr [[ADD_PTR9]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP9:%.*]] = zext <16 x i8> [[TMP8]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP10:%.*]] = sub nsw <16 x i16> [[TMP7]], [[TMP9]]
; CHECK-LTO-NEXT:    [[TMP11:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP10]], i1 true)
; CHECK-LTO-NEXT:    [[TMP52:%.*]] = zext nneg <16 x i16> [[TMP11]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP60:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP52]])
; CHECK-LTO-NEXT:    [[OP_RDX_1:%.*]] = add i32 [[TMP60]], [[TMP44]]
; CHECK-LTO-NEXT:    [[ADD_PTR_1:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_1:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP12:%.*]] = load <16 x i8>, ptr [[ADD_PTR_1]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP13:%.*]] = zext <16 x i8> [[TMP12]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP14:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_1]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP15:%.*]] = zext <16 x i8> [[TMP14]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP16:%.*]] = sub nsw <16 x i16> [[TMP13]], [[TMP15]]
; CHECK-LTO-NEXT:    [[TMP17:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP16]], i1 true)
; CHECK-LTO-NEXT:    [[TMP68:%.*]] = zext nneg <16 x i16> [[TMP17]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP76:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP68]])
; CHECK-LTO-NEXT:    [[OP_RDX_2:%.*]] = add i32 [[TMP76]], [[OP_RDX_1]]
; CHECK-LTO-NEXT:    [[ADD_PTR_2:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_1]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_2:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_1]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP18:%.*]] = load <16 x i8>, ptr [[ADD_PTR_2]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP19:%.*]] = zext <16 x i8> [[TMP18]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP20:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_2]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP21:%.*]] = zext <16 x i8> [[TMP20]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP22:%.*]] = sub nsw <16 x i16> [[TMP19]], [[TMP21]]
; CHECK-LTO-NEXT:    [[TMP23:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP22]], i1 true)
; CHECK-LTO-NEXT:    [[TMP84:%.*]] = zext nneg <16 x i16> [[TMP23]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP92:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP84]])
; CHECK-LTO-NEXT:    [[OP_RDX_3:%.*]] = add i32 [[TMP92]], [[OP_RDX_2]]
; CHECK-LTO-NEXT:    [[ADD_PTR_3:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_2]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_3:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_2]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP24:%.*]] = load <16 x i8>, ptr [[ADD_PTR_3]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP25:%.*]] = zext <16 x i8> [[TMP24]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP26:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_3]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP27:%.*]] = zext <16 x i8> [[TMP26]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP28:%.*]] = sub nsw <16 x i16> [[TMP25]], [[TMP27]]
; CHECK-LTO-NEXT:    [[TMP29:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP28]], i1 true)
; CHECK-LTO-NEXT:    [[TMP100:%.*]] = zext nneg <16 x i16> [[TMP29]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP108:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP100]])
; CHECK-LTO-NEXT:    [[OP_RDX_4:%.*]] = add i32 [[TMP108]], [[OP_RDX_3]]
; CHECK-LTO-NEXT:    [[ADD_PTR_4:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_3]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_4:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_3]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP30:%.*]] = load <16 x i8>, ptr [[ADD_PTR_4]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP31:%.*]] = zext <16 x i8> [[TMP30]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP32:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_4]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP33:%.*]] = zext <16 x i8> [[TMP32]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP34:%.*]] = sub nsw <16 x i16> [[TMP31]], [[TMP33]]
; CHECK-LTO-NEXT:    [[TMP35:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP34]], i1 true)
; CHECK-LTO-NEXT:    [[TMP116:%.*]] = zext nneg <16 x i16> [[TMP35]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP117:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP116]])
; CHECK-LTO-NEXT:    [[OP_RDX_5:%.*]] = add i32 [[TMP117]], [[OP_RDX_4]]
; CHECK-LTO-NEXT:    [[ADD_PTR_5:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_4]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_5:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_4]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP37:%.*]] = load <16 x i8>, ptr [[ADD_PTR_5]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP38:%.*]] = zext <16 x i8> [[TMP37]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP39:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_5]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP40:%.*]] = zext <16 x i8> [[TMP39]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP41:%.*]] = sub nsw <16 x i16> [[TMP38]], [[TMP40]]
; CHECK-LTO-NEXT:    [[TMP42:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP41]], i1 true)
; CHECK-LTO-NEXT:    [[TMP43:%.*]] = zext nneg <16 x i16> [[TMP42]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP118:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP43]])
; CHECK-LTO-NEXT:    [[OP_RDX_6:%.*]] = add i32 [[TMP118]], [[OP_RDX_5]]
; CHECK-LTO-NEXT:    [[ADD_PTR_6:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_5]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_6:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_5]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP45:%.*]] = load <16 x i8>, ptr [[ADD_PTR_6]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP46:%.*]] = zext <16 x i8> [[TMP45]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP47:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_6]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP48:%.*]] = zext <16 x i8> [[TMP47]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP49:%.*]] = sub nsw <16 x i16> [[TMP46]], [[TMP48]]
; CHECK-LTO-NEXT:    [[TMP50:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP49]], i1 true)
; CHECK-LTO-NEXT:    [[TMP51:%.*]] = zext nneg <16 x i16> [[TMP50]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP120:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP51]])
; CHECK-LTO-NEXT:    [[OP_RDX_7:%.*]] = add i32 [[TMP120]], [[OP_RDX_6]]
; CHECK-LTO-NEXT:    [[ADD_PTR_7:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_6]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_7:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_6]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP53:%.*]] = load <16 x i8>, ptr [[ADD_PTR_7]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP54:%.*]] = zext <16 x i8> [[TMP53]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP55:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_7]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP56:%.*]] = zext <16 x i8> [[TMP55]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP57:%.*]] = sub nsw <16 x i16> [[TMP54]], [[TMP56]]
; CHECK-LTO-NEXT:    [[TMP58:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP57]], i1 true)
; CHECK-LTO-NEXT:    [[TMP59:%.*]] = zext nneg <16 x i16> [[TMP58]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP121:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP59]])
; CHECK-LTO-NEXT:    [[OP_RDX_8:%.*]] = add i32 [[TMP121]], [[OP_RDX_7]]
; CHECK-LTO-NEXT:    [[ADD_PTR_8:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_7]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_8:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_7]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP61:%.*]] = load <16 x i8>, ptr [[ADD_PTR_8]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP62:%.*]] = zext <16 x i8> [[TMP61]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP63:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_8]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP64:%.*]] = zext <16 x i8> [[TMP63]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP65:%.*]] = sub nsw <16 x i16> [[TMP62]], [[TMP64]]
; CHECK-LTO-NEXT:    [[TMP66:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP65]], i1 true)
; CHECK-LTO-NEXT:    [[TMP67:%.*]] = zext nneg <16 x i16> [[TMP66]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP122:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP67]])
; CHECK-LTO-NEXT:    [[OP_RDX_9:%.*]] = add i32 [[TMP122]], [[OP_RDX_8]]
; CHECK-LTO-NEXT:    [[ADD_PTR_9:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_8]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_9:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_8]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP69:%.*]] = load <16 x i8>, ptr [[ADD_PTR_9]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP70:%.*]] = zext <16 x i8> [[TMP69]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP71:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_9]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP72:%.*]] = zext <16 x i8> [[TMP71]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP73:%.*]] = sub nsw <16 x i16> [[TMP70]], [[TMP72]]
; CHECK-LTO-NEXT:    [[TMP74:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP73]], i1 true)
; CHECK-LTO-NEXT:    [[TMP75:%.*]] = zext nneg <16 x i16> [[TMP74]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP123:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP75]])
; CHECK-LTO-NEXT:    [[OP_RDX_10:%.*]] = add i32 [[TMP123]], [[OP_RDX_9]]
; CHECK-LTO-NEXT:    [[ADD_PTR_10:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_9]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_10:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_9]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP77:%.*]] = load <16 x i8>, ptr [[ADD_PTR_10]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP78:%.*]] = zext <16 x i8> [[TMP77]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP79:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_10]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP80:%.*]] = zext <16 x i8> [[TMP79]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP81:%.*]] = sub nsw <16 x i16> [[TMP78]], [[TMP80]]
; CHECK-LTO-NEXT:    [[TMP82:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP81]], i1 true)
; CHECK-LTO-NEXT:    [[TMP83:%.*]] = zext nneg <16 x i16> [[TMP82]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP124:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP83]])
; CHECK-LTO-NEXT:    [[OP_RDX_11:%.*]] = add i32 [[TMP124]], [[OP_RDX_10]]
; CHECK-LTO-NEXT:    [[ADD_PTR_11:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_10]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_11:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_10]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP85:%.*]] = load <16 x i8>, ptr [[ADD_PTR_11]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP86:%.*]] = zext <16 x i8> [[TMP85]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP87:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_11]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP88:%.*]] = zext <16 x i8> [[TMP87]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP89:%.*]] = sub nsw <16 x i16> [[TMP86]], [[TMP88]]
; CHECK-LTO-NEXT:    [[TMP90:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP89]], i1 true)
; CHECK-LTO-NEXT:    [[TMP91:%.*]] = zext nneg <16 x i16> [[TMP90]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP125:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP91]])
; CHECK-LTO-NEXT:    [[OP_RDX_12:%.*]] = add i32 [[TMP125]], [[OP_RDX_11]]
; CHECK-LTO-NEXT:    [[ADD_PTR_12:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_11]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_12:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_11]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP93:%.*]] = load <16 x i8>, ptr [[ADD_PTR_12]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP94:%.*]] = zext <16 x i8> [[TMP93]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP95:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_12]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP96:%.*]] = zext <16 x i8> [[TMP95]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP97:%.*]] = sub nsw <16 x i16> [[TMP94]], [[TMP96]]
; CHECK-LTO-NEXT:    [[TMP98:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP97]], i1 true)
; CHECK-LTO-NEXT:    [[TMP99:%.*]] = zext nneg <16 x i16> [[TMP98]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP126:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP99]])
; CHECK-LTO-NEXT:    [[OP_RDX_13:%.*]] = add i32 [[TMP126]], [[OP_RDX_12]]
; CHECK-LTO-NEXT:    [[ADD_PTR_13:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_12]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_13:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_12]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP101:%.*]] = load <16 x i8>, ptr [[ADD_PTR_13]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP102:%.*]] = zext <16 x i8> [[TMP101]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP103:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_13]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP104:%.*]] = zext <16 x i8> [[TMP103]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP105:%.*]] = sub nsw <16 x i16> [[TMP102]], [[TMP104]]
; CHECK-LTO-NEXT:    [[TMP106:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP105]], i1 true)
; CHECK-LTO-NEXT:    [[TMP107:%.*]] = zext nneg <16 x i16> [[TMP106]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP119:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP107]])
; CHECK-LTO-NEXT:    [[OP_RDX_14:%.*]] = add i32 [[TMP119]], [[OP_RDX_13]]
; CHECK-LTO-NEXT:    [[ADD_PTR_14:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR_13]], i64 [[IDX_EXT]]
; CHECK-LTO-NEXT:    [[ADD_PTR9_14:%.*]] = getelementptr inbounds i8, ptr [[ADD_PTR9_13]], i64 [[IDX_EXT8]]
; CHECK-LTO-NEXT:    [[TMP109:%.*]] = load <16 x i8>, ptr [[ADD_PTR_14]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP110:%.*]] = zext <16 x i8> [[TMP109]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP111:%.*]] = load <16 x i8>, ptr [[ADD_PTR9_14]], align 1, !tbaa [[TBAA0]]
; CHECK-LTO-NEXT:    [[TMP112:%.*]] = zext <16 x i8> [[TMP111]] to <16 x i16>
; CHECK-LTO-NEXT:    [[TMP113:%.*]] = sub nsw <16 x i16> [[TMP110]], [[TMP112]]
; CHECK-LTO-NEXT:    [[TMP114:%.*]] = tail call <16 x i16> @llvm.abs.v16i16(<16 x i16> [[TMP113]], i1 true)
; CHECK-LTO-NEXT:    [[TMP115:%.*]] = zext nneg <16 x i16> [[TMP114]] to <16 x i32>
; CHECK-LTO-NEXT:    [[TMP127:%.*]] = tail call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP115]])
; CHECK-LTO-NEXT:    [[OP_RDX_16:%.*]] = add i32 [[TMP127]], [[OP_RDX_14]]
; CHECK-LTO-NEXT:    br label %[[FOR_COND_CLEANUP]]
; CHECK-LTO:       [[FOR_COND_CLEANUP]]:
; CHECK-LTO-NEXT:    [[OP_RDX_15:%.*]] = phi i32 [ [[TMP227]], %[[MIDDLE_BLOCK]] ], [ [[OP_RDX_16]], %[[FOR_COND1_PREHEADER_PREHEADER]] ]
; CHECK-LTO-NEXT:    ret i32 [[OP_RDX_15]]
;
entry:
  %p1.addr = alloca ptr, align 8
  %s_p1.addr = alloca i32, align 4
  %p2.addr = alloca ptr, align 8
  %s_p2.addr = alloca i32, align 4
  %i_sum = alloca i32, align 4
  %y = alloca i32, align 4
  %cleanup.dest.slot = alloca i32, align 4
  %x = alloca i32, align 4
  store ptr %p1, ptr %p1.addr, align 8, !tbaa !4
  store i32 %s_p1, ptr %s_p1.addr, align 4, !tbaa !9
  store ptr %p2, ptr %p2.addr, align 8, !tbaa !4
  store i32 %s_p2, ptr %s_p2.addr, align 4, !tbaa !9
  call void @llvm.lifetime.start.p0(ptr %i_sum) #3
  store i32 0, ptr %i_sum, align 4, !tbaa !9
  call void @llvm.lifetime.start.p0(ptr %y) #3
  store i32 0, ptr %y, align 4, !tbaa !9
  br label %for.cond

for.cond:                                         ; preds = %for.inc10, %entry
  %0 = load i32, ptr %y, align 4, !tbaa !9
  %cmp = icmp slt i32 %0, 16
  br i1 %cmp, label %for.body, label %for.cond.cleanup

for.cond.cleanup:                                 ; preds = %for.cond
  store i32 2, ptr %cleanup.dest.slot, align 4
  call void @llvm.lifetime.end.p0(ptr %y) #3
  br label %for.end12

for.body:                                         ; preds = %for.cond
  call void @llvm.lifetime.start.p0(ptr %x) #3
  store i32 0, ptr %x, align 4, !tbaa !9
  br label %for.cond1

for.cond1:                                        ; preds = %for.inc, %for.body
  %1 = load i32, ptr %x, align 4, !tbaa !9
  %cmp2 = icmp slt i32 %1, 16
  br i1 %cmp2, label %for.body4, label %for.cond.cleanup3

for.cond.cleanup3:                                ; preds = %for.cond1
  store i32 5, ptr %cleanup.dest.slot, align 4
  call void @llvm.lifetime.end.p0(ptr %x) #3
  br label %for.end

for.body4:                                        ; preds = %for.cond1
  %2 = load ptr, ptr %p1.addr, align 8, !tbaa !4
  %3 = load i32, ptr %x, align 4, !tbaa !9
  %idxprom = sext i32 %3 to i64
  %arrayidx = getelementptr inbounds i8, ptr %2, i64 %idxprom
  %4 = load i8, ptr %arrayidx, align 1, !tbaa !11
  %conv = zext i8 %4 to i32
  %5 = load ptr, ptr %p2.addr, align 8, !tbaa !4
  %6 = load i32, ptr %x, align 4, !tbaa !9
  %idxprom5 = sext i32 %6 to i64
  %arrayidx6 = getelementptr inbounds i8, ptr %5, i64 %idxprom5
  %7 = load i8, ptr %arrayidx6, align 1, !tbaa !11
  %conv7 = zext i8 %7 to i32
  %sub = sub nsw i32 %conv, %conv7
  %8 = call i32 @llvm.abs.i32(i32 %sub, i1 true)
  %9 = load i32, ptr %i_sum, align 4, !tbaa !9
  %add = add nsw i32 %9, %8
  store i32 %add, ptr %i_sum, align 4, !tbaa !9
  br label %for.inc

for.inc:                                          ; preds = %for.body4
  %10 = load i32, ptr %x, align 4, !tbaa !9
  %inc = add nsw i32 %10, 1
  store i32 %inc, ptr %x, align 4, !tbaa !9
  br label %for.cond1, !llvm.loop !12

for.end:                                          ; preds = %for.cond.cleanup3
  %11 = load i32, ptr %s_p1.addr, align 4, !tbaa !9
  %12 = load ptr, ptr %p1.addr, align 8, !tbaa !4
  %idx.ext = sext i32 %11 to i64
  %add.ptr = getelementptr inbounds i8, ptr %12, i64 %idx.ext
  store ptr %add.ptr, ptr %p1.addr, align 8, !tbaa !4
  %13 = load i32, ptr %s_p2.addr, align 4, !tbaa !9
  %14 = load ptr, ptr %p2.addr, align 8, !tbaa !4
  %idx.ext8 = sext i32 %13 to i64
  %add.ptr9 = getelementptr inbounds i8, ptr %14, i64 %idx.ext8
  store ptr %add.ptr9, ptr %p2.addr, align 8, !tbaa !4
  br label %for.inc10

for.inc10:                                        ; preds = %for.end
  %15 = load i32, ptr %y, align 4, !tbaa !9
  %inc11 = add nsw i32 %15, 1
  store i32 %inc11, ptr %y, align 4, !tbaa !9
  br label %for.cond, !llvm.loop !14

for.end12:                                        ; preds = %for.cond.cleanup
  %16 = load i32, ptr %i_sum, align 4, !tbaa !9
  store i32 1, ptr %cleanup.dest.slot, align 4
  call void @llvm.lifetime.end.p0(ptr %i_sum) #3
  ret i32 %16
}

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.start.p0(ptr) #1

; Function Attrs: nocallback nofree nosync nounwind speculatable willreturn memory(none)
declare i32 @llvm.abs.i32(i32, i1 immarg) #2

; Function Attrs: nocallback nofree nosync nounwind willreturn memory(argmem: readwrite)
declare void @llvm.lifetime.end.p0(ptr) #1

attributes #0 = { nounwind uwtable vscale_range(1,16) "frame-pointer"="non-leaf" "no-trapping-math"="true" "stack-protector-buffer-size"="8" "target-cpu"="generic" "target-features"="+bf16,+bti,+ccidx,+complxnum,+crc,+dit,+dotprod,+ete,+flagm,+fp-armv8,+fp16fml,+fullfp16,+i8mm,+jsconv,+lse,+mte,+neon,+pauth,+perfmon,+predres,+rand,+ras,+rcpc,+rdm,+sb,+spe,+ssbs,+sve,+sve-bitperm,+sve2,+trbe,+v8.1a,+v8.2a,+v8.3a,+v8.4a,+v8.5a,+v8a,+v9a,-fmv" }
attributes #1 = { nocallback nofree nosync nounwind willreturn memory(argmem: readwrite) }
attributes #2 = { nocallback nofree nosync nounwind speculatable willreturn memory(none) }
attributes #3 = { nounwind }


!4 = !{!5, !5, i64 0}
!5 = !{!"p1 omnipotent char", !6, i64 0}
!6 = !{!"any pointer", !7, i64 0}
!7 = !{!"omnipotent char", !8, i64 0}
!8 = !{!"Simple C/C++ TBAA"}
!9 = !{!10, !10, i64 0}
!10 = !{!"int", !7, i64 0}
!11 = !{!7, !7, i64 0}
!12 = distinct !{!12, !13}
!13 = !{!"llvm.loop.mustprogress"}
!14 = distinct !{!14, !13}
;.
; CHECK-O3: [[TBAA0]] = !{[[META1:![0-9]+]], [[META1]], i64 0}
; CHECK-O3: [[META1]] = !{!"omnipotent char", [[META2:![0-9]+]], i64 0}
; CHECK-O3: [[META2]] = !{!"Simple C/C++ TBAA"}
; CHECK-O3: [[LOOP3]] = distinct !{[[LOOP3]], [[META4:![0-9]+]], [[META5:![0-9]+]], [[META6:![0-9]+]]}
; CHECK-O3: [[META4]] = !{!"llvm.loop.mustprogress"}
; CHECK-O3: [[META5]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK-O3: [[META6]] = !{!"llvm.loop.unroll.runtime.disable"}
; CHECK-O3: [[LOOP7]] = distinct !{[[LOOP7]], [[META4]], [[META5]]}
;.
; CHECK-LTO: [[TBAA0]] = !{[[META1:![0-9]+]], [[META1]], i64 0}
; CHECK-LTO: [[META1]] = !{!"omnipotent char", [[META2:![0-9]+]], i64 0}
; CHECK-LTO: [[META2]] = !{!"Simple C/C++ TBAA"}
; CHECK-LTO: [[LOOP3]] = distinct !{[[LOOP3]], [[META4:![0-9]+]], [[META5:![0-9]+]], [[META6:![0-9]+]]}
; CHECK-LTO: [[META4]] = !{!"llvm.loop.mustprogress"}
; CHECK-LTO: [[META5]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK-LTO: [[META6]] = !{!"llvm.loop.unroll.runtime.disable"}
;.
