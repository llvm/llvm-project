; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -mtriple=amdgcn--amdpal -passes=load-store-vectorizer -mattr=+require-naturally-aligned-buffer-access -S -o - %s | FileCheck --check-prefix=ALIGNED %s
; RUN: opt -mtriple=amdgcn--amdpal -passes=load-store-vectorizer -S -o - %s | FileCheck --check-prefixes=UNALIGNED %s

; The test checks that require-naturally-aligned-buffer-access target feature prevents merging loads if the target load would not be naturally aligned.

define amdgpu_kernel void @merge_align_4(ptr addrspace(7) captures(none) %p) #0 {
;
; ALIGNED-LABEL: define amdgpu_kernel void @merge_align_4(
; ALIGNED-SAME: ptr addrspace(7) captures(none) [[P:%.*]]) #[[ATTR0:[0-9]+]] {
; ALIGNED-NEXT:  [[ENTRY:.*:]]
; ALIGNED-NEXT:    [[GEP_M8:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 -8
; ALIGNED-NEXT:    [[LD_M8:%.*]] = load i32, ptr addrspace(7) [[GEP_M8]], align 4
; ALIGNED-NEXT:    [[GEP_M4:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 -4
; ALIGNED-NEXT:    [[LD_M4:%.*]] = load i32, ptr addrspace(7) [[GEP_M4]], align 4
; ALIGNED-NEXT:    [[GEP_0:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 0
; ALIGNED-NEXT:    [[LD_0:%.*]] = load i32, ptr addrspace(7) [[GEP_0]], align 4
; ALIGNED-NEXT:    [[GEP_4:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i64 4
; ALIGNED-NEXT:    [[LD_4:%.*]] = load i32, ptr addrspace(7) [[GEP_4]], align 4
; ALIGNED-NEXT:    ret void
;
; UNALIGNED-LABEL: define amdgpu_kernel void @merge_align_4(
; UNALIGNED-SAME: ptr addrspace(7) captures(none) [[P:%.*]]) {
; UNALIGNED-NEXT:  [[ENTRY:.*:]]
; UNALIGNED-NEXT:    [[GEP_M8:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 -8
; UNALIGNED-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr addrspace(7) [[GEP_M8]], align 4
; UNALIGNED-NEXT:    [[LD_M81:%.*]] = extractelement <4 x i32> [[TMP0]], i32 0
; UNALIGNED-NEXT:    [[LD_M42:%.*]] = extractelement <4 x i32> [[TMP0]], i32 1
; UNALIGNED-NEXT:    [[LD_03:%.*]] = extractelement <4 x i32> [[TMP0]], i32 2
; UNALIGNED-NEXT:    [[LD_44:%.*]] = extractelement <4 x i32> [[TMP0]], i32 3
; UNALIGNED-NEXT:    ret void
;
entry:
  %gep_m8 = getelementptr i8, ptr addrspace(7) %p, i32 -8
  %ld_m8 = load i32, ptr addrspace(7) %gep_m8, align 4
  %gep_m4 = getelementptr i8, ptr addrspace(7) %p, i32 -4
  %ld_m4 = load i32, ptr addrspace(7) %gep_m4, align 4
  %gep_0 = getelementptr i8, ptr addrspace(7) %p, i32 0
  %ld_0 = load i32, ptr addrspace(7) %gep_0, align 4
  %gep_4 = getelementptr i8, ptr addrspace(7) %p, i64 4
  %ld_4 = load i32, ptr addrspace(7) %gep_4, align 4
  ret void
}

; The test checks that require-naturally-aligned-buffer-access target feature does not prevent merging loads if the target load would be naturally aligned.

define amdgpu_kernel void @merge_align_16(ptr addrspace(7) captures(none) %p) #0 {
; ALIGNED-LABEL: define amdgpu_kernel void @merge_align_16(
; ALIGNED-SAME: ptr addrspace(7) captures(none) [[P:%.*]]) #[[ATTR0]] {
; ALIGNED-NEXT:  [[ENTRY:.*:]]
; ALIGNED-NEXT:    [[GEP_M8:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 -8
; ALIGNED-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr addrspace(7) [[GEP_M8]], align 16
; ALIGNED-NEXT:    [[LD_M81:%.*]] = extractelement <4 x i32> [[TMP0]], i32 0
; ALIGNED-NEXT:    [[LD_M42:%.*]] = extractelement <4 x i32> [[TMP0]], i32 1
; ALIGNED-NEXT:    [[LD_03:%.*]] = extractelement <4 x i32> [[TMP0]], i32 2
; ALIGNED-NEXT:    [[LD_44:%.*]] = extractelement <4 x i32> [[TMP0]], i32 3
; ALIGNED-NEXT:    ret void
;
; UNALIGNED-LABEL: define amdgpu_kernel void @merge_align_16(
; UNALIGNED-SAME: ptr addrspace(7) captures(none) [[P:%.*]]) {
; UNALIGNED-NEXT:  [[ENTRY:.*:]]
; UNALIGNED-NEXT:    [[GEP_M8:%.*]] = getelementptr i8, ptr addrspace(7) [[P]], i32 -8
; UNALIGNED-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr addrspace(7) [[GEP_M8]], align 16
; UNALIGNED-NEXT:    [[LD_M81:%.*]] = extractelement <4 x i32> [[TMP0]], i32 0
; UNALIGNED-NEXT:    [[LD_M42:%.*]] = extractelement <4 x i32> [[TMP0]], i32 1
; UNALIGNED-NEXT:    [[LD_03:%.*]] = extractelement <4 x i32> [[TMP0]], i32 2
; UNALIGNED-NEXT:    [[LD_44:%.*]] = extractelement <4 x i32> [[TMP0]], i32 3
; UNALIGNED-NEXT:    ret void
;
entry:
  %gep_m8 = getelementptr i8, ptr addrspace(7) %p, i32 -8
  %ld_m8 = load i32, ptr addrspace(7) %gep_m8, align 16
  %gep_m4 = getelementptr i8, ptr addrspace(7) %p, i32 -4
  %ld_m4 = load i32, ptr addrspace(7) %gep_m4, align 4
  %gep_0 = getelementptr i8, ptr addrspace(7) %p, i32 0
  %ld_0 = load i32, ptr addrspace(7) %gep_0, align 8
  %gep_4 = getelementptr i8, ptr addrspace(7) %p, i64 4
  %ld_4 = load i32, ptr addrspace(7) %gep_4, align 4
  ret void
}
