; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; RUN: opt -mtriple=amdgcn-amd-amdhsa -passes=load-store-vectorizer -S -o - %s | FileCheck %s

; We expect the merged vector load to retain nontemporal and tbaa, and normalization to handle
; other load-only metadata.
define void @lsv_copy_load_metadata(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_load_metadata(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[P]], align 4, !tbaa [[CHAR_TBAA0:![0-9]+]], !invariant.load [[META3:![0-9]+]], !nontemporal [[META4:![0-9]+]]
; CHECK-NEXT:    [[LD01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-NEXT:    [[LD1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-NEXT:    [[LD1_MUT_BC:%.*]] = bitcast i32 [[LD1_MUT2]] to <2 x i16>
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  %ld0 = load i32, ptr %p, align 4, !tbaa !0, !nontemporal !5, !invariant.load !6
  %ld1 = load <2 x i16>, ptr %p1, align 4, !tbaa !0, !nontemporal !5, !invariant.load !6
  ret void
}

; Check that metadata on stores is preserved when LSV normalizes mixed-typed
; chains (exercises copyMetadataForAccess on stores).
define void @lsv_copy_store_metadata(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_metadata(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 7, i32 bitcast (<2 x i16> <i16 4, i16 5> to i32)>, ptr [[P]], align 4, !nontemporal [[META4]]
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 7, ptr %p, align 4, !nontemporal !5
  store <2 x i16> <i16 4, i16 5>, ptr %p1, align 4, !nontemporal !5
  ret void
}

; Copy alias.scope and noalias metadata on vectorized stores.
define void @lsv_copy_store_alias_metadata(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_alias_metadata(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 1, i32 bitcast (<2 x i16> <i16 2, i16 3> to i32)>, ptr [[P]], align 4, !alias.scope [[META5:![0-9]+]], !noalias [[META5]]
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 1, ptr %p, align 4, !alias.scope !11, !noalias !11
  store <2 x i16> <i16 2, i16 3>, ptr %p1, align 4, !alias.scope !11, !noalias !11
  ret void
}

; Copy access group metadata on vectorized stores.
define void @lsv_copy_store_access_group(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_access_group(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 9, i32 bitcast (<2 x i16> <i16 8, i16 7> to i32)>, ptr [[P]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 9, ptr %p, align 4, !llvm.access.group !14
  store <2 x i16> <i16 8, i16 7>, ptr %p1, align 4, !llvm.access.group !14
  ret void
}

; Copy noundef metadata on vectorized stores.
define void @lsv_copy_store_noundef(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_noundef(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 42, i32 bitcast (<2 x i16> <i16 6, i16 5> to i32)>, ptr [[P]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 42, ptr %p, align 4, !noundef !15
  store <2 x i16> <i16 6, i16 5>, ptr %p1, align 4, !noundef !15
  ret void
}

; Copy noalias.addrspace metadata on vectorized stores.
define void @lsv_copy_store_noalias_addrspace(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_noalias_addrspace(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 11, i32 bitcast (<2 x i16> <i16 10, i16 9> to i32)>, ptr [[P]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 11, ptr %p, align 4, !noalias.addrspace !16
  store <2 x i16> <i16 10, i16 9>, ptr %p1, align 4, !noalias.addrspace !16
  ret void
}

; Copy llvm.mem.parallel_loop_access metadata on vectorized stores.
define void @lsv_copy_store_mem_parallel_loop_access(ptr %p) {
; CHECK-LABEL: define void @lsv_copy_store_mem_parallel_loop_access(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    store <2 x i32> <i32 13, i32 bitcast (<2 x i16> <i16 12, i16 11> to i32)>, ptr [[P]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 13, ptr %p, align 4, !llvm.mem.parallel_loop_access !17
  store <2 x i16> <i16 12, i16 11>, ptr %p1, align 4, !llvm.mem.parallel_loop_access !17
  ret void
}

; Normalized type is not a pointer in the following test, avoid copying
; dereferenceable_or_null metadata.
define void @lsv_no_copy_deref_or_null(ptr %p) {
; CHECK-LABEL: define void @lsv_no_copy_deref_or_null(
; CHECK-SAME: ptr [[P:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i64>, ptr [[P]], align 8
; CHECK-NEXT:    [[LD0_MUT1:%.*]] = extractelement <2 x i64> [[TMP0]], i32 0
; CHECK-NEXT:    [[LD12:%.*]] = extractelement <2 x i64> [[TMP0]], i32 1
; CHECK-NEXT:    [[LD0_MUT_BC:%.*]] = inttoptr i64 [[LD0_MUT1]] to ptr
; CHECK-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  %ld0 = load ptr, ptr %p, align 4, !dereferenceable_or_null !7
  %ld1 = load i64, ptr %p1, align 4
  ret void
}

!0 = !{!3, !3, i64 0}
!3 = !{!"omnipotent char", !4, i64 0}
!4 = !{!"Simple C/C++ TBAA"}
!5 = !{i32 1}
!6 = !{}
!7 = !{i64 8}
!8 = !{i64 1, i64 256}
!11 = !{!12}
!12 = distinct !{!12, !13}
!13 = distinct !{!13}
!14 = distinct !{}
!15 = !{}
!16 = !{i32 5, i32 6}
!17 = !{!18}
!18 = distinct !{}
attributes #0 = { nounwind }
attributes #1 = { nounwind readnone }
;.
; CHECK: [[CHAR_TBAA0]] = !{[[META1:![0-9]+]], [[META1]], i64 0}
; CHECK: [[META1]] = !{!"omnipotent char", [[META2:![0-9]+]], i64 0}
; CHECK: [[META2]] = !{!"Simple C/C++ TBAA"}
; CHECK: [[META3]] = !{}
; CHECK: [[META4]] = !{i32 1}
; CHECK: [[META5]] = !{[[META6:![0-9]+]]}
; CHECK: [[META6]] = distinct !{[[META6]], [[META7:![0-9]+]]}
; CHECK: [[META7]] = distinct !{[[META7]]}
;.
