; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -mtriple=amdgcn-amd-amdhsa -passes=load-store-vectorizer -mattr=+relaxed-buffer-oob-mode -S -o - %s | FileCheck --check-prefixes=CHECK,CHECK-OOB-RELAXED %s
; RUN: opt -mtriple=amdgcn-amd-amdhsa -passes=load-store-vectorizer -S -o - %s | FileCheck --check-prefixes=CHECK,CHECK-OOB-STRICT %s

define amdgpu_kernel void @merge_v2i32_v2i32(ptr addrspace(1) nocapture %a, ptr addrspace(1) nocapture readonly %b) #0 {
; CHECK-LABEL: define amdgpu_kernel void @merge_v2i32_v2i32(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]], ptr addrspace(1) readonly captures(none) [[B:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr addrspace(1) [[B]], align 4
; CHECK-NEXT:    [[LD_C1:%.*]] = shufflevector <4 x i32> [[TMP0]], <4 x i32> poison, <2 x i32> <i32 0, i32 1>
; CHECK-NEXT:    [[LD_C_IDX_12:%.*]] = shufflevector <4 x i32> [[TMP0]], <4 x i32> poison, <2 x i32> <i32 2, i32 3>
; CHECK-NEXT:    store <4 x i32> zeroinitializer, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds <2 x i32>, ptr addrspace(1) %a, i64 1
  %b.1 = getelementptr inbounds <2 x i32>, ptr addrspace(1) %b, i64 1

  %ld.c = load <2 x i32>, ptr addrspace(1) %b, align 4
  %ld.c.idx.1 = load <2 x i32>, ptr addrspace(1) %b.1, align 4

  store <2 x i32> zeroinitializer, ptr addrspace(1) %a, align 4
  store <2 x i32> zeroinitializer, ptr addrspace(1) %a.1, align 4

  ret void
}

define amdgpu_kernel void @merge_v1i32_v1i32(ptr addrspace(1) nocapture %a, ptr addrspace(1) nocapture readonly %b) #0 {
; CHECK-LABEL: define amdgpu_kernel void @merge_v1i32_v1i32(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]], ptr addrspace(1) readonly captures(none) [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr addrspace(1) [[B]], align 4
; CHECK-NEXT:    [[LD_C1:%.*]] = shufflevector <2 x i32> [[TMP0]], <2 x i32> poison, <1 x i32> zeroinitializer
; CHECK-NEXT:    [[LD_C_IDX_12:%.*]] = shufflevector <2 x i32> [[TMP0]], <2 x i32> poison, <1 x i32> <i32 1>
; CHECK-NEXT:    store <2 x i32> zeroinitializer, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds <1 x i32>, ptr addrspace(1) %a, i64 1
  %b.1 = getelementptr inbounds <1 x i32>, ptr addrspace(1) %b, i64 1

  %ld.c = load <1 x i32>, ptr addrspace(1) %b, align 4
  %ld.c.idx.1 = load <1 x i32>, ptr addrspace(1) %b.1, align 4

  store <1 x i32> zeroinitializer, ptr addrspace(1) %a, align 4
  store <1 x i32> zeroinitializer, ptr addrspace(1) %a.1, align 4

  ret void
}

define amdgpu_kernel void @no_merge_v3i32_v3i32(ptr addrspace(1) nocapture %a, ptr addrspace(1) nocapture readonly %b) #0 {
; CHECK-LABEL: define amdgpu_kernel void @no_merge_v3i32_v3i32(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]], ptr addrspace(1) readonly captures(none) [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[A_1:%.*]] = getelementptr inbounds <3 x i32>, ptr addrspace(1) [[A]], i64 1
; CHECK-NEXT:    [[B_1:%.*]] = getelementptr inbounds <3 x i32>, ptr addrspace(1) [[B]], i64 1
; CHECK-NEXT:    [[LD_C:%.*]] = load <3 x i32>, ptr addrspace(1) [[B]], align 4
; CHECK-NEXT:    [[LD_C_IDX_1:%.*]] = load <3 x i32>, ptr addrspace(1) [[B_1]], align 4
; CHECK-NEXT:    store <3 x i32> zeroinitializer, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    store <3 x i32> zeroinitializer, ptr addrspace(1) [[A_1]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds <3 x i32>, ptr addrspace(1) %a, i64 1
  %b.1 = getelementptr inbounds <3 x i32>, ptr addrspace(1) %b, i64 1

  %ld.c = load <3 x i32>, ptr addrspace(1) %b, align 4
  %ld.c.idx.1 = load <3 x i32>, ptr addrspace(1) %b.1, align 4

  store <3 x i32> zeroinitializer, ptr addrspace(1) %a, align 4
  store <3 x i32> zeroinitializer, ptr addrspace(1) %a.1, align 4

  ret void
}

define amdgpu_kernel void @merge_v2i16_v2i16(ptr addrspace(1) nocapture %a, ptr addrspace(1) nocapture readonly %b) #0 {
; CHECK-LABEL: define amdgpu_kernel void @merge_v2i16_v2i16(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]], ptr addrspace(1) readonly captures(none) [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr addrspace(1) [[B]], align 4
; CHECK-NEXT:    [[LD_C1:%.*]] = shufflevector <4 x i16> [[TMP0]], <4 x i16> poison, <2 x i32> <i32 0, i32 1>
; CHECK-NEXT:    [[LD_C_IDX_12:%.*]] = shufflevector <4 x i16> [[TMP0]], <4 x i16> poison, <2 x i32> <i32 2, i32 3>
; CHECK-NEXT:    store <4 x i16> zeroinitializer, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds <2 x i16>, ptr addrspace(1) %a, i64 1
  %b.1 = getelementptr inbounds <2 x i16>, ptr addrspace(1) %b, i64 1

  %ld.c = load <2 x i16>, ptr addrspace(1) %b, align 4
  %ld.c.idx.1 = load <2 x i16>, ptr addrspace(1) %b.1, align 4

  store <2 x i16> zeroinitializer, ptr addrspace(1) %a, align 4
  store <2 x i16> zeroinitializer, ptr addrspace(1) %a.1, align 4

  ret void
}

define amdgpu_kernel void @merge_fat_ptrs(ptr addrspace(7) nocapture %a, ptr addrspace(7) nocapture readonly %b) #0 {
; CHECK-OOB-RELAXED-LABEL: define amdgpu_kernel void @merge_fat_ptrs(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(7) captures(none) [[A:%.*]], ptr addrspace(7) readonly captures(none) [[B:%.*]]) #[[ATTR0]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[TMP0:%.*]] = load <4 x i16>, ptr addrspace(7) [[B]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[LD_C1:%.*]] = shufflevector <4 x i16> [[TMP0]], <4 x i16> poison, <2 x i32> <i32 0, i32 1>
; CHECK-OOB-RELAXED-NEXT:    [[LD_C_IDX_12:%.*]] = shufflevector <4 x i16> [[TMP0]], <4 x i16> poison, <2 x i32> <i32 2, i32 3>
; CHECK-OOB-RELAXED-NEXT:    store <4 x i16> zeroinitializer, ptr addrspace(7) [[A]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define amdgpu_kernel void @merge_fat_ptrs(
; CHECK-OOB-STRICT-SAME: ptr addrspace(7) captures(none) [[A:%.*]], ptr addrspace(7) readonly captures(none) [[B:%.*]]) #[[ATTR0]] {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[A_1:%.*]] = getelementptr inbounds <2 x i16>, ptr addrspace(7) [[A]], i32 1
; CHECK-OOB-STRICT-NEXT:    [[B_1:%.*]] = getelementptr inbounds <2 x i16>, ptr addrspace(7) [[B]], i32 1
; CHECK-OOB-STRICT-NEXT:    [[LD_C:%.*]] = load <2 x i16>, ptr addrspace(7) [[B]], align 4
; CHECK-OOB-STRICT-NEXT:    [[LD_C_IDX_1:%.*]] = load <2 x i16>, ptr addrspace(7) [[B_1]], align 4
; CHECK-OOB-STRICT-NEXT:    store <2 x i16> zeroinitializer, ptr addrspace(7) [[A]], align 4
; CHECK-OOB-STRICT-NEXT:    store <2 x i16> zeroinitializer, ptr addrspace(7) [[A_1]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds <2 x i16>, ptr addrspace(7) %a, i32 1
  %b.1 = getelementptr inbounds <2 x i16>, ptr addrspace(7) %b, i32 1

  %ld.c = load <2 x i16>, ptr addrspace(7) %b, align 4
  %ld.c.idx.1 = load <2 x i16>, ptr addrspace(7) %b.1, align 4

  store <2 x i16> zeroinitializer, ptr addrspace(7) %a, align 4
  store <2 x i16> zeroinitializer, ptr addrspace(7) %a.1, align 4

  ret void
}

define amdgpu_kernel void @merge_load_i32_v2i16(ptr addrspace(1) nocapture %a) #0 {
; CHECK-LABEL: define amdgpu_kernel void @merge_load_i32_v2i16(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    [[LD_01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-NEXT:    [[LD_1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-NEXT:    [[LD_1_TOORIG:%.*]] = bitcast i32 [[LD_1_MUT2]] to <2 x i16>
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds i32, ptr addrspace(1) %a, i32 1

  %ld.0 = load i32, ptr addrspace(1) %a
  %ld.1 = load <2 x i16>, ptr addrspace(1) %a.1

  ret void
}

define amdgpu_kernel void @no_merge_load_i32_v2i8(ptr addrspace(1) nocapture %a) #0 {
; CHECK-LABEL: define amdgpu_kernel void @no_merge_load_i32_v2i8(
; CHECK-SAME: ptr addrspace(1) captures(none) [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[A_1:%.*]] = getelementptr inbounds i32, ptr addrspace(1) [[A]], i32 1
; CHECK-NEXT:    [[LD_0:%.*]] = load i32, ptr addrspace(1) [[A]], align 4
; CHECK-NEXT:    [[LD_1:%.*]] = load <2 x i8>, ptr addrspace(1) [[A_1]], align 2
; CHECK-NEXT:    ret void
;
entry:
  %a.1 = getelementptr inbounds i32, ptr addrspace(1) %a, i32 1

  %ld.0 = load i32, ptr addrspace(1) %a
  %ld.1 = load <2 x i8>, ptr addrspace(1) %a.1

  ret void
}

define void @test_normalize_loads(ptr %p) {
; CHECK-OOB-RELAXED-LABEL: define void @test_normalize_loads(
; CHECK-OOB-RELAXED-SAME: ptr [[P:%.*]]) #[[ATTR1:[0-9]+]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[P]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[L01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-OOB-RELAXED-NEXT:    [[L1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-OOB-RELAXED-NEXT:    [[L1_MUT_BC:%.*]] = bitcast i32 [[L1_MUT2]] to <2 x i16>
; CHECK-OOB-RELAXED-NEXT:    [[L0_EXT:%.*]] = zext i32 [[L01]] to i64
; CHECK-OOB-RELAXED-NEXT:    [[L1_CAST:%.*]] = bitcast <2 x i16> [[L1_MUT_BC]] to i32
; CHECK-OOB-RELAXED-NEXT:    [[L1_EXT:%.*]] = zext i32 [[L1_CAST]] to i64
; CHECK-OOB-RELAXED-NEXT:    [[ADD:%.*]] = add i64 [[L0_EXT]], [[L1_EXT]]
; CHECK-OOB-RELAXED-NEXT:    store i64 [[ADD]], ptr null, align 8
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @test_normalize_loads(
; CHECK-OOB-STRICT-SAME: ptr [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[P]], align 4
; CHECK-OOB-STRICT-NEXT:    [[L01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-OOB-STRICT-NEXT:    [[L1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-OOB-STRICT-NEXT:    [[L1_MUT_BC:%.*]] = bitcast i32 [[L1_MUT2]] to <2 x i16>
; CHECK-OOB-STRICT-NEXT:    [[L0_EXT:%.*]] = zext i32 [[L01]] to i64
; CHECK-OOB-STRICT-NEXT:    [[L1_CAST:%.*]] = bitcast <2 x i16> [[L1_MUT_BC]] to i32
; CHECK-OOB-STRICT-NEXT:    [[L1_EXT:%.*]] = zext i32 [[L1_CAST]] to i64
; CHECK-OOB-STRICT-NEXT:    [[ADD:%.*]] = add i64 [[L0_EXT]], [[L1_EXT]]
; CHECK-OOB-STRICT-NEXT:    store i64 [[ADD]], ptr null, align 8
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  %l0 = load i32, ptr %p
  %l1 = load <2 x i16>, ptr %p1
  %l0_ext = zext i32 %l0 to i64
  %l1_cast = bitcast <2 x i16> %l1 to i32
  %l1_ext = zext i32 %l1_cast to i64
  %add = add i64 %l0_ext, %l1_ext
  store i64 %add, ptr null
  ret void
}

define void @test_normalize_stores(ptr %p) {
; CHECK-OOB-RELAXED-LABEL: define void @test_normalize_stores(
; CHECK-OOB-RELAXED-SAME: ptr [[P:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    store <2 x i32> <i32 123, i32 bitcast (<2 x i16> <i16 4, i16 5> to i32)>, ptr [[P]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @test_normalize_stores(
; CHECK-OOB-STRICT-SAME: ptr [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    store <2 x i32> <i32 123, i32 bitcast (<2 x i16> <i16 4, i16 5> to i32)>, ptr [[P]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 123, ptr %p
  store <2 x i16> <i16 4, i16 5>, ptr %p1
  ret void
}

; TODO: Fix the below test
; Check that metadata on loads is preserved when LSV normalizes mixed-typed
; chains (exercises copyMetadataForAccess on loads).
define void @lsv_copy_load_metadata(ptr %p) {
; CHECK-OOB-RELAXED-LABEL: define void @lsv_copy_load_metadata(
; CHECK-OOB-RELAXED-SAME: ptr [[P:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[P]], align 4, !tbaa [[TBAA0:![0-9]+]], !invariant.load [[META3:![0-9]+]], !nontemporal [[META4:![0-9]+]]
; CHECK-OOB-RELAXED-NEXT:    [[L01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-OOB-RELAXED-NEXT:    [[L1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-OOB-RELAXED-NEXT:    [[L1_MUT_BC:%.*]] = bitcast i32 [[L1_MUT2]] to <2 x i16>
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @lsv_copy_load_metadata(
; CHECK-OOB-STRICT-SAME: ptr [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[P]], align 4, !tbaa [[TBAA0:![0-9]+]], !invariant.load [[META3:![0-9]+]], !nontemporal [[META4:![0-9]+]]
; CHECK-OOB-STRICT-NEXT:    [[L01:%.*]] = extractelement <2 x i32> [[TMP0]], i32 0
; CHECK-OOB-STRICT-NEXT:    [[L1_MUT2:%.*]] = extractelement <2 x i32> [[TMP0]], i32 1
; CHECK-OOB-STRICT-NEXT:    [[L1_MUT_BC:%.*]] = bitcast i32 [[L1_MUT2]] to <2 x i16>
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  %ld0 = load i32, ptr %p, align 4, !tbaa !0, !nontemporal !5, !invariant.load !6
  %ld1 = load <2 x i16>, ptr %p1, align 4, !tbaa !0, !nontemporal !5, !invariant.load !6
  ret void
}

; Check that metadata on stores is preserved when LSV normalizes mixed-typed
; chains (exercises copyMetadataForAccess on stores).
define void @lsv_copy_store_metadata(ptr %p) {
; CHECK-OOB-RELAXED-LABEL: define void @lsv_copy_store_metadata(
; CHECK-OOB-RELAXED-SAME: ptr [[P:%.*]]) #[[ATTR0]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    store <2 x i32> <i32 7, i32 bitcast (<2 x i16> <i16 4, i16 5> to i32)>, ptr [[P]], align 4, !nontemporal [[META4]]
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @lsv_copy_store_metadata(
; CHECK-OOB-STRICT-SAME: ptr [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    store <2 x i32> <i32 7, i32 bitcast (<2 x i16> <i16 4, i16 5> to i32)>, ptr [[P]], align 4, !nontemporal [[META4]]
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %p1 = getelementptr i32, ptr %p, i64 1
  store i32 7, ptr %p, align 4, !nontemporal !5
  store <2 x i16> <i16 4, i16 5>, ptr %p1, align 4, !nontemporal !5
  ret void
}

!0 = !{!3, !3, i64 0}
!3 = !{!"omnipotent char", !4, i64 0}
!4 = !{!"Simple C/C++ TBAA"}
!5 = !{i32 1}
!6 = !{}
attributes #0 = { nounwind }
attributes #1 = { nounwind readnone }


; Non power-of-two combined span (12 bytes) must not merge chains.
define void @no_merge_non_pot_span(ptr addrspace(1) %p) {
; CHECK-OOB-RELAXED-LABEL: define void @no_merge_non_pot_span(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[P:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[L0:%.*]] = load i32, ptr addrspace(1) [[P]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[P8:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[P]], i64 8
; CHECK-OOB-RELAXED-NEXT:    [[L1:%.*]] = load float, ptr addrspace(1) [[P8]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @no_merge_non_pot_span(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[L0:%.*]] = load i32, ptr addrspace(1) [[P]], align 4
; CHECK-OOB-STRICT-NEXT:    [[P8:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[P]], i64 8
; CHECK-OOB-STRICT-NEXT:    [[L1:%.*]] = load float, ptr addrspace(1) [[P8]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %l0 = load i32, ptr addrspace(1) %p, align 4
  %p8 = getelementptr inbounds i8, ptr addrspace(1) %p, i64 8
  %l1 = load float, ptr addrspace(1) %p8, align 4
  ret void
}

define void @no_merge_diff_ptrop(ptr addrspace(1) %ptr1, ptr addrspace(2) %ptr2) {
; CHECK-OOB-RELAXED-LABEL: define void @no_merge_diff_ptrop(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[PTR1:%.*]], ptr addrspace(2) [[PTR2:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:    [[LOAD_0:%.*]] = load i32, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[LOAD_1:%.*]] = load i32, ptr addrspace(2) [[PTR2]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @no_merge_diff_ptrop(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[PTR1:%.*]], ptr addrspace(2) [[PTR2:%.*]]) {
; CHECK-OOB-STRICT-NEXT:    [[LOAD_0:%.*]] = load i32, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-STRICT-NEXT:    [[LOAD_1:%.*]] = load i32, ptr addrspace(2) [[PTR2]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
  %load.0 = load i32, ptr addrspace(1) %ptr1
  %load.1 = load i32, ptr addrspace(2) %ptr2
  ret void
}

define void @no_merge_load_store(ptr addrspace(1) %ptr1) {
; CHECK-OOB-RELAXED-LABEL: define void @no_merge_load_store(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[PTR1:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:    [[LOAD_0:%.*]] = load i32, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-RELAXED-NEXT:    store i32 111, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @no_merge_load_store(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[PTR1:%.*]]) {
; CHECK-OOB-STRICT-NEXT:    [[LOAD_0:%.*]] = load i32, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-STRICT-NEXT:    store i32 111, ptr addrspace(1) [[PTR1]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
  %load.0 = load i32, ptr addrspace(1) %ptr1
  store i32 111, ptr addrspace(1) %ptr1
  ret void
}

; Stores in this test should not be vectorized as the total byte span
; from the end of %gep.a to the end of %gep.b is not a power of 2. This
; is a necessary condition for splitChainByAlignment.
define void @check_contiguity_of_base_ptrs(ptr addrspace(1) %ptr) {
; CHECK-OOB-RELAXED-LABEL: define void @check_contiguity_of_base_ptrs(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[PTR:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:    store i32 274, ptr addrspace(1) [[PTR]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[GEP_A:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(1) [[PTR]], i64 4
; CHECK-OOB-RELAXED-NEXT:    store i64 3610770474484254748, ptr addrspace(1) [[GEP_A]], align 8
; CHECK-OOB-RELAXED-NEXT:    [[GEP_B:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(1) [[PTR]], i64 12
; CHECK-OOB-RELAXED-NEXT:    store <2 x i32> <i32 1819043144, i32 1867980911>, ptr addrspace(1) [[GEP_B]], align 4
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @check_contiguity_of_base_ptrs(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[PTR:%.*]]) {
; CHECK-OOB-STRICT-NEXT:    store i32 274, ptr addrspace(1) [[PTR]], align 4
; CHECK-OOB-STRICT-NEXT:    [[GEP_A:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(1) [[PTR]], i64 4
; CHECK-OOB-STRICT-NEXT:    store i64 3610770474484254748, ptr addrspace(1) [[GEP_A]], align 8
; CHECK-OOB-STRICT-NEXT:    [[GEP_B:%.*]] = getelementptr inbounds nuw i8, ptr addrspace(1) [[PTR]], i64 12
; CHECK-OOB-STRICT-NEXT:    store <2 x i32> <i32 1819043144, i32 1867980911>, ptr addrspace(1) [[GEP_B]], align 4
; CHECK-OOB-STRICT-NEXT:    ret void
;
  store i32 274, ptr addrspace(1) %ptr, align 4
  %gep.a = getelementptr inbounds nuw i8, ptr addrspace(1) %ptr, i64 4
  store i64 3610770474484254748, ptr addrspace(1) %gep.a, align 8
  %gep.b = getelementptr inbounds nuw i8, ptr addrspace(1) %ptr, i64 12
  store <2 x i32> <i32 1819043144, i32 1867980911>, ptr addrspace(1) %gep.b, align 4
  ret void
}

; Offset is unknown in the following test, LSV should fail to vectorize.
define amdgpu_kernel void @assert_computeLeaderDelta(ptr addrspace(1) %a, i64 %idx) {
; CHECK-OOB-RELAXED-LABEL: define amdgpu_kernel void @assert_computeLeaderDelta(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[A:%.*]], i64 [[IDX:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[LD0:%.*]] = load i32, ptr addrspace(1) [[A]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[P1:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[A]], i64 [[IDX]]
; CHECK-OOB-RELAXED-NEXT:    [[LD1:%.*]] = load <2 x i16>, ptr addrspace(1) [[P1]], align 2
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define amdgpu_kernel void @assert_computeLeaderDelta(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[A:%.*]], i64 [[IDX:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[LD0:%.*]] = load i32, ptr addrspace(1) [[A]], align 4
; CHECK-OOB-STRICT-NEXT:    [[P1:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[A]], i64 [[IDX]]
; CHECK-OOB-STRICT-NEXT:    [[LD1:%.*]] = load <2 x i16>, ptr addrspace(1) [[P1]], align 2
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %ld0 = load i32, ptr addrspace(1) %a, align 4
  %p1 = getelementptr inbounds i8, ptr addrspace(1) %a, i64 %idx
  %ld1 = load <2 x i16>, ptr addrspace(1) %p1, align 2
  ret void
}


; Overlapping ranges after rebasing should prevent merging across chains.
define void @no_merge_overlap_after_rebase(ptr addrspace(1) %p) {
; CHECK-OOB-RELAXED-LABEL: define void @no_merge_overlap_after_rebase(
; CHECK-OOB-RELAXED-SAME: ptr addrspace(1) [[P:%.*]]) #[[ATTR1]] {
; CHECK-OOB-RELAXED-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-RELAXED-NEXT:    [[L0:%.*]] = load i32, ptr addrspace(1) [[P]], align 4
; CHECK-OOB-RELAXED-NEXT:    [[P1:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[P]], i64 2
; CHECK-OOB-RELAXED-NEXT:    [[L1:%.*]] = load <2 x i16>, ptr addrspace(1) [[P1]], align 2
; CHECK-OOB-RELAXED-NEXT:    ret void
;
; CHECK-OOB-STRICT-LABEL: define void @no_merge_overlap_after_rebase(
; CHECK-OOB-STRICT-SAME: ptr addrspace(1) [[P:%.*]]) {
; CHECK-OOB-STRICT-NEXT:  [[ENTRY:.*:]]
; CHECK-OOB-STRICT-NEXT:    [[L0:%.*]] = load i32, ptr addrspace(1) [[P]], align 4
; CHECK-OOB-STRICT-NEXT:    [[P1:%.*]] = getelementptr inbounds i8, ptr addrspace(1) [[P]], i64 2
; CHECK-OOB-STRICT-NEXT:    [[L1:%.*]] = load <2 x i16>, ptr addrspace(1) [[P1]], align 2
; CHECK-OOB-STRICT-NEXT:    ret void
;
entry:
  %l0 = load i32, ptr addrspace(1) %p, align 4
  %p1 = getelementptr inbounds i8, ptr addrspace(1) %p, i64 2
  %l1 = load <2 x i16>, ptr addrspace(1) %p1, align 2
  ret void
}
