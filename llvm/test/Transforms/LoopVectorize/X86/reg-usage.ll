; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -debug-only=loop-vectorize -loop-vectorize -vectorizer-maximize-bandwidth -mtriple=x86_64-unknown-linux -S 2>&1 | FileCheck %s
; RUN: opt < %s -debug-only=loop-vectorize -loop-vectorize -vectorizer-maximize-bandwidth -mtriple=x86_64-unknown-linux -mattr=+avx512f -S 2>&1 | FileCheck %s --check-prefix=AVX512F
; REQUIRES: asserts

@a = global [1024 x i8] zeroinitializer, align 16
@b = global [1024 x i8] zeroinitializer, align 16

define i32 @foo() {
; This function has a loop of SAD pattern. Here we check when VF = 16 the
; register usage doesn't exceed 16.
;
; CHECK-LABEL: @foo(
; CHECK-NEXT:  iter.check:
; CHECK-NEXT:    br i1 false, label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
; CHECK:       vector.main.loop.iter.check:
; CHECK-NEXT:    br i1 false, label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <16 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP13:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP0]]
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i8, i8* [[TMP1]], i32 0
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast i8* [[TMP2]] to <16 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <16 x i8>, <16 x i8>* [[TMP3]], align 1
; CHECK-NEXT:    [[TMP4:%.*]] = zext <16 x i8> [[WIDE_LOAD]] to <16 x i32>
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP0]]
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP5]], i32 0
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP6]] to <16 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <16 x i8>, <16 x i8>* [[TMP7]], align 1
; CHECK-NEXT:    [[TMP8:%.*]] = zext <16 x i8> [[WIDE_LOAD1]] to <16 x i32>
; CHECK-NEXT:    [[TMP9:%.*]] = sub nsw <16 x i32> [[TMP4]], [[TMP8]]
; CHECK-NEXT:    [[TMP10:%.*]] = icmp sgt <16 x i32> [[TMP9]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; CHECK-NEXT:    [[TMP11:%.*]] = sub nsw <16 x i32> zeroinitializer, [[TMP9]]
; CHECK-NEXT:    [[TMP12:%.*]] = select <16 x i1> [[TMP10]], <16 x i32> [[TMP9]], <16 x i32> [[TMP11]]
; CHECK-NEXT:    [[TMP13]] = add <16 x i32> [[TMP12]], [[VEC_PHI]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 16
; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; CHECK-NEXT:    br i1 [[TMP14]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP15:%.*]] = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP13]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
; CHECK:       vec.epilog.iter.check:
; CHECK-NEXT:    br i1 true, label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
; CHECK:       vec.epilog.ph:
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ], [ [[TMP15]], [[VEC_EPILOG_ITER_CHECK]] ]
; CHECK-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
; CHECK-NEXT:    [[TMP16:%.*]] = insertelement <8 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
; CHECK-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
; CHECK:       vec.epilog.vector.body:
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT8:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI5:%.*]] = phi <8 x i32> [ [[TMP16]], [[VEC_EPILOG_PH]] ], [ [[TMP30:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP17:%.*]] = add i64 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[TMP18:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP17]]
; CHECK-NEXT:    [[TMP19:%.*]] = getelementptr inbounds i8, i8* [[TMP18]], i32 0
; CHECK-NEXT:    [[TMP20:%.*]] = bitcast i8* [[TMP19]] to <8 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD6:%.*]] = load <8 x i8>, <8 x i8>* [[TMP20]], align 1
; CHECK-NEXT:    [[TMP21:%.*]] = zext <8 x i8> [[WIDE_LOAD6]] to <8 x i32>
; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP17]]
; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds i8, i8* [[TMP22]], i32 0
; CHECK-NEXT:    [[TMP24:%.*]] = bitcast i8* [[TMP23]] to <8 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD7:%.*]] = load <8 x i8>, <8 x i8>* [[TMP24]], align 1
; CHECK-NEXT:    [[TMP25:%.*]] = zext <8 x i8> [[WIDE_LOAD7]] to <8 x i32>
; CHECK-NEXT:    [[TMP26:%.*]] = sub nsw <8 x i32> [[TMP21]], [[TMP25]]
; CHECK-NEXT:    [[TMP27:%.*]] = icmp sgt <8 x i32> [[TMP26]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; CHECK-NEXT:    [[TMP28:%.*]] = sub nsw <8 x i32> zeroinitializer, [[TMP26]]
; CHECK-NEXT:    [[TMP29:%.*]] = select <8 x i1> [[TMP27]], <8 x i32> [[TMP26]], <8 x i32> [[TMP28]]
; CHECK-NEXT:    [[TMP30]] = add <8 x i32> [[TMP29]], [[VEC_PHI5]]
; CHECK-NEXT:    [[INDEX_NEXT8]] = add nuw i64 [[OFFSET_IDX]], 8
; CHECK-NEXT:    [[TMP31:%.*]] = icmp eq i64 [[INDEX_NEXT8]], 1024
; CHECK-NEXT:    br i1 [[TMP31]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; CHECK:       vec.epilog.middle.block:
; CHECK-NEXT:    [[TMP32:%.*]] = call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> [[TMP30]])
; CHECK-NEXT:    [[CMP_N3:%.*]] = icmp eq i64 1024, 1024
; CHECK-NEXT:    br i1 [[CMP_N3]], label [[FOR_COND_CLEANUP_LOOPEXIT:%.*]], label [[VEC_EPILOG_SCALAR_PH]]
; CHECK:       vec.epilog.scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[ITER_CHECK:%.*]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX9:%.*]] = phi i32 [ 0, [[ITER_CHECK]] ], [ [[TMP15]], [[VEC_EPILOG_ITER_CHECK]] ], [ [[TMP32]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.cond.cleanup.loopexit:
; CHECK-NEXT:    [[ADD_LCSSA2:%.*]] = phi i32 [ [[ADD:%.*]], [[FOR_BODY]] ], [ [[TMP32]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_COND_CLEANUP]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    [[ADD_LCSSA:%.*]] = phi i32 [ [[TMP15]], [[MIDDLE_BLOCK]] ], [ [[ADD_LCSSA2]], [[FOR_COND_CLEANUP_LOOPEXIT]] ]
; CHECK-NEXT:    ret i32 [[ADD_LCSSA]]
; CHECK:       for.body:
; CHECK-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[S_015:%.*]] = phi i32 [ [[BC_MERGE_RDX9]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[ADD]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[INDVARS_IV]]
; CHECK-NEXT:    [[TMP33:%.*]] = load i8, i8* [[ARRAYIDX]], align 1
; CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP33]] to i32
; CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[INDVARS_IV]]
; CHECK-NEXT:    [[TMP34:%.*]] = load i8, i8* [[ARRAYIDX2]], align 1
; CHECK-NEXT:    [[CONV3:%.*]] = zext i8 [[TMP34]] to i32
; CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[CONV]], [[CONV3]]
; CHECK-NEXT:    [[ISPOS:%.*]] = icmp sgt i32 [[SUB]], -1
; CHECK-NEXT:    [[NEG:%.*]] = sub nsw i32 0, [[SUB]]
; CHECK-NEXT:    [[TMP35:%.*]] = select i1 [[ISPOS]], i32 [[SUB]], i32 [[NEG]]
; CHECK-NEXT:    [[ADD]] = add nsw i32 [[TMP35]], [[S_015]]
; CHECK-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 1024
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP_LOOPEXIT]], label [[FOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
;
; AVX512F-LABEL: @foo(
; AVX512F-NEXT:  iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
; AVX512F:       vector.main.loop.iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
; AVX512F:       vector.ph:
; AVX512F-NEXT:    br label [[VECTOR_BODY:%.*]]
; AVX512F:       vector.body:
; AVX512F-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI:%.*]] = phi <64 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP26:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI1:%.*]] = phi <64 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP27:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; AVX512F-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 64
; AVX512F-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP0]]
; AVX512F-NEXT:    [[TMP3:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP1]]
; AVX512F-NEXT:    [[TMP4:%.*]] = getelementptr inbounds i8, i8* [[TMP2]], i32 0
; AVX512F-NEXT:    [[TMP5:%.*]] = bitcast i8* [[TMP4]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD:%.*]] = load <64 x i8>, <64 x i8>* [[TMP5]], align 1
; AVX512F-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP2]], i32 64
; AVX512F-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP6]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD2:%.*]] = load <64 x i8>, <64 x i8>* [[TMP7]], align 1
; AVX512F-NEXT:    [[TMP8:%.*]] = zext <64 x i8> [[WIDE_LOAD]] to <64 x i32>
; AVX512F-NEXT:    [[TMP9:%.*]] = zext <64 x i8> [[WIDE_LOAD2]] to <64 x i32>
; AVX512F-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP0]]
; AVX512F-NEXT:    [[TMP11:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP1]]
; AVX512F-NEXT:    [[TMP12:%.*]] = getelementptr inbounds i8, i8* [[TMP10]], i32 0
; AVX512F-NEXT:    [[TMP13:%.*]] = bitcast i8* [[TMP12]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD3:%.*]] = load <64 x i8>, <64 x i8>* [[TMP13]], align 1
; AVX512F-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i8, i8* [[TMP10]], i32 64
; AVX512F-NEXT:    [[TMP15:%.*]] = bitcast i8* [[TMP14]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD4:%.*]] = load <64 x i8>, <64 x i8>* [[TMP15]], align 1
; AVX512F-NEXT:    [[TMP16:%.*]] = zext <64 x i8> [[WIDE_LOAD3]] to <64 x i32>
; AVX512F-NEXT:    [[TMP17:%.*]] = zext <64 x i8> [[WIDE_LOAD4]] to <64 x i32>
; AVX512F-NEXT:    [[TMP18:%.*]] = sub nsw <64 x i32> [[TMP8]], [[TMP16]]
; AVX512F-NEXT:    [[TMP19:%.*]] = sub nsw <64 x i32> [[TMP9]], [[TMP17]]
; AVX512F-NEXT:    [[TMP20:%.*]] = icmp sgt <64 x i32> [[TMP18]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP21:%.*]] = icmp sgt <64 x i32> [[TMP19]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP22:%.*]] = sub nsw <64 x i32> zeroinitializer, [[TMP18]]
; AVX512F-NEXT:    [[TMP23:%.*]] = sub nsw <64 x i32> zeroinitializer, [[TMP19]]
; AVX512F-NEXT:    [[TMP24:%.*]] = select <64 x i1> [[TMP20]], <64 x i32> [[TMP18]], <64 x i32> [[TMP22]]
; AVX512F-NEXT:    [[TMP25:%.*]] = select <64 x i1> [[TMP21]], <64 x i32> [[TMP19]], <64 x i32> [[TMP23]]
; AVX512F-NEXT:    [[TMP26]] = add <64 x i32> [[TMP24]], [[VEC_PHI]]
; AVX512F-NEXT:    [[TMP27]] = add <64 x i32> [[TMP25]], [[VEC_PHI1]]
; AVX512F-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 128
; AVX512F-NEXT:    [[TMP28:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; AVX512F-NEXT:    br i1 [[TMP28]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; AVX512F:       middle.block:
; AVX512F-NEXT:    [[BIN_RDX:%.*]] = add <64 x i32> [[TMP27]], [[TMP26]]
; AVX512F-NEXT:    [[TMP29:%.*]] = call i32 @llvm.vector.reduce.add.v64i32(<64 x i32> [[BIN_RDX]])
; AVX512F-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; AVX512F-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
; AVX512F:       vec.epilog.iter.check:
; AVX512F-NEXT:    br i1 true, label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
; AVX512F:       vec.epilog.ph:
; AVX512F-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ], [ [[TMP29]], [[VEC_EPILOG_ITER_CHECK]] ]
; AVX512F-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
; AVX512F-NEXT:    [[TMP30:%.*]] = insertelement <32 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
; AVX512F-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
; AVX512F:       vec.epilog.vector.body:
; AVX512F-NEXT:    [[OFFSET_IDX:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT11:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI8:%.*]] = phi <32 x i32> [ [[TMP30]], [[VEC_EPILOG_PH]] ], [ [[TMP44:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP31:%.*]] = add i64 [[OFFSET_IDX]], 0
; AVX512F-NEXT:    [[TMP32:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP31]]
; AVX512F-NEXT:    [[TMP33:%.*]] = getelementptr inbounds i8, i8* [[TMP32]], i32 0
; AVX512F-NEXT:    [[TMP34:%.*]] = bitcast i8* [[TMP33]] to <32 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD9:%.*]] = load <32 x i8>, <32 x i8>* [[TMP34]], align 1
; AVX512F-NEXT:    [[TMP35:%.*]] = zext <32 x i8> [[WIDE_LOAD9]] to <32 x i32>
; AVX512F-NEXT:    [[TMP36:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP31]]
; AVX512F-NEXT:    [[TMP37:%.*]] = getelementptr inbounds i8, i8* [[TMP36]], i32 0
; AVX512F-NEXT:    [[TMP38:%.*]] = bitcast i8* [[TMP37]] to <32 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD10:%.*]] = load <32 x i8>, <32 x i8>* [[TMP38]], align 1
; AVX512F-NEXT:    [[TMP39:%.*]] = zext <32 x i8> [[WIDE_LOAD10]] to <32 x i32>
; AVX512F-NEXT:    [[TMP40:%.*]] = sub nsw <32 x i32> [[TMP35]], [[TMP39]]
; AVX512F-NEXT:    [[TMP41:%.*]] = icmp sgt <32 x i32> [[TMP40]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP42:%.*]] = sub nsw <32 x i32> zeroinitializer, [[TMP40]]
; AVX512F-NEXT:    [[TMP43:%.*]] = select <32 x i1> [[TMP41]], <32 x i32> [[TMP40]], <32 x i32> [[TMP42]]
; AVX512F-NEXT:    [[TMP44]] = add <32 x i32> [[TMP43]], [[VEC_PHI8]]
; AVX512F-NEXT:    [[INDEX_NEXT11]] = add nuw i64 [[OFFSET_IDX]], 32
; AVX512F-NEXT:    [[TMP45:%.*]] = icmp eq i64 [[INDEX_NEXT11]], 1024
; AVX512F-NEXT:    br i1 [[TMP45]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; AVX512F:       vec.epilog.middle.block:
; AVX512F-NEXT:    [[TMP46:%.*]] = call i32 @llvm.vector.reduce.add.v32i32(<32 x i32> [[TMP44]])
; AVX512F-NEXT:    [[CMP_N6:%.*]] = icmp eq i64 1024, 1024
; AVX512F-NEXT:    br i1 [[CMP_N6]], label [[FOR_COND_CLEANUP_LOOPEXIT:%.*]], label [[VEC_EPILOG_SCALAR_PH]]
; AVX512F:       vec.epilog.scalar.ph:
; AVX512F-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[ITER_CHECK:%.*]] ]
; AVX512F-NEXT:    [[BC_MERGE_RDX12:%.*]] = phi i32 [ 0, [[ITER_CHECK]] ], [ [[TMP29]], [[VEC_EPILOG_ITER_CHECK]] ], [ [[TMP46]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    br label [[FOR_BODY:%.*]]
; AVX512F:       for.cond.cleanup.loopexit:
; AVX512F-NEXT:    [[ADD_LCSSA5:%.*]] = phi i32 [ [[ADD:%.*]], [[FOR_BODY]] ], [ [[TMP46]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    br label [[FOR_COND_CLEANUP]]
; AVX512F:       for.cond.cleanup:
; AVX512F-NEXT:    [[ADD_LCSSA:%.*]] = phi i32 [ [[TMP29]], [[MIDDLE_BLOCK]] ], [ [[ADD_LCSSA5]], [[FOR_COND_CLEANUP_LOOPEXIT]] ]
; AVX512F-NEXT:    ret i32 [[ADD_LCSSA]]
; AVX512F:       for.body:
; AVX512F-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[S_015:%.*]] = phi i32 [ [[BC_MERGE_RDX12]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[ADD]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[INDVARS_IV]]
; AVX512F-NEXT:    [[TMP47:%.*]] = load i8, i8* [[ARRAYIDX]], align 1
; AVX512F-NEXT:    [[CONV:%.*]] = zext i8 [[TMP47]] to i32
; AVX512F-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[INDVARS_IV]]
; AVX512F-NEXT:    [[TMP48:%.*]] = load i8, i8* [[ARRAYIDX2]], align 1
; AVX512F-NEXT:    [[CONV3:%.*]] = zext i8 [[TMP48]] to i32
; AVX512F-NEXT:    [[SUB:%.*]] = sub nsw i32 [[CONV]], [[CONV3]]
; AVX512F-NEXT:    [[ISPOS:%.*]] = icmp sgt i32 [[SUB]], -1
; AVX512F-NEXT:    [[NEG:%.*]] = sub nsw i32 0, [[SUB]]
; AVX512F-NEXT:    [[TMP49:%.*]] = select i1 [[ISPOS]], i32 [[SUB]], i32 [[NEG]]
; AVX512F-NEXT:    [[ADD]] = add nsw i32 [[TMP49]], [[S_015]]
; AVX512F-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; AVX512F-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 1024
; AVX512F-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP_LOOPEXIT]], label [[FOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
;

entry:
  br label %for.body

for.cond.cleanup:
  %add.lcssa = phi i32 [ %add, %for.body ]
  ret i32 %add.lcssa

for.body:
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %s.015 = phi i32 [ 0, %entry ], [ %add, %for.body ]
  %arrayidx = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 %indvars.iv
  %0 = load i8, i8* %arrayidx, align 1
  %conv = zext i8 %0 to i32
  %arrayidx2 = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 %indvars.iv
  %1 = load i8, i8* %arrayidx2, align 1
  %conv3 = zext i8 %1 to i32
  %sub = sub nsw i32 %conv, %conv3
  %ispos = icmp sgt i32 %sub, -1
  %neg = sub nsw i32 0, %sub
  %2 = select i1 %ispos, i32 %sub, i32 %neg
  %add = add nsw i32 %2, %s.015
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

define i32 @goo() {
; For indvars.iv used in a computating chain only feeding into getelementptr or cmp,
; it will not have vector version and the vector register usage will not exceed the
; available vector register number.
; CHECK-LABEL: @goo(
; CHECK-NEXT:  iter.check:
; CHECK-NEXT:    br i1 false, label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
; CHECK:       vector.main.loop.iter.check:
; CHECK-NEXT:    br i1 false, label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <16 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP15:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw i64 [[TMP0]], 3
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP1]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds i8, i8* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast i8* [[TMP3]] to <16 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <16 x i8>, <16 x i8>* [[TMP4]], align 1
; CHECK-NEXT:    [[TMP5:%.*]] = zext <16 x i8> [[WIDE_LOAD]] to <16 x i32>
; CHECK-NEXT:    [[TMP6:%.*]] = add nsw i64 [[TMP0]], 2
; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP6]]
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i8, i8* [[TMP7]], i32 0
; CHECK-NEXT:    [[TMP9:%.*]] = bitcast i8* [[TMP8]] to <16 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <16 x i8>, <16 x i8>* [[TMP9]], align 1
; CHECK-NEXT:    [[TMP10:%.*]] = zext <16 x i8> [[WIDE_LOAD1]] to <16 x i32>
; CHECK-NEXT:    [[TMP11:%.*]] = sub nsw <16 x i32> [[TMP5]], [[TMP10]]
; CHECK-NEXT:    [[TMP12:%.*]] = icmp sgt <16 x i32> [[TMP11]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; CHECK-NEXT:    [[TMP13:%.*]] = sub nsw <16 x i32> zeroinitializer, [[TMP11]]
; CHECK-NEXT:    [[TMP14:%.*]] = select <16 x i1> [[TMP12]], <16 x i32> [[TMP11]], <16 x i32> [[TMP13]]
; CHECK-NEXT:    [[TMP15]] = add <16 x i32> [[TMP14]], [[VEC_PHI]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 16
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP17:%.*]] = call i32 @llvm.vector.reduce.add.v16i32(<16 x i32> [[TMP15]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
; CHECK:       vec.epilog.iter.check:
; CHECK-NEXT:    br i1 true, label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
; CHECK:       vec.epilog.ph:
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ], [ [[TMP17]], [[VEC_EPILOG_ITER_CHECK]] ]
; CHECK-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
; CHECK-NEXT:    [[TMP18:%.*]] = insertelement <8 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
; CHECK-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
; CHECK:       vec.epilog.vector.body:
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT8:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI5:%.*]] = phi <8 x i32> [ [[TMP18]], [[VEC_EPILOG_PH]] ], [ [[TMP34:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP19:%.*]] = add i64 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[TMP20:%.*]] = add nsw i64 [[TMP19]], 3
; CHECK-NEXT:    [[TMP21:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP20]]
; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds i8, i8* [[TMP21]], i32 0
; CHECK-NEXT:    [[TMP23:%.*]] = bitcast i8* [[TMP22]] to <8 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD6:%.*]] = load <8 x i8>, <8 x i8>* [[TMP23]], align 1
; CHECK-NEXT:    [[TMP24:%.*]] = zext <8 x i8> [[WIDE_LOAD6]] to <8 x i32>
; CHECK-NEXT:    [[TMP25:%.*]] = add nsw i64 [[TMP19]], 2
; CHECK-NEXT:    [[TMP26:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP25]]
; CHECK-NEXT:    [[TMP27:%.*]] = getelementptr inbounds i8, i8* [[TMP26]], i32 0
; CHECK-NEXT:    [[TMP28:%.*]] = bitcast i8* [[TMP27]] to <8 x i8>*
; CHECK-NEXT:    [[WIDE_LOAD7:%.*]] = load <8 x i8>, <8 x i8>* [[TMP28]], align 1
; CHECK-NEXT:    [[TMP29:%.*]] = zext <8 x i8> [[WIDE_LOAD7]] to <8 x i32>
; CHECK-NEXT:    [[TMP30:%.*]] = sub nsw <8 x i32> [[TMP24]], [[TMP29]]
; CHECK-NEXT:    [[TMP31:%.*]] = icmp sgt <8 x i32> [[TMP30]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; CHECK-NEXT:    [[TMP32:%.*]] = sub nsw <8 x i32> zeroinitializer, [[TMP30]]
; CHECK-NEXT:    [[TMP33:%.*]] = select <8 x i1> [[TMP31]], <8 x i32> [[TMP30]], <8 x i32> [[TMP32]]
; CHECK-NEXT:    [[TMP34]] = add <8 x i32> [[TMP33]], [[VEC_PHI5]]
; CHECK-NEXT:    [[INDEX_NEXT8]] = add nuw i64 [[OFFSET_IDX]], 8
; CHECK-NEXT:    [[TMP35:%.*]] = icmp eq i64 [[INDEX_NEXT8]], 1024
; CHECK-NEXT:    br i1 [[TMP35]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK:       vec.epilog.middle.block:
; CHECK-NEXT:    [[TMP36:%.*]] = call i32 @llvm.vector.reduce.add.v8i32(<8 x i32> [[TMP34]])
; CHECK-NEXT:    [[CMP_N3:%.*]] = icmp eq i64 1024, 1024
; CHECK-NEXT:    br i1 [[CMP_N3]], label [[FOR_COND_CLEANUP_LOOPEXIT:%.*]], label [[VEC_EPILOG_SCALAR_PH]]
; CHECK:       vec.epilog.scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[ITER_CHECK:%.*]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX9:%.*]] = phi i32 [ 0, [[ITER_CHECK]] ], [ [[TMP17]], [[VEC_EPILOG_ITER_CHECK]] ], [ [[TMP36]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.cond.cleanup.loopexit:
; CHECK-NEXT:    [[ADD_LCSSA2:%.*]] = phi i32 [ [[ADD:%.*]], [[FOR_BODY]] ], [ [[TMP36]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_COND_CLEANUP]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    [[ADD_LCSSA:%.*]] = phi i32 [ [[TMP17]], [[MIDDLE_BLOCK]] ], [ [[ADD_LCSSA2]], [[FOR_COND_CLEANUP_LOOPEXIT]] ]
; CHECK-NEXT:    ret i32 [[ADD_LCSSA]]
; CHECK:       for.body:
; CHECK-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[S_015:%.*]] = phi i32 [ [[BC_MERGE_RDX9]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[ADD]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[TMP1:%.*]] = add nsw i64 [[INDVARS_IV]], 3
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP1]]
; CHECK-NEXT:    [[TMP:%.*]] = load i8, i8* [[ARRAYIDX]], align 1
; CHECK-NEXT:    [[CONV:%.*]] = zext i8 [[TMP]] to i32
; CHECK-NEXT:    [[TMP2:%.*]] = add nsw i64 [[INDVARS_IV]], 2
; CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP2]]
; CHECK-NEXT:    [[TMP3:%.*]] = load i8, i8* [[ARRAYIDX2]], align 1
; CHECK-NEXT:    [[CONV3:%.*]] = zext i8 [[TMP3]] to i32
; CHECK-NEXT:    [[SUB:%.*]] = sub nsw i32 [[CONV]], [[CONV3]]
; CHECK-NEXT:    [[ISPOS:%.*]] = icmp sgt i32 [[SUB]], -1
; CHECK-NEXT:    [[NEG:%.*]] = sub nsw i32 0, [[SUB]]
; CHECK-NEXT:    [[TMP4:%.*]] = select i1 [[ISPOS]], i32 [[SUB]], i32 [[NEG]]
; CHECK-NEXT:    [[ADD]] = add nsw i32 [[TMP4]], [[S_015]]
; CHECK-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 1024
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP_LOOPEXIT]], label [[FOR_BODY]], !llvm.loop [[LOOP7:![0-9]+]]
;
; AVX512F-LABEL: @goo(
; AVX512F-NEXT:  iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
; AVX512F:       vector.main.loop.iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
; AVX512F:       vector.ph:
; AVX512F-NEXT:    br label [[VECTOR_BODY:%.*]]
; AVX512F:       vector.body:
; AVX512F-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI:%.*]] = phi <64 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP30:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI1:%.*]] = phi <64 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP31:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; AVX512F-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 64
; AVX512F-NEXT:    [[TMP2:%.*]] = add nsw i64 [[TMP0]], 3
; AVX512F-NEXT:    [[TMP3:%.*]] = add nsw i64 [[TMP1]], 3
; AVX512F-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP2]]
; AVX512F-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP3]]
; AVX512F-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i8, i8* [[TMP4]], i32 0
; AVX512F-NEXT:    [[TMP7:%.*]] = bitcast i8* [[TMP6]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD:%.*]] = load <64 x i8>, <64 x i8>* [[TMP7]], align 1
; AVX512F-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i8, i8* [[TMP4]], i32 64
; AVX512F-NEXT:    [[TMP9:%.*]] = bitcast i8* [[TMP8]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD2:%.*]] = load <64 x i8>, <64 x i8>* [[TMP9]], align 1
; AVX512F-NEXT:    [[TMP10:%.*]] = zext <64 x i8> [[WIDE_LOAD]] to <64 x i32>
; AVX512F-NEXT:    [[TMP11:%.*]] = zext <64 x i8> [[WIDE_LOAD2]] to <64 x i32>
; AVX512F-NEXT:    [[TMP12:%.*]] = add nsw i64 [[TMP0]], 2
; AVX512F-NEXT:    [[TMP13:%.*]] = add nsw i64 [[TMP1]], 2
; AVX512F-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP12]]
; AVX512F-NEXT:    [[TMP15:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP13]]
; AVX512F-NEXT:    [[TMP16:%.*]] = getelementptr inbounds i8, i8* [[TMP14]], i32 0
; AVX512F-NEXT:    [[TMP17:%.*]] = bitcast i8* [[TMP16]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD3:%.*]] = load <64 x i8>, <64 x i8>* [[TMP17]], align 1
; AVX512F-NEXT:    [[TMP18:%.*]] = getelementptr inbounds i8, i8* [[TMP14]], i32 64
; AVX512F-NEXT:    [[TMP19:%.*]] = bitcast i8* [[TMP18]] to <64 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD4:%.*]] = load <64 x i8>, <64 x i8>* [[TMP19]], align 1
; AVX512F-NEXT:    [[TMP20:%.*]] = zext <64 x i8> [[WIDE_LOAD3]] to <64 x i32>
; AVX512F-NEXT:    [[TMP21:%.*]] = zext <64 x i8> [[WIDE_LOAD4]] to <64 x i32>
; AVX512F-NEXT:    [[TMP22:%.*]] = sub nsw <64 x i32> [[TMP10]], [[TMP20]]
; AVX512F-NEXT:    [[TMP23:%.*]] = sub nsw <64 x i32> [[TMP11]], [[TMP21]]
; AVX512F-NEXT:    [[TMP24:%.*]] = icmp sgt <64 x i32> [[TMP22]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP25:%.*]] = icmp sgt <64 x i32> [[TMP23]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP26:%.*]] = sub nsw <64 x i32> zeroinitializer, [[TMP22]]
; AVX512F-NEXT:    [[TMP27:%.*]] = sub nsw <64 x i32> zeroinitializer, [[TMP23]]
; AVX512F-NEXT:    [[TMP28:%.*]] = select <64 x i1> [[TMP24]], <64 x i32> [[TMP22]], <64 x i32> [[TMP26]]
; AVX512F-NEXT:    [[TMP29:%.*]] = select <64 x i1> [[TMP25]], <64 x i32> [[TMP23]], <64 x i32> [[TMP27]]
; AVX512F-NEXT:    [[TMP30]] = add <64 x i32> [[TMP28]], [[VEC_PHI]]
; AVX512F-NEXT:    [[TMP31]] = add <64 x i32> [[TMP29]], [[VEC_PHI1]]
; AVX512F-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 128
; AVX512F-NEXT:    [[TMP32:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; AVX512F-NEXT:    br i1 [[TMP32]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; AVX512F:       middle.block:
; AVX512F-NEXT:    [[BIN_RDX:%.*]] = add <64 x i32> [[TMP31]], [[TMP30]]
; AVX512F-NEXT:    [[TMP33:%.*]] = call i32 @llvm.vector.reduce.add.v64i32(<64 x i32> [[BIN_RDX]])
; AVX512F-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; AVX512F-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
; AVX512F:       vec.epilog.iter.check:
; AVX512F-NEXT:    br i1 true, label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
; AVX512F:       vec.epilog.ph:
; AVX512F-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ], [ [[TMP33]], [[VEC_EPILOG_ITER_CHECK]] ]
; AVX512F-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
; AVX512F-NEXT:    [[TMP34:%.*]] = insertelement <32 x i32> zeroinitializer, i32 [[BC_MERGE_RDX]], i32 0
; AVX512F-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
; AVX512F:       vec.epilog.vector.body:
; AVX512F-NEXT:    [[OFFSET_IDX:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT11:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI8:%.*]] = phi <32 x i32> [ [[TMP34]], [[VEC_EPILOG_PH]] ], [ [[TMP50:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP35:%.*]] = add i64 [[OFFSET_IDX]], 0
; AVX512F-NEXT:    [[TMP36:%.*]] = add nsw i64 [[TMP35]], 3
; AVX512F-NEXT:    [[TMP37:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP36]]
; AVX512F-NEXT:    [[TMP38:%.*]] = getelementptr inbounds i8, i8* [[TMP37]], i32 0
; AVX512F-NEXT:    [[TMP39:%.*]] = bitcast i8* [[TMP38]] to <32 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD9:%.*]] = load <32 x i8>, <32 x i8>* [[TMP39]], align 1
; AVX512F-NEXT:    [[TMP40:%.*]] = zext <32 x i8> [[WIDE_LOAD9]] to <32 x i32>
; AVX512F-NEXT:    [[TMP41:%.*]] = add nsw i64 [[TMP35]], 2
; AVX512F-NEXT:    [[TMP42:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP41]]
; AVX512F-NEXT:    [[TMP43:%.*]] = getelementptr inbounds i8, i8* [[TMP42]], i32 0
; AVX512F-NEXT:    [[TMP44:%.*]] = bitcast i8* [[TMP43]] to <32 x i8>*
; AVX512F-NEXT:    [[WIDE_LOAD10:%.*]] = load <32 x i8>, <32 x i8>* [[TMP44]], align 1
; AVX512F-NEXT:    [[TMP45:%.*]] = zext <32 x i8> [[WIDE_LOAD10]] to <32 x i32>
; AVX512F-NEXT:    [[TMP46:%.*]] = sub nsw <32 x i32> [[TMP40]], [[TMP45]]
; AVX512F-NEXT:    [[TMP47:%.*]] = icmp sgt <32 x i32> [[TMP46]], <i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1, i32 -1>
; AVX512F-NEXT:    [[TMP48:%.*]] = sub nsw <32 x i32> zeroinitializer, [[TMP46]]
; AVX512F-NEXT:    [[TMP49:%.*]] = select <32 x i1> [[TMP47]], <32 x i32> [[TMP46]], <32 x i32> [[TMP48]]
; AVX512F-NEXT:    [[TMP50]] = add <32 x i32> [[TMP49]], [[VEC_PHI8]]
; AVX512F-NEXT:    [[INDEX_NEXT11]] = add nuw i64 [[OFFSET_IDX]], 32
; AVX512F-NEXT:    [[TMP51:%.*]] = icmp eq i64 [[INDEX_NEXT11]], 1024
; AVX512F-NEXT:    br i1 [[TMP51]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; AVX512F:       vec.epilog.middle.block:
; AVX512F-NEXT:    [[TMP52:%.*]] = call i32 @llvm.vector.reduce.add.v32i32(<32 x i32> [[TMP50]])
; AVX512F-NEXT:    [[CMP_N6:%.*]] = icmp eq i64 1024, 1024
; AVX512F-NEXT:    br i1 [[CMP_N6]], label [[FOR_COND_CLEANUP_LOOPEXIT:%.*]], label [[VEC_EPILOG_SCALAR_PH]]
; AVX512F:       vec.epilog.scalar.ph:
; AVX512F-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 1024, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[ITER_CHECK:%.*]] ]
; AVX512F-NEXT:    [[BC_MERGE_RDX12:%.*]] = phi i32 [ 0, [[ITER_CHECK]] ], [ [[TMP33]], [[VEC_EPILOG_ITER_CHECK]] ], [ [[TMP52]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    br label [[FOR_BODY:%.*]]
; AVX512F:       for.cond.cleanup.loopexit:
; AVX512F-NEXT:    [[ADD_LCSSA5:%.*]] = phi i32 [ [[ADD:%.*]], [[FOR_BODY]] ], [ [[TMP52]], [[VEC_EPILOG_MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    br label [[FOR_COND_CLEANUP]]
; AVX512F:       for.cond.cleanup:
; AVX512F-NEXT:    [[ADD_LCSSA:%.*]] = phi i32 [ [[TMP33]], [[MIDDLE_BLOCK]] ], [ [[ADD_LCSSA5]], [[FOR_COND_CLEANUP_LOOPEXIT]] ]
; AVX512F-NEXT:    ret i32 [[ADD_LCSSA]]
; AVX512F:       for.body:
; AVX512F-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[S_015:%.*]] = phi i32 [ [[BC_MERGE_RDX12]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[ADD]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[TMP1:%.*]] = add nsw i64 [[INDVARS_IV]], 3
; AVX512F-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 [[TMP1]]
; AVX512F-NEXT:    [[TMP:%.*]] = load i8, i8* [[ARRAYIDX]], align 1
; AVX512F-NEXT:    [[CONV:%.*]] = zext i8 [[TMP]] to i32
; AVX512F-NEXT:    [[TMP2:%.*]] = add nsw i64 [[INDVARS_IV]], 2
; AVX512F-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 [[TMP2]]
; AVX512F-NEXT:    [[TMP3:%.*]] = load i8, i8* [[ARRAYIDX2]], align 1
; AVX512F-NEXT:    [[CONV3:%.*]] = zext i8 [[TMP3]] to i32
; AVX512F-NEXT:    [[SUB:%.*]] = sub nsw i32 [[CONV]], [[CONV3]]
; AVX512F-NEXT:    [[ISPOS:%.*]] = icmp sgt i32 [[SUB]], -1
; AVX512F-NEXT:    [[NEG:%.*]] = sub nsw i32 0, [[SUB]]
; AVX512F-NEXT:    [[TMP4:%.*]] = select i1 [[ISPOS]], i32 [[SUB]], i32 [[NEG]]
; AVX512F-NEXT:    [[ADD]] = add nsw i32 [[TMP4]], [[S_015]]
; AVX512F-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; AVX512F-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 1024
; AVX512F-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP_LOOPEXIT]], label [[FOR_BODY]], !llvm.loop [[LOOP7:![0-9]+]]
;
entry:
  br label %for.body

for.cond.cleanup:                                 ; preds = %for.body
  %add.lcssa = phi i32 [ %add, %for.body ]
  ret i32 %add.lcssa

for.body:                                         ; preds = %for.body, %entry
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %s.015 = phi i32 [ 0, %entry ], [ %add, %for.body ]
  %tmp1 = add nsw i64 %indvars.iv, 3
  %arrayidx = getelementptr inbounds [1024 x i8], [1024 x i8]* @a, i64 0, i64 %tmp1
  %tmp = load i8, i8* %arrayidx, align 1
  %conv = zext i8 %tmp to i32
  %tmp2 = add nsw i64 %indvars.iv, 2
  %arrayidx2 = getelementptr inbounds [1024 x i8], [1024 x i8]* @b, i64 0, i64 %tmp2
  %tmp3 = load i8, i8* %arrayidx2, align 1
  %conv3 = zext i8 %tmp3 to i32
  %sub = sub nsw i32 %conv, %conv3
  %ispos = icmp sgt i32 %sub, -1
  %neg = sub nsw i32 0, %sub
  %tmp4 = select i1 %ispos, i32 %sub, i32 %neg
  %add = add nsw i32 %tmp4, %s.015
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 1024
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

define i64 @bar(i64* nocapture %a) {
; CHECK-LABEL: @bar(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_IND:%.*]] = phi <2 x i64> [ <i64 0, i64 1>, [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <2 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP12:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI2:%.*]] = phi <2 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP13:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[STEP_ADD:%.*]] = add <2 x i64> [[VEC_IND]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i64, i64* [[A:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds i64, i64* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast i64* [[TMP4]] to <2 x i64>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x i64>, <2 x i64>* [[TMP5]], align 8
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i64, i64* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast i64* [[TMP6]] to <2 x i64>*
; CHECK-NEXT:    [[WIDE_LOAD3:%.*]] = load <2 x i64>, <2 x i64>* [[TMP7]], align 8
; CHECK-NEXT:    [[TMP8:%.*]] = add nsw <2 x i64> [[WIDE_LOAD]], [[VEC_IND]]
; CHECK-NEXT:    [[TMP9:%.*]] = add nsw <2 x i64> [[WIDE_LOAD3]], [[STEP_ADD]]
; CHECK-NEXT:    [[TMP10:%.*]] = bitcast i64* [[TMP4]] to <2 x i64>*
; CHECK-NEXT:    store <2 x i64> [[TMP8]], <2 x i64>* [[TMP10]], align 8
; CHECK-NEXT:    [[TMP11:%.*]] = bitcast i64* [[TMP6]] to <2 x i64>*
; CHECK-NEXT:    store <2 x i64> [[TMP9]], <2 x i64>* [[TMP11]], align 8
; CHECK-NEXT:    [[TMP12]] = add <2 x i64> [[TMP8]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP13]] = add <2 x i64> [[TMP9]], [[VEC_PHI2]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[VEC_IND_NEXT]] = add <2 x i64> [[STEP_ADD]], <i64 2, i64 2>
; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; CHECK-NEXT:    br i1 [[TMP14]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[BIN_RDX:%.*]] = add <2 x i64> [[TMP13]], [[TMP12]]
; CHECK-NEXT:    [[TMP15:%.*]] = call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> [[BIN_RDX]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i64 [ 0, [[ENTRY]] ], [ [[TMP15]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    [[ADD2_LCSSA:%.*]] = phi i64 [ [[ADD2:%.*]], [[FOR_BODY]] ], [ [[TMP15]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    ret i64 [[ADD2_LCSSA]]
; CHECK:       for.body:
; CHECK-NEXT:    [[I_012:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INC:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[S_011:%.*]] = phi i64 [ [[BC_MERGE_RDX]], [[SCALAR_PH]] ], [ [[ADD2]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[I_012]]
; CHECK-NEXT:    [[TMP16:%.*]] = load i64, i64* [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[ADD:%.*]] = add nsw i64 [[TMP16]], [[I_012]]
; CHECK-NEXT:    store i64 [[ADD]], i64* [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[ADD2]] = add nsw i64 [[ADD]], [[S_011]]
; CHECK-NEXT:    [[INC]] = add nuw nsw i64 [[I_012]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INC]], 1024
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
;
; AVX512F-LABEL: @bar(
; AVX512F-NEXT:  entry:
; AVX512F-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; AVX512F:       vector.ph:
; AVX512F-NEXT:    br label [[VECTOR_BODY:%.*]]
; AVX512F:       vector.body:
; AVX512F-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_IND:%.*]] = phi <8 x i64> [ <i64 0, i64 1, i64 2, i64 3, i64 4, i64 5, i64 6, i64 7>, [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI:%.*]] = phi <8 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP24:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI4:%.*]] = phi <8 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP25:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI5:%.*]] = phi <8 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP26:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[VEC_PHI6:%.*]] = phi <8 x i64> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP27:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[STEP_ADD:%.*]] = add <8 x i64> [[VEC_IND]], <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
; AVX512F-NEXT:    [[STEP_ADD1:%.*]] = add <8 x i64> [[STEP_ADD]], <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
; AVX512F-NEXT:    [[STEP_ADD2:%.*]] = add <8 x i64> [[STEP_ADD1]], <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
; AVX512F-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; AVX512F-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 8
; AVX512F-NEXT:    [[TMP2:%.*]] = add i64 [[INDEX]], 16
; AVX512F-NEXT:    [[TMP3:%.*]] = add i64 [[INDEX]], 24
; AVX512F-NEXT:    [[TMP4:%.*]] = getelementptr inbounds i64, i64* [[A:%.*]], i64 [[TMP0]]
; AVX512F-NEXT:    [[TMP5:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[TMP1]]
; AVX512F-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[TMP2]]
; AVX512F-NEXT:    [[TMP7:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[TMP3]]
; AVX512F-NEXT:    [[TMP8:%.*]] = getelementptr inbounds i64, i64* [[TMP4]], i32 0
; AVX512F-NEXT:    [[TMP9:%.*]] = bitcast i64* [[TMP8]] to <8 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD:%.*]] = load <8 x i64>, <8 x i64>* [[TMP9]], align 8
; AVX512F-NEXT:    [[TMP10:%.*]] = getelementptr inbounds i64, i64* [[TMP4]], i32 8
; AVX512F-NEXT:    [[TMP11:%.*]] = bitcast i64* [[TMP10]] to <8 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD7:%.*]] = load <8 x i64>, <8 x i64>* [[TMP11]], align 8
; AVX512F-NEXT:    [[TMP12:%.*]] = getelementptr inbounds i64, i64* [[TMP4]], i32 16
; AVX512F-NEXT:    [[TMP13:%.*]] = bitcast i64* [[TMP12]] to <8 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD8:%.*]] = load <8 x i64>, <8 x i64>* [[TMP13]], align 8
; AVX512F-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i64, i64* [[TMP4]], i32 24
; AVX512F-NEXT:    [[TMP15:%.*]] = bitcast i64* [[TMP14]] to <8 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD9:%.*]] = load <8 x i64>, <8 x i64>* [[TMP15]], align 8
; AVX512F-NEXT:    [[TMP16:%.*]] = add nsw <8 x i64> [[WIDE_LOAD]], [[VEC_IND]]
; AVX512F-NEXT:    [[TMP17:%.*]] = add nsw <8 x i64> [[WIDE_LOAD7]], [[STEP_ADD]]
; AVX512F-NEXT:    [[TMP18:%.*]] = add nsw <8 x i64> [[WIDE_LOAD8]], [[STEP_ADD1]]
; AVX512F-NEXT:    [[TMP19:%.*]] = add nsw <8 x i64> [[WIDE_LOAD9]], [[STEP_ADD2]]
; AVX512F-NEXT:    [[TMP20:%.*]] = bitcast i64* [[TMP8]] to <8 x i64>*
; AVX512F-NEXT:    store <8 x i64> [[TMP16]], <8 x i64>* [[TMP20]], align 8
; AVX512F-NEXT:    [[TMP21:%.*]] = bitcast i64* [[TMP10]] to <8 x i64>*
; AVX512F-NEXT:    store <8 x i64> [[TMP17]], <8 x i64>* [[TMP21]], align 8
; AVX512F-NEXT:    [[TMP22:%.*]] = bitcast i64* [[TMP12]] to <8 x i64>*
; AVX512F-NEXT:    store <8 x i64> [[TMP18]], <8 x i64>* [[TMP22]], align 8
; AVX512F-NEXT:    [[TMP23:%.*]] = bitcast i64* [[TMP14]] to <8 x i64>*
; AVX512F-NEXT:    store <8 x i64> [[TMP19]], <8 x i64>* [[TMP23]], align 8
; AVX512F-NEXT:    [[TMP24]] = add <8 x i64> [[TMP16]], [[VEC_PHI]]
; AVX512F-NEXT:    [[TMP25]] = add <8 x i64> [[TMP17]], [[VEC_PHI4]]
; AVX512F-NEXT:    [[TMP26]] = add <8 x i64> [[TMP18]], [[VEC_PHI5]]
; AVX512F-NEXT:    [[TMP27]] = add <8 x i64> [[TMP19]], [[VEC_PHI6]]
; AVX512F-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 32
; AVX512F-NEXT:    [[VEC_IND_NEXT]] = add <8 x i64> [[STEP_ADD2]], <i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8, i64 8>
; AVX512F-NEXT:    [[TMP28:%.*]] = icmp eq i64 [[INDEX_NEXT]], 1024
; AVX512F-NEXT:    br i1 [[TMP28]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; AVX512F:       middle.block:
; AVX512F-NEXT:    [[BIN_RDX:%.*]] = add <8 x i64> [[TMP25]], [[TMP24]]
; AVX512F-NEXT:    [[BIN_RDX10:%.*]] = add <8 x i64> [[TMP26]], [[BIN_RDX]]
; AVX512F-NEXT:    [[BIN_RDX11:%.*]] = add <8 x i64> [[TMP27]], [[BIN_RDX10]]
; AVX512F-NEXT:    [[TMP29:%.*]] = call i64 @llvm.vector.reduce.add.v8i64(<8 x i64> [[BIN_RDX11]])
; AVX512F-NEXT:    [[CMP_N:%.*]] = icmp eq i64 1024, 1024
; AVX512F-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[SCALAR_PH]]
; AVX512F:       scalar.ph:
; AVX512F-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 1024, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; AVX512F-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i64 [ 0, [[ENTRY]] ], [ [[TMP29]], [[MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    br label [[FOR_BODY:%.*]]
; AVX512F:       for.cond.cleanup:
; AVX512F-NEXT:    [[ADD2_LCSSA:%.*]] = phi i64 [ [[ADD2:%.*]], [[FOR_BODY]] ], [ [[TMP29]], [[MIDDLE_BLOCK]] ]
; AVX512F-NEXT:    ret i64 [[ADD2_LCSSA]]
; AVX512F:       for.body:
; AVX512F-NEXT:    [[I_012:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INC:%.*]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[S_011:%.*]] = phi i64 [ [[BC_MERGE_RDX]], [[SCALAR_PH]] ], [ [[ADD2]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, i64* [[A]], i64 [[I_012]]
; AVX512F-NEXT:    [[TMP30:%.*]] = load i64, i64* [[ARRAYIDX]], align 8
; AVX512F-NEXT:    [[ADD:%.*]] = add nsw i64 [[TMP30]], [[I_012]]
; AVX512F-NEXT:    store i64 [[ADD]], i64* [[ARRAYIDX]], align 8
; AVX512F-NEXT:    [[ADD2]] = add nsw i64 [[ADD]], [[S_011]]
; AVX512F-NEXT:    [[INC]] = add nuw nsw i64 [[I_012]], 1
; AVX512F-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INC]], 1024
; AVX512F-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
;

entry:
  br label %for.body

for.cond.cleanup:
  %add2.lcssa = phi i64 [ %add2, %for.body ]
  ret i64 %add2.lcssa

for.body:
  %i.012 = phi i64 [ 0, %entry ], [ %inc, %for.body ]
  %s.011 = phi i64 [ 0, %entry ], [ %add2, %for.body ]
  %arrayidx = getelementptr inbounds i64, i64* %a, i64 %i.012
  %0 = load i64, i64* %arrayidx, align 8
  %add = add nsw i64 %0, %i.012
  store i64 %add, i64* %arrayidx, align 8
  %add2 = add nsw i64 %add, %s.011
  %inc = add nuw nsw i64 %i.012, 1
  %exitcond = icmp eq i64 %inc, 1024
  br i1 %exitcond, label %for.cond.cleanup, label %for.body
}

@d = external global [0 x i64], align 8
@e = external global [0 x i32], align 4
@c = external global [0 x i32], align 4

define void @hoo(i32 %n) {
; For c[i] = e[d[i]] in the loop, e[d[i]] is not consecutive but its index %tmp can
; be gathered into a vector. For VF == 16, the vector version of %tmp will be <16 x i64>
; so the max usage of AVX512 vector register will be 2.
; CHECK-LABEL: @hoo(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ 0, [[ENTRY:%.*]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [0 x i64], [0 x i64]* @d, i64 0, i64 [[INDVARS_IV]]
; CHECK-NEXT:    [[TMP:%.*]] = load i64, i64* [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @e, i64 0, i64 [[TMP]]
; CHECK-NEXT:    [[TMP1:%.*]] = load i32, i32* [[ARRAYIDX1]], align 4
; CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @c, i64 0, i64 [[INDVARS_IV]]
; CHECK-NEXT:    store i32 [[TMP1]], i32* [[ARRAYIDX3]], align 4
; CHECK-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 10000
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END:%.*]], label [[FOR_BODY]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;
; AVX512F-LABEL: @hoo(
; AVX512F-NEXT:  iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_SCALAR_PH:%.*]], label [[VECTOR_MAIN_LOOP_ITER_CHECK:%.*]]
; AVX512F:       vector.main.loop.iter.check:
; AVX512F-NEXT:    br i1 false, label [[VEC_EPILOG_PH:%.*]], label [[VECTOR_PH:%.*]]
; AVX512F:       vector.ph:
; AVX512F-NEXT:    br label [[VECTOR_BODY:%.*]]
; AVX512F:       vector.body:
; AVX512F-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; AVX512F-NEXT:    [[TMP1:%.*]] = getelementptr inbounds [0 x i64], [0 x i64]* @d, i64 0, i64 [[TMP0]]
; AVX512F-NEXT:    [[TMP2:%.*]] = getelementptr inbounds i64, i64* [[TMP1]], i32 0
; AVX512F-NEXT:    [[TMP3:%.*]] = bitcast i64* [[TMP2]] to <16 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD:%.*]] = load <16 x i64>, <16 x i64>* [[TMP3]], align 8
; AVX512F-NEXT:    [[TMP4:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @e, i64 0, <16 x i64> [[WIDE_LOAD]]
; AVX512F-NEXT:    [[WIDE_MASKED_GATHER:%.*]] = call <16 x i32> @llvm.masked.gather.v16i32.v16p0i32(<16 x i32*> [[TMP4]], i32 4, <16 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <16 x i32> undef)
; AVX512F-NEXT:    [[TMP5:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @c, i64 0, i64 [[TMP0]]
; AVX512F-NEXT:    [[TMP6:%.*]] = getelementptr inbounds i32, i32* [[TMP5]], i32 0
; AVX512F-NEXT:    [[TMP7:%.*]] = bitcast i32* [[TMP6]] to <16 x i32>*
; AVX512F-NEXT:    store <16 x i32> [[WIDE_MASKED_GATHER]], <16 x i32>* [[TMP7]], align 4
; AVX512F-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 16
; AVX512F-NEXT:    [[TMP8:%.*]] = icmp eq i64 [[INDEX_NEXT]], 10000
; AVX512F-NEXT:    br i1 [[TMP8]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
; AVX512F:       middle.block:
; AVX512F-NEXT:    [[CMP_N:%.*]] = icmp eq i64 10000, 10000
; AVX512F-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[VEC_EPILOG_ITER_CHECK:%.*]]
; AVX512F:       vec.epilog.iter.check:
; AVX512F-NEXT:    br i1 true, label [[VEC_EPILOG_SCALAR_PH]], label [[VEC_EPILOG_PH]]
; AVX512F:       vec.epilog.ph:
; AVX512F-NEXT:    [[VEC_EPILOG_RESUME_VAL:%.*]] = phi i64 [ 10000, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[VECTOR_MAIN_LOOP_ITER_CHECK]] ]
; AVX512F-NEXT:    br label [[VEC_EPILOG_VECTOR_BODY:%.*]]
; AVX512F:       vec.epilog.vector.body:
; AVX512F-NEXT:    [[OFFSET_IDX:%.*]] = phi i64 [ [[VEC_EPILOG_RESUME_VAL]], [[VEC_EPILOG_PH]] ], [ [[INDEX_NEXT5:%.*]], [[VEC_EPILOG_VECTOR_BODY]] ]
; AVX512F-NEXT:    [[TMP9:%.*]] = add i64 [[OFFSET_IDX]], 0
; AVX512F-NEXT:    [[TMP10:%.*]] = getelementptr inbounds [0 x i64], [0 x i64]* @d, i64 0, i64 [[TMP9]]
; AVX512F-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i64, i64* [[TMP10]], i32 0
; AVX512F-NEXT:    [[TMP12:%.*]] = bitcast i64* [[TMP11]] to <8 x i64>*
; AVX512F-NEXT:    [[WIDE_LOAD3:%.*]] = load <8 x i64>, <8 x i64>* [[TMP12]], align 8
; AVX512F-NEXT:    [[TMP13:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @e, i64 0, <8 x i64> [[WIDE_LOAD3]]
; AVX512F-NEXT:    [[WIDE_MASKED_GATHER4:%.*]] = call <8 x i32> @llvm.masked.gather.v8i32.v8p0i32(<8 x i32*> [[TMP13]], i32 4, <8 x i1> <i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true, i1 true>, <8 x i32> undef)
; AVX512F-NEXT:    [[TMP14:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @c, i64 0, i64 [[TMP9]]
; AVX512F-NEXT:    [[TMP15:%.*]] = getelementptr inbounds i32, i32* [[TMP14]], i32 0
; AVX512F-NEXT:    [[TMP16:%.*]] = bitcast i32* [[TMP15]] to <8 x i32>*
; AVX512F-NEXT:    store <8 x i32> [[WIDE_MASKED_GATHER4]], <8 x i32>* [[TMP16]], align 4
; AVX512F-NEXT:    [[INDEX_NEXT5]] = add nuw i64 [[OFFSET_IDX]], 8
; AVX512F-NEXT:    [[TMP17:%.*]] = icmp eq i64 [[INDEX_NEXT5]], 10000
; AVX512F-NEXT:    br i1 [[TMP17]], label [[VEC_EPILOG_MIDDLE_BLOCK:%.*]], label [[VEC_EPILOG_VECTOR_BODY]], !llvm.loop [[LOOP11:![0-9]+]]
; AVX512F:       vec.epilog.middle.block:
; AVX512F-NEXT:    [[CMP_N1:%.*]] = icmp eq i64 10000, 10000
; AVX512F-NEXT:    br i1 [[CMP_N1]], label [[FOR_END_LOOPEXIT:%.*]], label [[VEC_EPILOG_SCALAR_PH]]
; AVX512F:       vec.epilog.scalar.ph:
; AVX512F-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 10000, [[VEC_EPILOG_MIDDLE_BLOCK]] ], [ 10000, [[VEC_EPILOG_ITER_CHECK]] ], [ 0, [[ITER_CHECK:%.*]] ]
; AVX512F-NEXT:    br label [[FOR_BODY:%.*]]
; AVX512F:       for.body:
; AVX512F-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[VEC_EPILOG_SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; AVX512F-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds [0 x i64], [0 x i64]* @d, i64 0, i64 [[INDVARS_IV]]
; AVX512F-NEXT:    [[TMP:%.*]] = load i64, i64* [[ARRAYIDX]], align 8
; AVX512F-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @e, i64 0, i64 [[TMP]]
; AVX512F-NEXT:    [[TMP1:%.*]] = load i32, i32* [[ARRAYIDX1]], align 4
; AVX512F-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds [0 x i32], [0 x i32]* @c, i64 0, i64 [[INDVARS_IV]]
; AVX512F-NEXT:    store i32 [[TMP1]], i32* [[ARRAYIDX3]], align 4
; AVX512F-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; AVX512F-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], 10000
; AVX512F-NEXT:    br i1 [[EXITCOND]], label [[FOR_END_LOOPEXIT]], label [[FOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; AVX512F:       for.end.loopexit:
; AVX512F-NEXT:    br label [[FOR_END]]
; AVX512F:       for.end:
; AVX512F-NEXT:    ret void
;
; AVX512F-CHECK: LV(REG): Found max usage: 2 item
; AVX512F-CHECK: LV(REG): RegisterClass: Generic::ScalarRC, 2 registers
; AVX512F-CHECK: LV(REG): RegisterClass: Generic::VectorRC, 2 registers
; AVX512F-CHECK: LV(REG): Found invariant usage: 0 item

entry:
  br label %for.body

for.body:                                         ; preds = %for.body, %entry
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %arrayidx = getelementptr inbounds [0 x i64], [0 x i64]* @d, i64 0, i64 %indvars.iv
  %tmp = load i64, i64* %arrayidx, align 8
  %arrayidx1 = getelementptr inbounds [0 x i32], [0 x i32]* @e, i64 0, i64 %tmp
  %tmp1 = load i32, i32* %arrayidx1, align 4
  %arrayidx3 = getelementptr inbounds [0 x i32], [0 x i32]* @c, i64 0, i64 %indvars.iv
  store i32 %tmp1, i32* %arrayidx3, align 4
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, 10000
  br i1 %exitcond, label %for.end, label %for.body

for.end:                                          ; preds = %for.body
  ret void
}
