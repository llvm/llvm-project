; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -enable-interleaved-mem-accesses=true -force-vector-width=4 -loop-vectorize -S | FileCheck %s

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "x86_64-apple-macosx10.7.0"

; Verify that we do not get any loads of vectors with x86_fp80 elements.
;
; CHECK-NOT: load {{.*}} x x86_fp80
define x86_fp80 @foo(x86_fp80* %a) {
; CHECK-LABEL: @foo(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <4 x x86_fp80> [ <x86_fp80 undef, x86_fp80 0xK00000000000000000000, x86_fp80 0xK00000000000000000000, x86_fp80 0xK00000000000000000000>, [[VECTOR_PH]] ], [ [[TMP34:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = trunc i32 [[INDEX]] to i16
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = mul i16 [[TMP0]], 2
; CHECK-NEXT:    [[TMP1:%.*]] = add i16 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[TMP2:%.*]] = add i16 [[OFFSET_IDX]], 2
; CHECK-NEXT:    [[TMP3:%.*]] = add i16 [[OFFSET_IDX]], 4
; CHECK-NEXT:    [[TMP4:%.*]] = add i16 [[OFFSET_IDX]], 6
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A:%.*]], i16 [[TMP1]]
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP2]]
; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP3]]
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP4]]
; CHECK-NEXT:    [[TMP9:%.*]] = load x86_fp80, x86_fp80* [[TMP5]], align 1
; CHECK-NEXT:    [[TMP10:%.*]] = load x86_fp80, x86_fp80* [[TMP6]], align 1
; CHECK-NEXT:    [[TMP11:%.*]] = load x86_fp80, x86_fp80* [[TMP7]], align 1
; CHECK-NEXT:    [[TMP12:%.*]] = load x86_fp80, x86_fp80* [[TMP8]], align 1
; CHECK-NEXT:    [[TMP13:%.*]] = insertelement <4 x x86_fp80> poison, x86_fp80 [[TMP9]], i32 0
; CHECK-NEXT:    [[TMP14:%.*]] = insertelement <4 x x86_fp80> [[TMP13]], x86_fp80 [[TMP10]], i32 1
; CHECK-NEXT:    [[TMP15:%.*]] = insertelement <4 x x86_fp80> [[TMP14]], x86_fp80 [[TMP11]], i32 2
; CHECK-NEXT:    [[TMP16:%.*]] = insertelement <4 x x86_fp80> [[TMP15]], x86_fp80 [[TMP12]], i32 3
; CHECK-NEXT:    [[TMP17:%.*]] = or i16 [[TMP1]], 1
; CHECK-NEXT:    [[TMP18:%.*]] = or i16 [[TMP2]], 1
; CHECK-NEXT:    [[TMP19:%.*]] = or i16 [[TMP3]], 1
; CHECK-NEXT:    [[TMP20:%.*]] = or i16 [[TMP4]], 1
; CHECK-NEXT:    [[TMP21:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP17]]
; CHECK-NEXT:    [[TMP22:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP18]]
; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP19]]
; CHECK-NEXT:    [[TMP24:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[TMP20]]
; CHECK-NEXT:    [[TMP25:%.*]] = load x86_fp80, x86_fp80* [[TMP21]], align 1
; CHECK-NEXT:    [[TMP26:%.*]] = load x86_fp80, x86_fp80* [[TMP22]], align 1
; CHECK-NEXT:    [[TMP27:%.*]] = load x86_fp80, x86_fp80* [[TMP23]], align 1
; CHECK-NEXT:    [[TMP28:%.*]] = load x86_fp80, x86_fp80* [[TMP24]], align 1
; CHECK-NEXT:    [[TMP29:%.*]] = insertelement <4 x x86_fp80> poison, x86_fp80 [[TMP25]], i32 0
; CHECK-NEXT:    [[TMP30:%.*]] = insertelement <4 x x86_fp80> [[TMP29]], x86_fp80 [[TMP26]], i32 1
; CHECK-NEXT:    [[TMP31:%.*]] = insertelement <4 x x86_fp80> [[TMP30]], x86_fp80 [[TMP27]], i32 2
; CHECK-NEXT:    [[TMP32:%.*]] = insertelement <4 x x86_fp80> [[TMP31]], x86_fp80 [[TMP28]], i32 3
; CHECK-NEXT:    [[TMP33:%.*]] = fadd fast <4 x x86_fp80> [[TMP16]], [[VEC_PHI]]
; CHECK-NEXT:    [[TMP34]] = fadd fast <4 x x86_fp80> [[TMP33]], [[TMP32]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i32 [[INDEX]], 4
; CHECK-NEXT:    [[TMP35:%.*]] = icmp eq i32 [[INDEX_NEXT]], 200
; CHECK-NEXT:    br i1 [[TMP35]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[TMP36:%.*]] = call fast x86_fp80 @llvm.vector.reduce.fadd.v4f80(x86_fp80 0xK80000000000000000000, <4 x x86_fp80> [[TMP34]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i32 200, 200
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i16 [ 400, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi x86_fp80 [ undef, [[ENTRY]] ], [ [[TMP36]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.cond.cleanup:
; CHECK-NEXT:    [[DOTLCSSA:%.*]] = phi x86_fp80 [ [[TMP40:%.*]], [[FOR_BODY]] ], [ [[TMP36]], [[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    ret x86_fp80 [[DOTLCSSA]]
; CHECK:       for.body:
; CHECK-NEXT:    [[I_09:%.*]] = phi i16 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[ADD3:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[RES_08:%.*]] = phi x86_fp80 [ [[BC_MERGE_RDX]], [[SCALAR_PH]] ], [ [[TMP40]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[I_09]]
; CHECK-NEXT:    [[TMP37:%.*]] = load x86_fp80, x86_fp80* [[ARRAYIDX]], align 1
; CHECK-NEXT:    [[ADD:%.*]] = or i16 [[I_09]], 1
; CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds x86_fp80, x86_fp80* [[A]], i16 [[ADD]]
; CHECK-NEXT:    [[TMP38:%.*]] = load x86_fp80, x86_fp80* [[ARRAYIDX2]], align 1
; CHECK-NEXT:    [[TMP39:%.*]] = fadd fast x86_fp80 [[TMP37]], [[RES_08]]
; CHECK-NEXT:    [[TMP40]] = fadd fast x86_fp80 [[TMP39]], [[TMP38]]
; CHECK-NEXT:    [[ADD3]] = add nuw nsw i16 [[I_09]], 2
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult i16 [[ADD3]], 400
; CHECK-NEXT:    br i1 [[CMP]], label [[FOR_BODY]], label [[FOR_COND_CLEANUP]], !llvm.loop [[LOOP2:![0-9]+]]
;
entry:
  br label %for.body

for.cond.cleanup:
  ret x86_fp80 %3

for.body:
  %i.09 = phi i16 [ 0, %entry ], [ %add3, %for.body ]
  %res.08 = phi x86_fp80 [ undef, %entry ], [ %3, %for.body ]
  %arrayidx = getelementptr inbounds x86_fp80, x86_fp80* %a, i16 %i.09
  %0 = load x86_fp80, x86_fp80* %arrayidx, align 1
  %add = or i16 %i.09, 1
  %arrayidx2 = getelementptr inbounds x86_fp80, x86_fp80* %a, i16 %add
  %1 = load x86_fp80, x86_fp80* %arrayidx2, align 1
  %2 = fadd fast x86_fp80 %0, %res.08
  %3 = fadd fast x86_fp80 %2, %1
  %add3 = add nuw nsw i16 %i.09, 2
  %cmp = icmp ult i16 %add3, 400
  br i1 %cmp, label %for.body, label %for.cond.cleanup
}
