; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; REQUIRES: asserts

; RUN: opt -runtime-memory-check-threshold=9 -passes='loop-vectorize' -mtriple=x86_64-unknown-linux -S -debug %s 2>&1 | FileCheck %s

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"

target triple = "x86_64-unknown-linux"

declare double @llvm.pow.f64(double, double)

; Test case where the memory runtime checks and vector body is more expensive
; than running the scalar loop.
; TODO: should not be vectorized.
define void @test(double* nocapture %A, double* nocapture %B, double* nocapture %C, double* nocapture %D, double* nocapture %E) {
; CHECK-LABEL: @test(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[A1:%.*]] = bitcast double* [[A:%.*]] to i8*
; CHECK-NEXT:    [[B3:%.*]] = bitcast double* [[B:%.*]] to i8*
; CHECK-NEXT:    [[E6:%.*]] = bitcast double* [[E:%.*]] to i8*
; CHECK-NEXT:    [[C9:%.*]] = bitcast double* [[C:%.*]] to i8*
; CHECK-NEXT:    [[D12:%.*]] = bitcast double* [[D:%.*]] to i8*
; CHECK-NEXT:    br i1 false, label [[SCALAR_PH:%.*]], label [[VECTOR_MEMCHECK:%.*]]
; CHECK:       vector.memcheck:
; CHECK-NEXT:    [[SCEVGEP:%.*]] = getelementptr double, double* [[A]], i64 16
; CHECK-NEXT:    [[SCEVGEP2:%.*]] = bitcast double* [[SCEVGEP]] to i8*
; CHECK-NEXT:    [[SCEVGEP4:%.*]] = getelementptr double, double* [[B]], i64 16
; CHECK-NEXT:    [[SCEVGEP45:%.*]] = bitcast double* [[SCEVGEP4]] to i8*
; CHECK-NEXT:    [[SCEVGEP7:%.*]] = getelementptr double, double* [[E]], i64 16
; CHECK-NEXT:    [[SCEVGEP78:%.*]] = bitcast double* [[SCEVGEP7]] to i8*
; CHECK-NEXT:    [[SCEVGEP10:%.*]] = getelementptr double, double* [[C]], i64 16
; CHECK-NEXT:    [[SCEVGEP1011:%.*]] = bitcast double* [[SCEVGEP10]] to i8*
; CHECK-NEXT:    [[SCEVGEP13:%.*]] = getelementptr double, double* [[D]], i64 16
; CHECK-NEXT:    [[SCEVGEP1314:%.*]] = bitcast double* [[SCEVGEP13]] to i8*
; CHECK-NEXT:    [[BOUND0:%.*]] = icmp ult i8* [[A1]], [[SCEVGEP45]]
; CHECK-NEXT:    [[BOUND1:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP2]]
; CHECK-NEXT:    [[FOUND_CONFLICT:%.*]] = and i1 [[BOUND0]], [[BOUND1]]
; CHECK-NEXT:    [[BOUND015:%.*]] = icmp ult i8* [[A1]], [[SCEVGEP78]]
; CHECK-NEXT:    [[BOUND116:%.*]] = icmp ult i8* [[E6]], [[SCEVGEP2]]
; CHECK-NEXT:    [[FOUND_CONFLICT17:%.*]] = and i1 [[BOUND015]], [[BOUND116]]
; CHECK-NEXT:    [[CONFLICT_RDX:%.*]] = or i1 [[FOUND_CONFLICT]], [[FOUND_CONFLICT17]]
; CHECK-NEXT:    [[BOUND018:%.*]] = icmp ult i8* [[A1]], [[SCEVGEP1011]]
; CHECK-NEXT:    [[BOUND119:%.*]] = icmp ult i8* [[C9]], [[SCEVGEP2]]
; CHECK-NEXT:    [[FOUND_CONFLICT20:%.*]] = and i1 [[BOUND018]], [[BOUND119]]
; CHECK-NEXT:    [[CONFLICT_RDX21:%.*]] = or i1 [[CONFLICT_RDX]], [[FOUND_CONFLICT20]]
; CHECK-NEXT:    [[BOUND022:%.*]] = icmp ult i8* [[A1]], [[SCEVGEP1314]]
; CHECK-NEXT:    [[BOUND123:%.*]] = icmp ult i8* [[D12]], [[SCEVGEP2]]
; CHECK-NEXT:    [[FOUND_CONFLICT24:%.*]] = and i1 [[BOUND022]], [[BOUND123]]
; CHECK-NEXT:    [[CONFLICT_RDX25:%.*]] = or i1 [[CONFLICT_RDX21]], [[FOUND_CONFLICT24]]
; CHECK-NEXT:    [[BOUND026:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP78]]
; CHECK-NEXT:    [[BOUND127:%.*]] = icmp ult i8* [[E6]], [[SCEVGEP45]]
; CHECK-NEXT:    [[FOUND_CONFLICT28:%.*]] = and i1 [[BOUND026]], [[BOUND127]]
; CHECK-NEXT:    [[CONFLICT_RDX29:%.*]] = or i1 [[CONFLICT_RDX25]], [[FOUND_CONFLICT28]]
; CHECK-NEXT:    [[BOUND030:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP1011]]
; CHECK-NEXT:    [[BOUND131:%.*]] = icmp ult i8* [[C9]], [[SCEVGEP45]]
; CHECK-NEXT:    [[FOUND_CONFLICT32:%.*]] = and i1 [[BOUND030]], [[BOUND131]]
; CHECK-NEXT:    [[CONFLICT_RDX33:%.*]] = or i1 [[CONFLICT_RDX29]], [[FOUND_CONFLICT32]]
; CHECK-NEXT:    [[BOUND034:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP1314]]
; CHECK-NEXT:    [[BOUND135:%.*]] = icmp ult i8* [[D12]], [[SCEVGEP45]]
; CHECK-NEXT:    [[FOUND_CONFLICT36:%.*]] = and i1 [[BOUND034]], [[BOUND135]]
; CHECK-NEXT:    [[CONFLICT_RDX37:%.*]] = or i1 [[CONFLICT_RDX33]], [[FOUND_CONFLICT36]]
; CHECK-NEXT:    [[BOUND038:%.*]] = icmp ult i8* [[E6]], [[SCEVGEP1011]]
; CHECK-NEXT:    [[BOUND139:%.*]] = icmp ult i8* [[C9]], [[SCEVGEP78]]
; CHECK-NEXT:    [[FOUND_CONFLICT40:%.*]] = and i1 [[BOUND038]], [[BOUND139]]
; CHECK-NEXT:    [[CONFLICT_RDX41:%.*]] = or i1 [[CONFLICT_RDX37]], [[FOUND_CONFLICT40]]
; CHECK-NEXT:    [[BOUND042:%.*]] = icmp ult i8* [[E6]], [[SCEVGEP1314]]
; CHECK-NEXT:    [[BOUND143:%.*]] = icmp ult i8* [[D12]], [[SCEVGEP78]]
; CHECK-NEXT:    [[FOUND_CONFLICT44:%.*]] = and i1 [[BOUND042]], [[BOUND143]]
; CHECK-NEXT:    [[CONFLICT_RDX45:%.*]] = or i1 [[CONFLICT_RDX41]], [[FOUND_CONFLICT44]]
; CHECK-NEXT:    br i1 [[CONFLICT_RDX45]], label [[SCALAR_PH]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr inbounds double, double* [[A]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[TMP1]], i32 0
; CHECK-NEXT:    [[TMP3:%.*]] = bitcast double* [[TMP2]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP3]], align 4, !alias.scope !0, !noalias !3
; CHECK-NEXT:    [[TMP4:%.*]] = bitcast double* [[TMP2]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> zeroinitializer, <2 x double>* [[TMP4]], align 4, !alias.scope !0, !noalias !3
; CHECK-NEXT:    [[TMP5:%.*]] = call <2 x double> @llvm.pow.v2f64(<2 x double> [[WIDE_LOAD]], <2 x double> <double 2.000000e+00, double 2.000000e+00>)
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[B]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr inbounds double, double* [[TMP6]], i32 0
; CHECK-NEXT:    [[TMP8:%.*]] = bitcast double* [[TMP7]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD46:%.*]] = load <2 x double>, <2 x double>* [[TMP8]], align 4, !alias.scope !8, !noalias !9
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @llvm.pow.v2f64(<2 x double> [[WIDE_LOAD46]], <2 x double> [[TMP5]])
; CHECK-NEXT:    [[TMP10:%.*]] = bitcast double* [[TMP7]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> zeroinitializer, <2 x double>* [[TMP10]], align 4, !alias.scope !8, !noalias !9
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[C]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP11]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD47:%.*]] = load <2 x double>, <2 x double>* [[TMP13]], align 4, !alias.scope !10
; CHECK-NEXT:    [[TMP14:%.*]] = call <2 x double> @llvm.pow.v2f64(<2 x double> [[TMP5]], <2 x double> [[WIDE_LOAD47]])
; CHECK-NEXT:    [[TMP15:%.*]] = getelementptr inbounds double, double* [[D]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP16:%.*]] = getelementptr inbounds double, double* [[TMP15]], i32 0
; CHECK-NEXT:    [[TMP17:%.*]] = bitcast double* [[TMP16]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD48:%.*]] = load <2 x double>, <2 x double>* [[TMP17]], align 8, !alias.scope !11
; CHECK-NEXT:    [[TMP18:%.*]] = call <2 x double> @llvm.pow.v2f64(<2 x double> [[TMP14]], <2 x double> [[WIDE_LOAD48]])
; CHECK-NEXT:    [[TMP19:%.*]] = call <2 x double> @llvm.pow.v2f64(<2 x double> [[TMP18]], <2 x double> [[TMP14]])
; CHECK-NEXT:    [[TMP20:%.*]] = fmul <2 x double> <double 2.000000e+00, double 2.000000e+00>, [[TMP19]]
; CHECK-NEXT:    [[TMP21:%.*]] = fmul <2 x double> [[TMP20]], <double 2.000000e+00, double 2.000000e+00>
; CHECK-NEXT:    [[TMP22:%.*]] = fmul <2 x double> [[TMP20]], [[TMP21]]
; CHECK-NEXT:    [[TMP23:%.*]] = getelementptr inbounds double, double* [[E]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP24:%.*]] = getelementptr inbounds double, double* [[TMP23]], i32 0
; CHECK-NEXT:    [[TMP25:%.*]] = bitcast double* [[TMP24]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP22]], <2 x double>* [[TMP25]], align 4, !alias.scope !12, !noalias !13
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP26:%.*]] = icmp eq i64 [[INDEX_NEXT]], 16
; CHECK-NEXT:    br i1 [[TMP26]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP14:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 16, 16
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 16, [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ], [ 0, [[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-NEXT:    [[GEP_A:%.*]] = getelementptr inbounds double, double* [[A]], i64 [[IV]]
; CHECK-NEXT:    [[L_A:%.*]] = load double, double* [[GEP_A]], align 4
; CHECK-NEXT:    store double 0.000000e+00, double* [[GEP_A]], align 4
; CHECK-NEXT:    [[P_1:%.*]] = call double @llvm.pow.f64(double [[L_A]], double 2.000000e+00)
; CHECK-NEXT:    [[GEP_B:%.*]] = getelementptr inbounds double, double* [[B]], i64 [[IV]]
; CHECK-NEXT:    [[L_B:%.*]] = load double, double* [[GEP_B]], align 4
; CHECK-NEXT:    [[P_2:%.*]] = call double @llvm.pow.f64(double [[L_B]], double [[P_1]])
; CHECK-NEXT:    store double 0.000000e+00, double* [[GEP_B]], align 4
; CHECK-NEXT:    [[GEP_C:%.*]] = getelementptr inbounds double, double* [[C]], i64 [[IV]]
; CHECK-NEXT:    [[L_C:%.*]] = load double, double* [[GEP_C]], align 4
; CHECK-NEXT:    [[P_3:%.*]] = call double @llvm.pow.f64(double [[P_1]], double [[L_C]])
; CHECK-NEXT:    [[GEP_D:%.*]] = getelementptr inbounds double, double* [[D]], i64 [[IV]]
; CHECK-NEXT:    [[L_D:%.*]] = load double, double* [[GEP_D]], align 8
; CHECK-NEXT:    [[P_4:%.*]] = call double @llvm.pow.f64(double [[P_3]], double [[L_D]])
; CHECK-NEXT:    [[P_5:%.*]] = call double @llvm.pow.f64(double [[P_4]], double [[P_3]])
; CHECK-NEXT:    [[MUL:%.*]] = fmul double 2.000000e+00, [[P_5]]
; CHECK-NEXT:    [[MUL_2:%.*]] = fmul double [[MUL]], 2.000000e+00
; CHECK-NEXT:    [[MUL_3:%.*]] = fmul double [[MUL]], [[MUL_2]]
; CHECK-NEXT:    [[GEP_E:%.*]] = getelementptr inbounds double, double* [[E]], i64 [[IV]]
; CHECK-NEXT:    store double [[MUL_3]], double* [[GEP_E]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], 16
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP16:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;
entry:
  br label %for.body

for.body:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %for.body ]
  %gep.A = getelementptr inbounds double, double* %A, i64 %iv
  %l.A = load double, double* %gep.A, align 4
  store double 0.0, double* %gep.A, align 4
  %p.1 = call double @llvm.pow.f64(double %l.A, double 2.0)

  %gep.B = getelementptr inbounds double, double* %B, i64 %iv
  %l.B = load double, double* %gep.B, align 4
  %p.2 = call double @llvm.pow.f64(double %l.B, double %p.1)
  store double 0.0, double* %gep.B, align 4

  %gep.C = getelementptr inbounds double, double* %C, i64 %iv
  %l.C = load double, double* %gep.C, align 4
  %p.3 = call double @llvm.pow.f64(double %p.1, double %l.C)

  %gep.D = getelementptr inbounds double, double* %D, i64 %iv
  %l.D = load double, double* %gep.D
  %p.4 = call double @llvm.pow.f64(double %p.3, double %l.D)
  %p.5 = call double @llvm.pow.f64(double %p.4, double %p.3)
  %mul = fmul double 2.0, %p.5
  %mul.2 = fmul double %mul, 2.0
  %mul.3 = fmul double %mul, %mul.2
  %gep.E = getelementptr inbounds double, double* %E, i64 %iv
  store double %mul.3, double* %gep.E, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, 16
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}
