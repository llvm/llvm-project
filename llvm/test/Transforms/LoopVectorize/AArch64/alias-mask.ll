; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --filter-out-after "^scalar.ph:" --version 5
; RUN: opt -S -mtriple=aarch64-unknown-linux-gnu -mattr=+sve2 -passes=loop-vectorize -force-partial-aliasing-vectorization -prefer-predicate-over-epilogue=predicate-dont-vectorize %s | FileCheck %s --check-prefix=CHECK-TF

define void @alias_mask(ptr noalias %a, ptr %b, ptr %c, i64 %n) {
; CHECK-TF-LABEL: define void @alias_mask(
; CHECK-TF-SAME: ptr noalias [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[N:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[B2:%.*]] = ptrtoaddr ptr [[B]] to i64
; CHECK-TF-NEXT:    [[C1:%.*]] = ptrtoaddr ptr [[C]] to i64
; CHECK-TF-NEXT:    [[CMP11:%.*]] = icmp sgt i64 [[N]], 0
; CHECK-TF-NEXT:    br i1 [[CMP11]], label %[[FOR_BODY_PREHEADER:.*]], [[EXIT:label %.*]]
; CHECK-TF:       [[FOR_BODY_PREHEADER]]:
; CHECK-TF-NEXT:    br label %[[VECTOR_MIN_VF_CHECK:.*]]
; CHECK-TF:       [[VECTOR_MIN_VF_CHECK]]:
; CHECK-TF-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[B2]] to ptr
; CHECK-TF-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[C1]] to ptr
; CHECK-TF-NEXT:    [[ALIAS_LANE_MASK:%.*]] = call <vscale x 16 x i1> @llvm.loop.dependence.war.mask.nxv16i1(ptr [[TMP1]], ptr [[TMP2]], i64 1)
; CHECK-TF-NEXT:    [[TMP4:%.*]] = zext <vscale x 16 x i1> [[ALIAS_LANE_MASK]] to <vscale x 16 x i32>
; CHECK-TF-NEXT:    [[TMP5:%.*]] = call i32 @llvm.vector.reduce.add.nxv16i32(<vscale x 16 x i32> [[TMP4]])
; CHECK-TF-NEXT:    [[NUM_ACTIVE_LANES:%.*]] = zext i32 [[TMP5]] to i64
; CHECK-TF-NEXT:    [[CMP_VF:%.*]] = icmp ult i64 [[NUM_ACTIVE_LANES]], 2
; CHECK-TF-NEXT:    br i1 [[CMP_VF]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP7:%.*]] = sub i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP8:%.*]] = icmp ugt i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP9:%.*]] = select i1 [[TMP8]], i64 [[TMP7]], i64 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 [[N]])
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 16 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[TMP10:%.*]] = and <vscale x 16 x i1> [[ACTIVE_LANE_MASK]], [[ALIAS_LANE_MASK]]
; CHECK-TF-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i8, ptr [[A]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP11]], <vscale x 16 x i1> [[TMP10]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[TMP12:%.*]] = getelementptr inbounds i8, ptr [[B]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD3:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP12]], <vscale x 16 x i1> [[TMP10]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[TMP13:%.*]] = select <vscale x 16 x i1> [[TMP10]], <vscale x 16 x i8> [[WIDE_MASKED_LOAD]], <vscale x 16 x i8> splat (i8 1)
; CHECK-TF-NEXT:    [[TMP14:%.*]] = sdiv <vscale x 16 x i8> [[WIDE_MASKED_LOAD3]], [[TMP13]]
; CHECK-TF-NEXT:    [[TMP15:%.*]] = getelementptr inbounds i8, ptr [[C]], i64 [[INDEX]]
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> [[TMP14]], ptr align 1 [[TMP15]], <vscale x 16 x i1> [[TMP10]])
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 [[INDEX]], i64 [[TMP9]])
; CHECK-TF-NEXT:    [[TMP16:%.*]] = extractelement <vscale x 16 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP17:%.*]] = xor i1 [[TMP16]], true
; CHECK-TF-NEXT:    br i1 [[TMP17]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    br [[EXIT_LOOPEXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;

entry:
  %cmp11 = icmp sgt i64 %n, 0
  br i1 %cmp11, label %for.body, label %exit

for.body:                                         ; preds = %for.body.preheader, %for.body
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %for.body ]
  %gep.a = getelementptr inbounds i8, ptr %a, i64 %iv
  %load.a = load i8, ptr %gep.a, align 1
  %gep.b = getelementptr inbounds i8, ptr %b, i64 %iv
  %load.b = load i8, ptr %gep.b, align 1
  %div = sdiv i8 %load.b, %load.a
  %gep.c = getelementptr inbounds i8, ptr %c, i64 %iv
  store i8 %div, ptr %gep.c, align 1
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.next, %n
  br i1 %exitcond.not, label %exit, label %for.body

exit:                                 ; preds = %for.body, %entry
  ret void
}

; Note: This test could emit a `llvm.loop.dependence.raw` mask to avoid creating
; a dependency between the store and the load, but it is not necessary for
; correctness.
define i32 @alias_mask_read_after_write(ptr noalias %a, ptr %b, ptr %c, i64 %n) {
; CHECK-TF-LABEL: define i32 @alias_mask_read_after_write(
; CHECK-TF-SAME: ptr noalias [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[N:%.*]]) #[[ATTR0]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[C2:%.*]] = ptrtoaddr ptr [[C]] to i64
; CHECK-TF-NEXT:    [[B1:%.*]] = ptrtoaddr ptr [[B]] to i64
; CHECK-TF-NEXT:    [[CMP19:%.*]] = icmp sgt i64 [[N]], 0
; CHECK-TF-NEXT:    br i1 [[CMP19]], label %[[FOR_BODY_PREHEADER:.*]], [[EXIT:label %.*]]
; CHECK-TF:       [[FOR_BODY_PREHEADER]]:
; CHECK-TF-NEXT:    br label %[[VECTOR_MIN_VF_CHECK:.*]]
; CHECK-TF:       [[VECTOR_MIN_VF_CHECK]]:
; CHECK-TF-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[C2]] to ptr
; CHECK-TF-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[B1]] to ptr
; CHECK-TF-NEXT:    [[ALIAS_LANE_MASK:%.*]] = call <vscale x 4 x i1> @llvm.loop.dependence.war.mask.nxv4i1(ptr [[TMP1]], ptr [[TMP2]], i64 4)
; CHECK-TF-NEXT:    [[TMP4:%.*]] = zext <vscale x 4 x i1> [[ALIAS_LANE_MASK]] to <vscale x 4 x i32>
; CHECK-TF-NEXT:    [[TMP5:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[TMP4]])
; CHECK-TF-NEXT:    [[NUM_ACTIVE_LANES:%.*]] = zext i32 [[TMP5]] to i64
; CHECK-TF-NEXT:    [[CMP_VF:%.*]] = icmp ult i64 [[NUM_ACTIVE_LANES]], 2
; CHECK-TF-NEXT:    br i1 [[CMP_VF]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP7:%.*]] = sub i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP8:%.*]] = icmp ugt i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP9:%.*]] = select i1 [[TMP8]], i64 [[TMP7]], i64 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 [[N]])
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 4 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP16:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[TMP10:%.*]] = and <vscale x 4 x i1> [[ACTIVE_LANE_MASK]], [[ALIAS_LANE_MASK]]
; CHECK-TF-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i32, ptr [[A]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr align 2 [[TMP11]], <vscale x 4 x i1> [[TMP10]], <vscale x 4 x i32> poison)
; CHECK-TF-NEXT:    [[TMP12:%.*]] = getelementptr inbounds i32, ptr [[C]], i64 [[INDEX]]
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv4i32.p0(<vscale x 4 x i32> [[WIDE_MASKED_LOAD]], ptr align 2 [[TMP12]], <vscale x 4 x i1> [[TMP10]])
; CHECK-TF-NEXT:    [[TMP13:%.*]] = getelementptr inbounds i32, ptr [[B]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD3:%.*]] = call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr align 2 [[TMP13]], <vscale x 4 x i1> [[TMP10]], <vscale x 4 x i32> poison)
; CHECK-TF-NEXT:    [[TMP14:%.*]] = add <vscale x 4 x i32> [[WIDE_MASKED_LOAD]], [[VEC_PHI]]
; CHECK-TF-NEXT:    [[TMP15:%.*]] = add <vscale x 4 x i32> [[TMP14]], [[WIDE_MASKED_LOAD3]]
; CHECK-TF-NEXT:    [[TMP16]] = select <vscale x 4 x i1> [[TMP10]], <vscale x 4 x i32> [[TMP15]], <vscale x 4 x i32> [[VEC_PHI]]
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 [[INDEX]], i64 [[TMP9]])
; CHECK-TF-NEXT:    [[TMP17:%.*]] = extractelement <vscale x 4 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP18:%.*]] = xor i1 [[TMP17]], true
; CHECK-TF-NEXT:    br i1 [[TMP18]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    [[TMP19:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[TMP16]])
; CHECK-TF-NEXT:    br [[EXIT_LOOPEXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;


entry:
  %cmp19 = icmp sgt i64 %n, 0
  br i1 %cmp19, label %for.body, label %exit

for.body:                                         ; preds = %entry, %for.body
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %for.body ]
  %accum = phi i32 [ 0, %entry ], [ %add2, %for.body ]
  %gep.a = getelementptr inbounds i32, ptr %a, i64 %iv
  %load.a = load i32, ptr %gep.a, align 2
  %gep.c = getelementptr inbounds i32, ptr %c, i64 %iv
  store i32 %load.a, ptr %gep.c, align 2
  %gep.b = getelementptr inbounds i32, ptr %b, i64 %iv
  %load.b = load i32, ptr %gep.b, align 2
  %add = add i32 %load.a, %accum
  %add2 = add i32 %add, %load.b
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.next, %n
  br i1 %exitcond.not, label %exit, label %for.body

exit:                        ; preds = %entry, %for.body
  %result = phi i32 [ 0, %entry ], [ %add2, %for.body ]
  ret i32 %result
}

define void @alias_mask_multiple(ptr %a, ptr %b, ptr %c, i64 %n) {
; CHECK-TF-LABEL: define void @alias_mask_multiple(
; CHECK-TF-SAME: ptr [[A:%.*]], ptr [[B:%.*]], ptr [[C:%.*]], i64 [[N:%.*]]) #[[ATTR0]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[A7:%.*]] = ptrtoaddr ptr [[A]] to i64
; CHECK-TF-NEXT:    [[B3:%.*]] = ptrtoaddr ptr [[B]] to i64
; CHECK-TF-NEXT:    [[C1:%.*]] = ptrtoaddr ptr [[C]] to i64
; CHECK-TF-NEXT:    [[CMP11:%.*]] = icmp sgt i64 [[N]], 0
; CHECK-TF-NEXT:    br i1 [[CMP11]], label %[[FOR_BODY_PREHEADER:.*]], [[EXIT:label %.*]]
; CHECK-TF:       [[FOR_BODY_PREHEADER]]:
; CHECK-TF-NEXT:    br label %[[VECTOR_MIN_VF_CHECK:.*]]
; CHECK-TF:       [[VECTOR_MIN_VF_CHECK]]:
; CHECK-TF-NEXT:    [[TMP2:%.*]] = inttoptr i64 [[A7]] to ptr
; CHECK-TF-NEXT:    [[TMP3:%.*]] = inttoptr i64 [[C1]] to ptr
; CHECK-TF-NEXT:    [[ALIAS_LANE_MASK0:%.*]] = call <vscale x 16 x i1> @llvm.loop.dependence.war.mask.nxv16i1(ptr [[TMP2]], ptr [[TMP3]], i64 1)
; CHECK-TF-NEXT:    [[TMP5:%.*]] = inttoptr i64 [[B3]] to ptr
; CHECK-TF-NEXT:    [[TMP6:%.*]] = inttoptr i64 [[C1]] to ptr
; CHECK-TF-NEXT:    [[ALIAS_LANE_MASK1:%.*]] = call <vscale x 16 x i1> @llvm.loop.dependence.war.mask.nxv16i1(ptr [[TMP5]], ptr [[TMP6]], i64 1)
; CHECK-TF-NEXT:    [[TMP8:%.*]] = and <vscale x 16 x i1> [[ALIAS_LANE_MASK0]], [[ALIAS_LANE_MASK1]]
; CHECK-TF-NEXT:    [[TMP7:%.*]] = zext <vscale x 16 x i1> [[TMP8]] to <vscale x 16 x i32>
; CHECK-TF-NEXT:    [[TMP9:%.*]] = call i32 @llvm.vector.reduce.add.nxv16i32(<vscale x 16 x i32> [[TMP7]])
; CHECK-TF-NEXT:    [[NUM_ACTIVE_LANES:%.*]] = zext i32 [[TMP9]] to i64
; CHECK-TF-NEXT:    [[CMP_VF:%.*]] = icmp ult i64 [[NUM_ACTIVE_LANES]], 2
; CHECK-TF-NEXT:    br i1 [[CMP_VF]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP14:%.*]] = sub i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP15:%.*]] = icmp ugt i64 [[N]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[TMP16:%.*]] = select i1 [[TMP15]], i64 [[TMP14]], i64 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 [[N]])
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 16 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[TMP17:%.*]] = and <vscale x 16 x i1> [[ACTIVE_LANE_MASK]], [[TMP8]]
; CHECK-TF-NEXT:    [[TMP18:%.*]] = getelementptr inbounds i8, ptr [[A]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP18]], <vscale x 16 x i1> [[TMP17]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[TMP19:%.*]] = getelementptr inbounds i8, ptr [[B]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD8:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP19]], <vscale x 16 x i1> [[TMP17]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[TMP20:%.*]] = add <vscale x 16 x i8> [[WIDE_MASKED_LOAD8]], [[WIDE_MASKED_LOAD]]
; CHECK-TF-NEXT:    [[TMP21:%.*]] = getelementptr inbounds i8, ptr [[C]], i64 [[INDEX]]
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> [[TMP20]], ptr align 1 [[TMP21]], <vscale x 16 x i1> [[TMP17]])
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[NUM_ACTIVE_LANES]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 [[INDEX]], i64 [[TMP16]])
; CHECK-TF-NEXT:    [[TMP22:%.*]] = extractelement <vscale x 16 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP23:%.*]] = xor i1 [[TMP22]], true
; CHECK-TF-NEXT:    br i1 [[TMP23]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    br [[EXIT_LOOPEXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;

entry:
  %cmp11 = icmp sgt i64 %n, 0
  br i1 %cmp11, label %for.body, label %exit

for.body:                                         ; preds = %for.body.preheader, %for.body
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %for.body ]
  %gep.a = getelementptr inbounds i8, ptr %a, i64 %iv
  %load.a = load i8, ptr %gep.a, align 1
  %gep.b = getelementptr inbounds i8, ptr %b, i64 %iv
  %load.b = load i8, ptr %gep.b, align 1
  %add = add i8 %load.b, %load.a
  %gep.c = getelementptr inbounds i8, ptr %c, i64 %iv
  store i8 %add, ptr %gep.c, align 1
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.next, %n
  br i1 %exitcond.not, label %exit, label %for.body

exit:                                 ; preds = %for.body, %entry
  ret void
}

; Checks using a scalar outside the loop, with requires extracting the last
; active element.
define i8 @alias_masking_exit_value(ptr %ptrA, ptr %ptrB) {
; CHECK-TF-LABEL: define i8 @alias_masking_exit_value(
; CHECK-TF-SAME: ptr [[PTRA:%.*]], ptr [[PTRB:%.*]]) #[[ATTR0]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[PTRA2:%.*]] = ptrtoaddr ptr [[PTRA]] to i64
; CHECK-TF-NEXT:    [[PTRB1:%.*]] = ptrtoaddr ptr [[PTRB]] to i64
; CHECK-TF-NEXT:    br label %[[VECTOR_MIN_VF_CHECK:.*]]
; CHECK-TF:       [[VECTOR_MIN_VF_CHECK]]:
; CHECK-TF-NEXT:    [[TMP0:%.*]] = inttoptr i64 [[PTRA2]] to ptr
; CHECK-TF-NEXT:    [[TMP1:%.*]] = inttoptr i64 [[PTRB1]] to ptr
; CHECK-TF-NEXT:    [[ALIAS_LANE_MASK:%.*]] = call <vscale x 16 x i1> @llvm.loop.dependence.war.mask.nxv16i1(ptr [[TMP0]], ptr [[TMP1]], i64 1)
; CHECK-TF-NEXT:    [[TMP3:%.*]] = zext <vscale x 16 x i1> [[ALIAS_LANE_MASK]] to <vscale x 16 x i32>
; CHECK-TF-NEXT:    [[TMP4:%.*]] = call i32 @llvm.vector.reduce.add.nxv16i32(<vscale x 16 x i32> [[TMP3]])
; CHECK-TF-NEXT:    [[NUM_ACTIVE_LANES:%.*]] = zext i32 [[TMP4]] to i64
; CHECK-TF-NEXT:    [[TMP5:%.*]] = trunc i64 [[NUM_ACTIVE_LANES]] to i32
; CHECK-TF-NEXT:    [[CMP_VF:%.*]] = icmp ult i32 [[TMP5]], 2
; CHECK-TF-NEXT:    br i1 [[CMP_VF]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP8:%.*]] = sub i32 1000, [[TMP5]]
; CHECK-TF-NEXT:    [[TMP9:%.*]] = icmp ugt i32 1000, [[TMP5]]
; CHECK-TF-NEXT:    [[TMP10:%.*]] = select i1 [[TMP9]], i32 [[TMP8]], i32 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i32(i32 0, i32 1000)
; CHECK-TF-NEXT:    [[TMP11:%.*]] = call <vscale x 16 x i8> @llvm.stepvector.nxv16i8()
; CHECK-TF-NEXT:    [[TMP12:%.*]] = trunc i32 [[TMP5]] to i8
; CHECK-TF-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 16 x i8> poison, i8 [[TMP12]], i64 0
; CHECK-TF-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 16 x i8> [[BROADCAST_SPLATINSERT]], <vscale x 16 x i8> poison, <vscale x 16 x i32> zeroinitializer
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i32 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 16 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[VEC_IND:%.*]] = phi <vscale x 16 x i8> [ [[TMP11]], %[[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[TMP13:%.*]] = and <vscale x 16 x i1> [[ACTIVE_LANE_MASK]], [[ALIAS_LANE_MASK]]
; CHECK-TF-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i8, ptr [[PTRA]], i32 [[INDEX]]
; CHECK-TF-NEXT:    [[TMP15:%.*]] = getelementptr inbounds i8, ptr [[PTRB]], i32 [[INDEX]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP14]], <vscale x 16 x i1> [[TMP13]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[TMP16:%.*]] = add <vscale x 16 x i8> [[VEC_IND]], [[WIDE_MASKED_LOAD]]
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> [[TMP16]], ptr align 1 [[TMP15]], <vscale x 16 x i1> [[TMP13]])
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i32 [[INDEX]], [[TMP5]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i32(i32 [[INDEX]], i32 [[TMP10]])
; CHECK-TF-NEXT:    [[TMP17:%.*]] = extractelement <vscale x 16 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP18:%.*]] = xor i1 [[TMP17]], true
; CHECK-TF-NEXT:    [[VEC_IND_NEXT]] = add <vscale x 16 x i8> [[VEC_IND]], [[BROADCAST_SPLAT]]
; CHECK-TF-NEXT:    br i1 [[TMP18]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    [[TMP19:%.*]] = xor <vscale x 16 x i1> [[TMP13]], splat (i1 true)
; CHECK-TF-NEXT:    [[FIRST_INACTIVE_LANE:%.*]] = call i64 @llvm.experimental.cttz.elts.i64.nxv16i1(<vscale x 16 x i1> [[TMP19]], i1 false)
; CHECK-TF-NEXT:    [[LAST_ACTIVE_LANE:%.*]] = sub i64 [[FIRST_INACTIVE_LANE]], 1
; CHECK-TF-NEXT:    [[TMP20:%.*]] = extractelement <vscale x 16 x i8> [[TMP16]], i64 [[LAST_ACTIVE_LANE]]
; CHECK-TF-NEXT:    br [[EXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;
entry:
  br label %loop

loop:
  %iv = phi i32 [ 0, %entry ], [ %iv.next, %loop ]
  %gepA = getelementptr inbounds i8, ptr %ptrA, i32 %iv
  %gepB = getelementptr inbounds i8, ptr %ptrB, i32 %iv
  %loadA = load i8, ptr %gepA
  %iv.trunc = trunc i32 %iv to i8
  %add = add i8 %iv.trunc, %loadA
  store i8 %add, ptr %gepB
  %iv.next = add nsw i32 %iv, 1
  %ec = icmp eq i32 %iv.next, 1000
  br i1 %ec, label %exit, label %loop

exit:
  %exit.value = phi i8 [ %add, %loop ]
  ret i8 %exit.value
}

; Unsupported: Reversing the alias mask is not correct.
define void @alias_mask_reverse_iterate(ptr noalias %ptrA, ptr %ptrB, ptr %ptrC, i64 %n) {
; CHECK-TF-LABEL: define void @alias_mask_reverse_iterate(
; CHECK-TF-SAME: ptr noalias [[PTRA:%.*]], ptr [[PTRB:%.*]], ptr [[PTRC:%.*]], i64 [[N:%.*]]) #[[ATTR0]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[PTRC2:%.*]] = ptrtoaddr ptr [[PTRC]] to i64
; CHECK-TF-NEXT:    [[PTRB1:%.*]] = ptrtoaddr ptr [[PTRB]] to i64
; CHECK-TF-NEXT:    [[IV_START:%.*]] = add i64 [[N]], -1
; CHECK-TF-NEXT:    br label %[[VECTOR_MEMCHECK:.*]]
; CHECK-TF:       [[VECTOR_MEMCHECK]]:
; CHECK-TF-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-TF-NEXT:    [[TMP1:%.*]] = mul nuw i64 [[TMP0]], 16
; CHECK-TF-NEXT:    [[TMP2:%.*]] = sub i64 [[PTRB1]], [[PTRC2]]
; CHECK-TF-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP2]], [[TMP1]]
; CHECK-TF-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-TF-NEXT:    [[TMP6:%.*]] = shl nuw i64 [[TMP5]], 4
; CHECK-TF-NEXT:    [[TMP7:%.*]] = sub i64 [[IV_START]], [[TMP6]]
; CHECK-TF-NEXT:    [[TMP8:%.*]] = icmp ugt i64 [[IV_START]], [[TMP6]]
; CHECK-TF-NEXT:    [[TMP9:%.*]] = select i1 [[TMP8]], i64 [[TMP7]], i64 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 0, i64 [[IV_START]])
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 16 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[OFFSET_IDX:%.*]] = sub i64 [[IV_START]], [[INDEX]]
; CHECK-TF-NEXT:    [[TMP10:%.*]] = getelementptr inbounds i8, ptr [[PTRA]], i64 [[OFFSET_IDX]]
; CHECK-TF-NEXT:    [[TMP11:%.*]] = sub nuw nsw i64 [[TMP6]], 1
; CHECK-TF-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP11]], -1
; CHECK-TF-NEXT:    [[TMP13:%.*]] = getelementptr i8, ptr [[TMP10]], i64 [[TMP12]]
; CHECK-TF-NEXT:    [[REVERSE:%.*]] = call <vscale x 16 x i1> @llvm.vector.reverse.nxv16i1(<vscale x 16 x i1> [[ACTIVE_LANE_MASK]])
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP13]], <vscale x 16 x i1> [[REVERSE]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[REVERSE3:%.*]] = call <vscale x 16 x i8> @llvm.vector.reverse.nxv16i8(<vscale x 16 x i8> [[WIDE_MASKED_LOAD]])
; CHECK-TF-NEXT:    [[TMP14:%.*]] = getelementptr inbounds i8, ptr [[PTRB]], i64 [[OFFSET_IDX]]
; CHECK-TF-NEXT:    [[TMP15:%.*]] = getelementptr i8, ptr [[TMP14]], i64 [[TMP12]]
; CHECK-TF-NEXT:    [[REVERSE4:%.*]] = call <vscale x 16 x i1> @llvm.vector.reverse.nxv16i1(<vscale x 16 x i1> [[ACTIVE_LANE_MASK]])
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD5:%.*]] = call <vscale x 16 x i8> @llvm.masked.load.nxv16i8.p0(ptr align 1 [[TMP15]], <vscale x 16 x i1> [[REVERSE4]], <vscale x 16 x i8> poison)
; CHECK-TF-NEXT:    [[REVERSE6:%.*]] = call <vscale x 16 x i8> @llvm.vector.reverse.nxv16i8(<vscale x 16 x i8> [[WIDE_MASKED_LOAD5]])
; CHECK-TF-NEXT:    [[TMP16:%.*]] = add <vscale x 16 x i8> [[REVERSE6]], [[REVERSE3]]
; CHECK-TF-NEXT:    [[TMP17:%.*]] = getelementptr inbounds i8, ptr [[PTRC]], i64 [[OFFSET_IDX]]
; CHECK-TF-NEXT:    [[TMP18:%.*]] = getelementptr i8, ptr [[TMP17]], i64 [[TMP12]]
; CHECK-TF-NEXT:    [[REVERSE7:%.*]] = call <vscale x 16 x i8> @llvm.vector.reverse.nxv16i8(<vscale x 16 x i8> [[TMP16]])
; CHECK-TF-NEXT:    [[REVERSE8:%.*]] = call <vscale x 16 x i1> @llvm.vector.reverse.nxv16i1(<vscale x 16 x i1> [[ACTIVE_LANE_MASK]])
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv16i8.p0(<vscale x 16 x i8> [[REVERSE7]], ptr align 1 [[TMP18]], <vscale x 16 x i1> [[REVERSE8]])
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP6]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 16 x i1> @llvm.get.active.lane.mask.nxv16i1.i64(i64 [[INDEX]], i64 [[TMP9]])
; CHECK-TF-NEXT:    [[TMP19:%.*]] = extractelement <vscale x 16 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP20:%.*]] = xor i1 [[TMP19]], true
; CHECK-TF-NEXT:    br i1 [[TMP20]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    br [[EXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;
entry:
  %iv.start = add nsw i64 %n, -1
  br label %loop

loop:
  %iv = phi i64 [ %iv.start, %entry ], [ %iv.next, %loop ]
  %gep.A = getelementptr inbounds i8, ptr %ptrA, i64 %iv
  %loadA = load i8, ptr %gep.A, align 1
  %gep.B = getelementptr inbounds i8, ptr %ptrB, i64 %iv
  %loadB = load i8, ptr %gep.B, align 1
  %add = add i8 %loadB, %loadA
  %gep.C = getelementptr inbounds i8, ptr %ptrC, i64 %iv
  store i8 %add, ptr %gep.C, align 1
  %iv.next = add nsw i64 %iv, -1
  %ec = icmp eq i64 %iv.next, 0
  br i1 %ec, label %exit, label %loop

exit:
  ret void
}

; Test taken from: scalable-first-order-recurrence.ll. Check we don't use
; an alias-mask with first-order recurrences, as we cannot handle the
; splice.right with the alias-mask/clamped VF yet.
define i32 @recurrence_1(ptr nocapture readonly %a, ptr nocapture %b, i32 %n) {
; CHECK-TF-LABEL: define i32 @recurrence_1(
; CHECK-TF-SAME: ptr readonly captures(none) [[A:%.*]], ptr captures(none) [[B:%.*]], i32 [[N:%.*]]) #[[ATTR0]] {
; CHECK-TF-NEXT:  [[ENTRY:.*:]]
; CHECK-TF-NEXT:    [[A2:%.*]] = ptrtoaddr ptr [[A]] to i64
; CHECK-TF-NEXT:    [[B1:%.*]] = ptrtoaddr ptr [[B]] to i64
; CHECK-TF-NEXT:    br label %[[FOR_PREHEADER:.*]]
; CHECK-TF:       [[FOR_PREHEADER]]:
; CHECK-TF-NEXT:    [[PRE_LOAD:%.*]] = load i32, ptr [[A]], align 4
; CHECK-TF-NEXT:    [[TMP0:%.*]] = add i32 [[N]], -1
; CHECK-TF-NEXT:    [[TMP1:%.*]] = zext i32 [[TMP0]] to i64
; CHECK-TF-NEXT:    [[TMP2:%.*]] = add nuw nsw i64 [[TMP1]], 1
; CHECK-TF-NEXT:    br label %[[VECTOR_MEMCHECK:.*]]
; CHECK-TF:       [[VECTOR_MEMCHECK]]:
; CHECK-TF-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-TF-NEXT:    [[TMP4:%.*]] = mul nuw i64 [[TMP3]], 4
; CHECK-TF-NEXT:    [[TMP5:%.*]] = mul i64 [[TMP4]], 4
; CHECK-TF-NEXT:    [[TMP6:%.*]] = add i64 [[B1]], -4
; CHECK-TF-NEXT:    [[TMP7:%.*]] = sub i64 [[TMP6]], [[A2]]
; CHECK-TF-NEXT:    [[DIFF_CHECK:%.*]] = icmp ult i64 [[TMP7]], [[TMP5]]
; CHECK-TF-NEXT:    br i1 [[DIFF_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK-TF:       [[VECTOR_PH]]:
; CHECK-TF-NEXT:    [[TMP10:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-TF-NEXT:    [[TMP11:%.*]] = shl nuw i64 [[TMP10]], 2
; CHECK-TF-NEXT:    [[TMP12:%.*]] = sub i64 [[TMP2]], [[TMP11]]
; CHECK-TF-NEXT:    [[TMP13:%.*]] = icmp ugt i64 [[TMP2]], [[TMP11]]
; CHECK-TF-NEXT:    [[TMP14:%.*]] = select i1 [[TMP13]], i64 [[TMP12]], i64 0
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_ENTRY:%.*]] = call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 0, i64 [[TMP2]])
; CHECK-TF-NEXT:    [[TMP15:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-TF-NEXT:    [[TMP16:%.*]] = mul nuw i32 [[TMP15]], 4
; CHECK-TF-NEXT:    [[TMP17:%.*]] = sub i32 [[TMP16]], 1
; CHECK-TF-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 4 x i32> poison, i32 [[PRE_LOAD]], i32 [[TMP17]]
; CHECK-TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK-TF:       [[VECTOR_BODY]]:
; CHECK-TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = phi <vscale x 4 x i1> [ [[ACTIVE_LANE_MASK_ENTRY]], %[[VECTOR_PH]] ], [ [[ACTIVE_LANE_MASK_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 4 x i32> [ [[VECTOR_RECUR_INIT]], %[[VECTOR_PH]] ], [ [[WIDE_MASKED_LOAD:%.*]], %[[VECTOR_BODY]] ]
; CHECK-TF-NEXT:    [[TMP18:%.*]] = add nuw nsw i64 [[INDEX]], 1
; CHECK-TF-NEXT:    [[TMP19:%.*]] = getelementptr inbounds i32, ptr [[A]], i64 [[TMP18]]
; CHECK-TF-NEXT:    [[WIDE_MASKED_LOAD]] = call <vscale x 4 x i32> @llvm.masked.load.nxv4i32.p0(ptr align 4 [[TMP19]], <vscale x 4 x i1> [[ACTIVE_LANE_MASK]], <vscale x 4 x i32> poison)
; CHECK-TF-NEXT:    [[TMP20:%.*]] = call <vscale x 4 x i32> @llvm.vector.splice.right.nxv4i32(<vscale x 4 x i32> [[VECTOR_RECUR]], <vscale x 4 x i32> [[WIDE_MASKED_LOAD]], i32 1)
; CHECK-TF-NEXT:    [[TMP21:%.*]] = getelementptr inbounds i32, ptr [[B]], i64 [[INDEX]]
; CHECK-TF-NEXT:    [[TMP22:%.*]] = add <vscale x 4 x i32> [[WIDE_MASKED_LOAD]], [[TMP20]]
; CHECK-TF-NEXT:    call void @llvm.masked.store.nxv4i32.p0(<vscale x 4 x i32> [[TMP22]], ptr align 4 [[TMP21]], <vscale x 4 x i1> [[ACTIVE_LANE_MASK]])
; CHECK-TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP11]]
; CHECK-TF-NEXT:    [[ACTIVE_LANE_MASK_NEXT]] = call <vscale x 4 x i1> @llvm.get.active.lane.mask.nxv4i1.i64(i64 [[INDEX]], i64 [[TMP14]])
; CHECK-TF-NEXT:    [[TMP23:%.*]] = extractelement <vscale x 4 x i1> [[ACTIVE_LANE_MASK_NEXT]], i32 0
; CHECK-TF-NEXT:    [[TMP24:%.*]] = xor i1 [[TMP23]], true
; CHECK-TF-NEXT:    br i1 [[TMP24]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; CHECK-TF:       [[MIDDLE_BLOCK]]:
; CHECK-TF-NEXT:    [[TMP25:%.*]] = xor <vscale x 4 x i1> [[ACTIVE_LANE_MASK]], splat (i1 true)
; CHECK-TF-NEXT:    [[FIRST_INACTIVE_LANE:%.*]] = call i64 @llvm.experimental.cttz.elts.i64.nxv4i1(<vscale x 4 x i1> [[TMP25]], i1 false)
; CHECK-TF-NEXT:    [[LAST_ACTIVE_LANE:%.*]] = sub i64 [[FIRST_INACTIVE_LANE]], 1
; CHECK-TF-NEXT:    [[TMP26:%.*]] = sub i64 [[LAST_ACTIVE_LANE]], 1
; CHECK-TF-NEXT:    [[TMP27:%.*]] = extractelement <vscale x 4 x i32> [[WIDE_MASKED_LOAD]], i64 [[TMP26]]
; CHECK-TF-NEXT:    [[TMP28:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-TF-NEXT:    [[TMP29:%.*]] = mul nuw i32 [[TMP28]], 4
; CHECK-TF-NEXT:    [[TMP30:%.*]] = sub i32 [[TMP29]], 1
; CHECK-TF-NEXT:    [[TMP31:%.*]] = extractelement <vscale x 4 x i32> [[VECTOR_RECUR]], i32 [[TMP30]]
; CHECK-TF-NEXT:    [[TMP32:%.*]] = icmp eq i64 [[LAST_ACTIVE_LANE]], 0
; CHECK-TF-NEXT:    [[TMP33:%.*]] = select i1 [[TMP32]], i32 [[TMP31]], i32 [[TMP27]]
; CHECK-TF-NEXT:    br [[FOR_EXIT:label %.*]]
; CHECK-TF:       [[SCALAR_PH]]:
;

entry:
  br label %for.preheader

for.preheader:
  %pre_load = load i32, ptr %a
  br label %scalar.body

scalar.body:
  %0 = phi i32 [ %pre_load, %for.preheader ], [ %1, %scalar.body ]
  %indvars.iv = phi i64 [ 0, %for.preheader ], [ %indvars.iv.next, %scalar.body ]
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %arrayidx32 = getelementptr inbounds i32, ptr %a, i64 %indvars.iv.next
  %1 = load i32, ptr %arrayidx32
  %arrayidx34 = getelementptr inbounds i32, ptr %b, i64 %indvars.iv
  %add35 = add i32 %1, %0
  store i32 %add35, ptr %arrayidx34
  %lftr.wideiv = trunc i64 %indvars.iv.next to i32
  %exitcond = icmp eq i32 %lftr.wideiv, %n
  br i1 %exitcond, label %for.exit, label %scalar.body

for.exit:
  ret i32 %0
}
;.
; CHECK-TF: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
; CHECK-TF: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK-TF: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
; CHECK-TF: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
; CHECK-TF: [[LOOP6]] = distinct !{[[LOOP6]], [[META1]], [[META2]]}
; CHECK-TF: [[LOOP8]] = distinct !{[[LOOP8]], [[META1]], [[META2]]}
; CHECK-TF: [[LOOP10]] = distinct !{[[LOOP10]], [[META1]], [[META2]]}
; CHECK-TF: [[LOOP12]] = distinct !{[[LOOP12]], [[META1]], [[META2]]}
;.
