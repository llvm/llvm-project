; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -p loop-vectorize -S -mtriple aarch64-linux-gnu -mattr=+sve -prefer-control-flow %s \
; RUN:     -prefer-predicate-over-epilogue=scalar-epilogue | FileCheck %s
; RUN: opt -p loop-vectorize -S -mtriple aarch64-linux-gnu -mattr=+sve -prefer-control-flow %s \
; RUN:     -prefer-predicate-over-epilogue=predicate-else-scalar-epilogue -force-tail-folding-style=data | FileCheck %s --check-prefixes=TF

define void @conditional_store(ptr %addr, i64 %N, i64 %M) {
; CHECK-LABEL: define void @conditional_store(
; CHECK-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP0]], 2
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], [[TMP1]]
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP3:%.*]] = shl nuw i64 [[TMP5]], 1
; CHECK-NEXT:    [[TMP4:%.*]] = shl nuw i64 [[TMP3]], 1
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], [[TMP4]]
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr i64, ptr [[TMP2]], i64 [[TMP3]]
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x i64>, ptr [[TMP2]], align 8
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <vscale x 2 x i64>, ptr [[TMP6]], align 8
; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD1]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP9:%.*]] = freeze <vscale x 2 x i1> [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = freeze <vscale x 2 x i1> [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <vscale x 2 x i1> [[TMP9]], [[TMP10]]
; CHECK-NEXT:    [[TMP12:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP11]])
; CHECK-NEXT:    br i1 [[TMP12]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; CHECK:       [[VECTOR_IF_BB]]:
; CHECK-NEXT:    [[TMP13:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD]], splat (i64 1)
; CHECK-NEXT:    [[TMP14:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD1]], splat (i64 1)
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP13]], ptr align 8 [[TMP2]], <vscale x 2 x i1> [[TMP7]])
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP14]], ptr align 8 [[TMP6]], <vscale x 2 x i1> [[TMP8]])
; CHECK-NEXT:    br label %[[LOOP_IF_SPLIT]]
; CHECK:       [[LOOP_IF_SPLIT]]:
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP4]]
; CHECK-NEXT:    [[TMP26:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP26]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], %[[LOOP_CONT:.*]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, ptr [[ADDR]], i64 [[INDVARS_IV]]
; CHECK-NEXT:    [[TMP27:%.*]] = load i64, ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[OR_COND_NOT:%.*]] = icmp eq i64 [[TMP27]], [[M]]
; CHECK-NEXT:    br i1 [[OR_COND_NOT]], label %[[LOOP_IF:.*]], label %[[LOOP_CONT]]
; CHECK:       [[LOOP_IF]]:
; CHECK-NEXT:    [[XOR:%.*]] = add i64 [[TMP27]], 1
; CHECK-NEXT:    store i64 [[XOR]], ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    br label %[[LOOP_CONT]]
; CHECK:       [[LOOP_CONT]]:
; CHECK-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-NEXT:    [[EXITCOND_NOT:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND_NOT]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
; TF-LABEL: define void @conditional_store(
; TF-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0:[0-9]+]] {
; TF-NEXT:  [[ENTRY:.*:]]
; TF-NEXT:    br label %[[VECTOR_PH:.*]]
; TF:       [[VECTOR_PH]]:
; TF-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; TF-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP3]], 1
; TF-NEXT:    [[TMP2:%.*]] = sub i64 [[TMP1]], 1
; TF-NEXT:    [[N_RND_UP:%.*]] = add i64 [[N]], [[TMP2]]
; TF-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N_RND_UP]], [[TMP1]]
; TF-NEXT:    [[N_VEC:%.*]] = sub i64 [[N_RND_UP]], [[N_MOD_VF]]
; TF-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; TF-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; TF:       [[VECTOR_BODY]]:
; TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 [[INDEX]], i64 [[N]])
; TF-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr align 8 [[TMP0]], <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i64> poison)
; TF-NEXT:    [[TMP4:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], [[BROADCAST_SPLAT]]
; TF-NEXT:    [[TMP5:%.*]] = select <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i1> [[TMP4]], <vscale x 2 x i1> zeroinitializer
; TF-NEXT:    [[TMP9:%.*]] = freeze <vscale x 2 x i1> [[TMP5]]
; TF-NEXT:    [[TMP7:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP9]])
; TF-NEXT:    br i1 [[TMP7]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; TF:       [[VECTOR_IF_BB]]:
; TF-NEXT:    [[TMP8:%.*]] = add <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], splat (i64 1)
; TF-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP8]], ptr align 8 [[TMP0]], <vscale x 2 x i1> [[TMP5]])
; TF-NEXT:    br label %[[LOOP_IF_SPLIT]]
; TF:       [[LOOP_IF_SPLIT]]:
; TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP1]]
; TF-NEXT:    [[TMP6:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; TF-NEXT:    br i1 [[TMP6]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; TF:       [[MIDDLE_BLOCK]]:
; TF-NEXT:    br label %[[EXIT:.*]]
; TF:       [[EXIT]]:
; TF-NEXT:    ret void
;
entry:
  br label %loop

loop:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop.cont ]
  %arrayidx = getelementptr inbounds i64, ptr %addr, i64 %iv
  %2 = load i64, ptr %arrayidx, align 8
  %or.cond.not = icmp eq i64 %2, %M
  br i1 %or.cond.not, label %loop.if, label %loop.cont

loop.if:
  %stored.val = add i64 %2, 1
  store i64 %stored.val, ptr %arrayidx, align 8
  br label %loop.cont

loop.cont:
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond.not = icmp eq i64 %iv.next, %N
  br i1 %exitcond.not, label %exit, label %loop

exit:
  ret void
}

; Test for control flow along with live-out calculated in the conditional block.
define i64 @conditional_liveout_and_store(ptr %addr, i64 %N, i64 %M) {
; CHECK-LABEL: define i64 @conditional_liveout_and_store(
; CHECK-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP5]], 2
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], [[TMP1]]
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP3:%.*]] = shl nuw i64 [[TMP2]], 1
; CHECK-NEXT:    [[TMP4:%.*]] = shl nuw i64 [[TMP3]], 1
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], [[TMP4]]
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PREDPHI:%.*]], %[[LOOP_IF_SPLIT]] ]
; CHECK-NEXT:    [[VEC_PHI1:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PREDPHI3:%.*]], %[[LOOP_IF_SPLIT]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr i64, ptr [[TMP0]], i64 [[TMP3]]
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x i64>, ptr [[TMP0]], align 8
; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <vscale x 2 x i64>, ptr [[TMP6]], align 8
; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD2]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP9:%.*]] = freeze <vscale x 2 x i1> [[TMP7]]
; CHECK-NEXT:    [[TMP10:%.*]] = freeze <vscale x 2 x i1> [[TMP8]]
; CHECK-NEXT:    [[TMP11:%.*]] = or <vscale x 2 x i1> [[TMP9]], [[TMP10]]
; CHECK-NEXT:    [[TMP17:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP11]])
; CHECK-NEXT:    br i1 [[TMP17]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; CHECK:       [[VECTOR_IF_BB]]:
; CHECK-NEXT:    [[TMP13:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD]], splat (i64 1)
; CHECK-NEXT:    [[TMP14:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD2]], splat (i64 1)
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP13]], ptr align 8 [[TMP0]], <vscale x 2 x i1> [[TMP7]])
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP14]], ptr align 8 [[TMP6]], <vscale x 2 x i1> [[TMP8]])
; CHECK-NEXT:    br label %[[LOOP_IF_SPLIT]]
; CHECK:       [[LOOP_IF_SPLIT]]:
; CHECK-NEXT:    [[TMP15:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], splat (i64 1)
; CHECK-NEXT:    [[TMP16:%.*]] = add <vscale x 2 x i64> [[VEC_PHI1]], splat (i64 1)
; CHECK-NEXT:    [[PREDPHI]] = select <vscale x 2 x i1> [[TMP7]], <vscale x 2 x i64> [[TMP15]], <vscale x 2 x i64> [[VEC_PHI]]
; CHECK-NEXT:    [[PREDPHI3]] = select <vscale x 2 x i1> [[TMP8]], <vscale x 2 x i64> [[TMP16]], <vscale x 2 x i64> [[VEC_PHI1]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP4]]
; CHECK-NEXT:    [[TMP12:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP12]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 2 x i64> [[PREDPHI3]], [[PREDPHI]]
; CHECK-NEXT:    [[TMP18:%.*]] = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> [[BIN_RDX]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i64 [ [[TMP18]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP_CONT:.*]] ]
; CHECK-NEXT:    [[SUM:%.*]] = phi i64 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[SUM_NEXT:%.*]], %[[LOOP_CONT]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, ptr [[ADDR]], i64 [[IV]]
; CHECK-NEXT:    [[VAL:%.*]] = load i64, ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i64 [[VAL]], [[M]]
; CHECK-NEXT:    br i1 [[COND]], label %[[LOOP_IF:.*]], label %[[LOOP_CONT]]
; CHECK:       [[LOOP_IF]]:
; CHECK-NEXT:    [[NEW_VAL:%.*]] = add i64 [[VAL]], 1
; CHECK-NEXT:    store i64 [[NEW_VAL]], ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[SUM_INC:%.*]] = add i64 [[SUM]], 1
; CHECK-NEXT:    br label %[[LOOP_CONT]]
; CHECK:       [[LOOP_CONT]]:
; CHECK-NEXT:    [[SUM_NEXT]] = phi i64 [ [[SUM_INC]], %[[LOOP_IF]] ], [ [[SUM]], %[[LOOP]] ]
; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    [[SUM_NEXT_LCSSA:%.*]] = phi i64 [ [[SUM_NEXT]], %[[LOOP_CONT]] ], [ [[TMP18]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    ret i64 [[SUM_NEXT_LCSSA]]
;
; TF-LABEL: define i64 @conditional_liveout_and_store(
; TF-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0]] {
; TF-NEXT:  [[ENTRY:.*:]]
; TF-NEXT:    br label %[[VECTOR_PH:.*]]
; TF:       [[VECTOR_PH]]:
; TF-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; TF-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP3]], 1
; TF-NEXT:    [[TMP2:%.*]] = sub i64 [[TMP1]], 1
; TF-NEXT:    [[N_RND_UP:%.*]] = add i64 [[N]], [[TMP2]]
; TF-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N_RND_UP]], [[TMP1]]
; TF-NEXT:    [[N_VEC:%.*]] = sub i64 [[N_RND_UP]], [[N_MOD_VF]]
; TF-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; TF-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; TF:       [[VECTOR_BODY]]:
; TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; TF-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP10:%.*]], %[[LOOP_IF_SPLIT]] ]
; TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 [[INDEX]], i64 [[N]])
; TF-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr align 8 [[TMP0]], <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i64> poison)
; TF-NEXT:    [[TMP4:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], [[BROADCAST_SPLAT]]
; TF-NEXT:    [[TMP5:%.*]] = select <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i1> [[TMP4]], <vscale x 2 x i1> zeroinitializer
; TF-NEXT:    [[TMP6:%.*]] = freeze <vscale x 2 x i1> [[TMP5]]
; TF-NEXT:    [[TMP7:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP6]])
; TF-NEXT:    br i1 [[TMP7]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; TF:       [[VECTOR_IF_BB]]:
; TF-NEXT:    [[TMP11:%.*]] = add <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], splat (i64 1)
; TF-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP11]], ptr align 8 [[TMP0]], <vscale x 2 x i1> [[TMP5]])
; TF-NEXT:    br label %[[LOOP_IF_SPLIT]]
; TF:       [[LOOP_IF_SPLIT]]:
; TF-NEXT:    [[TMP12:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], splat (i64 1)
; TF-NEXT:    [[PREDPHI:%.*]] = select <vscale x 2 x i1> [[TMP4]], <vscale x 2 x i64> [[TMP12]], <vscale x 2 x i64> [[VEC_PHI]]
; TF-NEXT:    [[TMP10]] = select <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i64> [[PREDPHI]], <vscale x 2 x i64> [[VEC_PHI]]
; TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP1]]
; TF-NEXT:    [[TMP8:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; TF-NEXT:    br i1 [[TMP8]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP3:![0-9]+]]
; TF:       [[MIDDLE_BLOCK]]:
; TF-NEXT:    [[TMP9:%.*]] = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> [[TMP10]])
; TF-NEXT:    br label %[[EXIT:.*]]
; TF:       [[EXIT]]:
; TF-NEXT:    ret i64 [[TMP9]]
;
entry:
  br label %loop

loop:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop.cont ]
  %sum = phi i64 [ 0, %entry ], [ %sum.next, %loop.cont ]
  %arrayidx = getelementptr inbounds i64, ptr %addr, i64 %iv
  %val = load i64, ptr %arrayidx, align 8
  %cond = icmp eq i64 %val, %M
  br i1 %cond, label %loop.if, label %loop.cont

loop.if:
  %new_val = add i64 %val, 1
  store i64 %new_val, ptr %arrayidx, align 8
  %sum.inc = add i64 %sum, 1
  br label %loop.cont

loop.cont:
  %sum.next = phi i64 [ %sum.inc, %loop.if ], [ %sum, %loop ]
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %N
  br i1 %exitcond, label %exit, label %loop

exit:
  ret i64 %sum.next
}

; Test for control flow with live-out calculated in both paths.
define i64 @conditional_liveout_both_path(ptr %addr, i64 %N, i64 %M) {
; CHECK-LABEL: define i64 @conditional_liveout_both_path(
; CHECK-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[TMP5:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP5]], 2
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N]], [[TMP1]]
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-NEXT:    [[TMP3:%.*]] = shl nuw i64 [[TMP2]], 1
; CHECK-NEXT:    [[TMP4:%.*]] = shl nuw i64 [[TMP3]], 1
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], [[TMP4]]
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; CHECK-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PREDPHI:%.*]], %[[LOOP_IF_SPLIT]] ]
; CHECK-NEXT:    [[VEC_PHI1:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[PREDPHI3:%.*]], %[[LOOP_IF_SPLIT]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr i64, ptr [[TMP0]], i64 [[TMP3]]
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <vscale x 2 x i64>, ptr [[TMP0]], align 8
; CHECK-NEXT:    [[WIDE_LOAD2:%.*]] = load <vscale x 2 x i64>, ptr [[TMP6]], align 8
; CHECK-NEXT:    [[TMP7:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP8:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_LOAD2]], [[BROADCAST_SPLAT]]
; CHECK-NEXT:    [[TMP9:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], [[WIDE_LOAD]]
; CHECK-NEXT:    [[TMP10:%.*]] = add <vscale x 2 x i64> [[VEC_PHI1]], [[WIDE_LOAD2]]
; CHECK-NEXT:    [[TMP11:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD]], splat (i64 1)
; CHECK-NEXT:    [[TMP12:%.*]] = add <vscale x 2 x i64> [[WIDE_LOAD2]], splat (i64 1)
; CHECK-NEXT:    [[TMP13:%.*]] = freeze <vscale x 2 x i1> [[TMP7]]
; CHECK-NEXT:    [[TMP19:%.*]] = freeze <vscale x 2 x i1> [[TMP8]]
; CHECK-NEXT:    [[TMP15:%.*]] = or <vscale x 2 x i1> [[TMP13]], [[TMP19]]
; CHECK-NEXT:    [[TMP16:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP15]])
; CHECK-NEXT:    br i1 [[TMP16]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; CHECK:       [[VECTOR_IF_BB]]:
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP11]], ptr align 8 [[TMP0]], <vscale x 2 x i1> [[TMP7]])
; CHECK-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP12]], ptr align 8 [[TMP6]], <vscale x 2 x i1> [[TMP8]])
; CHECK-NEXT:    br label %[[LOOP_IF_SPLIT]]
; CHECK:       [[LOOP_IF_SPLIT]]:
; CHECK-NEXT:    [[TMP17:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], [[TMP11]]
; CHECK-NEXT:    [[TMP18:%.*]] = add <vscale x 2 x i64> [[VEC_PHI1]], [[TMP12]]
; CHECK-NEXT:    [[PREDPHI]] = select <vscale x 2 x i1> [[TMP7]], <vscale x 2 x i64> [[TMP17]], <vscale x 2 x i64> [[TMP9]]
; CHECK-NEXT:    [[PREDPHI3]] = select <vscale x 2 x i1> [[TMP8]], <vscale x 2 x i64> [[TMP18]], <vscale x 2 x i64> [[TMP10]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP4]]
; CHECK-NEXT:    [[TMP14:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP14]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 2 x i64> [[PREDPHI3]], [[PREDPHI]]
; CHECK-NEXT:    [[TMP20:%.*]] = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> [[BIN_RDX]])
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label %[[EXIT:.*]], label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i64 [ [[TMP20]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP_CONT:.*]] ]
; CHECK-NEXT:    [[ACC:%.*]] = phi i64 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[ACC_NEXT:%.*]], %[[LOOP_CONT]] ]
; CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds i64, ptr [[ADDR]], i64 [[IV]]
; CHECK-NEXT:    [[VAL:%.*]] = load i64, ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[COND:%.*]] = icmp eq i64 [[VAL]], [[M]]
; CHECK-NEXT:    br i1 [[COND]], label %[[LOOP_IF:.*]], label %[[LOOP_ELSE:.*]]
; CHECK:       [[LOOP_IF]]:
; CHECK-NEXT:    [[NEW_VAL:%.*]] = add i64 [[VAL]], 1
; CHECK-NEXT:    store i64 [[NEW_VAL]], ptr [[ARRAYIDX]], align 8
; CHECK-NEXT:    [[ACC_IF:%.*]] = add i64 [[ACC]], [[NEW_VAL]]
; CHECK-NEXT:    br label %[[LOOP_CONT]]
; CHECK:       [[LOOP_ELSE]]:
; CHECK-NEXT:    [[ACC_ELSE:%.*]] = add i64 [[ACC]], [[VAL]]
; CHECK-NEXT:    br label %[[LOOP_CONT]]
; CHECK:       [[LOOP_CONT]]:
; CHECK-NEXT:    [[ACC_NEXT]] = phi i64 [ [[ACC_IF]], %[[LOOP_IF]] ], [ [[ACC_ELSE]], %[[LOOP_ELSE]] ]
; CHECK-NEXT:    [[IV_NEXT]] = add nuw nsw i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label %[[EXIT]], label %[[LOOP]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    [[ACC_NEXT_LCSSA:%.*]] = phi i64 [ [[ACC_NEXT]], %[[LOOP_CONT]] ], [ [[TMP20]], %[[MIDDLE_BLOCK]] ]
; CHECK-NEXT:    ret i64 [[ACC_NEXT_LCSSA]]
;
; TF-LABEL: define i64 @conditional_liveout_both_path(
; TF-SAME: ptr [[ADDR:%.*]], i64 [[N:%.*]], i64 [[M:%.*]]) #[[ATTR0]] {
; TF-NEXT:  [[ENTRY:.*:]]
; TF-NEXT:    br label %[[VECTOR_PH:.*]]
; TF:       [[VECTOR_PH]]:
; TF-NEXT:    [[TMP3:%.*]] = call i64 @llvm.vscale.i64()
; TF-NEXT:    [[TMP1:%.*]] = shl nuw i64 [[TMP3]], 1
; TF-NEXT:    [[TMP2:%.*]] = sub i64 [[TMP1]], 1
; TF-NEXT:    [[N_RND_UP:%.*]] = add i64 [[N]], [[TMP2]]
; TF-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N_RND_UP]], [[TMP1]]
; TF-NEXT:    [[N_VEC:%.*]] = sub i64 [[N_RND_UP]], [[N_MOD_VF]]
; TF-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <vscale x 2 x i64> poison, i64 [[M]], i64 0
; TF-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <vscale x 2 x i64> [[BROADCAST_SPLATINSERT]], <vscale x 2 x i64> poison, <vscale x 2 x i32> zeroinitializer
; TF-NEXT:    br label %[[VECTOR_BODY:.*]]
; TF:       [[VECTOR_BODY]]:
; TF-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[LOOP_IF_SPLIT:.*]] ]
; TF-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 2 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP11:%.*]], %[[LOOP_IF_SPLIT]] ]
; TF-NEXT:    [[ACTIVE_LANE_MASK:%.*]] = call <vscale x 2 x i1> @llvm.get.active.lane.mask.nxv2i1.i64(i64 [[INDEX]], i64 [[N]])
; TF-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[ADDR]], i64 [[INDEX]]
; TF-NEXT:    [[WIDE_MASKED_LOAD:%.*]] = call <vscale x 2 x i64> @llvm.masked.load.nxv2i64.p0(ptr align 8 [[TMP0]], <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i64> poison)
; TF-NEXT:    [[TMP4:%.*]] = icmp eq <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], [[BROADCAST_SPLAT]]
; TF-NEXT:    [[TMP5:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], [[WIDE_MASKED_LOAD]]
; TF-NEXT:    [[TMP6:%.*]] = select <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i1> [[TMP4]], <vscale x 2 x i1> zeroinitializer
; TF-NEXT:    [[TMP7:%.*]] = add <vscale x 2 x i64> [[WIDE_MASKED_LOAD]], splat (i64 1)
; TF-NEXT:    [[TMP8:%.*]] = freeze <vscale x 2 x i1> [[TMP6]]
; TF-NEXT:    [[TMP12:%.*]] = call i1 @llvm.vector.reduce.or.nxv2i1(<vscale x 2 x i1> [[TMP8]])
; TF-NEXT:    br i1 [[TMP12]], label %[[VECTOR_IF_BB:.*]], label %[[LOOP_IF_SPLIT]]
; TF:       [[VECTOR_IF_BB]]:
; TF-NEXT:    call void @llvm.masked.store.nxv2i64.p0(<vscale x 2 x i64> [[TMP7]], ptr align 8 [[TMP0]], <vscale x 2 x i1> [[TMP6]])
; TF-NEXT:    br label %[[LOOP_IF_SPLIT]]
; TF:       [[LOOP_IF_SPLIT]]:
; TF-NEXT:    [[TMP13:%.*]] = add <vscale x 2 x i64> [[VEC_PHI]], [[TMP7]]
; TF-NEXT:    [[PREDPHI:%.*]] = select <vscale x 2 x i1> [[TMP4]], <vscale x 2 x i64> [[TMP13]], <vscale x 2 x i64> [[TMP5]]
; TF-NEXT:    [[TMP11]] = select <vscale x 2 x i1> [[ACTIVE_LANE_MASK]], <vscale x 2 x i64> [[PREDPHI]], <vscale x 2 x i64> [[VEC_PHI]]
; TF-NEXT:    [[INDEX_NEXT]] = add i64 [[INDEX]], [[TMP1]]
; TF-NEXT:    [[TMP9:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; TF-NEXT:    br i1 [[TMP9]], label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; TF:       [[MIDDLE_BLOCK]]:
; TF-NEXT:    [[TMP10:%.*]] = call i64 @llvm.vector.reduce.add.nxv2i64(<vscale x 2 x i64> [[TMP11]])
; TF-NEXT:    br label %[[EXIT:.*]]
; TF:       [[EXIT]]:
; TF-NEXT:    ret i64 [[TMP10]]
;
entry:
  br label %loop

loop:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop.cont ]
  %acc = phi i64 [ 0, %entry ], [ %acc.next, %loop.cont ]
  %arrayidx = getelementptr inbounds i64, ptr %addr, i64 %iv
  %val = load i64, ptr %arrayidx, align 8
  %cond = icmp eq i64 %val, %M
  br i1 %cond, label %loop.if, label %loop.else

loop.if:
  %new_val = add i64 %val, 1
  store i64 %new_val, ptr %arrayidx, align 8
  %acc.if = add i64 %acc, %new_val
  br label %loop.cont

loop.else:
  %acc.else = add i64 %acc, %val
  br label %loop.cont

loop.cont:
  %acc.next = phi i64 [ %acc.if, %loop.if ], [ %acc.else, %loop.else ]
  %iv.next = add nuw nsw i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %N
  br i1 %exitcond, label %exit, label %loop

exit:
  ret i64 %acc.next
}
;.
; CHECK: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
; CHECK: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
; CHECK: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
; CHECK: [[LOOP3]] = distinct !{[[LOOP3]], [[META2]], [[META1]]}
; CHECK: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
; CHECK: [[LOOP5]] = distinct !{[[LOOP5]], [[META2]], [[META1]]}
; CHECK: [[LOOP6]] = distinct !{[[LOOP6]], [[META1]], [[META2]]}
; CHECK: [[LOOP7]] = distinct !{[[LOOP7]], [[META2]], [[META1]]}
;.
; TF: [[LOOP0]] = distinct !{[[LOOP0]], [[META1:![0-9]+]], [[META2:![0-9]+]]}
; TF: [[META1]] = !{!"llvm.loop.isvectorized", i32 1}
; TF: [[META2]] = !{!"llvm.loop.unroll.runtime.disable"}
; TF: [[LOOP3]] = distinct !{[[LOOP3]], [[META1]], [[META2]]}
; TF: [[LOOP4]] = distinct !{[[LOOP4]], [[META1]], [[META2]]}
;.
