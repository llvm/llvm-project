; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -vector-library=Darwin_libsystem_m -inject-tli-mappings -loop-vectorize -S | FileCheck %s

target datalayout = "e-p:64:64:64-i1:8:8-i8:8:8-i16:16:16-i32:32:32-i64:64:64-f32:32:32-f64:64:64-v64:64:64-v128:128:128-a0:0:64-s0:64:64-f80:128:128-n8:16:32:64-S128"
target triple = "arm64-apple-darwin"

declare float @expf(float) nounwind readnone
define void @expf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @expf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_exp_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_exp_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @expf(float [[LV]]) #[[ATTR1:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @expf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @exp(double) nounwind readnone
define void @exp_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @exp_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_exp_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_exp_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @exp(double [[LV]]) #[[ATTR2:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @exp(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @acosf(float) nounwind readnone
define void @acos_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @acos_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_acos_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_acos_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP6:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @acosf(float [[LV]]) #[[ATTR3:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @acosf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @acos(double) nounwind readnone
define void @acos_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @acos_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_acos_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_acos_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP8:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @acos(double [[LV]]) #[[ATTR4:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP9:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @acos(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @asinf(float) nounwind readnone
define void @asinf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @asinf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_asin_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_asin_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP10:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @asinf(float [[LV]]) #[[ATTR5:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP11:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @asinf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @asin(double) nounwind readnone
define void @asin_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @asin_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_asin_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_asin_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @asin(double [[LV]]) #[[ATTR6:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP13:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @asin(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

  declare float @atanf(float) nounwind readnone
define void @atanf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @atanf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_atan_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_atan_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP14:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @atanf(float [[LV]]) #[[ATTR7:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP15:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @atanf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @atan(double) nounwind readnone
define void @atan_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @atan_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_atan_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_atan_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP16:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @atan(double [[LV]]) #[[ATTR8:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP17:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @atan(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @atan2f(float) nounwind readnone
define void @atan2f_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @atan2f_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_atan2_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_atan2_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP18:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @atan2f(float [[LV]]) #[[ATTR9:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP19:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @atan2f(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @atan2(double) nounwind readnone
define void @atan2_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @atan2_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_atan2_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_atan2_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP20:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @atan2(double [[LV]]) #[[ATTR10:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP21:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @atan2(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @cosf(float) nounwind readnone
define void @cosf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @cosf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_cos_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_cos_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP22:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @cosf(float [[LV]]) #[[ATTR11:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP23:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @cosf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @cos(double) nounwind readnone
define void @cos_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @cos_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_cos_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_cos_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP24:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @cos(double [[LV]]) #[[ATTR12:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP25:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @cos(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @cbrtf(float) nounwind readnone
define void @cbrtf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @cbrtf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_cbrt_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_cbrt_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP26:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @cbrtf(float [[LV]]) #[[ATTR13:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP27:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @cbrtf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @cbrt(double) nounwind readnone
define void @cbrt_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @cbrt_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_cbrt_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_cbrt_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP28:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @cbrt(double [[LV]]) #[[ATTR14:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP29:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @cbrt(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @erff(float) nounwind readnone
define void @erff_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @erff_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_erf_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_erf_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP30:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @erff(float [[LV]]) #[[ATTR15:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP31:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @erff(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @erf(double) nounwind readnone
define void @erf_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @erf_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_erf_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_erf_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP32:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @erf(double [[LV]]) #[[ATTR16:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP33:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @erf(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @powf(float) nounwind readnone
define void @powf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @powf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_pow_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_pow_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP34:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @powf(float [[LV]]) #[[ATTR17:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP35:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @powf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @pow(double) nounwind readnone
define void @pow_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @pow_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_pow_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_pow_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP36:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @pow(double [[LV]]) #[[ATTR18:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP37:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @pow(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @sinhf(float) nounwind readnone
define void @sinhf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @sinhf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_sinh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_sinh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP38:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @sinhf(float [[LV]]) #[[ATTR19:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP39:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @sinhf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @sinh(double) nounwind readnone
define void @sinh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @sinh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_sinh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_sinh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP40:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @sinh(double [[LV]]) #[[ATTR20:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP41:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @sinh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @coshf(float) nounwind readnone
define void @coshf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @coshf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_cosh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_cosh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP42:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @coshf(float [[LV]]) #[[ATTR21:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP43:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @coshf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @cosh(double) nounwind readnone
define void @cosh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @cosh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_cosh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_cosh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP44:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @cosh(double [[LV]]) #[[ATTR22:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP45:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @cosh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @tanhf(float) nounwind readnone
define void @tanhf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @tanhf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_tanh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_tanh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP46:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @tanhf(float [[LV]]) #[[ATTR23:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP47:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @tanhf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @tanh(double) nounwind readnone
define void @tanh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @tanh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_tanh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_tanh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP48:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @tanh(double [[LV]]) #[[ATTR24:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP49:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @tanh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @asinhf(float) nounwind readnone
define void @asinhf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @asinhf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_asinh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_asinh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP50:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @asinhf(float [[LV]]) #[[ATTR25:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP51:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @asinhf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @asinh(double) nounwind readnone
define void @asinh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @asinh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_asinh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_asinh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP52:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @asinh(double [[LV]]) #[[ATTR26:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP53:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @asinh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @acoshf(float) nounwind readnone
define void @acoshf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @acoshf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_acosh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_acosh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP54:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @acoshf(float [[LV]]) #[[ATTR27:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP55:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @acoshf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @acosh(double) nounwind readnone
define void @acosh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @acosh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_acosh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_acosh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP56:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @acosh(double [[LV]]) #[[ATTR28:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP57:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @acosh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare float @atanhf(float) nounwind readnone
define void @atanhf_v4f32(i64 %n, float* noalias %y, float* noalias %x) {
; CHECK-LABEL: @atanhf_v4f32(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 8
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 8
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds float, float* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast float* [[TMP4]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <4 x float>, <4 x float>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds float, float* [[TMP2]], i32 4
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast float* [[TMP6]] to <4 x float>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <4 x float>, <4 x float>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <4 x float> @_simd_atanh_f4(<4 x float> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <4 x float> @_simd_atanh_f4(<4 x float> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds float, float* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast float* [[TMP12]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP8]], <4 x float>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds float, float* [[TMP10]], i32 4
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast float* [[TMP14]] to <4 x float>*
; CHECK-NEXT:    store <4 x float> [[TMP9]], <4 x float>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 8
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP58:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds float, float* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load float, float* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call float @atanhf(float [[LV]]) #[[ATTR29:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds float, float* [[X]], i64 [[IV]]
; CHECK-NEXT:    store float [[CALL]], float* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP59:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds float, float* %y, i64 %iv
  %lv = load float, float* %gep.y, align 4
  %call = tail call float @atanhf(float %lv)
  %gep.x = getelementptr inbounds float, float* %x, i64 %iv
  store float %call, float* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}

declare double @atanh(double) nounwind readnone
define void @atanh_v2f64(i64 %n, double* noalias %y, double * noalias %x) {
; CHECK-LABEL: @atanh_v2f64(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[N:%.*]], 4
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], 4
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[N_MOD_VF]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[INDEX]], 0
; CHECK-NEXT:    [[TMP1:%.*]] = add i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr inbounds double, double* [[Y:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 0
; CHECK-NEXT:    [[TMP5:%.*]] = bitcast double* [[TMP4]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x double>, <2 x double>* [[TMP5]], align 4
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr inbounds double, double* [[TMP2]], i32 2
; CHECK-NEXT:    [[TMP7:%.*]] = bitcast double* [[TMP6]] to <2 x double>*
; CHECK-NEXT:    [[WIDE_LOAD1:%.*]] = load <2 x double>, <2 x double>* [[TMP7]], align 4
; CHECK-NEXT:    [[TMP8:%.*]] = call <2 x double> @_simd_atanh_d2(<2 x double> [[WIDE_LOAD]])
; CHECK-NEXT:    [[TMP9:%.*]] = call <2 x double> @_simd_atanh_d2(<2 x double> [[WIDE_LOAD1]])
; CHECK-NEXT:    [[TMP10:%.*]] = getelementptr inbounds double, double* [[X:%.*]], i64 [[TMP0]]
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[TMP1]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 0
; CHECK-NEXT:    [[TMP13:%.*]] = bitcast double* [[TMP12]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP8]], <2 x double>* [[TMP13]], align 4
; CHECK-NEXT:    [[TMP14:%.*]] = getelementptr inbounds double, double* [[TMP10]], i32 2
; CHECK-NEXT:    [[TMP15:%.*]] = bitcast double* [[TMP14]] to <2 x double>*
; CHECK-NEXT:    store <2 x double> [[TMP9]], <2 x double>* [[TMP15]], align 4
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 4
; CHECK-NEXT:    [[TMP16:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP16]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP60:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[N]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[FOR_END:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY:%.*]] ]
; CHECK-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK:       for.body:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[IV_NEXT:%.*]], [[FOR_BODY]] ], [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ]
; CHECK-NEXT:    [[GEP_Y:%.*]] = getelementptr inbounds double, double* [[Y]], i64 [[IV]]
; CHECK-NEXT:    [[LV:%.*]] = load double, double* [[GEP_Y]], align 4
; CHECK-NEXT:    [[CALL:%.*]] = tail call double @atanh(double [[LV]]) #[[ATTR30:[0-9]+]]
; CHECK-NEXT:    [[GEP_X:%.*]] = getelementptr inbounds double, double* [[X]], i64 [[IV]]
; CHECK-NEXT:    store double [[CALL]], double* [[GEP_X]], align 4
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[IV_NEXT]], [[N]]
; CHECK-NEXT:    br i1 [[EXITCOND]], label [[FOR_END]], label [[FOR_BODY]], !llvm.loop [[LOOP61:![0-9]+]]
; CHECK:       for.end:
; CHECK-NEXT:    ret void
;

entry:
  br label %for.body

for.body:
  %iv = phi i64 [ %iv.next, %for.body ], [ 0, %entry ]
  %gep.y = getelementptr inbounds double, double* %y, i64 %iv
  %lv = load double, double* %gep.y, align 4
  %call = tail call double @atanh(double %lv)
  %gep.x = getelementptr inbounds double, double* %x, i64 %iv
  store double %call, double* %gep.x, align 4
  %iv.next = add i64 %iv, 1
  %exitcond = icmp eq i64 %iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body

for.end:
  ret void
}
