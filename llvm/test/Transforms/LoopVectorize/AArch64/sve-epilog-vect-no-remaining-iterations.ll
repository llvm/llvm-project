; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --check-globals none --version 5
; RUN: opt -passes=loop-vectorize -S %s | FileCheck %s

target triple = "aarch64-linux-gnu"

define i64 @main_vector_loop_fixed_with_no_remaining_iterations(ptr %src, ptr noalias %dst, i32 %x) #0 {
; CHECK-LABEL: define i64 @main_vector_loop_fixed_with_no_remaining_iterations(
; CHECK-SAME: ptr [[SRC:%.*]], ptr noalias [[DST:%.*]], i32 [[X:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    br i1 true, label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    [[BROADCAST_SPLATINSERT:%.*]] = insertelement <16 x i32> poison, i32 [[X]], i64 0
; CHECK-NEXT:    [[BROADCAST_SPLAT:%.*]] = shufflevector <16 x i32> [[BROADCAST_SPLATINSERT]], <16 x i32> poison, <16 x i32> zeroinitializer
; CHECK-NEXT:    [[TMP0:%.*]] = call <16 x i32> @llvm.abs.v16i32(<16 x i32> [[BROADCAST_SPLAT]], i1 false)
; CHECK-NEXT:    [[TMP1:%.*]] = call <16 x i32> @llvm.abs.v16i32(<16 x i32> [[BROADCAST_SPLAT]], i1 false)
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, %[[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[VEC_PHI1:%.*]] = phi <16 x i64> [ zeroinitializer, %[[VECTOR_PH]] ], [ [[TMP17:%.*]], %[[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr { [4 x i8] }, ptr [[SRC]], i64 [[INDEX]], i32 0, i64 3
; CHECK-NEXT:    [[WIDE_VEC2:%.*]] = load <64 x i8>, ptr [[TMP4]], align 1
; CHECK-NEXT:    [[STRIDED_VEC3:%.*]] = shufflevector <64 x i8> [[WIDE_VEC2]], <64 x i8> poison, <16 x i32> <i32 0, i32 4, i32 8, i32 12, i32 16, i32 20, i32 24, i32 28, i32 32, i32 36, i32 40, i32 44, i32 48, i32 52, i32 56, i32 60>
; CHECK-NEXT:    [[TMP6:%.*]] = zext <16 x i8> [[STRIDED_VEC3]] to <16 x i32>
; CHECK-NEXT:    [[TMP8:%.*]] = call <16 x i32> @llvm.umin.v16i32(<16 x i32> [[TMP0]], <16 x i32> [[TMP6]])
; CHECK-NEXT:    [[TMP10:%.*]] = call <16 x i32> @llvm.umin.v16i32(<16 x i32> [[TMP1]], <16 x i32> [[TMP8]])
; CHECK-NEXT:    [[TMP11:%.*]] = getelementptr inbounds i8, ptr [[DST]], i64 [[INDEX]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr inbounds i8, ptr [[TMP11]], i32 0
; CHECK-NEXT:    store <16 x i8> zeroinitializer, ptr [[TMP12]], align 1
; CHECK-NEXT:    [[TMP15:%.*]] = zext <16 x i32> [[TMP10]] to <16 x i64>
; CHECK-NEXT:    [[TMP17]] = or <16 x i64> [[VEC_PHI1]], [[TMP15]]
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 16
; CHECK-NEXT:    br i1 true, label %[[MIDDLE_BLOCK:.*]], label %[[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    [[TMP18:%.*]] = call i64 @llvm.vector.reduce.or.v16i64(<16 x i64> [[TMP17]])
; CHECK-NEXT:    br label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 0, %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i64 [ [[TMP18]], %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
; CHECK-NEXT:    [[RED:%.*]] = phi i64 [ [[BC_MERGE_RDX]], %[[SCALAR_PH]] ], [ [[RED_NEXT:%.*]], %[[LOOP]] ]
; CHECK-NEXT:    [[GEP_SRC_I_I:%.*]] = getelementptr { [4 x i8] }, ptr [[SRC]], i64 [[IV]], i32 0, i64 3
; CHECK-NEXT:    [[L:%.*]] = load i8, ptr [[GEP_SRC_I_I]], align 1
; CHECK-NEXT:    [[L_EXT:%.*]] = zext i8 [[L]] to i32
; CHECK-NEXT:    [[ABS_0:%.*]] = call i32 @llvm.abs.i32(i32 [[X]], i1 false)
; CHECK-NEXT:    [[MIN_0:%.*]] = call i32 @llvm.umin.i32(i32 [[ABS_0]], i32 [[L_EXT]])
; CHECK-NEXT:    [[ABS_1:%.*]] = call i32 @llvm.abs.i32(i32 [[X]], i1 false)
; CHECK-NEXT:    [[MIN_1:%.*]] = call i32 @llvm.umin.i32(i32 [[ABS_1]], i32 [[MIN_0]])
; CHECK-NEXT:    [[GEP_DST:%.*]] = getelementptr inbounds i8, ptr [[DST]], i64 [[IV]]
; CHECK-NEXT:    store i8 0, ptr [[GEP_DST]], align 1
; CHECK-NEXT:    [[MIN_EXT:%.*]] = zext i32 [[MIN_1]] to i64
; CHECK-NEXT:    [[RED_NEXT]] = or i64 [[RED]], [[MIN_EXT]]
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 1
; CHECK-NEXT:    [[EXITCOND_NOT_I_I:%.*]] = icmp eq i64 [[IV_NEXT]], 16
; CHECK-NEXT:    br i1 [[EXITCOND_NOT_I_I]], label %[[EXIT:.*]], label %[[LOOP]], !llvm.loop [[LOOP3:![0-9]+]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    [[RED_NEXT_LCSSA:%.*]] = phi i64 [ [[RED_NEXT]], %[[LOOP]] ]
; CHECK-NEXT:    ret i64 [[RED_NEXT_LCSSA]]
;
entry:
  br label %loop

loop:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]
  %red = phi i64 [ 0, %entry ], [ %red.next, %loop ]
  %gep.src.i.i = getelementptr { [4 x i8] }, ptr %src, i64 %iv, i32 0, i64 3
  %l = load i8, ptr %gep.src.i.i, align 1
  %l.ext = zext i8 %l to i32
  %abs.0 = call i32 @llvm.abs.i32(i32 %x, i1 false)
  %min.0 = call i32 @llvm.umin.i32(i32 %abs.0, i32 %l.ext)
  %abs.1 = call i32 @llvm.abs.i32(i32 %x, i1 false)
  %min.1 = call i32 @llvm.umin.i32(i32 %abs.1, i32 %min.0)
  %gep.dst = getelementptr inbounds i8, ptr  %dst, i64 %iv
  store i8 0, ptr %gep.dst, align 1
  %min.ext = zext i32 %min.1 to i64
  %red.next = or i64 %red, %min.ext
  %iv.next = add i64 %iv, 1
  %exitcond.not.i.i = icmp eq i64 %iv.next, 16
  br i1 %exitcond.not.i.i, label %exit, label %loop

exit:
  ret i64 %red.next
}

; Test case for https://github.com/llvm/llvm-project/issues/149726.
; TODO: Should not try to vectorize with VF = 8, as the vector loop will never
;       execute.
define void @main_vector_loop_fixed_single_vector_iteration_with_runtime_checks(ptr noalias %A, ptr noalias %B, ptr noalias %C, ptr noalias %D, ptr noalias %E, ptr noalias %F, ptr noalias %G, ptr noalias %H, ptr noalias %I, ptr noalias %J, ptr noalias %K, ptr %L) #1 {
; CHECK-LABEL: define void @main_vector_loop_fixed_single_vector_iteration_with_runtime_checks(
; CHECK-SAME: ptr noalias [[A:%.*]], ptr noalias [[B:%.*]], ptr noalias [[C:%.*]], ptr noalias [[D:%.*]], ptr noalias [[E:%.*]], ptr noalias [[F:%.*]], ptr noalias [[G:%.*]], ptr noalias [[H:%.*]], ptr noalias [[I:%.*]], ptr noalias [[J:%.*]], ptr noalias [[K:%.*]], ptr [[L:%.*]]) #[[ATTR1:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    br i1 true, label %[[SCALAR_PH:.*]], label %[[VECTOR_PH:.*]]
; CHECK:       [[VECTOR_PH]]:
; CHECK-NEXT:    br label %[[VECTOR_BODY:.*]]
; CHECK:       [[VECTOR_BODY]]:
; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr i64, ptr [[J]], i64 0
; CHECK-NEXT:    [[WIDE_VEC:%.*]] = load <16 x i64>, ptr [[TMP0]], align 8
; CHECK-NEXT:    [[STRIDED_VEC:%.*]] = shufflevector <16 x i64> [[WIDE_VEC]], <16 x i64> poison, <8 x i32> <i32 0, i32 2, i32 4, i32 6, i32 8, i32 10, i32 12, i32 14>
; CHECK-NEXT:    [[TMP1:%.*]] = trunc <8 x i64> [[STRIDED_VEC]] to <8 x i16>
; CHECK-NEXT:    [[TMP2:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 0
; CHECK-NEXT:    [[TMP3:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 2
; CHECK-NEXT:    [[TMP4:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 4
; CHECK-NEXT:    [[TMP5:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 6
; CHECK-NEXT:    [[TMP6:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 8
; CHECK-NEXT:    [[TMP7:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 10
; CHECK-NEXT:    [[TMP8:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 12
; CHECK-NEXT:    [[TMP9:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 14
; CHECK-NEXT:    [[TMP10:%.*]] = extractelement <8 x i16> [[TMP1]], i32 0
; CHECK-NEXT:    store i16 [[TMP10]], ptr [[TMP2]], align 2
; CHECK-NEXT:    [[TMP11:%.*]] = extractelement <8 x i16> [[TMP1]], i32 1
; CHECK-NEXT:    store i16 [[TMP11]], ptr [[TMP3]], align 2
; CHECK-NEXT:    [[TMP12:%.*]] = extractelement <8 x i16> [[TMP1]], i32 2
; CHECK-NEXT:    store i16 [[TMP12]], ptr [[TMP4]], align 2
; CHECK-NEXT:    [[TMP13:%.*]] = extractelement <8 x i16> [[TMP1]], i32 3
; CHECK-NEXT:    store i16 [[TMP13]], ptr [[TMP5]], align 2
; CHECK-NEXT:    [[TMP14:%.*]] = extractelement <8 x i16> [[TMP1]], i32 4
; CHECK-NEXT:    store i16 [[TMP14]], ptr [[TMP6]], align 2
; CHECK-NEXT:    [[TMP15:%.*]] = extractelement <8 x i16> [[TMP1]], i32 5
; CHECK-NEXT:    store i16 [[TMP15]], ptr [[TMP7]], align 2
; CHECK-NEXT:    [[TMP16:%.*]] = extractelement <8 x i16> [[TMP1]], i32 6
; CHECK-NEXT:    store i16 [[TMP16]], ptr [[TMP8]], align 2
; CHECK-NEXT:    [[TMP17:%.*]] = extractelement <8 x i16> [[TMP1]], i32 7
; CHECK-NEXT:    store i16 [[TMP17]], ptr [[TMP9]], align 2
; CHECK-NEXT:    store i64 0, ptr [[A]], align 8
; CHECK-NEXT:    store i64 0, ptr [[B]], align 8
; CHECK-NEXT:    store i64 0, ptr [[C]], align 8
; CHECK-NEXT:    store i64 0, ptr [[D]], align 8
; CHECK-NEXT:    store i64 0, ptr [[E]], align 8
; CHECK-NEXT:    store i64 0, ptr [[F]], align 8
; CHECK-NEXT:    store i64 0, ptr [[G]], align 8
; CHECK-NEXT:    store i64 0, ptr [[H]], align 8
; CHECK-NEXT:    store i64 0, ptr [[I]], align 8
; CHECK-NEXT:    store i64 0, ptr [[L]], align 8
; CHECK-NEXT:    br label %[[MIDDLE_BLOCK:.*]]
; CHECK:       [[MIDDLE_BLOCK]]:
; CHECK-NEXT:    br label %[[SCALAR_PH]]
; CHECK:       [[SCALAR_PH]]:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ 0, %[[MIDDLE_BLOCK]] ], [ 0, %[[ENTRY]] ]
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], %[[SCALAR_PH]] ], [ [[IV_NEXT:%.*]], %[[LOOP]] ]
; CHECK-NEXT:    [[GEP_J:%.*]] = getelementptr i64, ptr [[J]], i64 [[IV]]
; CHECK-NEXT:    [[L:%.*]] = load i64, ptr [[GEP_J]], align 8
; CHECK-NEXT:    [[L_TRUNC:%.*]] = trunc i64 [[L]] to i16
; CHECK-NEXT:    [[GEP_K:%.*]] = getelementptr [0 x i16], ptr [[K]], i64 0, i64 [[IV]]
; CHECK-NEXT:    store i16 [[L_TRUNC]], ptr [[GEP_K]], align 2
; CHECK-NEXT:    store i64 0, ptr [[A]], align 8
; CHECK-NEXT:    store i64 0, ptr [[B]], align 8
; CHECK-NEXT:    store i64 0, ptr [[C]], align 8
; CHECK-NEXT:    store i64 0, ptr [[D]], align 8
; CHECK-NEXT:    store i64 0, ptr [[E]], align 8
; CHECK-NEXT:    store i64 0, ptr [[F]], align 8
; CHECK-NEXT:    store i64 0, ptr [[G]], align 8
; CHECK-NEXT:    store i64 0, ptr [[H]], align 8
; CHECK-NEXT:    store i64 0, ptr [[I]], align 8
; CHECK-NEXT:    store i64 0, ptr [[L]], align 8
; CHECK-NEXT:    [[IV_NEXT]] = add i64 [[IV]], 2
; CHECK-NEXT:    [[EC:%.*]] = icmp ult i64 [[IV]], 14
; CHECK-NEXT:    br i1 [[EC]], label %[[LOOP]], label %[[EXIT:.*]], !llvm.loop [[LOOP4:![0-9]+]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  br label %loop

loop:
  %iv = phi i64 [ 0, %entry ], [ %iv.next, %loop ]
  %gep.J = getelementptr i64, ptr %J, i64 %iv
  %l = load i64, ptr %gep.J, align 8
  %l.trunc = trunc i64 %l to i16
  %gep.K = getelementptr [0 x i16], ptr %K, i64 0, i64 %iv
  store i16 %l.trunc, ptr %gep.K, align 2
  store i64 0, ptr %A, align 8
  store i64 0, ptr %B, align 8
  store i64 0, ptr %C, align 8
  store i64 0, ptr %D, align 8
  store i64 0, ptr %E, align 8
  store i64 0, ptr %F, align 8
  store i64 0, ptr %G, align 8
  store i64 0, ptr %H, align 8
  store i64 0, ptr %I, align 8
  store i64 0, ptr %L, align 8
  %iv.next = add i64 %iv, 2
  %ec = icmp ult i64 %iv, 14
  br i1 %ec, label %loop, label %exit, !llvm.loop !0

exit:
  ret void
}

declare i32 @llvm.umin.i32(i32, i32)

declare i32 @llvm.abs.i32(i32, i1 immarg)

attributes #0 = { "target-cpu"="neoverse-512tvb" }
attributes #1 = { "target-cpu"="grace" }

!0 = distinct !{!0, !1, !2}
!1 = !{!"llvm.loop.mustprogress"}
!2 = !{!"llvm.loop.vectorize.enable", i1 true}
