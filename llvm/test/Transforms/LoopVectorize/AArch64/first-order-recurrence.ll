; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -loop-vectorize -force-vector-width=4 -force-vector-interleave=1 -mtriple aarch64-unknown-linux-gnu -mattr=+sve -S < %s | FileCheck %s --check-prefix=CHECK-VF4UF1
; RUN: opt -loop-vectorize -force-vector-width=4 -force-vector-interleave=2 -mtriple aarch64-unknown-linux-gnu -mattr=+sve -S < %s | FileCheck %s --check-prefix=CHECK-VF4UF2

; We vectorize this first order recurrence, with a set of insertelements for
; each unrolled part. Make sure these insertelements are generated in-order,
; because the shuffle of the first order recurrence will be added after the
; insertelement of the last part UF - 1, assuming the latter appears after the
; insertelements of all other parts.
;
; int PR33613(double *b, double j, int d) {
;   int a = 0;
;   for(int i = 0; i < 10240; i++, b+=25) {
;     double f = b[d]; // Scalarize to form insertelements
;     if (j * f)
;       a++;
;     j = f;
;   }
;   return a;
; }
;
define i32 @PR33613(double* %b, double %j, i32 %d) #0 {
; CHECK-VF4UF1-LABEL: @PR33613(
; CHECK-VF4UF1-NEXT:  entry:
; CHECK-VF4UF1-NEXT:    [[IDXPROM:%.*]] = sext i32 [[D:%.*]] to i64
; CHECK-VF4UF1-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 4
; CHECK-VF4UF1-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 10240, [[TMP1]]
; CHECK-VF4UF1-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK-VF4UF1:       vector.ph:
; CHECK-VF4UF1-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
; CHECK-VF4UF1-NEXT:    [[N_MOD_VF:%.*]] = urem i64 10240, [[TMP3]]
; CHECK-VF4UF1-NEXT:    [[N_VEC:%.*]] = sub i64 10240, [[N_MOD_VF]]
; CHECK-VF4UF1-NEXT:    [[TMP4:%.*]] = mul i64 [[N_VEC]], 25
; CHECK-VF4UF1-NEXT:    [[IND_END:%.*]] = getelementptr double, double* [[B:%.*]], i64 [[TMP4]]
; CHECK-VF4UF1-NEXT:    [[IND_END2:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-VF4UF1-NEXT:    [[TMP5:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP6:%.*]] = mul i32 [[TMP5]], 4
; CHECK-VF4UF1-NEXT:    [[TMP7:%.*]] = sub i32 [[TMP6]], 1
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 4 x double> poison, double [[J:%.*]], i32 [[TMP7]]
; CHECK-VF4UF1-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK-VF4UF1:       vector.body:
; CHECK-VF4UF1-NEXT:    [[POINTER_PHI:%.*]] = phi double* [ [[B]], [[VECTOR_PH]] ], [ [[PTR_IND:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ insertelement (<vscale x 4 x i32> zeroinitializer, i32 0, i32 0), [[VECTOR_PH]] ], [ [[TMP21:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 4 x double> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_MASKED_GATHER:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[TMP8:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP9:%.*]] = mul i64 [[TMP8]], 4
; CHECK-VF4UF1-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP9]], 1
; CHECK-VF4UF1-NEXT:    [[TMP11:%.*]] = mul i64 25, [[TMP10]]
; CHECK-VF4UF1-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP9]], 0
; CHECK-VF4UF1-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP12]], i32 0
; CHECK-VF4UF1-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-VF4UF1-NEXT:    [[TMP13:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-VF4UF1-NEXT:    [[TMP14:%.*]] = add <vscale x 4 x i64> [[DOTSPLAT]], [[TMP13]]
; CHECK-VF4UF1-NEXT:    [[VECTOR_GEP:%.*]] = mul <vscale x 4 x i64> [[TMP14]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 25, i32 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-VF4UF1-NEXT:    [[TMP15:%.*]] = getelementptr double, double* [[POINTER_PHI]], <vscale x 4 x i64> [[VECTOR_GEP]]
; CHECK-VF4UF1-NEXT:    [[TMP16:%.*]] = getelementptr inbounds double, <vscale x 4 x double*> [[TMP15]], i64 [[IDXPROM]]
; CHECK-VF4UF1-NEXT:    [[WIDE_MASKED_GATHER]] = call <vscale x 4 x double> @llvm.masked.gather.nxv4f64.nxv4p0f64(<vscale x 4 x double*> [[TMP16]], i32 8, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x double> undef)
; CHECK-VF4UF1-NEXT:    [[TMP17:%.*]] = call <vscale x 4 x double> @llvm.experimental.vector.splice.nxv4f64(<vscale x 4 x double> [[VECTOR_RECUR]], <vscale x 4 x double> [[WIDE_MASKED_GATHER]], i32 -1)
; CHECK-VF4UF1-NEXT:    [[TMP18:%.*]] = fmul <vscale x 4 x double> [[TMP17]], [[WIDE_MASKED_GATHER]]
; CHECK-VF4UF1-NEXT:    [[TMP19:%.*]] = fcmp une <vscale x 4 x double> [[TMP18]], zeroinitializer
; CHECK-VF4UF1-NEXT:    [[TMP20:%.*]] = zext <vscale x 4 x i1> [[TMP19]] to <vscale x 4 x i32>
; CHECK-VF4UF1-NEXT:    [[TMP21]] = add <vscale x 4 x i32> [[VEC_PHI]], [[TMP20]]
; CHECK-VF4UF1-NEXT:    [[TMP22:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP23:%.*]] = mul i64 [[TMP22]], 4
; CHECK-VF4UF1-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP23]]
; CHECK-VF4UF1-NEXT:    [[PTR_IND]] = getelementptr double, double* [[POINTER_PHI]], i64 [[TMP11]]
; CHECK-VF4UF1-NEXT:    [[TMP24:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-VF4UF1-NEXT:    br i1 [[TMP24]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK-VF4UF1:       middle.block:
; CHECK-VF4UF1-NEXT:    [[TMP25:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[TMP21]])
; CHECK-VF4UF1-NEXT:    [[CMP_N:%.*]] = icmp eq i64 10240, [[N_VEC]]
; CHECK-VF4UF1-NEXT:    [[TMP26:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP27:%.*]] = mul i32 [[TMP26]], 4
; CHECK-VF4UF1-NEXT:    [[TMP28:%.*]] = sub i32 [[TMP27]], 1
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 4 x double> [[WIDE_MASKED_GATHER]], i32 [[TMP28]]
; CHECK-VF4UF1-NEXT:    [[TMP29:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP30:%.*]] = mul i32 [[TMP29]], 4
; CHECK-VF4UF1-NEXT:    [[TMP31:%.*]] = sub i32 [[TMP30]], 2
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_EXTRACT_FOR_PHI:%.*]] = extractelement <vscale x 4 x double> [[WIDE_MASKED_GATHER]], i32 [[TMP31]]
; CHECK-VF4UF1-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[SCALAR_PH]]
; CHECK-VF4UF1:       scalar.ph:
; CHECK-VF4UF1-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi double [ [[J]], [[ENTRY:%.*]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF1-NEXT:    [[BC_RESUME_VAL:%.*]] = phi double* [ [[IND_END]], [[MIDDLE_BLOCK]] ], [ [[B]], [[ENTRY]] ]
; CHECK-VF4UF1-NEXT:    [[BC_RESUME_VAL1:%.*]] = phi i32 [ [[IND_END2]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY]] ]
; CHECK-VF4UF1-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[ENTRY]] ], [ [[TMP25]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF1-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK-VF4UF1:       for.cond.cleanup:
; CHECK-VF4UF1-NEXT:    [[A_1_LCSSA:%.*]] = phi i32 [ [[A_1:%.*]], [[FOR_BODY]] ], [ [[TMP25]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF1-NEXT:    ret i32 [[A_1_LCSSA]]
; CHECK-VF4UF1:       for.body:
; CHECK-VF4UF1-NEXT:    [[B_ADDR_012:%.*]] = phi double* [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[ADD_PTR:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[I_011:%.*]] = phi i32 [ [[BC_RESUME_VAL1]], [[SCALAR_PH]] ], [ [[INC1:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[A_010:%.*]] = phi i32 [ [[BC_MERGE_RDX]], [[SCALAR_PH]] ], [ [[A_1]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[SCALAR_RECUR:%.*]] = phi double [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP32:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, double* [[B_ADDR_012]], i64 [[IDXPROM]]
; CHECK-VF4UF1-NEXT:    [[TMP32]] = load double, double* [[ARRAYIDX]], align 8
; CHECK-VF4UF1-NEXT:    [[MUL:%.*]] = fmul double [[SCALAR_RECUR]], [[TMP32]]
; CHECK-VF4UF1-NEXT:    [[TOBOOL:%.*]] = fcmp une double [[MUL]], 0.000000e+00
; CHECK-VF4UF1-NEXT:    [[INC:%.*]] = zext i1 [[TOBOOL]] to i32
; CHECK-VF4UF1-NEXT:    [[A_1]] = add nsw i32 [[A_010]], [[INC]]
; CHECK-VF4UF1-NEXT:    [[INC1]] = add nuw nsw i32 [[I_011]], 1
; CHECK-VF4UF1-NEXT:    [[ADD_PTR]] = getelementptr inbounds double, double* [[B_ADDR_012]], i64 25
; CHECK-VF4UF1-NEXT:    [[EXITCOND:%.*]] = icmp eq i32 [[INC1]], 10240
; CHECK-VF4UF1-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
;
; CHECK-VF4UF2-LABEL: @PR33613(
; CHECK-VF4UF2-NEXT:  entry:
; CHECK-VF4UF2-NEXT:    [[IDXPROM:%.*]] = sext i32 [[D:%.*]] to i64
; CHECK-VF4UF2-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
; CHECK-VF4UF2-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 10240, [[TMP1]]
; CHECK-VF4UF2-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_PH:%.*]]
; CHECK-VF4UF2:       vector.ph:
; CHECK-VF4UF2-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 8
; CHECK-VF4UF2-NEXT:    [[N_MOD_VF:%.*]] = urem i64 10240, [[TMP3]]
; CHECK-VF4UF2-NEXT:    [[N_VEC:%.*]] = sub i64 10240, [[N_MOD_VF]]
; CHECK-VF4UF2-NEXT:    [[TMP4:%.*]] = mul i64 [[N_VEC]], 25
; CHECK-VF4UF2-NEXT:    [[IND_END:%.*]] = getelementptr double, double* [[B:%.*]], i64 [[TMP4]]
; CHECK-VF4UF2-NEXT:    [[IND_END2:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-VF4UF2-NEXT:    [[TMP5:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP6:%.*]] = mul i32 [[TMP5]], 4
; CHECK-VF4UF2-NEXT:    [[TMP7:%.*]] = sub i32 [[TMP6]], 1
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 4 x double> poison, double [[J:%.*]], i32 [[TMP7]]
; CHECK-VF4UF2-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK-VF4UF2:       vector.body:
; CHECK-VF4UF2-NEXT:    [[POINTER_PHI:%.*]] = phi double* [ [[B]], [[VECTOR_PH]] ], [ [[PTR_IND:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[VEC_PHI:%.*]] = phi <vscale x 4 x i32> [ insertelement (<vscale x 4 x i32> zeroinitializer, i32 0, i32 0), [[VECTOR_PH]] ], [ [[TMP30:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[VEC_PHI6:%.*]] = phi <vscale x 4 x i32> [ zeroinitializer, [[VECTOR_PH]] ], [ [[TMP31:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 4 x double> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_MASKED_GATHER7:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[TMP8:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP9:%.*]] = mul i64 [[TMP8]], 4
; CHECK-VF4UF2-NEXT:    [[TMP10:%.*]] = mul i64 [[TMP9]], 2
; CHECK-VF4UF2-NEXT:    [[TMP11:%.*]] = mul i64 25, [[TMP10]]
; CHECK-VF4UF2-NEXT:    [[TMP12:%.*]] = mul i64 [[TMP9]], 0
; CHECK-VF4UF2-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP12]], i32 0
; CHECK-VF4UF2-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-VF4UF2-NEXT:    [[TMP13:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-VF4UF2-NEXT:    [[TMP14:%.*]] = add <vscale x 4 x i64> [[DOTSPLAT]], [[TMP13]]
; CHECK-VF4UF2-NEXT:    [[VECTOR_GEP:%.*]] = mul <vscale x 4 x i64> [[TMP14]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 25, i32 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-VF4UF2-NEXT:    [[TMP15:%.*]] = getelementptr double, double* [[POINTER_PHI]], <vscale x 4 x i64> [[VECTOR_GEP]]
; CHECK-VF4UF2-NEXT:    [[TMP16:%.*]] = mul i64 [[TMP9]], 1
; CHECK-VF4UF2-NEXT:    [[DOTSPLATINSERT3:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP16]], i32 0
; CHECK-VF4UF2-NEXT:    [[DOTSPLAT4:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT3]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-VF4UF2-NEXT:    [[TMP17:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-VF4UF2-NEXT:    [[TMP18:%.*]] = add <vscale x 4 x i64> [[DOTSPLAT4]], [[TMP17]]
; CHECK-VF4UF2-NEXT:    [[VECTOR_GEP5:%.*]] = mul <vscale x 4 x i64> [[TMP18]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 25, i32 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-VF4UF2-NEXT:    [[TMP19:%.*]] = getelementptr double, double* [[POINTER_PHI]], <vscale x 4 x i64> [[VECTOR_GEP5]]
; CHECK-VF4UF2-NEXT:    [[TMP20:%.*]] = getelementptr inbounds double, <vscale x 4 x double*> [[TMP15]], i64 [[IDXPROM]]
; CHECK-VF4UF2-NEXT:    [[TMP21:%.*]] = getelementptr inbounds double, <vscale x 4 x double*> [[TMP19]], i64 [[IDXPROM]]
; CHECK-VF4UF2-NEXT:    [[WIDE_MASKED_GATHER:%.*]] = call <vscale x 4 x double> @llvm.masked.gather.nxv4f64.nxv4p0f64(<vscale x 4 x double*> [[TMP20]], i32 8, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x double> undef)
; CHECK-VF4UF2-NEXT:    [[WIDE_MASKED_GATHER7]] = call <vscale x 4 x double> @llvm.masked.gather.nxv4f64.nxv4p0f64(<vscale x 4 x double*> [[TMP21]], i32 8, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x double> undef)
; CHECK-VF4UF2-NEXT:    [[TMP22:%.*]] = call <vscale x 4 x double> @llvm.experimental.vector.splice.nxv4f64(<vscale x 4 x double> [[VECTOR_RECUR]], <vscale x 4 x double> [[WIDE_MASKED_GATHER]], i32 -1)
; CHECK-VF4UF2-NEXT:    [[TMP23:%.*]] = call <vscale x 4 x double> @llvm.experimental.vector.splice.nxv4f64(<vscale x 4 x double> [[WIDE_MASKED_GATHER]], <vscale x 4 x double> [[WIDE_MASKED_GATHER7]], i32 -1)
; CHECK-VF4UF2-NEXT:    [[TMP24:%.*]] = fmul <vscale x 4 x double> [[TMP22]], [[WIDE_MASKED_GATHER]]
; CHECK-VF4UF2-NEXT:    [[TMP25:%.*]] = fmul <vscale x 4 x double> [[TMP23]], [[WIDE_MASKED_GATHER7]]
; CHECK-VF4UF2-NEXT:    [[TMP26:%.*]] = fcmp une <vscale x 4 x double> [[TMP24]], zeroinitializer
; CHECK-VF4UF2-NEXT:    [[TMP27:%.*]] = fcmp une <vscale x 4 x double> [[TMP25]], zeroinitializer
; CHECK-VF4UF2-NEXT:    [[TMP28:%.*]] = zext <vscale x 4 x i1> [[TMP26]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP29:%.*]] = zext <vscale x 4 x i1> [[TMP27]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP30]] = add <vscale x 4 x i32> [[VEC_PHI]], [[TMP28]]
; CHECK-VF4UF2-NEXT:    [[TMP31]] = add <vscale x 4 x i32> [[VEC_PHI6]], [[TMP29]]
; CHECK-VF4UF2-NEXT:    [[TMP32:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP33:%.*]] = mul i64 [[TMP32]], 8
; CHECK-VF4UF2-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP33]]
; CHECK-VF4UF2-NEXT:    [[PTR_IND]] = getelementptr double, double* [[POINTER_PHI]], i64 [[TMP11]]
; CHECK-VF4UF2-NEXT:    [[TMP34:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-VF4UF2-NEXT:    br i1 [[TMP34]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP0:![0-9]+]]
; CHECK-VF4UF2:       middle.block:
; CHECK-VF4UF2-NEXT:    [[BIN_RDX:%.*]] = add <vscale x 4 x i32> [[TMP31]], [[TMP30]]
; CHECK-VF4UF2-NEXT:    [[TMP35:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[BIN_RDX]])
; CHECK-VF4UF2-NEXT:    [[CMP_N:%.*]] = icmp eq i64 10240, [[N_VEC]]
; CHECK-VF4UF2-NEXT:    [[TMP36:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP37:%.*]] = mul i32 [[TMP36]], 4
; CHECK-VF4UF2-NEXT:    [[TMP38:%.*]] = sub i32 [[TMP37]], 1
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 4 x double> [[WIDE_MASKED_GATHER7]], i32 [[TMP38]]
; CHECK-VF4UF2-NEXT:    [[TMP39:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP40:%.*]] = mul i32 [[TMP39]], 4
; CHECK-VF4UF2-NEXT:    [[TMP41:%.*]] = sub i32 [[TMP40]], 2
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_EXTRACT_FOR_PHI:%.*]] = extractelement <vscale x 4 x double> [[WIDE_MASKED_GATHER7]], i32 [[TMP41]]
; CHECK-VF4UF2-NEXT:    br i1 [[CMP_N]], label [[FOR_COND_CLEANUP:%.*]], label [[SCALAR_PH]]
; CHECK-VF4UF2:       scalar.ph:
; CHECK-VF4UF2-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi double [ [[J]], [[ENTRY:%.*]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF2-NEXT:    [[BC_RESUME_VAL:%.*]] = phi double* [ [[IND_END]], [[MIDDLE_BLOCK]] ], [ [[B]], [[ENTRY]] ]
; CHECK-VF4UF2-NEXT:    [[BC_RESUME_VAL1:%.*]] = phi i32 [ [[IND_END2]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY]] ]
; CHECK-VF4UF2-NEXT:    [[BC_MERGE_RDX:%.*]] = phi i32 [ 0, [[ENTRY]] ], [ [[TMP35]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF2-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK-VF4UF2:       for.cond.cleanup:
; CHECK-VF4UF2-NEXT:    [[A_1_LCSSA:%.*]] = phi i32 [ [[A_1:%.*]], [[FOR_BODY]] ], [ [[TMP35]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF2-NEXT:    ret i32 [[A_1_LCSSA]]
; CHECK-VF4UF2:       for.body:
; CHECK-VF4UF2-NEXT:    [[B_ADDR_012:%.*]] = phi double* [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[ADD_PTR:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[I_011:%.*]] = phi i32 [ [[BC_RESUME_VAL1]], [[SCALAR_PH]] ], [ [[INC1:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[A_010:%.*]] = phi i32 [ [[BC_MERGE_RDX]], [[SCALAR_PH]] ], [ [[A_1]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[SCALAR_RECUR:%.*]] = phi double [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP42:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds double, double* [[B_ADDR_012]], i64 [[IDXPROM]]
; CHECK-VF4UF2-NEXT:    [[TMP42]] = load double, double* [[ARRAYIDX]], align 8
; CHECK-VF4UF2-NEXT:    [[MUL:%.*]] = fmul double [[SCALAR_RECUR]], [[TMP42]]
; CHECK-VF4UF2-NEXT:    [[TOBOOL:%.*]] = fcmp une double [[MUL]], 0.000000e+00
; CHECK-VF4UF2-NEXT:    [[INC:%.*]] = zext i1 [[TOBOOL]] to i32
; CHECK-VF4UF2-NEXT:    [[A_1]] = add nsw i32 [[A_010]], [[INC]]
; CHECK-VF4UF2-NEXT:    [[INC1]] = add nuw nsw i32 [[I_011]], 1
; CHECK-VF4UF2-NEXT:    [[ADD_PTR]] = getelementptr inbounds double, double* [[B_ADDR_012]], i64 25
; CHECK-VF4UF2-NEXT:    [[EXITCOND:%.*]] = icmp eq i32 [[INC1]], 10240
; CHECK-VF4UF2-NEXT:    br i1 [[EXITCOND]], label [[FOR_COND_CLEANUP]], label [[FOR_BODY]], !llvm.loop [[LOOP2:![0-9]+]]
;
entry:
  %idxprom = sext i32 %d to i64
  br label %for.body

for.cond.cleanup:
  %a.1.lcssa = phi i32 [ %a.1, %for.body ]
  ret i32 %a.1.lcssa

for.body:
  %b.addr.012 = phi double* [ %b, %entry ], [ %add.ptr, %for.body ]
  %i.011 = phi i32 [ 0, %entry ], [ %inc1, %for.body ]
  %a.010 = phi i32 [ 0, %entry ], [ %a.1, %for.body ]
  %j.addr.09 = phi double [ %j, %entry ], [ %0, %for.body ]
  %arrayidx = getelementptr inbounds double, double* %b.addr.012, i64 %idxprom
  %0 = load double, double* %arrayidx, align 8
  %mul = fmul double %j.addr.09, %0
  %tobool = fcmp une double %mul, 0.000000e+00
  %inc = zext i1 %tobool to i32
  %a.1 = add nsw i32 %a.010, %inc
  %inc1 = add nuw nsw i32 %i.011, 1
  %add.ptr = getelementptr inbounds double, double* %b.addr.012, i64 25
  %exitcond = icmp eq i32 %inc1, 10240
  br i1 %exitcond, label %for.cond.cleanup, label %for.body, !llvm.loop !0
}

; PR34711: given three consecutive instructions such that the first will be
; widened, the second is a cast that will be widened and needs to sink after the
; third, and the third is a first-order-recurring load that will be replicated
; instead of widened. Although the cast and the first instruction will both be
; widened, and are originally adjacent to each other, make sure the replicated
; load ends up appearing between them.
;
; void PR34711(short[2] *a, int *b, int *c, int n) {
;   for(int i = 0; i < n; i++) {
;     c[i] = 7;
;     b[i] = (a[i][0] * a[i][1]);
;   }
; }
;
; Check that the sext sank after the load in the vector loop.
define void @PR34711([2 x i16]* %a, i32* %b, i32* %c, i64 %n) #0 {
; CHECK-VF4UF1-LABEL: @PR34711(
; CHECK-VF4UF1-NEXT:  entry:
; CHECK-VF4UF1-NEXT:    [[C1:%.*]] = bitcast i32* [[C:%.*]] to i8*
; CHECK-VF4UF1-NEXT:    [[B3:%.*]] = bitcast i32* [[B:%.*]] to i8*
; CHECK-VF4UF1-NEXT:    [[PRE_INDEX:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A:%.*]], i64 0, i64 0
; CHECK-VF4UF1-NEXT:    [[DOTPRE:%.*]] = load i16, i16* [[PRE_INDEX]], align 2
; CHECK-VF4UF1-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 4
; CHECK-VF4UF1-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ule i64 [[N:%.*]], [[TMP1]]
; CHECK-VF4UF1-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_MEMCHECK:%.*]]
; CHECK-VF4UF1:       vector.memcheck:
; CHECK-VF4UF1-NEXT:    [[SCEVGEP:%.*]] = getelementptr i32, i32* [[C]], i64 [[N]]
; CHECK-VF4UF1-NEXT:    [[SCEVGEP2:%.*]] = bitcast i32* [[SCEVGEP]] to i8*
; CHECK-VF4UF1-NEXT:    [[SCEVGEP4:%.*]] = getelementptr i32, i32* [[B]], i64 [[N]]
; CHECK-VF4UF1-NEXT:    [[SCEVGEP45:%.*]] = bitcast i32* [[SCEVGEP4]] to i8*
; CHECK-VF4UF1-NEXT:    [[SCEVGEP6:%.*]] = getelementptr [2 x i16], [2 x i16]* [[A]], i64 0, i64 1
; CHECK-VF4UF1-NEXT:    [[SCEVGEP67:%.*]] = bitcast i16* [[SCEVGEP6]] to i8*
; CHECK-VF4UF1-NEXT:    [[SCEVGEP8:%.*]] = getelementptr [2 x i16], [2 x i16]* [[A]], i64 [[N]], i64 0
; CHECK-VF4UF1-NEXT:    [[SCEVGEP89:%.*]] = bitcast i16* [[SCEVGEP8]] to i8*
; CHECK-VF4UF1-NEXT:    [[BOUND0:%.*]] = icmp ult i8* [[C1]], [[SCEVGEP45]]
; CHECK-VF4UF1-NEXT:    [[BOUND1:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP2]]
; CHECK-VF4UF1-NEXT:    [[FOUND_CONFLICT:%.*]] = and i1 [[BOUND0]], [[BOUND1]]
; CHECK-VF4UF1-NEXT:    [[BOUND010:%.*]] = icmp ult i8* [[C1]], [[SCEVGEP89]]
; CHECK-VF4UF1-NEXT:    [[BOUND111:%.*]] = icmp ult i8* [[SCEVGEP67]], [[SCEVGEP2]]
; CHECK-VF4UF1-NEXT:    [[FOUND_CONFLICT12:%.*]] = and i1 [[BOUND010]], [[BOUND111]]
; CHECK-VF4UF1-NEXT:    [[CONFLICT_RDX:%.*]] = or i1 [[FOUND_CONFLICT]], [[FOUND_CONFLICT12]]
; CHECK-VF4UF1-NEXT:    [[BOUND013:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP89]]
; CHECK-VF4UF1-NEXT:    [[BOUND114:%.*]] = icmp ult i8* [[SCEVGEP67]], [[SCEVGEP45]]
; CHECK-VF4UF1-NEXT:    [[FOUND_CONFLICT15:%.*]] = and i1 [[BOUND013]], [[BOUND114]]
; CHECK-VF4UF1-NEXT:    [[CONFLICT_RDX16:%.*]] = or i1 [[CONFLICT_RDX]], [[FOUND_CONFLICT15]]
; CHECK-VF4UF1-NEXT:    br i1 [[CONFLICT_RDX16]], label [[SCALAR_PH]], label [[VECTOR_PH:%.*]]
; CHECK-VF4UF1:       vector.ph:
; CHECK-VF4UF1-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 4
; CHECK-VF4UF1-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], [[TMP3]]
; CHECK-VF4UF1-NEXT:    [[TMP4:%.*]] = icmp eq i64 [[N_MOD_VF]], 0
; CHECK-VF4UF1-NEXT:    [[TMP5:%.*]] = select i1 [[TMP4]], i64 [[TMP3]], i64 [[N_MOD_VF]]
; CHECK-VF4UF1-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[TMP5]]
; CHECK-VF4UF1-NEXT:    [[TMP6:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP7:%.*]] = mul i32 [[TMP6]], 4
; CHECK-VF4UF1-NEXT:    [[TMP8:%.*]] = sub i32 [[TMP7]], 1
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 4 x i16> poison, i16 [[DOTPRE]], i32 [[TMP8]]
; CHECK-VF4UF1-NEXT:    [[TMP9:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-VF4UF1-NEXT:    [[TMP10:%.*]] = add <vscale x 4 x i64> [[TMP9]], zeroinitializer
; CHECK-VF4UF1-NEXT:    [[TMP11:%.*]] = mul <vscale x 4 x i64> [[TMP10]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 1, i32 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-VF4UF1-NEXT:    [[INDUCTION:%.*]] = add <vscale x 4 x i64> zeroinitializer, [[TMP11]]
; CHECK-VF4UF1-NEXT:    [[TMP12:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP12]], 4
; CHECK-VF4UF1-NEXT:    [[TMP14:%.*]] = mul i64 1, [[TMP13]]
; CHECK-VF4UF1-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP14]], i32 0
; CHECK-VF4UF1-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-VF4UF1-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK-VF4UF1:       vector.body:
; CHECK-VF4UF1-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 4 x i16> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_MASKED_GATHER:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[VEC_IND:%.*]] = phi <vscale x 4 x i64> [ [[INDUCTION]], [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[TMP15:%.*]] = add i64 [[INDEX]], 0
; CHECK-VF4UF1-NEXT:    [[TMP16:%.*]] = getelementptr inbounds i32, i32* [[C]], i64 [[TMP15]]
; CHECK-VF4UF1-NEXT:    [[TMP17:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A]], <vscale x 4 x i64> [[VEC_IND]], i64 1
; CHECK-VF4UF1-NEXT:    [[TMP18:%.*]] = getelementptr inbounds i32, i32* [[TMP16]], i32 0
; CHECK-VF4UF1-NEXT:    [[TMP19:%.*]] = bitcast i32* [[TMP18]] to <vscale x 4 x i32>*
; CHECK-VF4UF1-NEXT:    store <vscale x 4 x i32> shufflevector (<vscale x 4 x i32> insertelement (<vscale x 4 x i32> poison, i32 7, i32 0), <vscale x 4 x i32> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i32>* [[TMP19]], align 4, !alias.scope !4, !noalias !7
; CHECK-VF4UF1-NEXT:    [[WIDE_MASKED_GATHER]] = call <vscale x 4 x i16> @llvm.masked.gather.nxv4i16.nxv4p0i16(<vscale x 4 x i16*> [[TMP17]], i32 2, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i16> undef), !alias.scope !10
; CHECK-VF4UF1-NEXT:    [[TMP20:%.*]] = call <vscale x 4 x i16> @llvm.experimental.vector.splice.nxv4i16(<vscale x 4 x i16> [[VECTOR_RECUR]], <vscale x 4 x i16> [[WIDE_MASKED_GATHER]], i32 -1)
; CHECK-VF4UF1-NEXT:    [[TMP21:%.*]] = sext <vscale x 4 x i16> [[TMP20]] to <vscale x 4 x i32>
; CHECK-VF4UF1-NEXT:    [[TMP22:%.*]] = sext <vscale x 4 x i16> [[WIDE_MASKED_GATHER]] to <vscale x 4 x i32>
; CHECK-VF4UF1-NEXT:    [[TMP23:%.*]] = mul nsw <vscale x 4 x i32> [[TMP22]], [[TMP21]]
; CHECK-VF4UF1-NEXT:    [[TMP24:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[TMP15]]
; CHECK-VF4UF1-NEXT:    [[TMP25:%.*]] = getelementptr inbounds i32, i32* [[TMP24]], i32 0
; CHECK-VF4UF1-NEXT:    [[TMP26:%.*]] = bitcast i32* [[TMP25]] to <vscale x 4 x i32>*
; CHECK-VF4UF1-NEXT:    store <vscale x 4 x i32> [[TMP23]], <vscale x 4 x i32>* [[TMP26]], align 4, !alias.scope !11, !noalias !10
; CHECK-VF4UF1-NEXT:    [[TMP27:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF1-NEXT:    [[TMP28:%.*]] = mul i64 [[TMP27]], 4
; CHECK-VF4UF1-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP28]]
; CHECK-VF4UF1-NEXT:    [[VEC_IND_NEXT]] = add <vscale x 4 x i64> [[VEC_IND]], [[DOTSPLAT]]
; CHECK-VF4UF1-NEXT:    [[TMP29:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-VF4UF1-NEXT:    br i1 [[TMP29]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; CHECK-VF4UF1:       middle.block:
; CHECK-VF4UF1-NEXT:    [[TMP30:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP31:%.*]] = mul i32 [[TMP30]], 4
; CHECK-VF4UF1-NEXT:    [[TMP32:%.*]] = sub i32 [[TMP31]], 1
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 4 x i16> [[WIDE_MASKED_GATHER]], i32 [[TMP32]]
; CHECK-VF4UF1-NEXT:    [[TMP33:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF1-NEXT:    [[TMP34:%.*]] = mul i32 [[TMP33]], 4
; CHECK-VF4UF1-NEXT:    [[TMP35:%.*]] = sub i32 [[TMP34]], 2
; CHECK-VF4UF1-NEXT:    [[VECTOR_RECUR_EXTRACT_FOR_PHI:%.*]] = extractelement <vscale x 4 x i16> [[WIDE_MASKED_GATHER]], i32 [[TMP35]]
; CHECK-VF4UF1-NEXT:    br label [[SCALAR_PH]]
; CHECK-VF4UF1:       scalar.ph:
; CHECK-VF4UF1-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i16 [ [[DOTPRE]], [[VECTOR_MEMCHECK]] ], [ [[DOTPRE]], [[ENTRY:%.*]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF1-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY]] ], [ 0, [[VECTOR_MEMCHECK]] ]
; CHECK-VF4UF1-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK-VF4UF1:       for.body:
; CHECK-VF4UF1-NEXT:    [[SCALAR_RECUR:%.*]] = phi i16 [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP36:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF1-NEXT:    [[ARRAYCIDX:%.*]] = getelementptr inbounds i32, i32* [[C]], i64 [[INDVARS_IV]]
; CHECK-VF4UF1-NEXT:    [[CUR_INDEX:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A]], i64 [[INDVARS_IV]], i64 1
; CHECK-VF4UF1-NEXT:    store i32 7, i32* [[ARRAYCIDX]], align 4
; CHECK-VF4UF1-NEXT:    [[CONV:%.*]] = sext i16 [[SCALAR_RECUR]] to i32
; CHECK-VF4UF1-NEXT:    [[TMP36]] = load i16, i16* [[CUR_INDEX]], align 2
; CHECK-VF4UF1-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP36]] to i32
; CHECK-VF4UF1-NEXT:    [[MUL:%.*]] = mul nsw i32 [[CONV3]], [[CONV]]
; CHECK-VF4UF1-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[INDVARS_IV]]
; CHECK-VF4UF1-NEXT:    store i32 [[MUL]], i32* [[ARRAYIDX5]], align 4
; CHECK-VF4UF1-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-VF4UF1-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], [[N]]
; CHECK-VF4UF1-NEXT:    br i1 [[EXITCOND]], label [[FOR_END:%.*]], label [[FOR_BODY]], !llvm.loop [[LOOP13:![0-9]+]]
; CHECK-VF4UF1:       for.end:
; CHECK-VF4UF1-NEXT:    ret void
;
; CHECK-VF4UF2-LABEL: @PR34711(
; CHECK-VF4UF2-NEXT:  entry:
; CHECK-VF4UF2-NEXT:    [[C1:%.*]] = bitcast i32* [[C:%.*]] to i8*
; CHECK-VF4UF2-NEXT:    [[B3:%.*]] = bitcast i32* [[B:%.*]] to i8*
; CHECK-VF4UF2-NEXT:    [[PRE_INDEX:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A:%.*]], i64 0, i64 0
; CHECK-VF4UF2-NEXT:    [[DOTPRE:%.*]] = load i16, i16* [[PRE_INDEX]], align 2
; CHECK-VF4UF2-NEXT:    [[TMP0:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP1:%.*]] = mul i64 [[TMP0]], 8
; CHECK-VF4UF2-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ule i64 [[N:%.*]], [[TMP1]]
; CHECK-VF4UF2-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_MEMCHECK:%.*]]
; CHECK-VF4UF2:       vector.memcheck:
; CHECK-VF4UF2-NEXT:    [[SCEVGEP:%.*]] = getelementptr i32, i32* [[C]], i64 [[N]]
; CHECK-VF4UF2-NEXT:    [[SCEVGEP2:%.*]] = bitcast i32* [[SCEVGEP]] to i8*
; CHECK-VF4UF2-NEXT:    [[SCEVGEP4:%.*]] = getelementptr i32, i32* [[B]], i64 [[N]]
; CHECK-VF4UF2-NEXT:    [[SCEVGEP45:%.*]] = bitcast i32* [[SCEVGEP4]] to i8*
; CHECK-VF4UF2-NEXT:    [[SCEVGEP6:%.*]] = getelementptr [2 x i16], [2 x i16]* [[A]], i64 0, i64 1
; CHECK-VF4UF2-NEXT:    [[SCEVGEP67:%.*]] = bitcast i16* [[SCEVGEP6]] to i8*
; CHECK-VF4UF2-NEXT:    [[SCEVGEP8:%.*]] = getelementptr [2 x i16], [2 x i16]* [[A]], i64 [[N]], i64 0
; CHECK-VF4UF2-NEXT:    [[SCEVGEP89:%.*]] = bitcast i16* [[SCEVGEP8]] to i8*
; CHECK-VF4UF2-NEXT:    [[BOUND0:%.*]] = icmp ult i8* [[C1]], [[SCEVGEP45]]
; CHECK-VF4UF2-NEXT:    [[BOUND1:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP2]]
; CHECK-VF4UF2-NEXT:    [[FOUND_CONFLICT:%.*]] = and i1 [[BOUND0]], [[BOUND1]]
; CHECK-VF4UF2-NEXT:    [[BOUND010:%.*]] = icmp ult i8* [[C1]], [[SCEVGEP89]]
; CHECK-VF4UF2-NEXT:    [[BOUND111:%.*]] = icmp ult i8* [[SCEVGEP67]], [[SCEVGEP2]]
; CHECK-VF4UF2-NEXT:    [[FOUND_CONFLICT12:%.*]] = and i1 [[BOUND010]], [[BOUND111]]
; CHECK-VF4UF2-NEXT:    [[CONFLICT_RDX:%.*]] = or i1 [[FOUND_CONFLICT]], [[FOUND_CONFLICT12]]
; CHECK-VF4UF2-NEXT:    [[BOUND013:%.*]] = icmp ult i8* [[B3]], [[SCEVGEP89]]
; CHECK-VF4UF2-NEXT:    [[BOUND114:%.*]] = icmp ult i8* [[SCEVGEP67]], [[SCEVGEP45]]
; CHECK-VF4UF2-NEXT:    [[FOUND_CONFLICT15:%.*]] = and i1 [[BOUND013]], [[BOUND114]]
; CHECK-VF4UF2-NEXT:    [[CONFLICT_RDX16:%.*]] = or i1 [[CONFLICT_RDX]], [[FOUND_CONFLICT15]]
; CHECK-VF4UF2-NEXT:    br i1 [[CONFLICT_RDX16]], label [[SCALAR_PH]], label [[VECTOR_PH:%.*]]
; CHECK-VF4UF2:       vector.ph:
; CHECK-VF4UF2-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP3:%.*]] = mul i64 [[TMP2]], 8
; CHECK-VF4UF2-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[N]], [[TMP3]]
; CHECK-VF4UF2-NEXT:    [[TMP4:%.*]] = icmp eq i64 [[N_MOD_VF]], 0
; CHECK-VF4UF2-NEXT:    [[TMP5:%.*]] = select i1 [[TMP4]], i64 [[TMP3]], i64 [[N_MOD_VF]]
; CHECK-VF4UF2-NEXT:    [[N_VEC:%.*]] = sub i64 [[N]], [[TMP5]]
; CHECK-VF4UF2-NEXT:    [[TMP6:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP7:%.*]] = mul i32 [[TMP6]], 4
; CHECK-VF4UF2-NEXT:    [[TMP8:%.*]] = sub i32 [[TMP7]], 1
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_INIT:%.*]] = insertelement <vscale x 4 x i16> poison, i16 [[DOTPRE]], i32 [[TMP8]]
; CHECK-VF4UF2-NEXT:    [[TMP9:%.*]] = call <vscale x 4 x i64> @llvm.experimental.stepvector.nxv4i64()
; CHECK-VF4UF2-NEXT:    [[TMP10:%.*]] = add <vscale x 4 x i64> [[TMP9]], zeroinitializer
; CHECK-VF4UF2-NEXT:    [[TMP11:%.*]] = mul <vscale x 4 x i64> [[TMP10]], shufflevector (<vscale x 4 x i64> insertelement (<vscale x 4 x i64> poison, i64 1, i32 0), <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer)
; CHECK-VF4UF2-NEXT:    [[INDUCTION:%.*]] = add <vscale x 4 x i64> zeroinitializer, [[TMP11]]
; CHECK-VF4UF2-NEXT:    [[TMP12:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP13:%.*]] = mul i64 [[TMP12]], 4
; CHECK-VF4UF2-NEXT:    [[TMP14:%.*]] = mul i64 1, [[TMP13]]
; CHECK-VF4UF2-NEXT:    [[DOTSPLATINSERT:%.*]] = insertelement <vscale x 4 x i64> poison, i64 [[TMP14]], i32 0
; CHECK-VF4UF2-NEXT:    [[DOTSPLAT:%.*]] = shufflevector <vscale x 4 x i64> [[DOTSPLATINSERT]], <vscale x 4 x i64> poison, <vscale x 4 x i32> zeroinitializer
; CHECK-VF4UF2-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK-VF4UF2:       vector.body:
; CHECK-VF4UF2-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR:%.*]] = phi <vscale x 4 x i16> [ [[VECTOR_RECUR_INIT]], [[VECTOR_PH]] ], [ [[WIDE_MASKED_GATHER18:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[VEC_IND:%.*]] = phi <vscale x 4 x i64> [ [[INDUCTION]], [[VECTOR_PH]] ], [ [[VEC_IND_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[STEP_ADD:%.*]] = add <vscale x 4 x i64> [[VEC_IND]], [[DOTSPLAT]]
; CHECK-VF4UF2-NEXT:    [[TMP15:%.*]] = add i64 [[INDEX]], 0
; CHECK-VF4UF2-NEXT:    [[TMP16:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP17:%.*]] = mul i64 [[TMP16]], 4
; CHECK-VF4UF2-NEXT:    [[TMP18:%.*]] = add i64 [[TMP17]], 0
; CHECK-VF4UF2-NEXT:    [[TMP19:%.*]] = mul i64 [[TMP18]], 1
; CHECK-VF4UF2-NEXT:    [[TMP20:%.*]] = add i64 [[INDEX]], [[TMP19]]
; CHECK-VF4UF2-NEXT:    [[TMP21:%.*]] = getelementptr inbounds i32, i32* [[C]], i64 [[TMP15]]
; CHECK-VF4UF2-NEXT:    [[TMP22:%.*]] = getelementptr inbounds i32, i32* [[C]], i64 [[TMP20]]
; CHECK-VF4UF2-NEXT:    [[TMP23:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A]], <vscale x 4 x i64> [[VEC_IND]], i64 1
; CHECK-VF4UF2-NEXT:    [[TMP24:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A]], <vscale x 4 x i64> [[STEP_ADD]], i64 1
; CHECK-VF4UF2-NEXT:    [[TMP25:%.*]] = getelementptr inbounds i32, i32* [[TMP21]], i32 0
; CHECK-VF4UF2-NEXT:    [[TMP26:%.*]] = bitcast i32* [[TMP25]] to <vscale x 4 x i32>*
; CHECK-VF4UF2-NEXT:    store <vscale x 4 x i32> shufflevector (<vscale x 4 x i32> insertelement (<vscale x 4 x i32> poison, i32 7, i32 0), <vscale x 4 x i32> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i32>* [[TMP26]], align 4, !alias.scope !4, !noalias !7
; CHECK-VF4UF2-NEXT:    [[TMP27:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP28:%.*]] = mul i32 [[TMP27]], 4
; CHECK-VF4UF2-NEXT:    [[TMP29:%.*]] = getelementptr inbounds i32, i32* [[TMP21]], i32 [[TMP28]]
; CHECK-VF4UF2-NEXT:    [[TMP30:%.*]] = bitcast i32* [[TMP29]] to <vscale x 4 x i32>*
; CHECK-VF4UF2-NEXT:    store <vscale x 4 x i32> shufflevector (<vscale x 4 x i32> insertelement (<vscale x 4 x i32> poison, i32 7, i32 0), <vscale x 4 x i32> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i32>* [[TMP30]], align 4, !alias.scope !4, !noalias !7
; CHECK-VF4UF2-NEXT:    [[WIDE_MASKED_GATHER:%.*]] = call <vscale x 4 x i16> @llvm.masked.gather.nxv4i16.nxv4p0i16(<vscale x 4 x i16*> [[TMP23]], i32 2, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i16> undef), !alias.scope !10
; CHECK-VF4UF2-NEXT:    [[WIDE_MASKED_GATHER18]] = call <vscale x 4 x i16> @llvm.masked.gather.nxv4i16.nxv4p0i16(<vscale x 4 x i16*> [[TMP24]], i32 2, <vscale x 4 x i1> shufflevector (<vscale x 4 x i1> insertelement (<vscale x 4 x i1> poison, i1 true, i32 0), <vscale x 4 x i1> poison, <vscale x 4 x i32> zeroinitializer), <vscale x 4 x i16> undef), !alias.scope !10
; CHECK-VF4UF2-NEXT:    [[TMP31:%.*]] = call <vscale x 4 x i16> @llvm.experimental.vector.splice.nxv4i16(<vscale x 4 x i16> [[VECTOR_RECUR]], <vscale x 4 x i16> [[WIDE_MASKED_GATHER]], i32 -1)
; CHECK-VF4UF2-NEXT:    [[TMP32:%.*]] = call <vscale x 4 x i16> @llvm.experimental.vector.splice.nxv4i16(<vscale x 4 x i16> [[WIDE_MASKED_GATHER]], <vscale x 4 x i16> [[WIDE_MASKED_GATHER18]], i32 -1)
; CHECK-VF4UF2-NEXT:    [[TMP33:%.*]] = sext <vscale x 4 x i16> [[TMP31]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP34:%.*]] = sext <vscale x 4 x i16> [[TMP32]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP35:%.*]] = sext <vscale x 4 x i16> [[WIDE_MASKED_GATHER]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP36:%.*]] = sext <vscale x 4 x i16> [[WIDE_MASKED_GATHER18]] to <vscale x 4 x i32>
; CHECK-VF4UF2-NEXT:    [[TMP37:%.*]] = mul nsw <vscale x 4 x i32> [[TMP35]], [[TMP33]]
; CHECK-VF4UF2-NEXT:    [[TMP38:%.*]] = mul nsw <vscale x 4 x i32> [[TMP36]], [[TMP34]]
; CHECK-VF4UF2-NEXT:    [[TMP39:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[TMP15]]
; CHECK-VF4UF2-NEXT:    [[TMP40:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[TMP20]]
; CHECK-VF4UF2-NEXT:    [[TMP41:%.*]] = getelementptr inbounds i32, i32* [[TMP39]], i32 0
; CHECK-VF4UF2-NEXT:    [[TMP42:%.*]] = bitcast i32* [[TMP41]] to <vscale x 4 x i32>*
; CHECK-VF4UF2-NEXT:    store <vscale x 4 x i32> [[TMP37]], <vscale x 4 x i32>* [[TMP42]], align 4, !alias.scope !11, !noalias !10
; CHECK-VF4UF2-NEXT:    [[TMP43:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP44:%.*]] = mul i32 [[TMP43]], 4
; CHECK-VF4UF2-NEXT:    [[TMP45:%.*]] = getelementptr inbounds i32, i32* [[TMP39]], i32 [[TMP44]]
; CHECK-VF4UF2-NEXT:    [[TMP46:%.*]] = bitcast i32* [[TMP45]] to <vscale x 4 x i32>*
; CHECK-VF4UF2-NEXT:    store <vscale x 4 x i32> [[TMP38]], <vscale x 4 x i32>* [[TMP46]], align 4, !alias.scope !11, !noalias !10
; CHECK-VF4UF2-NEXT:    [[TMP47:%.*]] = call i64 @llvm.vscale.i64()
; CHECK-VF4UF2-NEXT:    [[TMP48:%.*]] = mul i64 [[TMP47]], 8
; CHECK-VF4UF2-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], [[TMP48]]
; CHECK-VF4UF2-NEXT:    [[VEC_IND_NEXT]] = add <vscale x 4 x i64> [[STEP_ADD]], [[DOTSPLAT]]
; CHECK-VF4UF2-NEXT:    [[TMP49:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-VF4UF2-NEXT:    br i1 [[TMP49]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP12:![0-9]+]]
; CHECK-VF4UF2:       middle.block:
; CHECK-VF4UF2-NEXT:    [[TMP50:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP51:%.*]] = mul i32 [[TMP50]], 4
; CHECK-VF4UF2-NEXT:    [[TMP52:%.*]] = sub i32 [[TMP51]], 1
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_EXTRACT:%.*]] = extractelement <vscale x 4 x i16> [[WIDE_MASKED_GATHER18]], i32 [[TMP52]]
; CHECK-VF4UF2-NEXT:    [[TMP53:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-VF4UF2-NEXT:    [[TMP54:%.*]] = mul i32 [[TMP53]], 4
; CHECK-VF4UF2-NEXT:    [[TMP55:%.*]] = sub i32 [[TMP54]], 2
; CHECK-VF4UF2-NEXT:    [[VECTOR_RECUR_EXTRACT_FOR_PHI:%.*]] = extractelement <vscale x 4 x i16> [[WIDE_MASKED_GATHER18]], i32 [[TMP55]]
; CHECK-VF4UF2-NEXT:    br label [[SCALAR_PH]]
; CHECK-VF4UF2:       scalar.ph:
; CHECK-VF4UF2-NEXT:    [[SCALAR_RECUR_INIT:%.*]] = phi i16 [ [[DOTPRE]], [[VECTOR_MEMCHECK]] ], [ [[DOTPRE]], [[ENTRY:%.*]] ], [ [[VECTOR_RECUR_EXTRACT]], [[MIDDLE_BLOCK]] ]
; CHECK-VF4UF2-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[N_VEC]], [[MIDDLE_BLOCK]] ], [ 0, [[ENTRY]] ], [ 0, [[VECTOR_MEMCHECK]] ]
; CHECK-VF4UF2-NEXT:    br label [[FOR_BODY:%.*]]
; CHECK-VF4UF2:       for.body:
; CHECK-VF4UF2-NEXT:    [[SCALAR_RECUR:%.*]] = phi i16 [ [[SCALAR_RECUR_INIT]], [[SCALAR_PH]] ], [ [[TMP56:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[INDVARS_IV:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[INDVARS_IV_NEXT:%.*]], [[FOR_BODY]] ]
; CHECK-VF4UF2-NEXT:    [[ARRAYCIDX:%.*]] = getelementptr inbounds i32, i32* [[C]], i64 [[INDVARS_IV]]
; CHECK-VF4UF2-NEXT:    [[CUR_INDEX:%.*]] = getelementptr inbounds [2 x i16], [2 x i16]* [[A]], i64 [[INDVARS_IV]], i64 1
; CHECK-VF4UF2-NEXT:    store i32 7, i32* [[ARRAYCIDX]], align 4
; CHECK-VF4UF2-NEXT:    [[CONV:%.*]] = sext i16 [[SCALAR_RECUR]] to i32
; CHECK-VF4UF2-NEXT:    [[TMP56]] = load i16, i16* [[CUR_INDEX]], align 2
; CHECK-VF4UF2-NEXT:    [[CONV3:%.*]] = sext i16 [[TMP56]] to i32
; CHECK-VF4UF2-NEXT:    [[MUL:%.*]] = mul nsw i32 [[CONV3]], [[CONV]]
; CHECK-VF4UF2-NEXT:    [[ARRAYIDX5:%.*]] = getelementptr inbounds i32, i32* [[B]], i64 [[INDVARS_IV]]
; CHECK-VF4UF2-NEXT:    store i32 [[MUL]], i32* [[ARRAYIDX5]], align 4
; CHECK-VF4UF2-NEXT:    [[INDVARS_IV_NEXT]] = add nuw nsw i64 [[INDVARS_IV]], 1
; CHECK-VF4UF2-NEXT:    [[EXITCOND:%.*]] = icmp eq i64 [[INDVARS_IV_NEXT]], [[N]]
; CHECK-VF4UF2-NEXT:    br i1 [[EXITCOND]], label [[FOR_END:%.*]], label [[FOR_BODY]], !llvm.loop [[LOOP13:![0-9]+]]
; CHECK-VF4UF2:       for.end:
; CHECK-VF4UF2-NEXT:    ret void
;
entry:
  %pre.index = getelementptr inbounds [2 x i16], [2 x i16]* %a, i64 0, i64 0
  %.pre = load i16, i16* %pre.index
  br label %for.body

for.body:
  %0 = phi i16 [ %.pre, %entry ], [ %1, %for.body ]
  %indvars.iv = phi i64 [ 0, %entry ], [ %indvars.iv.next, %for.body ]
  %arraycidx = getelementptr inbounds i32, i32* %c, i64 %indvars.iv
  %cur.index = getelementptr inbounds [2 x i16], [2 x i16]* %a, i64 %indvars.iv, i64 1
  store i32 7, i32* %arraycidx   ; 1st instruction, to be widened.
  %conv = sext i16 %0 to i32     ; 2nd, cast to sink after third.
  %1 = load i16, i16* %cur.index ; 3rd, first-order-recurring load not widened.
  %conv3 = sext i16 %1 to i32
  %mul = mul nsw i32 %conv3, %conv
  %arrayidx5 = getelementptr inbounds i32, i32* %b, i64 %indvars.iv
  store i32 %mul, i32* %arrayidx5
  %indvars.iv.next = add nuw nsw i64 %indvars.iv, 1
  %exitcond = icmp eq i64 %indvars.iv.next, %n
  br i1 %exitcond, label %for.end, label %for.body, !llvm.loop !0

for.end:
  ret void
}

attributes #0 = { vscale_range(1, 16) }
!0 = distinct !{!0, !1}
!1 = !{!"llvm.loop.vectorize.scalable.enable", i1 true}
