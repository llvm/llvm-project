; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt -loop-vectorize -force-vector-width=2 %s -S -debug 2>&1 | FileCheck %s
; RUN: opt -passes='loop-vectorize' -force-vector-width=2 %s -S -debug 2>&1 | FileCheck %s

; REQUIRES: asserts

target datalayout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-f80:128-n8:16:32:64-S128"

; FIXME
; Test case for PR47751. Make sure the runtime check includes a required
; addition of the size of the element type (a pointer) for the end bound.

define void @test(i64 %arg, i32 %arg1, i8** %base) {
; CHECK-LABEL: @test(
; CHECK-NEXT:  entry:
; CHECK-NEXT:    [[TMP0:%.*]] = add i64 [[ARG:%.*]], 1
; CHECK-NEXT:    [[SMIN19:%.*]] = call i64 @llvm.smin.i64(i64 [[ARG]], i64 1)
; CHECK-NEXT:    [[TMP1:%.*]] = sub i64 [[TMP0]], [[SMIN19]]
; CHECK-NEXT:    [[MIN_ITERS_CHECK:%.*]] = icmp ult i64 [[TMP1]], 2
; CHECK-NEXT:    br i1 [[MIN_ITERS_CHECK]], label [[SCALAR_PH:%.*]], label [[VECTOR_SCEVCHECK:%.*]]
; CHECK:       vector.scevcheck:
; CHECK-NEXT:    [[SMIN:%.*]] = call i64 @llvm.smin.i64(i64 [[ARG]], i64 1)
; CHECK-NEXT:    [[TMP2:%.*]] = sub i64 [[ARG]], [[SMIN]]
; CHECK-NEXT:    [[TMP3:%.*]] = add nsw i32 [[ARG1:%.*]], -1
; CHECK-NEXT:    [[TMP4:%.*]] = trunc i64 [[TMP2]] to i32
; CHECK-NEXT:    [[MUL:%.*]] = call { i32, i1 } @llvm.umul.with.overflow.i32(i32 1, i32 [[TMP4]])
; CHECK-NEXT:    [[MUL_RESULT:%.*]] = extractvalue { i32, i1 } [[MUL]], 0
; CHECK-NEXT:    [[MUL_OVERFLOW:%.*]] = extractvalue { i32, i1 } [[MUL]], 1
; CHECK-NEXT:    [[TMP5:%.*]] = sub i32 [[TMP3]], [[MUL_RESULT]]
; CHECK-NEXT:    [[TMP6:%.*]] = icmp ugt i32 [[TMP5]], [[TMP3]]
; CHECK-NEXT:    [[TMP7:%.*]] = or i1 [[TMP6]], [[MUL_OVERFLOW]]
; CHECK-NEXT:    [[TMP8:%.*]] = icmp ugt i64 [[TMP2]], 4294967295
; CHECK-NEXT:    [[TMP9:%.*]] = or i1 [[TMP7]], [[TMP8]]
; CHECK-NEXT:    [[TMP10:%.*]] = zext i32 [[TMP3]] to i64
; CHECK-NEXT:    [[SCEVGEP:%.*]] = getelementptr i8*, i8** [[BASE:%.*]], i64 [[TMP10]]
; CHECK-NEXT:    [[MUL1:%.*]] = call { i64, i1 } @llvm.umul.with.overflow.i64(i64 8, i64 [[TMP2]])
; CHECK-NEXT:    [[MUL_RESULT2:%.*]] = extractvalue { i64, i1 } [[MUL1]], 0
; CHECK-NEXT:    [[MUL_OVERFLOW3:%.*]] = extractvalue { i64, i1 } [[MUL1]], 1
; CHECK-NEXT:    [[SCEVGEP4:%.*]] = bitcast i8** [[SCEVGEP]] to i8*
; CHECK-NEXT:    [[TMP11:%.*]] = sub i64 0, [[MUL_RESULT2]]
; CHECK-NEXT:    [[TMP12:%.*]] = getelementptr i8, i8* [[SCEVGEP4]], i64 [[TMP11]]
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ugt i8* [[TMP12]], [[SCEVGEP4]]
; CHECK-NEXT:    [[TMP14:%.*]] = or i1 [[TMP13]], [[MUL_OVERFLOW3]]
; CHECK-NEXT:    [[SCEVGEP5:%.*]] = getelementptr i8*, i8** [[BASE]], i64 [[ARG]]
; CHECK-NEXT:    [[MUL6:%.*]] = call { i64, i1 } @llvm.umul.with.overflow.i64(i64 8, i64 [[TMP2]])
; CHECK-NEXT:    [[MUL_RESULT7:%.*]] = extractvalue { i64, i1 } [[MUL6]], 0
; CHECK-NEXT:    [[MUL_OVERFLOW8:%.*]] = extractvalue { i64, i1 } [[MUL6]], 1
; CHECK-NEXT:    [[SCEVGEP59:%.*]] = bitcast i8** [[SCEVGEP5]] to i8*
; CHECK-NEXT:    [[TMP15:%.*]] = sub i64 0, [[MUL_RESULT7]]
; CHECK-NEXT:    [[TMP16:%.*]] = getelementptr i8, i8* [[SCEVGEP59]], i64 [[TMP15]]
; CHECK-NEXT:    [[TMP17:%.*]] = icmp ugt i8* [[TMP16]], [[SCEVGEP59]]
; CHECK-NEXT:    [[TMP18:%.*]] = or i1 [[TMP17]], [[MUL_OVERFLOW8]]
; CHECK-NEXT:    [[TMP19:%.*]] = or i1 [[TMP9]], [[TMP14]]
; CHECK-NEXT:    [[TMP20:%.*]] = or i1 [[TMP19]], [[TMP18]]
; CHECK-NEXT:    br i1 [[TMP20]], label [[SCALAR_PH]], label [[VECTOR_MEMCHECK:%.*]]
; CHECK:       vector.memcheck:
; CHECK-NEXT:    [[SMIN10:%.*]] = call i64 @llvm.smin.i64(i64 [[ARG]], i64 1)
; CHECK-NEXT:    [[TMP21:%.*]] = add nsw i32 [[ARG1]], -1
; CHECK-NEXT:    [[TMP22:%.*]] = zext i32 [[TMP21]] to i64
; CHECK-NEXT:    [[TMP23:%.*]] = add i64 [[SMIN10]], [[TMP22]]
; CHECK-NEXT:    [[TMP24:%.*]] = sub i64 [[TMP23]], [[ARG]]
; CHECK-NEXT:    [[SCEVGEP11:%.*]] = getelementptr i8*, i8** [[BASE]], i64 [[TMP24]]
; CHECK-NEXT:    [[SCEVGEP1112:%.*]] = bitcast i8** [[SCEVGEP11]] to i8*
; CHECK-NEXT:    [[TMP25:%.*]] = add nuw nsw i64 [[TMP22]], 1
; CHECK-NEXT:    [[SCEVGEP13:%.*]] = getelementptr i8*, i8** [[BASE]], i64 [[TMP25]]
; CHECK-NEXT:    [[SCEVGEP1314:%.*]] = bitcast i8** [[SCEVGEP13]] to i8*
; CHECK-NEXT:    [[SCEVGEP15:%.*]] = getelementptr i8*, i8** [[BASE]], i64 [[SMIN10]]
; CHECK-NEXT:    [[SCEVGEP1516:%.*]] = bitcast i8** [[SCEVGEP15]] to i8*
; CHECK-NEXT:    [[TMP26:%.*]] = add i64 [[ARG]], 1
; CHECK-NEXT:    [[SCEVGEP17:%.*]] = getelementptr i8*, i8** [[BASE]], i64 [[TMP26]]
; CHECK-NEXT:    [[SCEVGEP1718:%.*]] = bitcast i8** [[SCEVGEP17]] to i8*
; CHECK-NEXT:    [[BOUND0:%.*]] = icmp ult i8* [[SCEVGEP1112]], [[SCEVGEP1718]]
; CHECK-NEXT:    [[BOUND1:%.*]] = icmp ult i8* [[SCEVGEP1516]], [[SCEVGEP1314]]
; CHECK-NEXT:    [[FOUND_CONFLICT:%.*]] = and i1 [[BOUND0]], [[BOUND1]]
; CHECK-NEXT:    br i1 [[FOUND_CONFLICT]], label [[SCALAR_PH]], label [[VECTOR_PH:%.*]]
; CHECK:       vector.ph:
; CHECK-NEXT:    [[N_MOD_VF:%.*]] = urem i64 [[TMP1]], 2
; CHECK-NEXT:    [[N_VEC:%.*]] = sub i64 [[TMP1]], [[N_MOD_VF]]
; CHECK-NEXT:    [[IND_END:%.*]] = sub i64 [[ARG]], [[N_VEC]]
; CHECK-NEXT:    [[CAST_CRD:%.*]] = trunc i64 [[N_VEC]] to i32
; CHECK-NEXT:    [[IND_END21:%.*]] = sub i32 [[ARG1]], [[CAST_CRD]]
; CHECK-NEXT:    br label [[VECTOR_BODY:%.*]]
; CHECK:       vector.body:
; CHECK-NEXT:    [[INDEX:%.*]] = phi i64 [ 0, [[VECTOR_PH]] ], [ [[INDEX_NEXT:%.*]], [[VECTOR_BODY]] ]
; CHECK-NEXT:    [[TMP27:%.*]] = trunc i64 [[INDEX]] to i32
; CHECK-NEXT:    [[OFFSET_IDX:%.*]] = sub i32 [[ARG1]], [[TMP27]]
; CHECK-NEXT:    [[TMP28:%.*]] = add i32 [[OFFSET_IDX]], 0
; CHECK-NEXT:    [[OFFSET_IDX22:%.*]] = sub i64 [[ARG]], [[INDEX]]
; CHECK-NEXT:    [[TMP29:%.*]] = add i64 [[OFFSET_IDX22]], 0
; CHECK-NEXT:    [[TMP30:%.*]] = add nsw i32 [[TMP28]], -1
; CHECK-NEXT:    [[TMP31:%.*]] = zext i32 [[TMP30]] to i64
; CHECK-NEXT:    [[TMP32:%.*]] = getelementptr inbounds i8*, i8** [[BASE]], i64 [[TMP31]]
; CHECK-NEXT:    [[TMP33:%.*]] = getelementptr inbounds i8*, i8** [[TMP32]], i32 0
; CHECK-NEXT:    [[TMP34:%.*]] = getelementptr inbounds i8*, i8** [[TMP33]], i32 -1
; CHECK-NEXT:    [[TMP35:%.*]] = bitcast i8** [[TMP34]] to <2 x i8*>*
; CHECK-NEXT:    [[WIDE_LOAD:%.*]] = load <2 x i8*>, <2 x i8*>* [[TMP35]], align 8, !alias.scope !0, !noalias !3
; CHECK-NEXT:    [[REVERSE:%.*]] = shufflevector <2 x i8*> [[WIDE_LOAD]], <2 x i8*> poison, <2 x i32> <i32 1, i32 0>
; CHECK-NEXT:    [[TMP36:%.*]] = getelementptr inbounds i8*, i8** [[BASE]], i64 [[TMP29]]
; CHECK-NEXT:    [[TMP37:%.*]] = getelementptr inbounds i8*, i8** [[TMP36]], i32 0
; CHECK-NEXT:    [[TMP38:%.*]] = getelementptr inbounds i8*, i8** [[TMP37]], i32 -1
; CHECK-NEXT:    [[TMP39:%.*]] = bitcast i8** [[TMP38]] to <2 x i8*>*
; CHECK-NEXT:    [[WIDE_LOAD23:%.*]] = load <2 x i8*>, <2 x i8*>* [[TMP39]], align 8, !alias.scope !3
; CHECK-NEXT:    [[REVERSE24:%.*]] = shufflevector <2 x i8*> [[WIDE_LOAD23]], <2 x i8*> poison, <2 x i32> <i32 1, i32 0>
; CHECK-NEXT:    [[REVERSE25:%.*]] = shufflevector <2 x i8*> [[REVERSE24]], <2 x i8*> poison, <2 x i32> <i32 1, i32 0>
; CHECK-NEXT:    [[TMP40:%.*]] = bitcast i8** [[TMP34]] to <2 x i8*>*
; CHECK-NEXT:    store <2 x i8*> [[REVERSE25]], <2 x i8*>* [[TMP40]], align 8, !alias.scope !0, !noalias !3
; CHECK-NEXT:    [[REVERSE26:%.*]] = shufflevector <2 x i8*> [[REVERSE]], <2 x i8*> poison, <2 x i32> <i32 1, i32 0>
; CHECK-NEXT:    [[TMP41:%.*]] = bitcast i8** [[TMP38]] to <2 x i8*>*
; CHECK-NEXT:    store <2 x i8*> [[REVERSE26]], <2 x i8*>* [[TMP41]], align 8, !alias.scope !3
; CHECK-NEXT:    [[INDEX_NEXT]] = add nuw i64 [[INDEX]], 2
; CHECK-NEXT:    [[TMP42:%.*]] = icmp eq i64 [[INDEX_NEXT]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[TMP42]], label [[MIDDLE_BLOCK:%.*]], label [[VECTOR_BODY]], !llvm.loop [[LOOP5:![0-9]+]]
; CHECK:       middle.block:
; CHECK-NEXT:    [[CMP_N:%.*]] = icmp eq i64 [[TMP1]], [[N_VEC]]
; CHECK-NEXT:    br i1 [[CMP_N]], label [[EXIT:%.*]], label [[SCALAR_PH]]
; CHECK:       scalar.ph:
; CHECK-NEXT:    [[BC_RESUME_VAL:%.*]] = phi i64 [ [[IND_END]], [[MIDDLE_BLOCK]] ], [ [[ARG]], [[ENTRY:%.*]] ], [ [[ARG]], [[VECTOR_SCEVCHECK]] ], [ [[ARG]], [[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    [[BC_RESUME_VAL20:%.*]] = phi i32 [ [[IND_END21]], [[MIDDLE_BLOCK]] ], [ [[ARG1]], [[ENTRY]] ], [ [[ARG1]], [[VECTOR_SCEVCHECK]] ], [ [[ARG1]], [[VECTOR_MEMCHECK]] ]
; CHECK-NEXT:    br label [[LOOP:%.*]]
; CHECK:       loop:
; CHECK-NEXT:    [[IV_1:%.*]] = phi i64 [ [[BC_RESUME_VAL]], [[SCALAR_PH]] ], [ [[IV_1_NEXT:%.*]], [[LOOP]] ]
; CHECK-NEXT:    [[IV_2:%.*]] = phi i32 [ [[BC_RESUME_VAL20]], [[SCALAR_PH]] ], [ [[IV_2_NEXT:%.*]], [[LOOP]] ]
; CHECK-NEXT:    [[IV_2_NEXT]] = add nsw i32 [[IV_2]], -1
; CHECK-NEXT:    [[IV_2_EXT:%.*]] = zext i32 [[IV_2_NEXT]] to i64
; CHECK-NEXT:    [[IDX_1:%.*]] = getelementptr inbounds i8*, i8** [[BASE]], i64 [[IV_2_EXT]]
; CHECK-NEXT:    [[V_1:%.*]] = load i8*, i8** [[IDX_1]], align 8
; CHECK-NEXT:    [[IDX_2:%.*]] = getelementptr inbounds i8*, i8** [[BASE]], i64 [[IV_1]]
; CHECK-NEXT:    [[V_2:%.*]] = load i8*, i8** [[IDX_2]], align 8
; CHECK-NEXT:    store i8* [[V_2]], i8** [[IDX_1]], align 8
; CHECK-NEXT:    store i8* [[V_1]], i8** [[IDX_2]], align 8
; CHECK-NEXT:    [[TMP11:%.*]] = icmp sgt i64 [[IV_1]], 1
; CHECK-NEXT:    [[IV_1_NEXT]] = add nsw i64 [[IV_1]], -1
; CHECK-NEXT:    br i1 [[TMP11]], label [[LOOP]], label [[EXIT]], !llvm.loop [[LOOP7:![0-9]+]]
; CHECK:       exit:
; CHECK-NEXT:    ret void
;


entry:
  br label %loop

loop:
  %iv.1 = phi i64 [ %arg, %entry ], [ %iv.1.next, %loop ]
  %iv.2 = phi i32 [ %arg1, %entry ], [ %iv.2.next, %loop ]
  %iv.2.next = add nsw i32 %iv.2, -1
  %iv.2.ext = zext i32 %iv.2.next to i64
  %idx.1 = getelementptr inbounds i8*, i8** %base, i64 %iv.2.ext
  %v.1 = load i8*, i8** %idx.1, align 8
  %idx.2 = getelementptr inbounds i8*, i8** %base, i64 %iv.1
  %v.2 = load i8*, i8** %idx.2, align 8
  store i8* %v.2, i8** %idx.1, align 8
  store i8* %v.1, i8** %idx.2, align 8
  %tmp11 = icmp sgt i64 %iv.1, 1
  %iv.1.next = add nsw i64 %iv.1, -1
  br i1 %tmp11, label %loop, label %exit

exit:                                             ; preds = %bb3
  ret void
}
