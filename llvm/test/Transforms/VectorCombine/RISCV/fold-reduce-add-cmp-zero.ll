; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S -passes=vector-combine -mtriple=riscv64 -mattr=+v < %s | FileCheck %s

; Fold (icmp eq/ne (reduce.add (sext <N x i1>)), 0) to reduce.umax.

define i1 @reduce_add_sext_v4i1_eq(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_eq(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sext)
  %res = icmp eq i32 %red, 0
  ret i1 %res
}

; Same with ne predicate
define i1 @reduce_add_sext_v4i1_ne(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_ne(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[RES:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[CMP]])
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sext)
  %res = icmp ne i32 %red, 0
  ret i1 %res
}

define i1 @reduce_add_sext_v4i1_to_i8(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_to_i8(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i8>
  %red = call i8 @llvm.vector.reduce.add.v4i8(<4 x i8> %sext)
  %res = icmp eq i8 %red, 0
  ret i1 %res
}

define i1 @reduce_add_sext_v8i1(<8 x i16> %a, <8 x i16> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v8i1(
; CHECK-SAME: <8 x i16> [[A:%.*]], <8 x i16> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp slt <8 x i16> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v8i1(<8 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp slt <8 x i16> %a, %b
  %sext = sext <8 x i1> %cmp to <8 x i16>
  %red = call i16 @llvm.vector.reduce.add.v8i16(<8 x i16> %sext)
  %res = icmp eq i16 %red, 0
  ret i1 %res
}

define i1 @reduce_add_sext_v2i1(<2 x i64> %a, <2 x i64> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v2i1(
; CHECK-SAME: <2 x i64> [[A:%.*]], <2 x i64> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp ult <2 x i64> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v2i1(<2 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp ult <2 x i64> %a, %b
  %sext = sext <2 x i1> %cmp to <2 x i64>
  %red = call i64 @llvm.vector.reduce.add.v2i64(<2 x i64> %sext)
  %res = icmp eq i64 %red, 0
  ret i1 %res
}

; Negative: comparison with non-zero value
define i1 @reduce_add_sext_v4i1_nonzero_cmp(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_nonzero_cmp(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[SEXT:%.*]] = sext <4 x i1> [[CMP]] to <4 x i32>
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SEXT]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i32 [[RED]], -2
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sext)
  %res = icmp eq i32 %red, -2
  ret i1 %res
}

; Negative: multiple uses of reduce
define i1 @reduce_add_sext_v4i1_multiuse(<4 x i32> %a, <4 x i32> %b, ptr %p) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_multiuse(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]], ptr [[P:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[SEXT:%.*]] = sext <4 x i1> [[CMP]] to <4 x i32>
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[SEXT]])
; CHECK-NEXT:    store i32 [[RED]], ptr [[P]], align 4
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sext)
  store i32 %red, ptr %p
  %res = icmp eq i32 %red, 0
  ret i1 %res
}

; slt with non-positive (sext i1) elements: sum < 0 iff any != 0
define i1 @reduce_add_sext_v4i1_slt(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_v4i1_slt(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[RES:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[CMP]])
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %sext = sext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %sext)
  %res = icmp slt i32 %red, 0
  ret i1 %res
}

; Same with zext instead of sext
define i1 @reduce_add_zext_v4i1(<4 x i32> %a, <4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_zext_v4i1(
; CHECK-SAME: <4 x i32> [[A:%.*]], <4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v4i1(<4 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <4 x i32> %a, %b
  %zext = zext <4 x i1> %cmp to <4 x i32>
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %zext)
  %res = icmp eq i32 %red, 0
  ret i1 %res
}

; Negative: scalable vector
define i1 @reduce_add_sext_nxv4i1(<vscale x 4 x i32> %a, <vscale x 4 x i32> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_nxv4i1(
; CHECK-SAME: <vscale x 4 x i32> [[A:%.*]], <vscale x 4 x i32> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <vscale x 4 x i32> [[A]], [[B]]
; CHECK-NEXT:    [[SEXT:%.*]] = sext <vscale x 4 x i1> [[CMP]] to <vscale x 4 x i32>
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> [[SEXT]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <vscale x 4 x i32> %a, %b
  %sext = sext <vscale x 4 x i1> %cmp to <vscale x 4 x i32>
  %red = call i32 @llvm.vector.reduce.add.nxv4i32(<vscale x 4 x i32> %sext)
  %res = icmp eq i32 %red, 0
  ret i1 %res
}

; Negative: unknown input
define i1 @reduce_add_unknown_input(<4 x i32> %x) {
; CHECK-LABEL: define i1 @reduce_add_unknown_input(
; CHECK-SAME: <4 x i32> [[X:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[RED:%.*]] = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> [[X]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i32 [[RED]], 0
; CHECK-NEXT:    ret i1 [[RES]]
;
  %red = call i32 @llvm.vector.reduce.add.v4i32(<4 x i32> %x)
  %res = icmp eq i32 %red, 0
  ret i1 %res
}

; Negative: 256 elements can wrap to zero in i8
define i1 @reduce_add_sext_overflow(<256 x i8> %a, <256 x i8> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_overflow(
; CHECK-SAME: <256 x i8> [[A:%.*]], <256 x i8> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <256 x i8> [[A]], [[B]]
; CHECK-NEXT:    [[SEXT:%.*]] = sext <256 x i1> [[CMP]] to <256 x i8>
; CHECK-NEXT:    [[RED:%.*]] = call i8 @llvm.vector.reduce.add.v256i8(<256 x i8> [[SEXT]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i8 [[RED]], 0
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <256 x i8> %a, %b
  %sext = sext <256 x i1> %cmp to <256 x i8>
  %red = call i8 @llvm.vector.reduce.add.v256i8(<256 x i8> %sext)
  %res = icmp eq i8 %red, 0
  ret i1 %res
}

; 255 elements cannot wrap to zero in i8
define i1 @reduce_add_sext_no_overflow(<255 x i8> %a, <255 x i8> %b) {
; CHECK-LABEL: define i1 @reduce_add_sext_no_overflow(
; CHECK-SAME: <255 x i8> [[A:%.*]], <255 x i8> [[B:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[CMP:%.*]] = icmp eq <255 x i8> [[A]], [[B]]
; CHECK-NEXT:    [[TMP1:%.*]] = call i1 @llvm.vector.reduce.or.v255i1(<255 x i1> [[CMP]])
; CHECK-NEXT:    [[RES:%.*]] = icmp eq i1 [[TMP1]], false
; CHECK-NEXT:    ret i1 [[RES]]
;
  %cmp = icmp eq <255 x i8> %a, %b
  %sext = sext <255 x i1> %cmp to <255 x i8>
  %red = call i8 @llvm.vector.reduce.add.v255i8(<255 x i8> %sext)
  %res = icmp eq i8 %red, 0
  ret i1 %res
}
