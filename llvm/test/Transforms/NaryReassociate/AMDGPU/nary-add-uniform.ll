; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; REQUIRES: amdgpu-registered-target
; RUN: opt < %s -mtriple=amdgcn-amd-amdhsa -passes='nary-reassociate' -S | FileCheck %s

declare i32 @llvm.amdgcn.workitem.id.x()
declare i32 @llvm.amdgcn.workitem.id.y()
declare void @use(i32)

; Test that NaryReassociate prefers grouping uniform values together when
; uniformity analysis is available and both reassociation options exist.
;
; For I = (A op B) op RHS, the pass can form:
;   - (A op RHS) op B
;   - (B op RHS) op A
;
; When both dominating expressions exist, prefer the one grouping uniforms.

; Both %d_u2 and %u1_u2 exist as dominating expressions.
; For (d + u1) + u2:
;   - Without UA preference: would try (d + u2) first, find %d_u2, return %d_u2 + u1
;   - With UA preference: B=u1 and RHS=u2 are uniform, A=d is divergent
;                         So prefer (u1 + u2) + d, returning %u1_u2 + d
;

define amdgpu_kernel void @prefer_uniform_grouping(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @prefer_uniform_grouping(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_U2:%.*]] = add i32 [[D]], [[U2]]
; CHECK-NEXT:    [[U1_U2:%.*]] = add i32 [[U1]], [[U2]]
; CHECK-NEXT:    call void @use(i32 [[D_U2]])
; CHECK-NEXT:    call void @use(i32 [[U1_U2]])
; CHECK-NEXT:    [[RESULT:%.*]] = add i32 [[U1_U2]], [[D]]
; CHECK-NEXT:    call void @use(i32 [[RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  ; Create both possible reassociation targets
  %d_u2 = add i32 %d, %u2        ; divergent + uniform
  %u1_u2 = add i32 %u1, %u2      ; uniform + uniform (should be preferred!)

  call void @use(i32 %d_u2)
  call void @use(i32 %u1_u2)

  ; (d + u1) + u2: both (d + u2) and (u1 + u2) exist
  ; Should prefer (u1 + u2) + d to group uniforms
  %tmp = add i32 %d, %u1
  %result = add i32 %tmp, %u2
  call void @use(i32 %result)

  ret void
}

define amdgpu_kernel void @prefer_uniform_grouping_mul(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @prefer_uniform_grouping_mul(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_U2:%.*]] = mul i32 [[D]], [[U2]]
; CHECK-NEXT:    [[U1_U2:%.*]] = mul i32 [[U1]], [[U2]]
; CHECK-NEXT:    call void @use(i32 [[D_U2]])
; CHECK-NEXT:    call void @use(i32 [[U1_U2]])
; CHECK-NEXT:    [[RESULT:%.*]] = mul i32 [[U1_U2]], [[D]]
; CHECK-NEXT:    call void @use(i32 [[RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  %d_u2 = mul i32 %d, %u2
  %u1_u2 = mul i32 %u1, %u2

  call void @use(i32 %d_u2)
  call void @use(i32 %u1_u2)

  %tmp = mul i32 %d, %u1
  %result = mul i32 %tmp, %u2
  call void @use(i32 %result)

  ret void
}

define amdgpu_kernel void @only_one_option(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @only_one_option(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[U1_U2:%.*]] = add i32 [[U1]], [[U2]]
; CHECK-NEXT:    call void @use(i32 [[U1_U2]])
; CHECK-NEXT:    [[RESULT:%.*]] = add i32 [[U1_U2]], [[D]]
; CHECK-NEXT:    call void @use(i32 [[RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  ; Only u1 + u2 exists, not d + u2
  %u1_u2 = add i32 %u1, %u2
  call void @use(i32 %u1_u2)

  %tmp = add i32 %d, %u1
  %result = add i32 %tmp, %u2
  call void @use(i32 %result)

  ret void
}

; When no dominating expression exists, no reassociation happens
define amdgpu_kernel void @no_dominating_expr(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @no_dominating_expr(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[TMP:%.*]] = add i32 [[D]], [[U1]]
; CHECK-NEXT:    [[RESULT:%.*]] = add i32 [[TMP]], [[U2]]
; CHECK-NEXT:    call void @use(i32 [[RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  ; No dominating expressions exist
  %tmp = add i32 %d, %u1
  %result = add i32 %tmp, %u2
  call void @use(i32 %result)

  ret void
}

; Test smax: prefer grouping uniform values together
; For smax(smax(A, B), RHS):
;   - smax(smax(A, RHS), B): groups A and RHS
;   - smax(smax(B, RHS), A): groups B and RHS
; When B and RHS are uniform but A is divergent, prefer smax(smax(B, RHS), A)
define amdgpu_kernel void @prefer_uniform_grouping_smax(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @prefer_uniform_grouping_smax(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_U2:%.*]] = call i32 @llvm.smax.i32(i32 [[D]], i32 [[U2]])
; CHECK-NEXT:    [[U1_U2:%.*]] = call i32 @llvm.smax.i32(i32 [[U1]], i32 [[U2]])
; CHECK-NEXT:    call void @use(i32 [[D_U2]])
; CHECK-NEXT:    call void @use(i32 [[U1_U2]])
; CHECK-NEXT:    [[RESULT_NARY:%.*]] = call i32 @llvm.smax.i32(i32 [[U1_U2]], i32 [[D]])
; CHECK-NEXT:    call void @use(i32 [[RESULT_NARY]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  ; Create both possible reassociation targets
  %d_u2 = call i32 @llvm.smax.i32(i32 %d, i32 %u2)    ; divergent, uniform
  %u1_u2 = call i32 @llvm.smax.i32(i32 %u1, i32 %u2)  ; uniform, uniform (preferred!)

  call void @use(i32 %d_u2)
  call void @use(i32 %u1_u2)

  ; smax(smax(d, u1), u2): both smax(d, u2) and smax(u1, u2) exist
  ; Should prefer smax(smax(u1, u2), d) to group uniforms
  %tmp = call i32 @llvm.smax.i32(i32 %d, i32 %u1)
  %result = call i32 @llvm.smax.i32(i32 %tmp, i32 %u2)
  call void @use(i32 %result)

  ret void
}

; Test umin: prefer grouping uniform values together
define amdgpu_kernel void @prefer_uniform_grouping_umin(i32 %u1, i32 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @prefer_uniform_grouping_umin(
; CHECK-SAME: i32 [[U1:%.*]], i32 [[U2:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_U2:%.*]] = call i32 @llvm.umin.i32(i32 [[D]], i32 [[U2]])
; CHECK-NEXT:    [[U1_U2:%.*]] = call i32 @llvm.umin.i32(i32 [[U1]], i32 [[U2]])
; CHECK-NEXT:    call void @use(i32 [[D_U2]])
; CHECK-NEXT:    call void @use(i32 [[U1_U2]])
; CHECK-NEXT:    [[RESULT_NARY:%.*]] = call i32 @llvm.umin.i32(i32 [[U1_U2]], i32 [[D]])
; CHECK-NEXT:    call void @use(i32 [[RESULT_NARY]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()

  %d_u2 = call i32 @llvm.umin.i32(i32 %d, i32 %u2)
  %u1_u2 = call i32 @llvm.umin.i32(i32 %u1, i32 %u2)

  call void @use(i32 %d_u2)
  call void @use(i32 %u1_u2)

  %tmp = call i32 @llvm.umin.i32(i32 %d, i32 %u1)
  %result = call i32 @llvm.umin.i32(i32 %tmp, i32 %u2)
  call void @use(i32 %result)

  ret void
}

; Test GEP with LHS=uniform, RHS=divergent
define amdgpu_kernel void @gep_lhs_uniform_rhs_divergent(ptr %base, i64 %u_offset) {
; CHECK-LABEL: define amdgpu_kernel void @gep_lhs_uniform_rhs_divergent(
; CHECK-SAME: ptr [[BASE:%.*]], i64 [[U_OFFSET:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_EXT:%.*]] = zext i32 [[D]] to i64
; CHECK-NEXT:    [[GEP_U:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[U_OFFSET]]
; CHECK-NEXT:    [[GEP_D:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[D_EXT]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_U]])
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_D]])
; CHECK-NEXT:    [[GEP_RESULT:%.*]] = getelementptr i32, ptr [[GEP_D]], i64 [[U_OFFSET]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()
  %d_ext = zext i32 %d to i64

  ; Create BOTH dominating GEPs so there's a choice
  %gep_u = getelementptr i32, ptr %base, i64 %u_offset  ; uniform index
  %gep_d = getelementptr i32, ptr %base, i64 %d_ext     ; divergent index

  call void @use_ptr(ptr %gep_u)
  call void @use_ptr(ptr %gep_d)

  ; idx = u_offset + d_ext (LHS=uniform, RHS=divergent)
  %idx = add i64 %u_offset, %d_ext
  %gep_result = getelementptr i32, ptr %base, i64 %idx
  call void @use_ptr(ptr %gep_result)

  ret void
}

; Test GEP with LHS=divergent, RHS=uniform
define amdgpu_kernel void @gep_lhs_divergent_rhs_uniform(ptr %base, i64 %u_offset) {
; CHECK-LABEL: define amdgpu_kernel void @gep_lhs_divergent_rhs_uniform(
; CHECK-SAME: ptr [[BASE:%.*]], i64 [[U_OFFSET:%.*]]) {
; CHECK-NEXT:    [[D:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D_EXT:%.*]] = zext i32 [[D]] to i64
; CHECK-NEXT:    [[GEP_U:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[U_OFFSET]]
; CHECK-NEXT:    [[GEP_D:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[D_EXT]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_U]])
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_D]])
; CHECK-NEXT:    [[GEP_RESULT:%.*]] = getelementptr i32, ptr [[GEP_D]], i64 [[U_OFFSET]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_RESULT]])
; CHECK-NEXT:    ret void
;
  %d = call i32 @llvm.amdgcn.workitem.id.x()
  %d_ext = zext i32 %d to i64

  ; Create BOTH dominating GEPs so there's a choice
  %gep_u = getelementptr i32, ptr %base, i64 %u_offset  ; uniform index
  %gep_d = getelementptr i32, ptr %base, i64 %d_ext     ; divergent index

  call void @use_ptr(ptr %gep_u)
  call void @use_ptr(ptr %gep_d)

  ; idx = d_ext + u_offset (LHS=divergent, RHS=uniform)
  %idx = add i64 %d_ext, %u_offset
  %gep_result = getelementptr i32, ptr %base, i64 %idx
  call void @use_ptr(ptr %gep_result)

  ret void
}

; Test GEP with both LHS and RHS uniform - no preference needed
define amdgpu_kernel void @gep_both_uniform(ptr %base, i64 %u1, i64 %u2) {
; CHECK-LABEL: define amdgpu_kernel void @gep_both_uniform(
; CHECK-SAME: ptr [[BASE:%.*]], i64 [[U1:%.*]], i64 [[U2:%.*]]) {
; CHECK-NEXT:    [[GEP_U1:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[U1]]
; CHECK-NEXT:    [[GEP_U2:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[U2]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_U1]])
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_U2]])
; CHECK-NEXT:    [[GEP_RESULT:%.*]] = getelementptr i32, ptr [[GEP_U1]], i64 [[U2]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_RESULT]])
; CHECK-NEXT:    ret void
;
  ; Create both dominating GEPs with uniform indices
  %gep_u1 = getelementptr i32, ptr %base, i64 %u1
  %gep_u2 = getelementptr i32, ptr %base, i64 %u2

  call void @use_ptr(ptr %gep_u1)
  call void @use_ptr(ptr %gep_u2)

  ; idx = u1 + u2 (both uniform - no preference needed)
  %idx = add i64 %u1, %u2
  %gep_result = getelementptr i32, ptr %base, i64 %idx
  call void @use_ptr(ptr %gep_result)

  ret void
}

; Test GEP with both LHS and RHS divergent - no preference needed
define amdgpu_kernel void @gep_both_divergent(ptr %base) {
; CHECK-LABEL: define amdgpu_kernel void @gep_both_divergent(
; CHECK-SAME: ptr [[BASE:%.*]]) {
; CHECK-NEXT:    [[D1:%.*]] = call i32 @llvm.amdgcn.workitem.id.x()
; CHECK-NEXT:    [[D2:%.*]] = call i32 @llvm.amdgcn.workitem.id.y()
; CHECK-NEXT:    [[D1_EXT:%.*]] = zext i32 [[D1]] to i64
; CHECK-NEXT:    [[D2_EXT:%.*]] = zext i32 [[D2]] to i64
; CHECK-NEXT:    [[GEP_D1:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[D1_EXT]]
; CHECK-NEXT:    [[GEP_D2:%.*]] = getelementptr i32, ptr [[BASE]], i64 [[D2_EXT]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_D1]])
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_D2]])
; CHECK-NEXT:    [[GEP_RESULT:%.*]] = getelementptr i32, ptr [[GEP_D1]], i64 [[D2_EXT]]
; CHECK-NEXT:    call void @use_ptr(ptr [[GEP_RESULT]])
; CHECK-NEXT:    ret void
;
  %d1 = call i32 @llvm.amdgcn.workitem.id.x()
  %d2 = call i32 @llvm.amdgcn.workitem.id.y()
  %d1_ext = zext i32 %d1 to i64
  %d2_ext = zext i32 %d2 to i64

  ; Create both dominating GEPs with divergent indices
  %gep_d1 = getelementptr i32, ptr %base, i64 %d1_ext
  %gep_d2 = getelementptr i32, ptr %base, i64 %d2_ext

  call void @use_ptr(ptr %gep_d1)
  call void @use_ptr(ptr %gep_d2)

  ; idx = d1_ext + d2_ext (both divergent - no preference needed)
  %idx = add i64 %d1_ext, %d2_ext
  %gep_result = getelementptr i32, ptr %base, i64 %idx
  call void @use_ptr(ptr %gep_result)

  ret void
}

declare i32 @llvm.smax.i32(i32, i32)
declare i32 @llvm.smin.i32(i32, i32)
declare i32 @llvm.umax.i32(i32, i32)
declare i32 @llvm.umin.i32(i32, i32)
declare void @use_ptr(ptr)
