; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S -passes=instcombine < %s | FileCheck %s

target triple = "aarch64-unknown-linux-gnu"

define <vscale x 16 x i1> @aarch64_sve_rev_inv(<vscale x 16 x i1> %0) #0{
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_inv(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev(<vscale x 16 x i1> %1)
  ret <vscale x 16 x i1> %2
}

define <vscale x 16 x i1> @aarch64_sve_rev_b16_inv(<vscale x 16 x i1> %0) #0{
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_b16_inv(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> %1)
  ret <vscale x 16 x i1> %2
}

define <vscale x 16 x i1> @aarch64_sve_rev_b32_inv(<vscale x 16 x i1> %0) #0{
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_b32_inv(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b32(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b32(<vscale x 16 x i1> %1)
  ret <vscale x 16 x i1> %2
}

define <vscale x 16 x i1> @aarch64_sve_rev_b64_inv(<vscale x 16 x i1> %0)#0 {
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_b64_inv(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP0]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b64(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b64(<vscale x 16 x i1> %1)
  ret <vscale x 16 x i1> %2
}


define <vscale x 4 x i32> @aarch64_sve_revb_inv(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revb_inv(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    ret <vscale x 4 x i32> [[A]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

define <vscale x 16 x i8> @aarch64_sve_revd_inv(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %a) #0 {
; CHECK-LABEL: define <vscale x 16 x i8> @aarch64_sve_revd_inv(
; CHECK-SAME: <vscale x 16 x i8> [[PRED:%.*]], <vscale x 16 x i1> [[PASSTHRU:%.*]], <vscale x 16 x i8> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    ret <vscale x 16 x i8> [[A]]
;
  %1 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %a)
  %2 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %1)
  ret <vscale x 16 x i8> %2
}

define <vscale x 4 x i32> @aarch64_sve_revh_inv(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revh_inv(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    ret <vscale x 4 x i32> [[A]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

define <vscale x 2 x i64> @aarch64_sve_revw_inv(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a) #0 {
; CHECK-LABEL: define <vscale x 2 x i64> @aarch64_sve_revw_inv(
; CHECK-SAME: <vscale x 2 x i64> [[PRED:%.*]], <vscale x 2 x i1> [[PASSTHRU:%.*]], <vscale x 2 x i64> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    ret <vscale x 2 x i64> [[A]]
;
  %1 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a)
  %2 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %1)
  ret <vscale x 2 x i64> %2
}


; negative test
define <vscale x 4 x i32> @aarch64_sve_revb_inv_pred_mismatch(<vscale x 4 x i32> %pred, <vscale x 4 x i32> %pred1, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revb_inv_pred_mismatch(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i32> [[PRED1:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> [[PRED1]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred1, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

; negative test
define <vscale x 16 x i8> @aarch64_sve_revd_inv_pred_mismatch(<vscale x 16 x i8> %pred, <vscale x 16 x i8> %pred1, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %a) #0 {
; CHECK-LABEL: define <vscale x 16 x i8> @aarch64_sve_revd_inv_pred_mismatch(
; CHECK-SAME: <vscale x 16 x i8> [[PRED:%.*]], <vscale x 16 x i8> [[PRED1:%.*]], <vscale x 16 x i1> [[PASSTHRU:%.*]], <vscale x 16 x i8> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> [[PRED]], <vscale x 16 x i1> [[PASSTHRU]], <vscale x 16 x i8> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> [[PRED1]], <vscale x 16 x i1> [[PASSTHRU]], <vscale x 16 x i8> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
;
  %1 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %a)
  %2 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred1, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %1)
  ret <vscale x 16 x i8> %2
}

; negative test
define <vscale x 4 x i32> @aarch64_sve_revh_inv_pred_mismatch(<vscale x 4 x i32> %pred, <vscale x 4 x i32> %pred1, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revh_inv_pred_mismatch(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i32> [[PRED1:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> [[PRED1]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred1, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

; negative test
define <vscale x 2 x i64> @aarch64_sve_revw_inv_pred_mismatch(<vscale x 2 x i64> %pred, <vscale x 2 x i64> %pred1, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a) #0 {
; CHECK-LABEL: define <vscale x 2 x i64> @aarch64_sve_revw_inv_pred_mismatch(
; CHECK-SAME: <vscale x 2 x i64> [[PRED:%.*]], <vscale x 2 x i64> [[PRED1:%.*]], <vscale x 2 x i1> [[PASSTHRU:%.*]], <vscale x 2 x i64> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED1]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
;
  %1 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a)
  %2 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred1, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %1)
  ret <vscale x 2 x i64> %2
}

; negative test
define <vscale x 4 x i32> @aarch64_sve_revb_inv_passthru_mismatch(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i1> %passthru1, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revb_inv_passthru_mismatch(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i1> [[PASSTHRU1:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU1]], <vscale x 4 x i32> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revb.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru1, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

; negative test
define <vscale x 16 x i8> @aarch64_sve_revd_inv_passthru_mismatch(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i1> %passthru1, <vscale x 16 x i8> %a) #0 {
; CHECK-LABEL: define <vscale x 16 x i8> @aarch64_sve_revd_inv_passthru_mismatch(
; CHECK-SAME: <vscale x 16 x i8> [[PRED:%.*]], <vscale x 16 x i1> [[PASSTHRU:%.*]], <vscale x 16 x i1> [[PASSTHRU1:%.*]], <vscale x 16 x i8> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> [[PRED]], <vscale x 16 x i1> [[PASSTHRU]], <vscale x 16 x i8> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> [[PRED]], <vscale x 16 x i1> [[PASSTHRU1]], <vscale x 16 x i8> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 16 x i8> [[TMP2]]
;
  %1 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru, <vscale x 16 x i8> %a)
  %2 = call <vscale x 16 x i8> @llvm.aarch64.sve.revd.nxv16i8(<vscale x 16 x i8> %pred, <vscale x 16 x i1> %passthru1, <vscale x 16 x i8> %1)
  ret <vscale x 16 x i8> %2
}

; negative test
define <vscale x 4 x i32> @aarch64_sve_revh_inv_passthru_mismatch(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i1> %passthru1, <vscale x 4 x i32> %a) #0 {
; CHECK-LABEL: define <vscale x 4 x i32> @aarch64_sve_revh_inv_passthru_mismatch(
; CHECK-SAME: <vscale x 4 x i32> [[PRED:%.*]], <vscale x 4 x i1> [[PASSTHRU:%.*]], <vscale x 4 x i1> [[PASSTHRU1:%.*]], <vscale x 4 x i32> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU]], <vscale x 4 x i32> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> [[PRED]], <vscale x 4 x i1> [[PASSTHRU1]], <vscale x 4 x i32> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 4 x i32> [[TMP2]]
;
  %1 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru, <vscale x 4 x i32> %a)
  %2 = call <vscale x 4 x i32> @llvm.aarch64.sve.revh.nxv4i32(<vscale x 4 x i32> %pred, <vscale x 4 x i1> %passthru1, <vscale x 4 x i32> %1)
  ret <vscale x 4 x i32> %2
}

; negative test
define <vscale x 2 x i64> @aarch64_sve_revw_inv_passthru_mismatch(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i1> %passthru1, <vscale x 2 x i64> %a) #0 {
; CHECK-LABEL: define <vscale x 2 x i64> @aarch64_sve_revw_inv_passthru_mismatch(
; CHECK-SAME: <vscale x 2 x i64> [[PRED:%.*]], <vscale x 2 x i1> [[PASSTHRU:%.*]], <vscale x 2 x i1> [[PASSTHRU1:%.*]], <vscale x 2 x i64> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU1]], <vscale x 2 x i64> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
;
  %1 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a)
  %2 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru1, <vscale x 2 x i64> %1)
  ret <vscale x 2 x i64> %2
}

; negative test
define <vscale x 16 x i1> @aarch64_sve_rev_mismatch(<vscale x 16 x i1> %0) #0{
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_mismatch(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b32(<vscale x 16 x i1> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP2]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b32(<vscale x 16 x i1> %1)
  ret <vscale x 16 x i1> %2
}

; negative test
define <vscale x 2 x i64> @aarch64_sve_rev_mismatch_1(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a) #0 {
; CHECK-LABEL: define <vscale x 2 x i64> @aarch64_sve_rev_mismatch_1(
; CHECK-SAME: <vscale x 2 x i64> [[PRED:%.*]], <vscale x 2 x i1> [[PASSTHRU:%.*]], <vscale x 2 x i64> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revh.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[TMP1]])
; CHECK-NEXT:    ret <vscale x 2 x i64> [[TMP2]]
;
  %1 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a)
  %2 = call <vscale x 2 x i64> @llvm.aarch64.sve.revh.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %1)
  ret <vscale x 2 x i64> %2
}

; negative test
define <vscale x 16 x i1> @aarch64_sve_rev_inv_multi_use(<vscale x 16 x i1> %0) #0 {
; CHECK-LABEL: define <vscale x 16 x i1> @aarch64_sve_rev_inv_multi_use(
; CHECK-SAME: <vscale x 16 x i1> [[TMP0:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = or <vscale x 16 x i1> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    ret <vscale x 16 x i1> [[TMP3]]
;
entry:
  %1 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> %0)
  %2 = call <vscale x 16 x i1> @llvm.aarch64.sve.rev.b16(<vscale x 16 x i1> %1)
  %3 = or <vscale x 16 x i1> %1, %2
  ret <vscale x 16 x i1> %3
}


; negative test
define <vscale x 2 x i64> @aarch64_sve_revw_inv_multi_use(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a) #0 {
; CHECK-LABEL: define <vscale x 2 x i64> @aarch64_sve_revw_inv_multi_use(
; CHECK-SAME: <vscale x 2 x i64> [[PRED:%.*]], <vscale x 2 x i1> [[PASSTHRU:%.*]], <vscale x 2 x i64> [[A:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[A]])
; CHECK-NEXT:    [[TMP2:%.*]] = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> [[PRED]], <vscale x 2 x i1> [[PASSTHRU]], <vscale x 2 x i64> [[TMP1]])
; CHECK-NEXT:    [[TMP3:%.*]] = or <vscale x 2 x i64> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    ret <vscale x 2 x i64> [[TMP3]]
;
  %1 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %a)
  %2 = call <vscale x 2 x i64> @llvm.aarch64.sve.revw.nxv2i64(<vscale x 2 x i64> %pred, <vscale x 2 x i1> %passthru, <vscale x 2 x i64> %1)
  %3 = or <vscale x 2 x i64> %1, %2
  ret <vscale x 2 x i64> %3
}

attributes #0 = { "target-features"="+sve" }
