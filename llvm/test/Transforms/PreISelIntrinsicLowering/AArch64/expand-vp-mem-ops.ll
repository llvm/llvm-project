; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; RUN: opt -mtriple=aarch64-linux-gnu -mattr=+sve -passes=pre-isel-intrinsic-lowering \
; RUN:    -force-mem-intrinsic-expansion -S < %s | FileCheck %s

define void @memcpy1024_i64(ptr %dst, ptr %src) {
; CHECK-LABEL: define void @memcpy1024_i64(
; CHECK-SAME: ptr [[DST:%.*]], ptr [[SRC:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    br label %[[MEMCPY_VEC_LOOP:.*]]
; CHECK:       [[MEMCPY_VEC_LOOP]]:
; CHECK-NEXT:    [[SRC_F:%.*]] = phi ptr [ [[SRC]], %[[ENTRY]] ], [ [[TMP6:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[DST_F:%.*]] = phi ptr [ [[DST]], %[[ENTRY]] ], [ [[TMP7:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[LEN_F:%.*]] = phi i64 [ 1024, %[[ENTRY]] ], [ [[TMP9:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.experimental.get.vector.length.i64(i64 [[LEN_F]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = and <vscale x 8 x i1> [[TMP1]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE:%.*]] = mul nuw i32 [[VSCALE]], 8
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[SRC_F]], <vscale x 8 x i1> [[TMP2]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP5:%.*]] = and <vscale x 8 x i1> [[TMP4]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE1:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE2:%.*]] = mul nuw i32 [[VSCALE1]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP3]], ptr align 1 [[DST_F]], <vscale x 8 x i1> [[TMP5]])
; CHECK-NEXT:    [[TMP6]] = getelementptr i8, ptr [[SRC_F]], i32 [[TMP0]]
; CHECK-NEXT:    [[TMP7]] = getelementptr i8, ptr [[DST_F]], i32 [[TMP0]]
; CHECK-NEXT:    [[TMP8:%.*]] = zext i32 [[TMP0]] to i64
; CHECK-NEXT:    [[TMP9]] = sub i64 [[LEN_F]], [[TMP8]]
; CHECK-NEXT:    [[TMP10:%.*]] = icmp ugt i64 [[TMP9]], 0
; CHECK-NEXT:    br i1 [[TMP10]], label %[[MEMCPY_VEC_LOOP]], label %[[MEMCPY_VEC_EXIT:.*]]
; CHECK:       [[MEMCPY_VEC_EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  call void @llvm.memcpy.p0.p0.i64(ptr %dst, ptr %src, i64 1024, i1 false)
  ret void
}

define void @memcpy1024_i32(ptr %dst, ptr %src) {
; CHECK-LABEL: define void @memcpy1024_i32(
; CHECK-SAME: ptr [[DST:%.*]], ptr [[SRC:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    br label %[[MEMCPY_VEC_LOOP:.*]]
; CHECK:       [[MEMCPY_VEC_LOOP]]:
; CHECK-NEXT:    [[SRC_F:%.*]] = phi ptr [ [[SRC]], %[[ENTRY]] ], [ [[TMP6:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[DST_F:%.*]] = phi ptr [ [[DST]], %[[ENTRY]] ], [ [[TMP7:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[LEN_F:%.*]] = phi i32 [ 1024, %[[ENTRY]] ], [ [[TMP8:%.*]], %[[MEMCPY_VEC_LOOP]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.experimental.get.vector.length.i32(i32 [[LEN_F]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = and <vscale x 8 x i1> [[TMP1]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE:%.*]] = mul nuw i32 [[VSCALE]], 8
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[SRC_F]], <vscale x 8 x i1> [[TMP2]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP5:%.*]] = and <vscale x 8 x i1> [[TMP4]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE1:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE2:%.*]] = mul nuw i32 [[VSCALE1]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP3]], ptr align 1 [[DST_F]], <vscale x 8 x i1> [[TMP5]])
; CHECK-NEXT:    [[TMP6]] = getelementptr i8, ptr [[SRC_F]], i32 [[TMP0]]
; CHECK-NEXT:    [[TMP7]] = getelementptr i8, ptr [[DST_F]], i32 [[TMP0]]
; CHECK-NEXT:    [[TMP8]] = sub i32 [[LEN_F]], [[TMP0]]
; CHECK-NEXT:    [[TMP9:%.*]] = icmp ugt i32 [[TMP8]], 0
; CHECK-NEXT:    br i1 [[TMP9]], label %[[MEMCPY_VEC_LOOP]], label %[[MEMCPY_VEC_EXIT:.*]]
; CHECK:       [[MEMCPY_VEC_EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  call void @llvm.memcpy.p0.p0.i32(ptr %dst, ptr %src, i32 1024, i1 false)
  ret void
}

define void @memset1024(ptr %dst, i8 %value) {
; CHECK-LABEL: define void @memset1024(
; CHECK-SAME: ptr [[DST:%.*]], i8 [[VALUE:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    br label %[[MEMSET_VEC_LOOP:.*]]
; CHECK:       [[MEMSET_VEC_LOOP]]:
; CHECK-NEXT:    [[SRC_F:%.*]] = phi ptr [ [[DST]], %[[ENTRY]] ], [ [[TMP6:%.*]], %[[MEMSET_VEC_LOOP]] ]
; CHECK-NEXT:    [[LEN_F:%.*]] = phi i64 [ 1024, %[[ENTRY]] ], [ [[TMP8:%.*]], %[[MEMSET_VEC_LOOP]] ]
; CHECK-NEXT:    [[TMP0:%.*]] = call i32 @llvm.experimental.get.vector.length.i64(i64 [[LEN_F]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP1:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP2:%.*]] = and <vscale x 8 x i1> [[TMP1]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE1:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE2:%.*]] = mul nuw i32 [[VSCALE1]], 8
; CHECK-NEXT:    [[TMP3:%.*]] = call <vscale x 8 x i8> @llvm.experimental.vp.splat.nxv8i8(i8 [[VALUE]], <vscale x 8 x i1> [[TMP2]], i32 [[SCALABLE_SIZE2]])
; CHECK-NEXT:    [[TMP4:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP0]])
; CHECK-NEXT:    [[TMP5:%.*]] = and <vscale x 8 x i1> [[TMP4]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE:%.*]] = mul nuw i32 [[VSCALE]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP3]], ptr align 1 [[SRC_F]], <vscale x 8 x i1> [[TMP5]])
; CHECK-NEXT:    [[TMP6]] = getelementptr i8, ptr [[SRC_F]], i32 [[TMP0]]
; CHECK-NEXT:    [[TMP7:%.*]] = zext i32 [[TMP0]] to i64
; CHECK-NEXT:    [[TMP8]] = sub i64 [[LEN_F]], [[TMP7]]
; CHECK-NEXT:    [[TMP9:%.*]] = icmp ugt i64 [[TMP8]], 0
; CHECK-NEXT:    br i1 [[TMP9]], label %[[MEMSET_VEC_LOOP]], label %[[MEMSET_VEC_EXIT:.*]]
; CHECK:       [[MEMSET_VEC_EXIT]]:
; CHECK-NEXT:    ret void
;
entry:
  call void @llvm.memset.p0.i64(ptr %dst, i8 %value, i64 1024, i1 false)
  ret void
}

define void @memmove1024_i64(ptr %dst, ptr %src) {
; CHECK-LABEL: define void @memmove1024_i64(
; CHECK-SAME: ptr [[DST:%.*]], ptr [[SRC:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[COMPARE_ADDR:%.*]] = icmp ult ptr [[SRC]], [[DST]]
; CHECK-NEXT:    br i1 [[COMPARE_ADDR]], label %[[VEC_BACKWARD:.*]], label %[[VEC_FORWARD:.*]]
; CHECK:       [[VEC_BACKWARD]]:
; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr i8, ptr [[SRC]], i64 1024
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr [[DST]], i64 1024
; CHECK-NEXT:    br label %[[VEC_BACKWARD_LOOP:.*]]
; CHECK:       [[VEC_BACKWARD_LOOP]]:
; CHECK-NEXT:    [[SRC_B:%.*]] = phi ptr [ [[TMP0]], %[[VEC_BACKWARD]] ], [ [[TMP4:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[DST_B:%.*]] = phi ptr [ [[TMP1]], %[[VEC_BACKWARD]] ], [ [[TMP6:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[LEN_B:%.*]] = phi i64 [ 1024, %[[VEC_BACKWARD]] ], [ [[TMP13:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.experimental.get.vector.length.i64(i64 [[LEN_B]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP3:%.*]] = sub i32 0, [[TMP2]]
; CHECK-NEXT:    [[TMP4]] = getelementptr i8, ptr [[SRC_B]], i32 [[TMP3]]
; CHECK-NEXT:    [[TMP5:%.*]] = sub i32 0, [[TMP2]]
; CHECK-NEXT:    [[TMP6]] = getelementptr i8, ptr [[DST_B]], i32 [[TMP5]]
; CHECK-NEXT:    [[TMP7:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP2]])
; CHECK-NEXT:    [[TMP8:%.*]] = and <vscale x 8 x i1> [[TMP7]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE1:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE2:%.*]] = mul nuw i32 [[VSCALE1]], 8
; CHECK-NEXT:    [[TMP9:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[TMP4]], <vscale x 8 x i1> [[TMP8]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP10:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP2]])
; CHECK-NEXT:    [[TMP11:%.*]] = and <vscale x 8 x i1> [[TMP10]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE5:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE6:%.*]] = mul nuw i32 [[VSCALE5]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP9]], ptr align 1 [[TMP6]], <vscale x 8 x i1> [[TMP11]])
; CHECK-NEXT:    [[TMP12:%.*]] = zext i32 [[TMP2]] to i64
; CHECK-NEXT:    [[TMP13]] = sub i64 [[LEN_B]], [[TMP12]]
; CHECK-NEXT:    [[TMP14:%.*]] = icmp ugt i64 [[TMP13]], 0
; CHECK-NEXT:    br i1 [[TMP14]], label %[[VEC_BACKWARD_LOOP]], label %[[VEC_DONE:.*]]
; CHECK:       [[VEC_FORWARD]]:
; CHECK-NEXT:    [[SRC_F:%.*]] = phi ptr [ [[SRC]], %[[ENTRY]] ], [ [[TMP21:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[DST_F:%.*]] = phi ptr [ [[DST]], %[[ENTRY]] ], [ [[TMP22:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[LEN_F:%.*]] = phi i64 [ 1024, %[[ENTRY]] ], [ [[TMP24:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[TMP15:%.*]] = call i32 @llvm.experimental.get.vector.length.i64(i64 [[LEN_F]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP16:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP15]])
; CHECK-NEXT:    [[TMP17:%.*]] = and <vscale x 8 x i1> [[TMP16]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE:%.*]] = mul nuw i32 [[VSCALE]], 8
; CHECK-NEXT:    [[TMP18:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[SRC_F]], <vscale x 8 x i1> [[TMP17]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP19:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP15]])
; CHECK-NEXT:    [[TMP20:%.*]] = and <vscale x 8 x i1> [[TMP19]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE3:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE4:%.*]] = mul nuw i32 [[VSCALE3]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP18]], ptr align 1 [[DST_F]], <vscale x 8 x i1> [[TMP20]])
; CHECK-NEXT:    [[TMP21]] = getelementptr i8, ptr [[SRC_F]], i32 [[TMP15]]
; CHECK-NEXT:    [[TMP22]] = getelementptr i8, ptr [[DST_F]], i32 [[TMP15]]
; CHECK-NEXT:    [[TMP23:%.*]] = zext i32 [[TMP15]] to i64
; CHECK-NEXT:    [[TMP24]] = sub i64 [[LEN_F]], [[TMP23]]
; CHECK-NEXT:    [[TMP25:%.*]] = icmp ugt i64 [[TMP24]], 0
; CHECK-NEXT:    br i1 [[TMP25]], label %[[VEC_FORWARD]], label %[[VEC_DONE]]
; CHECK:       [[VEC_DONE]]:
; CHECK-NEXT:    ret void
;
entry:
  call void @llvm.memmove.p0.p0.i64(ptr %dst, ptr %src, i64 1024, i1 false)
  ret void
}

define void @memmove1024_i32(ptr %dst, ptr %src) {
; CHECK-LABEL: define void @memmove1024_i32(
; CHECK-SAME: ptr [[DST:%.*]], ptr [[SRC:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    [[COMPARE_ADDR:%.*]] = icmp ult ptr [[SRC]], [[DST]]
; CHECK-NEXT:    br i1 [[COMPARE_ADDR]], label %[[VEC_BACKWARD:.*]], label %[[VEC_FORWARD:.*]]
; CHECK:       [[VEC_BACKWARD]]:
; CHECK-NEXT:    [[TMP0:%.*]] = getelementptr i8, ptr [[SRC]], i32 1024
; CHECK-NEXT:    [[TMP1:%.*]] = getelementptr i8, ptr [[DST]], i32 1024
; CHECK-NEXT:    br label %[[VEC_BACKWARD_LOOP:.*]]
; CHECK:       [[VEC_BACKWARD_LOOP]]:
; CHECK-NEXT:    [[SRC_B:%.*]] = phi ptr [ [[TMP0]], %[[VEC_BACKWARD]] ], [ [[TMP4:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[DST_B:%.*]] = phi ptr [ [[TMP1]], %[[VEC_BACKWARD]] ], [ [[TMP6:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[LEN_B:%.*]] = phi i32 [ 1024, %[[VEC_BACKWARD]] ], [ [[TMP12:%.*]], %[[VEC_BACKWARD_LOOP]] ]
; CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.experimental.get.vector.length.i32(i32 [[LEN_B]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP3:%.*]] = sub i32 0, [[TMP2]]
; CHECK-NEXT:    [[TMP4]] = getelementptr i8, ptr [[SRC_B]], i32 [[TMP3]]
; CHECK-NEXT:    [[TMP5:%.*]] = sub i32 0, [[TMP2]]
; CHECK-NEXT:    [[TMP6]] = getelementptr i8, ptr [[DST_B]], i32 [[TMP5]]
; CHECK-NEXT:    [[TMP7:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP2]])
; CHECK-NEXT:    [[TMP8:%.*]] = and <vscale x 8 x i1> [[TMP7]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE1:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE2:%.*]] = mul nuw i32 [[VSCALE1]], 8
; CHECK-NEXT:    [[TMP9:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[TMP4]], <vscale x 8 x i1> [[TMP8]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP10:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP2]])
; CHECK-NEXT:    [[TMP11:%.*]] = and <vscale x 8 x i1> [[TMP10]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE5:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE6:%.*]] = mul nuw i32 [[VSCALE5]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP9]], ptr align 1 [[TMP6]], <vscale x 8 x i1> [[TMP11]])
; CHECK-NEXT:    [[TMP12]] = sub i32 [[LEN_B]], [[TMP2]]
; CHECK-NEXT:    [[TMP13:%.*]] = icmp ugt i32 [[TMP12]], 0
; CHECK-NEXT:    br i1 [[TMP13]], label %[[VEC_BACKWARD_LOOP]], label %[[VEC_DONE:.*]]
; CHECK:       [[VEC_FORWARD]]:
; CHECK-NEXT:    [[SRC_F:%.*]] = phi ptr [ [[SRC]], %[[ENTRY]] ], [ [[TMP20:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[DST_F:%.*]] = phi ptr [ [[DST]], %[[ENTRY]] ], [ [[TMP21:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[LEN_F:%.*]] = phi i32 [ 1024, %[[ENTRY]] ], [ [[TMP22:%.*]], %[[VEC_FORWARD]] ]
; CHECK-NEXT:    [[TMP14:%.*]] = call i32 @llvm.experimental.get.vector.length.i32(i32 [[LEN_F]], i32 8, i1 true)
; CHECK-NEXT:    [[TMP15:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP14]])
; CHECK-NEXT:    [[TMP16:%.*]] = and <vscale x 8 x i1> [[TMP15]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE:%.*]] = mul nuw i32 [[VSCALE]], 8
; CHECK-NEXT:    [[TMP17:%.*]] = call <vscale x 8 x i8> @llvm.masked.load.nxv8i8.p0(ptr align 1 [[SRC_F]], <vscale x 8 x i1> [[TMP16]], <vscale x 8 x i8> poison)
; CHECK-NEXT:    [[TMP18:%.*]] = call <vscale x 8 x i1> @llvm.get.active.lane.mask.nxv8i1.i32(i32 0, i32 [[TMP14]])
; CHECK-NEXT:    [[TMP19:%.*]] = and <vscale x 8 x i1> [[TMP18]], splat (i1 true)
; CHECK-NEXT:    [[VSCALE3:%.*]] = call i32 @llvm.vscale.i32()
; CHECK-NEXT:    [[SCALABLE_SIZE4:%.*]] = mul nuw i32 [[VSCALE3]], 8
; CHECK-NEXT:    call void @llvm.masked.store.nxv8i8.p0(<vscale x 8 x i8> [[TMP17]], ptr align 1 [[DST_F]], <vscale x 8 x i1> [[TMP19]])
; CHECK-NEXT:    [[TMP20]] = getelementptr i8, ptr [[SRC_F]], i32 [[TMP14]]
; CHECK-NEXT:    [[TMP21]] = getelementptr i8, ptr [[DST_F]], i32 [[TMP14]]
; CHECK-NEXT:    [[TMP22]] = sub i32 [[LEN_F]], [[TMP14]]
; CHECK-NEXT:    [[TMP23:%.*]] = icmp ugt i32 [[TMP22]], 0
; CHECK-NEXT:    br i1 [[TMP23]], label %[[VEC_FORWARD]], label %[[VEC_DONE]]
; CHECK:       [[VEC_DONE]]:
; CHECK-NEXT:    ret void
;
entry:
  call void @llvm.memmove.p0.p0.i32(ptr %dst, ptr %src, i32 1024, i1 false)
  ret void
}

declare void @llvm.memcpy.p0.p0.i64(ptr, ptr, i64, i1)
declare void @llvm.memcpy.p0.p0.i32(ptr, ptr, i32, i1)
declare void @llvm.memset.p0.i64(ptr nocapture, i8, i64, i1)
declare void @llvm.memmove.p0.p0.i64(ptr, ptr, i64, i1)
declare void @llvm.memmove.p0.p0.i32(ptr, ptr, i32, i1)
