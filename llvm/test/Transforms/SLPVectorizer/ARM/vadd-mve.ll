; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt < %s -passes=slp-vectorizer --mtriple arm-none-eabi -mattr=+mve -S -o - | FileCheck %s

define i64 @vadd_32_64(ptr readonly %a) {
; CHECK-LABEL: define i64 @vadd_32_64(
; CHECK-SAME: ptr readonly [[A:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x i32>, ptr [[A]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = sext <4 x i32> [[TMP0]] to <4 x i64>
; CHECK-NEXT:    [[TMP2:%.*]] = call i64 @llvm.vector.reduce.add.v4i64(<4 x i64> [[TMP1]])
; CHECK-NEXT:    ret i64 [[TMP2]]
;
entry:
  %0 = load i32, ptr %a, align 4
  %conv = sext i32 %0 to i64
  %arrayidx1 = getelementptr inbounds nuw i8, ptr %a, i32 4
  %1 = load i32, ptr %arrayidx1, align 4
  %conv2 = sext i32 %1 to i64
  %add = add nsw i64 %conv2, %conv
  %arrayidx3 = getelementptr inbounds nuw i8, ptr %a, i32 8
  %2 = load i32, ptr %arrayidx3, align 4
  %conv4 = sext i32 %2 to i64
  %add5 = add nsw i64 %add, %conv4
  %arrayidx6 = getelementptr inbounds nuw i8, ptr %a, i32 12
  %3 = load i32, ptr %arrayidx6, align 4
  %conv7 = sext i32 %3 to i64
  %add8 = add nsw i64 %add5, %conv7
  ret i64 %add8
}
