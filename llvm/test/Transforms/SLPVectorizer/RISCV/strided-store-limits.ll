; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5

; RUN: opt -mtriple=riscv64 -mattr=+m,+v -passes=slp-vectorizer \
; RUN: -slp-enable-strided-stores -slp-min-strided-loads=5 \
; RUN: -slp-max-stride=9 -S < %s | FileCheck %s

; Don't vectorize since stride of 10, -slp-max-stride=9
define void @constant_stride_too_large(ptr %pl, ptr %ps) {
; CHECK-LABEL: define void @constant_stride_too_large(
; CHECK-SAME: ptr [[PL:%.*]], ptr [[PS:%.*]]) #[[ATTR0:[0-9]+]] {
; CHECK-NEXT:    [[GEP_L0:%.*]] = getelementptr i8, ptr [[PL]], i64 0
; CHECK-NEXT:    [[GEP_L1:%.*]] = getelementptr i8, ptr [[PL]], i64 1
; CHECK-NEXT:    [[GEP_L2:%.*]] = getelementptr i8, ptr [[PL]], i64 2
; CHECK-NEXT:    [[GEP_L3:%.*]] = getelementptr i8, ptr [[PL]], i64 3
; CHECK-NEXT:    [[GEP_L4:%.*]] = getelementptr i8, ptr [[PL]], i64 4
; CHECK-NEXT:    [[GEP_L5:%.*]] = getelementptr i8, ptr [[PL]], i64 5
; CHECK-NEXT:    [[GEP_L6:%.*]] = getelementptr i8, ptr [[PL]], i64 6
; CHECK-NEXT:    [[GEP_L7:%.*]] = getelementptr i8, ptr [[PL]], i64 7
; CHECK-NEXT:    [[LOAD0:%.*]] = load i8, ptr [[GEP_L0]], align 1
; CHECK-NEXT:    [[LOAD1:%.*]] = load i8, ptr [[GEP_L1]], align 1
; CHECK-NEXT:    [[LOAD2:%.*]] = load i8, ptr [[GEP_L2]], align 1
; CHECK-NEXT:    [[LOAD3:%.*]] = load i8, ptr [[GEP_L3]], align 1
; CHECK-NEXT:    [[LOAD4:%.*]] = load i8, ptr [[GEP_L4]], align 1
; CHECK-NEXT:    [[LOAD5:%.*]] = load i8, ptr [[GEP_L5]], align 1
; CHECK-NEXT:    [[LOAD6:%.*]] = load i8, ptr [[GEP_L6]], align 1
; CHECK-NEXT:    [[LOAD7:%.*]] = load i8, ptr [[GEP_L7]], align 1
; CHECK-NEXT:    [[GEP_S0:%.*]] = getelementptr i8, ptr [[PS]], i64 0
; CHECK-NEXT:    [[GEP_S1:%.*]] = getelementptr i8, ptr [[PS]], i64 10
; CHECK-NEXT:    [[GEP_S2:%.*]] = getelementptr i8, ptr [[PS]], i64 20
; CHECK-NEXT:    [[GEP_S3:%.*]] = getelementptr i8, ptr [[PS]], i64 30
; CHECK-NEXT:    [[GEP_S4:%.*]] = getelementptr i8, ptr [[PS]], i64 40
; CHECK-NEXT:    [[GEP_S5:%.*]] = getelementptr i8, ptr [[PS]], i64 50
; CHECK-NEXT:    [[GEP_S6:%.*]] = getelementptr i8, ptr [[PS]], i64 60
; CHECK-NEXT:    [[GEP_S7:%.*]] = getelementptr i8, ptr [[PS]], i64 70
; CHECK-NEXT:    store i8 [[LOAD0]], ptr [[GEP_S0]], align 1
; CHECK-NEXT:    store i8 [[LOAD1]], ptr [[GEP_S1]], align 1
; CHECK-NEXT:    store i8 [[LOAD2]], ptr [[GEP_S2]], align 1
; CHECK-NEXT:    store i8 [[LOAD3]], ptr [[GEP_S3]], align 1
; CHECK-NEXT:    store i8 [[LOAD4]], ptr [[GEP_S4]], align 1
; CHECK-NEXT:    store i8 [[LOAD5]], ptr [[GEP_S5]], align 1
; CHECK-NEXT:    store i8 [[LOAD6]], ptr [[GEP_S6]], align 1
; CHECK-NEXT:    store i8 [[LOAD7]], ptr [[GEP_S7]], align 1
; CHECK-NEXT:    ret void
;
  %gep_l0 = getelementptr i8, ptr %pl, i64 0
  %gep_l1 = getelementptr i8, ptr %pl, i64 1
  %gep_l2 = getelementptr i8, ptr %pl, i64 2
  %gep_l3 = getelementptr i8, ptr %pl, i64 3
  %gep_l4 = getelementptr i8, ptr %pl, i64 4
  %gep_l5 = getelementptr i8, ptr %pl, i64 5
  %gep_l6 = getelementptr i8, ptr %pl, i64 6
  %gep_l7 = getelementptr i8, ptr %pl, i64 7

  %load0  = load i8, ptr %gep_l0
  %load1  = load i8, ptr %gep_l1
  %load2  = load i8, ptr %gep_l2
  %load3  = load i8, ptr %gep_l3
  %load4  = load i8, ptr %gep_l4
  %load5  = load i8, ptr %gep_l5
  %load6  = load i8, ptr %gep_l6
  %load7  = load i8, ptr %gep_l7

  %gep_s0 = getelementptr i8, ptr %ps, i64 0
  %gep_s1 = getelementptr i8, ptr %ps, i64 10
  %gep_s2 = getelementptr i8, ptr %ps, i64 20
  %gep_s3 = getelementptr i8, ptr %ps, i64 30
  %gep_s4 = getelementptr i8, ptr %ps, i64 40
  %gep_s5 = getelementptr i8, ptr %ps, i64 50
  %gep_s6 = getelementptr i8, ptr %ps, i64 60
  %gep_s7 = getelementptr i8, ptr %ps, i64 70

  store i8 %load0, ptr %gep_s0
  store i8 %load1, ptr %gep_s1
  store i8 %load2, ptr %gep_s2
  store i8 %load3, ptr %gep_s3
  store i8 %load4, ptr %gep_s4
  store i8 %load5, ptr %gep_s5
  store i8 %load6, ptr %gep_s6
  store i8 %load7, ptr %gep_s7

  ret void
}

; Vectorize since stride of 9, -slp-max-stride=9
define void @constant_stride_not_too_large(ptr %pl, ptr %ps) {
; CHECK-LABEL: define void @constant_stride_not_too_large(
; CHECK-SAME: ptr [[PL:%.*]], ptr [[PS:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[GEP_L0:%.*]] = getelementptr i8, ptr [[PL]], i64 0
; CHECK-NEXT:    [[GEP_S0:%.*]] = getelementptr i8, ptr [[PS]], i64 0
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[GEP_L0]], align 1
; CHECK-NEXT:    call void @llvm.experimental.vp.strided.store.v8i8.p0.i64(<8 x i8> [[TMP1]], ptr align 1 [[GEP_S0]], i64 9, <8 x i1> splat (i1 true), i32 8)
; CHECK-NEXT:    ret void
;
  %gep_l0 = getelementptr i8, ptr %pl, i64 0
  %gep_l1 = getelementptr i8, ptr %pl, i64 1
  %gep_l2 = getelementptr i8, ptr %pl, i64 2
  %gep_l3 = getelementptr i8, ptr %pl, i64 3
  %gep_l4 = getelementptr i8, ptr %pl, i64 4
  %gep_l5 = getelementptr i8, ptr %pl, i64 5
  %gep_l6 = getelementptr i8, ptr %pl, i64 6
  %gep_l7 = getelementptr i8, ptr %pl, i64 7

  %load0  = load i8, ptr %gep_l0
  %load1  = load i8, ptr %gep_l1
  %load2  = load i8, ptr %gep_l2
  %load3  = load i8, ptr %gep_l3
  %load4  = load i8, ptr %gep_l4
  %load5  = load i8, ptr %gep_l5
  %load6  = load i8, ptr %gep_l6
  %load7  = load i8, ptr %gep_l7

  %gep_s0 = getelementptr i8, ptr %ps, i64 0
  %gep_s1 = getelementptr i8, ptr %ps, i64 9
  %gep_s2 = getelementptr i8, ptr %ps, i64 18
  %gep_s3 = getelementptr i8, ptr %ps, i64 27
  %gep_s4 = getelementptr i8, ptr %ps, i64 36
  %gep_s5 = getelementptr i8, ptr %ps, i64 45
  %gep_s6 = getelementptr i8, ptr %ps, i64 54
  %gep_s7 = getelementptr i8, ptr %ps, i64 63

  store i8 %load0, ptr %gep_s0
  store i8 %load1, ptr %gep_s1
  store i8 %load2, ptr %gep_s2
  store i8 %load3, ptr %gep_s3
  store i8 %load4, ptr %gep_s4
  store i8 %load5, ptr %gep_s5
  store i8 %load6, ptr %gep_s6
  store i8 %load7, ptr %gep_s7

  ret void
}

; Don't vectorize since 4 elements, -slp-min-strided-loads=5
define void @constant_stride_too_few_elements(ptr %pl, ptr %ps) {
; CHECK-LABEL: define void @constant_stride_too_few_elements(
; CHECK-SAME: ptr [[PL:%.*]], ptr [[PS:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[GEP_L0:%.*]] = getelementptr i8, ptr [[PL]], i64 0
; CHECK-NEXT:    [[GEP_L1:%.*]] = getelementptr i8, ptr [[PL]], i64 1
; CHECK-NEXT:    [[GEP_L2:%.*]] = getelementptr i8, ptr [[PL]], i64 2
; CHECK-NEXT:    [[GEP_L3:%.*]] = getelementptr i8, ptr [[PL]], i64 3
; CHECK-NEXT:    [[LOAD0:%.*]] = load i8, ptr [[GEP_L0]], align 1
; CHECK-NEXT:    [[LOAD1:%.*]] = load i8, ptr [[GEP_L1]], align 1
; CHECK-NEXT:    [[LOAD2:%.*]] = load i8, ptr [[GEP_L2]], align 1
; CHECK-NEXT:    [[LOAD3:%.*]] = load i8, ptr [[GEP_L3]], align 1
; CHECK-NEXT:    [[GEP_S0:%.*]] = getelementptr i8, ptr [[PS]], i64 0
; CHECK-NEXT:    [[GEP_S1:%.*]] = getelementptr i8, ptr [[PS]], i64 2
; CHECK-NEXT:    [[GEP_S2:%.*]] = getelementptr i8, ptr [[PS]], i64 4
; CHECK-NEXT:    [[GEP_S3:%.*]] = getelementptr i8, ptr [[PS]], i64 6
; CHECK-NEXT:    store i8 [[LOAD0]], ptr [[GEP_S0]], align 1
; CHECK-NEXT:    store i8 [[LOAD1]], ptr [[GEP_S1]], align 1
; CHECK-NEXT:    store i8 [[LOAD2]], ptr [[GEP_S2]], align 1
; CHECK-NEXT:    store i8 [[LOAD3]], ptr [[GEP_S3]], align 1
; CHECK-NEXT:    ret void
;
  %gep_l0 = getelementptr i8, ptr %pl, i64 0
  %gep_l1 = getelementptr i8, ptr %pl, i64 1
  %gep_l2 = getelementptr i8, ptr %pl, i64 2
  %gep_l3 = getelementptr i8, ptr %pl, i64 3

  %load0  = load i8, ptr %gep_l0
  %load1  = load i8, ptr %gep_l1
  %load2  = load i8, ptr %gep_l2
  %load3  = load i8, ptr %gep_l3

  %gep_s0 = getelementptr i8, ptr %ps, i64 0
  %gep_s1 = getelementptr i8, ptr %ps, i64 2
  %gep_s2 = getelementptr i8, ptr %ps, i64 4
  %gep_s3 = getelementptr i8, ptr %ps, i64 6

  store i8 %load0, ptr %gep_s0
  store i8 %load1, ptr %gep_s1
  store i8 %load2, ptr %gep_s2
  store i8 %load3, ptr %gep_s3

  ret void
}

; Don't vectorize since 5 elements (so VF=4 because of power of 2), -slp-min-strided-loads=5
define void @constant_stride_just_enough_elements(ptr %pl, ptr %ps) {
; CHECK-LABEL: define void @constant_stride_just_enough_elements(
; CHECK-SAME: ptr [[PL:%.*]], ptr [[PS:%.*]]) #[[ATTR0]] {
; CHECK-NEXT:    [[GEP_L0:%.*]] = getelementptr i8, ptr [[PL]], i64 0
; CHECK-NEXT:    [[GEP_L1:%.*]] = getelementptr i8, ptr [[PL]], i64 1
; CHECK-NEXT:    [[GEP_L2:%.*]] = getelementptr i8, ptr [[PL]], i64 2
; CHECK-NEXT:    [[GEP_L3:%.*]] = getelementptr i8, ptr [[PL]], i64 3
; CHECK-NEXT:    [[GEP_L4:%.*]] = getelementptr i8, ptr [[PL]], i64 4
; CHECK-NEXT:    [[LOAD0:%.*]] = load i8, ptr [[GEP_L0]], align 1
; CHECK-NEXT:    [[LOAD1:%.*]] = load i8, ptr [[GEP_L1]], align 1
; CHECK-NEXT:    [[LOAD2:%.*]] = load i8, ptr [[GEP_L2]], align 1
; CHECK-NEXT:    [[LOAD3:%.*]] = load i8, ptr [[GEP_L3]], align 1
; CHECK-NEXT:    [[LOAD4:%.*]] = load i8, ptr [[GEP_L4]], align 1
; CHECK-NEXT:    [[GEP_S0:%.*]] = getelementptr i8, ptr [[PS]], i64 0
; CHECK-NEXT:    [[GEP_S1:%.*]] = getelementptr i8, ptr [[PS]], i64 2
; CHECK-NEXT:    [[GEP_S2:%.*]] = getelementptr i8, ptr [[PS]], i64 4
; CHECK-NEXT:    [[GEP_S3:%.*]] = getelementptr i8, ptr [[PS]], i64 6
; CHECK-NEXT:    [[GEP_S4:%.*]] = getelementptr i8, ptr [[PS]], i64 8
; CHECK-NEXT:    store i8 [[LOAD0]], ptr [[GEP_S0]], align 1
; CHECK-NEXT:    store i8 [[LOAD1]], ptr [[GEP_S1]], align 1
; CHECK-NEXT:    store i8 [[LOAD2]], ptr [[GEP_S2]], align 1
; CHECK-NEXT:    store i8 [[LOAD3]], ptr [[GEP_S3]], align 1
; CHECK-NEXT:    store i8 [[LOAD4]], ptr [[GEP_S4]], align 1
; CHECK-NEXT:    ret void
;
  %gep_l0 = getelementptr i8, ptr %pl, i64 0
  %gep_l1 = getelementptr i8, ptr %pl, i64 1
  %gep_l2 = getelementptr i8, ptr %pl, i64 2
  %gep_l3 = getelementptr i8, ptr %pl, i64 3
  %gep_l4 = getelementptr i8, ptr %pl, i64 4

  %load0  = load i8, ptr %gep_l0
  %load1  = load i8, ptr %gep_l1
  %load2  = load i8, ptr %gep_l2
  %load3  = load i8, ptr %gep_l3
  %load4  = load i8, ptr %gep_l4

  %gep_s0 = getelementptr i8, ptr %ps, i64 0
  %gep_s1 = getelementptr i8, ptr %ps, i64 2
  %gep_s2 = getelementptr i8, ptr %ps, i64 4
  %gep_s3 = getelementptr i8, ptr %ps, i64 6
  %gep_s4 = getelementptr i8, ptr %ps, i64 8

  store i8 %load0, ptr %gep_s0
  store i8 %load1, ptr %gep_s1
  store i8 %load2, ptr %gep_s2
  store i8 %load3, ptr %gep_s3
  store i8 %load4, ptr %gep_s4

  ret void
}
