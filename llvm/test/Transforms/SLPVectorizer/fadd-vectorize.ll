; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S --passes=slp-vectorizer < %s | FileCheck %s

define float @test_reduce(ptr %a) {
; CHECK-LABEL: define float @test_reduce(
; CHECK-SAME: ptr [[A:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> [[TMP0]])
; CHECK-NEXT:    ret float [[TMP1]]
;
entry:
  %0 = load float, ptr %a, align 4
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 4
  %1 = load float, ptr %arrayidx1, align 4
  %add = fadd float %0, %1
  %arrayidx2 = getelementptr inbounds i8, ptr %a, i64 8
  %2 = load float, ptr %arrayidx2, align 4
  %add3 = fadd float %add, %2
  %arrayidx4 = getelementptr inbounds i8, ptr %a, i64 12
  %3 = load float, ptr %arrayidx4, align 4
  %add5 = fadd float %add3, %3
  ret float %add5
}

define float @test_no_reduce(ptr %a) {
; CHECK-LABEL: define float @test_no_reduce(
; CHECK-SAME: ptr [[A:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A]], align 4
; CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds i8, ptr [[A]], i64 4
; CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[ARRAYIDX1]], align 4
; CHECK-NEXT:    [[ADD:%.*]] = fadd float [[TMP0]], [[TMP1]]
; CHECK-NEXT:    [[ARRAYIDX2:%.*]] = getelementptr inbounds i8, ptr [[A]], i64 8
; CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[ARRAYIDX2]], align 4
; CHECK-NEXT:    [[ARRAYIDX3:%.*]] = getelementptr inbounds i8, ptr [[A]], i64 12
; CHECK-NEXT:    [[TMP3:%.*]] = load float, ptr [[ARRAYIDX3]], align 4
; CHECK-NEXT:    [[ADD4:%.*]] = fadd float [[TMP2]], [[TMP3]]
; CHECK-NEXT:    [[ADD5:%.*]] = fadd float [[ADD]], [[ADD4]]
; CHECK-NEXT:    ret float [[ADD5]]
;
entry:
  %0 = load float, ptr %a, align 4
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 4
  %1 = load float, ptr %arrayidx1, align 4
  %add = fadd float %0, %1
  %arrayidx2 = getelementptr inbounds i8, ptr %a, i64 8
  %2 = load float, ptr %arrayidx2, align 4
  %arrayidx3 = getelementptr inbounds i8, ptr %a, i64 12
  %3 = load float, ptr %arrayidx3, align 4
  %add4 = fadd float %2, %3
  %add5 = fadd float %add, %add4
  ret float %add5
}

define float @test_reduce2(ptr %a, float %b) {
; CHECK-LABEL: define float @test_reduce2(
; CHECK-SAME: ptr [[A:%.*]], float [[B:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = call float @llvm.vector.reduce.fadd.v4f32(float -0.000000e+00, <4 x float> [[TMP0]])
; CHECK-NEXT:    [[ADDB:%.*]] = fadd float [[TMP1]], [[B]]
; CHECK-NEXT:    ret float [[TMP1]]
;
entry:
  %0 = load float, ptr %a, align 4
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 4
  %1 = load float, ptr %arrayidx1, align 4
  %add = fadd float %0, %1
  %arrayidx2 = getelementptr inbounds i8, ptr %a, i64 8
  %2 = load float, ptr %arrayidx2, align 4
  %add3 = fadd float %add, %2
  %arrayidx4 = getelementptr inbounds i8, ptr %a, i64 12
  %3 = load float, ptr %arrayidx4, align 4
  %add5 = fadd float %add3, %3
  %addb = fadd float %add5, %b
  ret float %add5
}

define float @test_reduce_multiple_use(ptr %a, float %b) {
; CHECK-LABEL: define float @test_reduce_multiple_use(
; CHECK-SAME: ptr [[A:%.*]], float [[B:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr [[A]], align 4
; CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[A]], align 4
; CHECK-NEXT:    [[ADDC:%.*]] = fadd float [[B]], [[TMP1]]
; CHECK-NEXT:    [[ADD6:%.*]] = call float @llvm.vector.reduce.fadd.v4f32(float [[ADDC]], <4 x float> [[TMP0]])
; CHECK-NEXT:    [[OP_RDX1:%.*]] = fadd float [[ADD6]], [[B]]
; CHECK-NEXT:    ret float [[OP_RDX1]]
;
entry:
  %0 = load float, ptr %a, align 4
  %arrayidx1 = getelementptr inbounds i8, ptr %a, i64 4
  %1 = load float, ptr %arrayidx1, align 4
  %addc = fadd float %b, %0
  %addb = fadd float %addc, %0
  %add = fadd float %addb, %1
  %arrayidx2 = getelementptr inbounds i8, ptr %a, i64 8
  %2 = load float, ptr %arrayidx2, align 4
  %add3 = fadd float %add, %2
  %arrayidx4 = getelementptr inbounds i8, ptr %a, i64 12
  %3 = load float, ptr %arrayidx4, align 4
  %add5 = fadd float %add3, %3
  %add6 = fadd float %add5, %b
  ret float %add6
}

define double @test_reduce_multiple_reductions(ptr %freq, double %sum) {
; CHECK-LABEL: define double @test_reduce_multiple_reductions(
; CHECK-SAME: ptr [[FREQ:%.*]], double [[SUM:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    [[TMP0:%.*]] = load <32 x double>, ptr [[FREQ]], align 8
; CHECK-NEXT:    [[ARRAYIDX_32:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 256
; CHECK-NEXT:    [[TMP1:%.*]] = load <16 x double>, ptr [[ARRAYIDX_32]], align 8
; CHECK-NEXT:    [[ARRAYIDX_48:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 384
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x double>, ptr [[ARRAYIDX_48]], align 8
; CHECK-NEXT:    [[ARRAYIDX_56:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 448
; CHECK-NEXT:    [[TMP3:%.*]] = load <4 x double>, ptr [[ARRAYIDX_56]], align 8
; CHECK-NEXT:    [[ARRAYIDX_60:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 480
; CHECK-NEXT:    [[TMP4:%.*]] = load double, ptr [[ARRAYIDX_60]], align 8
; CHECK-NEXT:    [[ARRAYIDX_61:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 488
; CHECK-NEXT:    [[TMP5:%.*]] = load double, ptr [[ARRAYIDX_61]], align 8
; CHECK-NEXT:    [[ARRAYIDX_62:%.*]] = getelementptr inbounds i8, ptr [[FREQ]], i64 496
; CHECK-NEXT:    [[TMP6:%.*]] = load double, ptr [[ARRAYIDX_62]], align 8
; CHECK-NEXT:    [[TMP7:%.*]] = call double @llvm.vector.reduce.fadd.v32f64(double -0.000000e+00, <32 x double> [[TMP0]])
; CHECK-NEXT:    [[TMP8:%.*]] = call double @llvm.vector.reduce.fadd.v16f64(double [[TMP7]], <16 x double> [[TMP1]])
; CHECK-NEXT:    [[TMP9:%.*]] = call double @llvm.vector.reduce.fadd.v8f64(double [[TMP8]], <8 x double> [[TMP2]])
; CHECK-NEXT:    [[TMP13:%.*]] = call double @llvm.vector.reduce.fadd.v4f64(double [[TMP9]], <4 x double> [[TMP3]])
; CHECK-NEXT:    [[OP_RDX:%.*]] = fadd double [[TMP13]], [[TMP4]]
; CHECK-NEXT:    [[ADD_61:%.*]] = fadd double [[OP_RDX]], [[TMP5]]
; CHECK-NEXT:    [[ADD_62:%.*]] = fadd double [[ADD_61]], [[TMP6]]
; CHECK-NEXT:    ret double [[ADD_62]]
;
entry:
  %0 = load double, ptr %freq, align 8
  %arrayidx.1 = getelementptr inbounds i8, ptr %freq, i64 8
  %1 = load double, ptr %arrayidx.1, align 8
  %add.1 = fadd double %0, %1
  %arrayidx.2 = getelementptr inbounds i8, ptr %freq, i64 16
  %2 = load double, ptr %arrayidx.2, align 8
  %add.2 = fadd double %add.1, %2
  %arrayidx.3 = getelementptr inbounds i8, ptr %freq, i64 24
  %3 = load double, ptr %arrayidx.3, align 8
  %add.3 = fadd double %add.2, %3
  %arrayidx.4 = getelementptr inbounds i8, ptr %freq, i64 32
  %4 = load double, ptr %arrayidx.4, align 8
  %add.4 = fadd double %add.3, %4
  %arrayidx.5 = getelementptr inbounds i8, ptr %freq, i64 40
  %5 = load double, ptr %arrayidx.5, align 8
  %add.5 = fadd double %add.4, %5
  %arrayidx.6 = getelementptr inbounds i8, ptr %freq, i64 48
  %6 = load double, ptr %arrayidx.6, align 8
  %add.6 = fadd double %add.5, %6
  %arrayidx.7 = getelementptr inbounds i8, ptr %freq, i64 56
  %7 = load double, ptr %arrayidx.7, align 8
  %add.7 = fadd double %add.6, %7
  %arrayidx.8 = getelementptr inbounds i8, ptr %freq, i64 64
  %8 = load double, ptr %arrayidx.8, align 8
  %add.8 = fadd double %add.7, %8
  %arrayidx.9 = getelementptr inbounds i8, ptr %freq, i64 72
  %9 = load double, ptr %arrayidx.9, align 8
  %add.9 = fadd double %add.8, %9
  %arrayidx.10 = getelementptr inbounds i8, ptr %freq, i64 80
  %10 = load double, ptr %arrayidx.10, align 8
  %add.10 = fadd double %add.9, %10
  %arrayidx.11 = getelementptr inbounds i8, ptr %freq, i64 88
  %11 = load double, ptr %arrayidx.11, align 8
  %add.11 = fadd double %add.10, %11
  %arrayidx.12 = getelementptr inbounds i8, ptr %freq, i64 96
  %12 = load double, ptr %arrayidx.12, align 8
  %add.12 = fadd double %add.11, %12
  %arrayidx.13 = getelementptr inbounds i8, ptr %freq, i64 104
  %13 = load double, ptr %arrayidx.13, align 8
  %add.13 = fadd double %add.12, %13
  %arrayidx.14 = getelementptr inbounds i8, ptr %freq, i64 112
  %14 = load double, ptr %arrayidx.14, align 8
  %add.14 = fadd double %add.13, %14
  %arrayidx.15 = getelementptr inbounds i8, ptr %freq, i64 120
  %15 = load double, ptr %arrayidx.15, align 8
  %add.15 = fadd double %add.14, %15
  %arrayidx.16 = getelementptr inbounds i8, ptr %freq, i64 128
  %16 = load double, ptr %arrayidx.16, align 8
  %add.16 = fadd double %add.15, %16
  %arrayidx.17 = getelementptr inbounds i8, ptr %freq, i64 136
  %17 = load double, ptr %arrayidx.17, align 8
  %add.17 = fadd double %add.16, %17
  %arrayidx.18 = getelementptr inbounds i8, ptr %freq, i64 144
  %18 = load double, ptr %arrayidx.18, align 8
  %add.18 = fadd double %add.17, %18
  %arrayidx.19 = getelementptr inbounds i8, ptr %freq, i64 152
  %19 = load double, ptr %arrayidx.19, align 8
  %add.19 = fadd double %add.18, %19
  %arrayidx.20 = getelementptr inbounds i8, ptr %freq, i64 160
  %20 = load double, ptr %arrayidx.20, align 8
  %add.20 = fadd double %add.19, %20
  %arrayidx.21 = getelementptr inbounds i8, ptr %freq, i64 168
  %21 = load double, ptr %arrayidx.21, align 8
  %add.21 = fadd double %add.20, %21
  %arrayidx.22 = getelementptr inbounds i8, ptr %freq, i64 176
  %22 = load double, ptr %arrayidx.22, align 8
  %add.22 = fadd double %add.21, %22
  %arrayidx.23 = getelementptr inbounds i8, ptr %freq, i64 184
  %23 = load double, ptr %arrayidx.23, align 8
  %add.23 = fadd double %add.22, %23
  %arrayidx.24 = getelementptr inbounds i8, ptr %freq, i64 192
  %24 = load double, ptr %arrayidx.24, align 8
  %add.24 = fadd double %add.23, %24
  %arrayidx.25 = getelementptr inbounds i8, ptr %freq, i64 200
  %25 = load double, ptr %arrayidx.25, align 8
  %add.25 = fadd double %add.24, %25
  %arrayidx.26 = getelementptr inbounds i8, ptr %freq, i64 208
  %26 = load double, ptr %arrayidx.26, align 8
  %add.26 = fadd double %add.25, %26
  %arrayidx.27 = getelementptr inbounds i8, ptr %freq, i64 216
  %27 = load double, ptr %arrayidx.27, align 8
  %add.27 = fadd double %add.26, %27
  %arrayidx.28 = getelementptr inbounds i8, ptr %freq, i64 224
  %28 = load double, ptr %arrayidx.28, align 8
  %add.28 = fadd double %add.27, %28
  %arrayidx.29 = getelementptr inbounds i8, ptr %freq, i64 232
  %29 = load double, ptr %arrayidx.29, align 8
  %add.29 = fadd double %add.28, %29
  %arrayidx.30 = getelementptr inbounds i8, ptr %freq, i64 240
  %30 = load double, ptr %arrayidx.30, align 8
  %add.30 = fadd double %add.29, %30
  %arrayidx.31 = getelementptr inbounds i8, ptr %freq, i64 248
  %31 = load double, ptr %arrayidx.31, align 8
  %add.31 = fadd double %add.30, %31
  %arrayidx.32 = getelementptr inbounds i8, ptr %freq, i64 256
  %32 = load double, ptr %arrayidx.32, align 8
  %add.32 = fadd double %add.31, %32
  %arrayidx.33 = getelementptr inbounds i8, ptr %freq, i64 264
  %33 = load double, ptr %arrayidx.33, align 8
  %add.33 = fadd double %add.32, %33
  %arrayidx.34 = getelementptr inbounds i8, ptr %freq, i64 272
  %34 = load double, ptr %arrayidx.34, align 8
  %add.34 = fadd double %add.33, %34
  %arrayidx.35 = getelementptr inbounds i8, ptr %freq, i64 280
  %35 = load double, ptr %arrayidx.35, align 8
  %add.35 = fadd double %add.34, %35
  %arrayidx.36 = getelementptr inbounds i8, ptr %freq, i64 288
  %36 = load double, ptr %arrayidx.36, align 8
  %add.36 = fadd double %add.35, %36
  %arrayidx.37 = getelementptr inbounds i8, ptr %freq, i64 296
  %37 = load double, ptr %arrayidx.37, align 8
  %add.37 = fadd double %add.36, %37
  %arrayidx.38 = getelementptr inbounds i8, ptr %freq, i64 304
  %38 = load double, ptr %arrayidx.38, align 8
  %add.38 = fadd double %add.37, %38
  %arrayidx.39 = getelementptr inbounds i8, ptr %freq, i64 312
  %39 = load double, ptr %arrayidx.39, align 8
  %add.39 = fadd double %add.38, %39
  %arrayidx.40 = getelementptr inbounds i8, ptr %freq, i64 320
  %40 = load double, ptr %arrayidx.40, align 8
  %add.40 = fadd double %add.39, %40
  %arrayidx.41 = getelementptr inbounds i8, ptr %freq, i64 328
  %41 = load double, ptr %arrayidx.41, align 8
  %add.41 = fadd double %add.40, %41
  %arrayidx.42 = getelementptr inbounds i8, ptr %freq, i64 336
  %42 = load double, ptr %arrayidx.42, align 8
  %add.42 = fadd double %add.41, %42
  %arrayidx.43 = getelementptr inbounds i8, ptr %freq, i64 344
  %43 = load double, ptr %arrayidx.43, align 8
  %add.43 = fadd double %add.42, %43
  %arrayidx.44 = getelementptr inbounds i8, ptr %freq, i64 352
  %44 = load double, ptr %arrayidx.44, align 8
  %add.44 = fadd double %add.43, %44
  %arrayidx.45 = getelementptr inbounds i8, ptr %freq, i64 360
  %45 = load double, ptr %arrayidx.45, align 8
  %add.45 = fadd double %add.44, %45
  %arrayidx.46 = getelementptr inbounds i8, ptr %freq, i64 368
  %46 = load double, ptr %arrayidx.46, align 8
  %add.46 = fadd double %add.45, %46
  %arrayidx.47 = getelementptr inbounds i8, ptr %freq, i64 376
  %47 = load double, ptr %arrayidx.47, align 8
  %add.47 = fadd double %add.46, %47
  %arrayidx.48 = getelementptr inbounds i8, ptr %freq, i64 384
  %48 = load double, ptr %arrayidx.48, align 8
  %add.48 = fadd double %add.47, %48
  %arrayidx.49 = getelementptr inbounds i8, ptr %freq, i64 392
  %49 = load double, ptr %arrayidx.49, align 8
  %add.49 = fadd double %add.48, %49
  %arrayidx.50 = getelementptr inbounds i8, ptr %freq, i64 400
  %50 = load double, ptr %arrayidx.50, align 8
  %add.50 = fadd double %add.49, %50
  %arrayidx.51 = getelementptr inbounds i8, ptr %freq, i64 408
  %51 = load double, ptr %arrayidx.51, align 8
  %add.51 = fadd double %add.50, %51
  %arrayidx.52 = getelementptr inbounds i8, ptr %freq, i64 416
  %52 = load double, ptr %arrayidx.52, align 8
  %add.52 = fadd double %add.51, %52
  %arrayidx.53 = getelementptr inbounds i8, ptr %freq, i64 424
  %53 = load double, ptr %arrayidx.53, align 8
  %add.53 = fadd double %add.52, %53
  %arrayidx.54 = getelementptr inbounds i8, ptr %freq, i64 432
  %54 = load double, ptr %arrayidx.54, align 8
  %add.54 = fadd double %add.53, %54
  %arrayidx.55 = getelementptr inbounds i8, ptr %freq, i64 440
  %55 = load double, ptr %arrayidx.55, align 8
  %add.55 = fadd double %add.54, %55
  %arrayidx.56 = getelementptr inbounds i8, ptr %freq, i64 448
  %56 = load double, ptr %arrayidx.56, align 8
  %add.56 = fadd double %add.55, %56
  %arrayidx.57 = getelementptr inbounds i8, ptr %freq, i64 456
  %57 = load double, ptr %arrayidx.57, align 8
  %add.57 = fadd double %add.56, %57
  %arrayidx.58 = getelementptr inbounds i8, ptr %freq, i64 464
  %58 = load double, ptr %arrayidx.58, align 8
  %add.58 = fadd double %add.57, %58
  %arrayidx.59 = getelementptr inbounds i8, ptr %freq, i64 472
  %59 = load double, ptr %arrayidx.59, align 8
  %add.59 = fadd double %add.58, %59
  %arrayidx.60 = getelementptr inbounds i8, ptr %freq, i64 480
  %60 = load double, ptr %arrayidx.60, align 8
  %add.60 = fadd double %add.59, %60
  %arrayidx.61 = getelementptr inbounds i8, ptr %freq, i64 488
  %61 = load double, ptr %arrayidx.61, align 8
  %add.61 = fadd double %add.60, %61
  %arrayidx.62 = getelementptr inbounds i8, ptr %freq, i64 496
  %62 = load double, ptr %arrayidx.62, align 8
  %add.62 = fadd double %add.61, %62
  ret double %add.62
}
