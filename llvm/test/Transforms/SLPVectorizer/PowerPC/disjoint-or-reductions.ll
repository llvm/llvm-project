; NOTE: Assertions have been autogenerated by utils/update_test_checks.py
; RUN: opt < %s -passes=slp-vectorizer -S -mtriple=powerpc64-linux-gnu -mcpu=pwr9 -mattr=+vsx | FileCheck %s

define i64 @bswap(ptr noalias %p, ptr noalias %p1) {
; CHECK-LABEL: @bswap(
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[P:%.*]], align 1
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[P1:%.*]], align 1
; CHECK-NEXT:    [[TMP3:%.*]] = add <8 x i8> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = zext <8 x i8> [[TMP3]] to <8 x i64>
; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw <8 x i64> [[TMP4]], <i64 56, i64 48, i64 40, i64 32, i64 24, i64 16, i64 8, i64 0>
; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.or.v8i64(<8 x i64> [[TMP5]])
; CHECK-NEXT:    ret i64 [[TMP6]]
;
  %g1 = getelementptr i8, ptr %p, i32 1
  %g2 = getelementptr i8, ptr %p, i32 2
  %g3 = getelementptr i8, ptr %p, i32 3
  %g4 = getelementptr i8, ptr %p, i32 4
  %g5 = getelementptr i8, ptr %p, i32 5
  %g6 = getelementptr i8, ptr %p, i32 6
  %g7 = getelementptr i8, ptr %p, i32 7

  %t0 = load i8, ptr %p
  %t1 = load i8, ptr %g1
  %t2 = load i8, ptr %g2
  %t3 = load i8, ptr %g3
  %t4 = load i8, ptr %g4
  %t5 = load i8, ptr %g5
  %t6 = load i8, ptr %g6
  %t7 = load i8, ptr %g7

  %g11 = getelementptr i8, ptr %p1, i32 1
  %g12 = getelementptr i8, ptr %p1, i32 2
  %g13 = getelementptr i8, ptr %p1, i32 3
  %g14 = getelementptr i8, ptr %p1, i32 4
  %g15 = getelementptr i8, ptr %p1, i32 5
  %g16 = getelementptr i8, ptr %p1, i32 6
  %g17 = getelementptr i8, ptr %p1, i32 7

  %t10 = load i8, ptr %p1
  %t11 = load i8, ptr %g11
  %t12 = load i8, ptr %g12
  %t13 = load i8, ptr %g13
  %t14 = load i8, ptr %g14
  %t15 = load i8, ptr %g15
  %t16 = load i8, ptr %g16
  %t17 = load i8, ptr %g17

  %a0 = add i8 %t0, %t10
  %a1 = add i8 %t1, %t11
  %a2 = add i8 %t2, %t12
  %a3 = add i8 %t3, %t13
  %a4 = add i8 %t4, %t14
  %a5 = add i8 %t5, %t15
  %a6 = add i8 %t6, %t16
  %a7 = add i8 %t7, %t17

  %z0 = zext i8 %a0 to i64
  %z1 = zext i8 %a1 to i64
  %z2 = zext i8 %a2 to i64
  %z3 = zext i8 %a3 to i64
  %z4 = zext i8 %a4 to i64
  %z5 = zext i8 %a5 to i64
  %z6 = zext i8 %a6 to i64
  %z7 = zext i8 %a7 to i64

  %sh0 = shl nuw i64 %z0, 56
  %sh1 = shl nuw nsw i64 %z1, 48
  %sh2 = shl nuw nsw i64 %z2, 40
  %sh3 = shl nuw nsw i64 %z3, 32
  %sh4 = shl nuw nsw i64 %z4, 24
  %sh5 = shl nuw nsw i64 %z5, 16
  %sh6 = shl nuw nsw i64 %z6, 8
;  %sh7 = shl nuw nsw i64 %z7, 0 <-- missing phantom shift

  %or01 = or disjoint i64 %sh0, %sh1
  %or012 = or disjoint i64 %or01, %sh2
  %or0123 = or disjoint i64 %or012, %sh3
  %or01234 = or disjoint i64 %or0123, %sh4
  %or012345 = or disjoint i64 %or01234, %sh5
  %or0123456 = or disjoint i64 %or012345, %sh6
  %or01234567 = or disjoint i64 %or0123456, %z7
  ret i64 %or01234567
}

define i64 @reorder(ptr noalias %p, ptr noalias %p1) {
; CHECK-LABEL: @reorder(
; CHECK-NEXT:    [[TMP1:%.*]] = load <8 x i8>, ptr [[P:%.*]], align 1
; CHECK-NEXT:    [[TMP2:%.*]] = load <8 x i8>, ptr [[P1:%.*]], align 1
; CHECK-NEXT:    [[TMP3:%.*]] = add <8 x i8> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = zext <8 x i8> [[TMP3]] to <8 x i64>
; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw <8 x i64> [[TMP4]], <i64 56, i64 16, i64 40, i64 32, i64 24, i64 48, i64 8, i64 0>
; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.or.v8i64(<8 x i64> [[TMP5]])
; CHECK-NEXT:    ret i64 [[TMP6]]
;
  %g1 = getelementptr i8, ptr %p, i32 1
  %g2 = getelementptr i8, ptr %p, i32 2
  %g3 = getelementptr i8, ptr %p, i32 3
  %g4 = getelementptr i8, ptr %p, i32 4
  %g5 = getelementptr i8, ptr %p, i32 5
  %g6 = getelementptr i8, ptr %p, i32 6
  %g7 = getelementptr i8, ptr %p, i32 7

  %t0 = load i8, ptr %p
  %t1 = load i8, ptr %g1
  %t2 = load i8, ptr %g2
  %t3 = load i8, ptr %g3
  %t4 = load i8, ptr %g4
  %t5 = load i8, ptr %g5
  %t6 = load i8, ptr %g6
  %t7 = load i8, ptr %g7

  %g11 = getelementptr i8, ptr %p1, i32 1
  %g12 = getelementptr i8, ptr %p1, i32 2
  %g13 = getelementptr i8, ptr %p1, i32 3
  %g14 = getelementptr i8, ptr %p1, i32 4
  %g15 = getelementptr i8, ptr %p1, i32 5
  %g16 = getelementptr i8, ptr %p1, i32 6
  %g17 = getelementptr i8, ptr %p1, i32 7

  %t10 = load i8, ptr %p1
  %t11 = load i8, ptr %g11
  %t12 = load i8, ptr %g12
  %t13 = load i8, ptr %g13
  %t14 = load i8, ptr %g14
  %t15 = load i8, ptr %g15
  %t16 = load i8, ptr %g16
  %t17 = load i8, ptr %g17

  %a0 = add i8 %t0, %t10
  %a1 = add i8 %t1, %t11
  %a2 = add i8 %t2, %t12
  %a3 = add i8 %t3, %t13
  %a4 = add i8 %t4, %t14
  %a5 = add i8 %t5, %t15
  %a6 = add i8 %t6, %t16
  %a7 = add i8 %t7, %t17

  %z0 = zext i8 %a0 to i64
  %z1 = zext i8 %a1 to i64
  %z2 = zext i8 %a2 to i64
  %z3 = zext i8 %a3 to i64
  %z4 = zext i8 %a4 to i64
  %z5 = zext i8 %a5 to i64
  %z6 = zext i8 %a6 to i64
  %z7 = zext i8 %a7 to i64

  %sh0 = shl nuw i64 %z0, 56
  %sh1 = shl nuw nsw i64 %z1, 16
  %sh2 = shl nuw nsw i64 %z2, 40
  %sh3 = shl nuw nsw i64 %z3, 32
  %sh4 = shl nuw nsw i64 %z4, 24
  %sh5 = shl nuw nsw i64 %z5, 48
  %sh6 = shl nuw nsw i64 %z6, 8
;  %sh7 = shl nuw nsw i64 %z7, 0 <-- missing phantom shift

  %or01 = or disjoint i64 %sh0, %sh1
  %or012 = or disjoint i64 %or01, %sh2
  %or0123 = or disjoint i64 %or012, %sh3
  %or01234 = or disjoint i64 %or0123, %sh4
  %or012345 = or disjoint i64 %or01234, %sh5
  %or0123456 = or disjoint i64 %or012345, %sh6
  %or01234567 = or disjoint i64 %or0123456, %z7
  ret i64 %or01234567
}

define i64 @swap_i16(ptr noalias %p, ptr noalias %p1) {
; CHECK-LABEL: @swap_i16(
; CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[P:%.*]], align 2
; CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[P1:%.*]], align 2
; CHECK-NEXT:    [[TMP3:%.*]] = add <4 x i16> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = zext <4 x i16> [[TMP3]] to <4 x i64>
; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw <4 x i64> [[TMP4]], <i64 48, i64 32, i64 16, i64 0>
; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.or.v4i64(<4 x i64> [[TMP5]])
; CHECK-NEXT:    ret i64 [[TMP6]]
;
  %g1 = getelementptr i16, ptr %p, i32 1
  %g2 = getelementptr i16, ptr %p, i32 2
  %g3 = getelementptr i16, ptr %p, i32 3

  %t0 = load i16, ptr %p
  %t1 = load i16, ptr %g1
  %t2 = load i16, ptr %g2
  %t3 = load i16, ptr %g3

  %g11 = getelementptr i16, ptr %p1, i32 1
  %g12 = getelementptr i16, ptr %p1, i32 2
  %g13 = getelementptr i16, ptr %p1, i32 3

  %t10 = load i16, ptr %p1
  %t11 = load i16, ptr %g11
  %t12 = load i16, ptr %g12
  %t13 = load i16, ptr %g13

  %a0 = add i16 %t0, %t10
  %a1 = add i16 %t1, %t11
  %a2 = add i16 %t2, %t12
  %a3 = add i16 %t3, %t13

  %z0 = zext i16 %a0 to i64
  %z1 = zext i16 %a1 to i64
  %z2 = zext i16 %a2 to i64
  %z3 = zext i16 %a3 to i64

  %sh0 = shl nuw i64 %z0, 48
  %sh1 = shl nuw nsw i64 %z1, 32
  %sh2 = shl nuw nsw i64 %z2, 16

  %or01 = or disjoint i64 %sh0, %sh1
  %or012 = or disjoint i64 %or01, %sh2
  %or0123 = or disjoint i64 %or012, %z3
  ret i64 %or0123
}

define i64 @reorder_i16(ptr noalias %p, ptr noalias %p1) {
; CHECK-LABEL: @reorder_i16(
; CHECK-NEXT:    [[TMP1:%.*]] = load <4 x i16>, ptr [[P:%.*]], align 2
; CHECK-NEXT:    [[TMP2:%.*]] = load <4 x i16>, ptr [[P1:%.*]], align 2
; CHECK-NEXT:    [[TMP3:%.*]] = add <4 x i16> [[TMP1]], [[TMP2]]
; CHECK-NEXT:    [[TMP4:%.*]] = zext <4 x i16> [[TMP3]] to <4 x i64>
; CHECK-NEXT:    [[TMP5:%.*]] = shl nuw <4 x i64> [[TMP4]], <i64 16, i64 32, i64 48, i64 0>
; CHECK-NEXT:    [[TMP6:%.*]] = call i64 @llvm.vector.reduce.or.v4i64(<4 x i64> [[TMP5]])
; CHECK-NEXT:    ret i64 [[TMP6]]
;
  %g1 = getelementptr i16, ptr %p, i32 1
  %g2 = getelementptr i16, ptr %p, i32 2
  %g3 = getelementptr i16, ptr %p, i32 3

  %t0 = load i16, ptr %p
  %t1 = load i16, ptr %g1
  %t2 = load i16, ptr %g2
  %t3 = load i16, ptr %g3

  %g11 = getelementptr i16, ptr %p1, i32 1
  %g12 = getelementptr i16, ptr %p1, i32 2
  %g13 = getelementptr i16, ptr %p1, i32 3

  %t10 = load i16, ptr %p1
  %t11 = load i16, ptr %g11
  %t12 = load i16, ptr %g12
  %t13 = load i16, ptr %g13

  %a0 = add i16 %t0, %t10
  %a1 = add i16 %t1, %t11
  %a2 = add i16 %t2, %t12
  %a3 = add i16 %t3, %t13

  %z0 = zext i16 %a0 to i64
  %z1 = zext i16 %a1 to i64
  %z2 = zext i16 %a2 to i64
  %z3 = zext i16 %a3 to i64

  %sh0 = shl nuw i64 %z0, 16
  %sh1 = shl nuw nsw i64 %z1, 32
  %sh2 = shl nuw nsw i64 %z2, 48

  %or01 = or disjoint i64 %sh0, %sh1
  %or012 = or disjoint i64 %or01, %sh2
  %or0123 = or disjoint i64 %or012, %z3
  ret i64 %or0123
}

