; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 6
; RUN: opt -passes=slp-vectorizer -S -mcpu=gfx1250 -mtriple=amdgcn-amd-amdhsa -slp-look-through-intrinsics=true -o - %s | FileCheck %s --check-prefix=ENABLED
; RUN: opt -passes=slp-vectorizer -S -mcpu=gfx1250 -mtriple=amdgcn-amd-amdhsa -slp-look-through-intrinsics=false -o - %s | FileCheck %s --check-prefix=DISABLED
target triple = "amdgcn-amd-amdhsa"

define amdgpu_kernel void @test_amdgcn_exp2(ptr addrspace(1) %input, ptr addrspace(1) %scales, ptr addrspace(1) %output) {
; ENABLED-LABEL: define amdgpu_kernel void @test_amdgcn_exp2(
; ENABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[SCALES:%.*]], ptr addrspace(1) [[OUTPUT:%.*]]) #[[ATTR0:[0-9]+]] {
; ENABLED-NEXT:  [[ENTRY:.*:]]
; ENABLED-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr addrspace(1) [[INPUT]], align 4
; ENABLED-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr addrspace(1) [[SCALES]], align 4
; ENABLED-NEXT:    [[TMP2:%.*]] = fmul contract <4 x float> [[TMP0]], splat (float 0x3FC0527DC0000000)
; ENABLED-NEXT:    [[TMP3:%.*]] = fsub contract <4 x float> [[TMP2]], [[TMP1]]
; ENABLED-NEXT:    [[TMP4:%.*]] = extractelement <4 x float> [[TMP3]], i32 0
; ENABLED-NEXT:    [[EXP0:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP4]])
; ENABLED-NEXT:    [[TMP5:%.*]] = extractelement <4 x float> [[TMP3]], i32 1
; ENABLED-NEXT:    [[EXP1:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP5]])
; ENABLED-NEXT:    [[TMP6:%.*]] = extractelement <4 x float> [[TMP3]], i32 2
; ENABLED-NEXT:    [[EXP2:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP6]])
; ENABLED-NEXT:    [[TMP7:%.*]] = extractelement <4 x float> [[TMP3]], i32 3
; ENABLED-NEXT:    [[EXP3:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP7]])
; ENABLED-NEXT:    [[TMP8:%.*]] = insertelement <4 x float> poison, float [[EXP0]], i32 0
; ENABLED-NEXT:    [[TMP9:%.*]] = insertelement <4 x float> [[TMP8]], float [[EXP1]], i32 1
; ENABLED-NEXT:    [[TMP10:%.*]] = insertelement <4 x float> [[TMP9]], float [[EXP2]], i32 2
; ENABLED-NEXT:    [[TMP11:%.*]] = insertelement <4 x float> [[TMP10]], float [[EXP3]], i32 3
; ENABLED-NEXT:    [[TMP12:%.*]] = call fast float @llvm.vector.reduce.fadd.v4f32(float 0.000000e+00, <4 x float> [[TMP11]])
; ENABLED-NEXT:    store float [[TMP12]], ptr addrspace(1) [[OUTPUT]], align 4
; ENABLED-NEXT:    ret void
;
; DISABLED-LABEL: define amdgpu_kernel void @test_amdgcn_exp2(
; DISABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[SCALES:%.*]], ptr addrspace(1) [[OUTPUT:%.*]]) #[[ATTR0:[0-9]+]] {
; DISABLED-NEXT:  [[ENTRY:.*:]]
; DISABLED-NEXT:    [[IN0:%.*]] = load float, ptr addrspace(1) [[INPUT]], align 4
; DISABLED-NEXT:    [[PTR1:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 1
; DISABLED-NEXT:    [[IN1:%.*]] = load float, ptr addrspace(1) [[PTR1]], align 4
; DISABLED-NEXT:    [[PTR2:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 2
; DISABLED-NEXT:    [[IN2:%.*]] = load float, ptr addrspace(1) [[PTR2]], align 4
; DISABLED-NEXT:    [[PTR3:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 3
; DISABLED-NEXT:    [[IN3:%.*]] = load float, ptr addrspace(1) [[PTR3]], align 4
; DISABLED-NEXT:    [[SCALE0:%.*]] = load float, ptr addrspace(1) [[SCALES]], align 4
; DISABLED-NEXT:    [[SPTR1:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 1
; DISABLED-NEXT:    [[SCALE1:%.*]] = load float, ptr addrspace(1) [[SPTR1]], align 4
; DISABLED-NEXT:    [[SPTR2:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 2
; DISABLED-NEXT:    [[SCALE2:%.*]] = load float, ptr addrspace(1) [[SPTR2]], align 4
; DISABLED-NEXT:    [[SPTR3:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 3
; DISABLED-NEXT:    [[SCALE3:%.*]] = load float, ptr addrspace(1) [[SPTR3]], align 4
; DISABLED-NEXT:    [[MUL0:%.*]] = fmul contract float [[IN0]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL1:%.*]] = fmul contract float [[IN1]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL2:%.*]] = fmul contract float [[IN2]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL3:%.*]] = fmul contract float [[IN3]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[SUB0:%.*]] = fsub contract float [[MUL0]], [[SCALE0]]
; DISABLED-NEXT:    [[SUB1:%.*]] = fsub contract float [[MUL1]], [[SCALE1]]
; DISABLED-NEXT:    [[SUB2:%.*]] = fsub contract float [[MUL2]], [[SCALE2]]
; DISABLED-NEXT:    [[SUB3:%.*]] = fsub contract float [[MUL3]], [[SCALE3]]
; DISABLED-NEXT:    [[EXP0:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB0]])
; DISABLED-NEXT:    [[EXP1:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB1]])
; DISABLED-NEXT:    [[EXP2:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB2]])
; DISABLED-NEXT:    [[EXP3:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB3]])
; DISABLED-NEXT:    [[TMP0:%.*]] = insertelement <2 x float> poison, float [[EXP0]], i32 0
; DISABLED-NEXT:    [[TMP1:%.*]] = insertelement <2 x float> [[TMP0]], float [[EXP2]], i32 1
; DISABLED-NEXT:    [[TMP2:%.*]] = insertelement <2 x float> poison, float [[EXP1]], i32 0
; DISABLED-NEXT:    [[TMP3:%.*]] = insertelement <2 x float> [[TMP2]], float [[EXP3]], i32 1
; DISABLED-NEXT:    [[TMP4:%.*]] = fadd fast <2 x float> [[TMP1]], [[TMP3]]
; DISABLED-NEXT:    [[SUM01:%.*]] = extractelement <2 x float> [[TMP4]], i32 0
; DISABLED-NEXT:    [[SUM23:%.*]] = extractelement <2 x float> [[TMP4]], i32 1
; DISABLED-NEXT:    [[SUM:%.*]] = fadd fast float [[SUM01]], [[SUM23]]
; DISABLED-NEXT:    store float [[SUM]], ptr addrspace(1) [[OUTPUT]], align 4
; DISABLED-NEXT:    ret void
;
entry:

  %in0 = load float, ptr addrspace(1) %input, align 4
  %ptr1 = getelementptr float, ptr addrspace(1) %input, i64 1
  %in1 = load float, ptr addrspace(1) %ptr1, align 4
  %ptr2 = getelementptr float, ptr addrspace(1) %input, i64 2
  %in2 = load float, ptr addrspace(1) %ptr2, align 4
  %ptr3 = getelementptr float, ptr addrspace(1) %input, i64 3
  %in3 = load float, ptr addrspace(1) %ptr3, align 4

  %scale0 = load float, ptr addrspace(1) %scales, align 4
  %sptr1 = getelementptr float, ptr addrspace(1) %scales, i64 1
  %scale1 = load float, ptr addrspace(1) %sptr1, align 4
  %sptr2 = getelementptr float, ptr addrspace(1) %scales, i64 2
  %scale2 = load float, ptr addrspace(1) %sptr2, align 4
  %sptr3 = getelementptr float, ptr addrspace(1) %scales, i64 3
  %scale3 = load float, ptr addrspace(1) %sptr3, align 4

  %mul0 = fmul contract float %in0, 0x3FC0527DC0000000
  %mul1 = fmul contract float %in1, 0x3FC0527DC0000000
  %mul2 = fmul contract float %in2, 0x3FC0527DC0000000
  %mul3 = fmul contract float %in3, 0x3FC0527DC0000000

  %sub0 = fsub contract float %mul0, %scale0
  %sub1 = fsub contract float %mul1, %scale1
  %sub2 = fsub contract float %mul2, %scale2
  %sub3 = fsub contract float %mul3, %scale3

  %exp0 = tail call float @llvm.amdgcn.exp2.f32(float %sub0)
  %exp1 = tail call float @llvm.amdgcn.exp2.f32(float %sub1)
  %exp2 = tail call float @llvm.amdgcn.exp2.f32(float %sub2)
  %exp3 = tail call float @llvm.amdgcn.exp2.f32(float %sub3)

  %sum01 = fadd fast float %exp0, %exp1
  %sum23 = fadd fast float %exp2, %exp3
  %sum = fadd fast float %sum01, %sum23

  store float %sum, ptr addrspace(1) %output, align 4

  ret void
}


define amdgpu_kernel void @test_amdgcn_log(ptr addrspace(1) %input, ptr addrspace(1) %scales, ptr addrspace(1) %output) {
; ENABLED-LABEL: define amdgpu_kernel void @test_amdgcn_log(
; ENABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[SCALES:%.*]], ptr addrspace(1) [[OUTPUT:%.*]]) #[[ATTR0]] {
; ENABLED-NEXT:  [[ENTRY:.*:]]
; ENABLED-NEXT:    [[TMP0:%.*]] = load <4 x float>, ptr addrspace(1) [[INPUT]], align 4
; ENABLED-NEXT:    [[TMP1:%.*]] = load <4 x float>, ptr addrspace(1) [[SCALES]], align 4
; ENABLED-NEXT:    [[TMP2:%.*]] = fmul contract <4 x float> [[TMP0]], splat (float 0x3FC0527DC0000000)
; ENABLED-NEXT:    [[TMP3:%.*]] = fadd contract <4 x float> [[TMP2]], [[TMP1]]
; ENABLED-NEXT:    [[TMP4:%.*]] = extractelement <4 x float> [[TMP3]], i32 0
; ENABLED-NEXT:    [[LOG0:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[TMP4]])
; ENABLED-NEXT:    [[TMP5:%.*]] = extractelement <4 x float> [[TMP3]], i32 1
; ENABLED-NEXT:    [[LOG1:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[TMP5]])
; ENABLED-NEXT:    [[TMP6:%.*]] = extractelement <4 x float> [[TMP3]], i32 2
; ENABLED-NEXT:    [[LOG2:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[TMP6]])
; ENABLED-NEXT:    [[TMP7:%.*]] = extractelement <4 x float> [[TMP3]], i32 3
; ENABLED-NEXT:    [[LOG3:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[TMP7]])
; ENABLED-NEXT:    [[TMP8:%.*]] = insertelement <4 x float> poison, float [[LOG0]], i32 0
; ENABLED-NEXT:    [[TMP9:%.*]] = insertelement <4 x float> [[TMP8]], float [[LOG1]], i32 1
; ENABLED-NEXT:    [[TMP10:%.*]] = insertelement <4 x float> [[TMP9]], float [[LOG2]], i32 2
; ENABLED-NEXT:    [[TMP11:%.*]] = insertelement <4 x float> [[TMP10]], float [[LOG3]], i32 3
; ENABLED-NEXT:    [[TMP12:%.*]] = call fast float @llvm.vector.reduce.fadd.v4f32(float 0.000000e+00, <4 x float> [[TMP11]])
; ENABLED-NEXT:    store float [[TMP12]], ptr addrspace(1) [[OUTPUT]], align 4
; ENABLED-NEXT:    ret void
;
; DISABLED-LABEL: define amdgpu_kernel void @test_amdgcn_log(
; DISABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[SCALES:%.*]], ptr addrspace(1) [[OUTPUT:%.*]]) #[[ATTR0]] {
; DISABLED-NEXT:  [[ENTRY:.*:]]
; DISABLED-NEXT:    [[IN0:%.*]] = load float, ptr addrspace(1) [[INPUT]], align 4
; DISABLED-NEXT:    [[PTR1:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 1
; DISABLED-NEXT:    [[IN1:%.*]] = load float, ptr addrspace(1) [[PTR1]], align 4
; DISABLED-NEXT:    [[PTR2:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 2
; DISABLED-NEXT:    [[IN2:%.*]] = load float, ptr addrspace(1) [[PTR2]], align 4
; DISABLED-NEXT:    [[PTR3:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 3
; DISABLED-NEXT:    [[IN3:%.*]] = load float, ptr addrspace(1) [[PTR3]], align 4
; DISABLED-NEXT:    [[SCALE0:%.*]] = load float, ptr addrspace(1) [[SCALES]], align 4
; DISABLED-NEXT:    [[SPTR1:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 1
; DISABLED-NEXT:    [[SCALE1:%.*]] = load float, ptr addrspace(1) [[SPTR1]], align 4
; DISABLED-NEXT:    [[SPTR2:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 2
; DISABLED-NEXT:    [[SCALE2:%.*]] = load float, ptr addrspace(1) [[SPTR2]], align 4
; DISABLED-NEXT:    [[SPTR3:%.*]] = getelementptr float, ptr addrspace(1) [[SCALES]], i64 3
; DISABLED-NEXT:    [[SCALE3:%.*]] = load float, ptr addrspace(1) [[SPTR3]], align 4
; DISABLED-NEXT:    [[MUL0:%.*]] = fmul contract float [[IN0]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL1:%.*]] = fmul contract float [[IN1]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL2:%.*]] = fmul contract float [[IN2]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL3:%.*]] = fmul contract float [[IN3]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[SUB0:%.*]] = fadd contract float [[MUL0]], [[SCALE0]]
; DISABLED-NEXT:    [[SUB1:%.*]] = fadd contract float [[MUL1]], [[SCALE1]]
; DISABLED-NEXT:    [[SUB2:%.*]] = fadd contract float [[MUL2]], [[SCALE2]]
; DISABLED-NEXT:    [[SUB3:%.*]] = fadd contract float [[MUL3]], [[SCALE3]]
; DISABLED-NEXT:    [[LOG0:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[SUB0]])
; DISABLED-NEXT:    [[LOG1:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[SUB1]])
; DISABLED-NEXT:    [[LOG2:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[SUB2]])
; DISABLED-NEXT:    [[LOG3:%.*]] = tail call float @llvm.amdgcn.log.f32(float [[SUB3]])
; DISABLED-NEXT:    [[TMP0:%.*]] = insertelement <2 x float> poison, float [[LOG0]], i32 0
; DISABLED-NEXT:    [[TMP1:%.*]] = insertelement <2 x float> [[TMP0]], float [[LOG2]], i32 1
; DISABLED-NEXT:    [[TMP2:%.*]] = insertelement <2 x float> poison, float [[LOG1]], i32 0
; DISABLED-NEXT:    [[TMP3:%.*]] = insertelement <2 x float> [[TMP2]], float [[LOG3]], i32 1
; DISABLED-NEXT:    [[TMP4:%.*]] = fadd fast <2 x float> [[TMP1]], [[TMP3]]
; DISABLED-NEXT:    [[SUM01:%.*]] = extractelement <2 x float> [[TMP4]], i32 0
; DISABLED-NEXT:    [[SUM23:%.*]] = extractelement <2 x float> [[TMP4]], i32 1
; DISABLED-NEXT:    [[SUM:%.*]] = fadd fast float [[SUM01]], [[SUM23]]
; DISABLED-NEXT:    store float [[SUM]], ptr addrspace(1) [[OUTPUT]], align 4
; DISABLED-NEXT:    ret void
;
entry:

  %in0 = load float, ptr addrspace(1) %input, align 4
  %ptr1 = getelementptr float, ptr addrspace(1) %input, i64 1
  %in1 = load float, ptr addrspace(1) %ptr1, align 4
  %ptr2 = getelementptr float, ptr addrspace(1) %input, i64 2
  %in2 = load float, ptr addrspace(1) %ptr2, align 4
  %ptr3 = getelementptr float, ptr addrspace(1) %input, i64 3
  %in3 = load float, ptr addrspace(1) %ptr3, align 4

  %scale0 = load float, ptr addrspace(1) %scales, align 4
  %sptr1 = getelementptr float, ptr addrspace(1) %scales, i64 1
  %scale1 = load float, ptr addrspace(1) %sptr1, align 4
  %sptr2 = getelementptr float, ptr addrspace(1) %scales, i64 2
  %scale2 = load float, ptr addrspace(1) %sptr2, align 4
  %sptr3 = getelementptr float, ptr addrspace(1) %scales, i64 3
  %scale3 = load float, ptr addrspace(1) %sptr3, align 4

  %mul0 = fmul contract float %in0, 0x3FC0527DC0000000
  %mul1 = fmul contract float %in1, 0x3FC0527DC0000000
  %mul2 = fmul contract float %in2, 0x3FC0527DC0000000
  %mul3 = fmul contract float %in3, 0x3FC0527DC0000000

  %sub0 = fadd contract float %mul0, %scale0
  %sub1 = fadd contract float %mul1, %scale1
  %sub2 = fadd contract float %mul2, %scale2
  %sub3 = fadd contract float %mul3, %scale3

  %log0 = tail call float @llvm.amdgcn.log.f32(float %sub0)
  %log1 = tail call float @llvm.amdgcn.log.f32(float %sub1)
  %log2 = tail call float @llvm.amdgcn.log.f32(float %sub2)
  %log3 = tail call float @llvm.amdgcn.log.f32(float %sub3)

  %sum01 = fadd fast float %log0, %log1
  %sum23 = fadd fast float %log2, %log3
  %sum = fadd fast float %sum01, %sum23

  store float %sum, ptr addrspace(1) %output, align 4

  ret void
}


define amdgpu_kernel void @test_with_wmma( ptr addrspace(1) %input, ptr addrspace(1) %output, float %scaled_max, <16 x i32> %A, <16 x i32> %B, i32 %scale_idx) {
; ENABLED-LABEL: define amdgpu_kernel void @test_with_wmma(
; ENABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[OUTPUT:%.*]], float [[SCALED_MAX:%.*]], <16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[SCALE_IDX:%.*]]) #[[ATTR0]] {
; ENABLED-NEXT:  [[ENTRY:.*:]]
; ENABLED-NEXT:    [[TMP0:%.*]] = load <2 x float>, ptr addrspace(1) [[INPUT]], align 4
; ENABLED-NEXT:    [[TMP1:%.*]] = fmul contract <2 x float> [[TMP0]], splat (float 0x3FC0527DC0000000)
; ENABLED-NEXT:    [[TMP2:%.*]] = insertelement <2 x float> poison, float [[SCALED_MAX]], i32 0
; ENABLED-NEXT:    [[TMP3:%.*]] = shufflevector <2 x float> [[TMP2]], <2 x float> poison, <2 x i32> zeroinitializer
; ENABLED-NEXT:    [[TMP4:%.*]] = fsub contract <2 x float> [[TMP1]], [[TMP3]]
; ENABLED-NEXT:    [[TMP5:%.*]] = extractelement <2 x float> [[TMP4]], i32 0
; ENABLED-NEXT:    [[EXP0:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP5]])
; ENABLED-NEXT:    [[TMP6:%.*]] = extractelement <2 x float> [[TMP4]], i32 1
; ENABLED-NEXT:    [[EXP1:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[TMP6]])
; ENABLED-NEXT:    [[TMP7:%.*]] = insertelement <2 x float> poison, float [[EXP0]], i32 0
; ENABLED-NEXT:    [[TMP8:%.*]] = insertelement <2 x float> [[TMP7]], float [[EXP1]], i32 1
; ENABLED-NEXT:    [[VEC_I32:%.*]] = bitcast <2 x float> [[TMP8]] to <2 x i32>
; ENABLED-NEXT:    [[SCALE0:%.*]] = extractelement <2 x i32> [[VEC_I32]], i64 0
; ENABLED-NEXT:    [[SCALE1:%.*]] = extractelement <2 x i32> [[VEC_I32]], i64 1
; ENABLED-NEXT:    [[WMMA0:%.*]] = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(i32 0, <16 x i32> [[A]], i32 0, <16 x i32> [[B]], i16 0, <8 x float> zeroinitializer, i32 0, i32 0, i32 [[SCALE0]], i32 0, i32 0, i32 [[SCALE_IDX]], i1 false, i1 false)
; ENABLED-NEXT:    [[WMMA1:%.*]] = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(i32 0, <16 x i32> [[A]], i32 0, <16 x i32> [[B]], i16 0, <8 x float> [[WMMA0]], i32 0, i32 0, i32 [[SCALE1]], i32 0, i32 0, i32 [[SCALE_IDX]], i1 false, i1 false)
; ENABLED-NEXT:    store <8 x float> [[WMMA1]], ptr addrspace(1) [[OUTPUT]], align 32
; ENABLED-NEXT:    ret void
;
; DISABLED-LABEL: define amdgpu_kernel void @test_with_wmma(
; DISABLED-SAME: ptr addrspace(1) [[INPUT:%.*]], ptr addrspace(1) [[OUTPUT:%.*]], float [[SCALED_MAX:%.*]], <16 x i32> [[A:%.*]], <16 x i32> [[B:%.*]], i32 [[SCALE_IDX:%.*]]) #[[ATTR0]] {
; DISABLED-NEXT:  [[ENTRY:.*:]]
; DISABLED-NEXT:    [[IN0:%.*]] = load float, ptr addrspace(1) [[INPUT]], align 4
; DISABLED-NEXT:    [[PTR1:%.*]] = getelementptr float, ptr addrspace(1) [[INPUT]], i64 1
; DISABLED-NEXT:    [[IN1:%.*]] = load float, ptr addrspace(1) [[PTR1]], align 4
; DISABLED-NEXT:    [[MUL0:%.*]] = fmul contract float [[IN0]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[MUL1:%.*]] = fmul contract float [[IN1]], 0x3FC0527DC0000000
; DISABLED-NEXT:    [[SUB0:%.*]] = fsub contract float [[MUL0]], [[SCALED_MAX]]
; DISABLED-NEXT:    [[SUB1:%.*]] = fsub contract float [[MUL1]], [[SCALED_MAX]]
; DISABLED-NEXT:    [[EXP0:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB0]])
; DISABLED-NEXT:    [[EXP1:%.*]] = tail call float @llvm.amdgcn.exp2.f32(float [[SUB1]])
; DISABLED-NEXT:    [[VEC0:%.*]] = insertelement <2 x float> poison, float [[EXP0]], i64 0
; DISABLED-NEXT:    [[VEC1:%.*]] = insertelement <2 x float> [[VEC0]], float [[EXP1]], i64 1
; DISABLED-NEXT:    [[VEC_I32:%.*]] = bitcast <2 x float> [[VEC1]] to <2 x i32>
; DISABLED-NEXT:    [[SCALE0:%.*]] = extractelement <2 x i32> [[VEC_I32]], i64 0
; DISABLED-NEXT:    [[SCALE1:%.*]] = extractelement <2 x i32> [[VEC_I32]], i64 1
; DISABLED-NEXT:    [[WMMA0:%.*]] = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(i32 0, <16 x i32> [[A]], i32 0, <16 x i32> [[B]], i16 0, <8 x float> zeroinitializer, i32 0, i32 0, i32 [[SCALE0]], i32 0, i32 0, i32 [[SCALE_IDX]], i1 false, i1 false)
; DISABLED-NEXT:    [[WMMA1:%.*]] = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(i32 0, <16 x i32> [[A]], i32 0, <16 x i32> [[B]], i16 0, <8 x float> [[WMMA0]], i32 0, i32 0, i32 [[SCALE1]], i32 0, i32 0, i32 [[SCALE_IDX]], i1 false, i1 false)
; DISABLED-NEXT:    store <8 x float> [[WMMA1]], ptr addrspace(1) [[OUTPUT]], align 32
; DISABLED-NEXT:    ret void
;
entry:

  %in0 = load float, ptr addrspace(1) %input, align 4
  %ptr1 = getelementptr float, ptr addrspace(1) %input, i64 1
  %in1 = load float, ptr addrspace(1) %ptr1, align 4

  %mul0 = fmul contract float %in0, 0x3FC0527DC0000000
  %mul1 = fmul contract float %in1, 0x3FC0527DC0000000

  %sub0 = fsub contract float %mul0, %scaled_max
  %sub1 = fsub contract float %mul1, %scaled_max

  %exp0 = tail call float @llvm.amdgcn.exp2.f32(float %sub0)
  %exp1 = tail call float @llvm.amdgcn.exp2.f32(float %sub1)

  %vec0 = insertelement <2 x float> poison, float %exp0, i64 0
  %vec1 = insertelement <2 x float> %vec0, float %exp1, i64 1

  %vec_i32 = bitcast <2 x float> %vec1 to <2 x i32>

  %scale0 = extractelement <2 x i32> %vec_i32, i64 0
  %scale1 = extractelement <2 x i32> %vec_i32, i64 1

  %wmma0 = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(
  i32 0, <16 x i32> %A, i32 0, <16 x i32> %B, i16 0, <8 x float> zeroinitializer,
  i32 0, i32 0, i32 %scale0, i32 0, i32 0, i32 %scale_idx, i1 false, i1 false)

  %wmma1 = tail call <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(
  i32 0, <16 x i32> %A, i32 0, <16 x i32> %B, i16 0, <8 x float> %wmma0,
  i32 0, i32 0, i32 %scale1, i32 0, i32 0, i32 %scale_idx, i1 false, i1 false)

  store <8 x float> %wmma1, ptr addrspace(1) %output, align 32
  ret void
}

declare float @llvm.amdgcn.exp2.f32(float)
declare <8 x float> @llvm.amdgcn.wmma.scale.f32.16x16x128.f8f6f4.v8f32.v16i32.v16i32(i32 immarg, <16 x i32>, i32 immarg, <16 x i32>, i16 immarg, <8 x float>, i32 immarg, i32 immarg, i32, i32 immarg, i32 immarg, i32, i1 immarg, i1 immarg)
