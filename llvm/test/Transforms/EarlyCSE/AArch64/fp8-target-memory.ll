; NOTE: Assertions have been autogenerated by utils/update_test_checks.py UTC_ARGS: --version 5
; RUN: opt -S -passes='early-cse<memssa>' < %s | FileCheck %s
target datalayout = "e-m:e-i64:64-i128:128-n32:64-S128"
target triple = "aarch64--linux-gnu"

;; Only FPMR as inaccessible memory
define <4 x half> @test_fpmr_inaccessible(<4 x half>%vd, <8 x i8> %vn, <8 x i8> %vm, <16 x i8> %vm2, i64 %fpm) {
; CHECK-LABEL: define <4 x half> @test_fpmr_inaccessible(
; CHECK-SAME: <4 x half> [[VD:%.*]], <8 x i8> [[VN:%.*]], <8 x i8> [[VM:%.*]], <16 x i8> [[VM2:%.*]], i64 [[FPM:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM]])
; CHECK-NEXT:    [[FDOT1:%.*]] = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> [[VD]], <8 x i8> [[VN]], <16 x i8> [[VM2]], i32 2)
; CHECK-NEXT:    [[FDOT2:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT1]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    [[FDOT3:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT2]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    ret <4 x half> [[FDOT3]]
;
entry:
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot1 = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> %vd, <8 x i8> %vn, <16 x i8> %vm2, i32 2)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot2 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> %fdot1, <8 x i8> %vn, <8 x i8> %vm)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot3 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half>  %fdot2, <8 x i8> %vn, <8 x i8> %vm)
  ret <4 x half> %fdot3
}


;; Write to ZA should not block the optimization for FPRM
;; sme.fp8.fdot.lane.za16  ZA: write FPMR: read
define void @test_fpmr_za_inaccessible(i32 %slice.0, <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2, <vscale x 16 x i8> %zm, i64 %fpm) {
; CHECK-LABEL: define void @test_fpmr_za_inaccessible(
; CHECK-SAME: i32 [[SLICE_0:%.*]], <vscale x 16 x i8> [[ZN1:%.*]], <vscale x 16 x i8> [[ZN2:%.*]], <vscale x 16 x i8> [[ZM:%.*]], i64 [[FPM:%.*]]) {
; CHECK-NEXT:    [[SLICE:%.*]] = add i32 [[SLICE_0]], 7
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM]])
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZM]], i32 1)
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZM]], i32 0)
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE_0]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZM]], i32 0)
; CHECK-NEXT:    ret void
;
  %slice = add i32 %slice.0, 7
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice,
  <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2,
  <vscale x 16 x i8> %zm, i32 1)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice,
  <vscale x 16 x i8> %zn2, <vscale x 16 x i8> %zn1,
  <vscale x 16 x i8> %zm, i32 0)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice.0,
  <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2,
  <vscale x 16 x i8> %zm, i32 0)
  ret void
}


define dso_local <4 x half> @test_loop_fpmr_inaccessible(<4 x half>%vd, <8 x i8> %vn, <8 x i8> %vm, <16 x i8> %vm2, i64%fpm) {
; CHECK-LABEL: define dso_local <4 x half> @test_loop_fpmr_inaccessible(
; CHECK-SAME: <4 x half> [[VD:%.*]], <8 x i8> [[VN:%.*]], <8 x i8> [[VM:%.*]], <16 x i8> [[VM2:%.*]], i64 [[FPM:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*]]:
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM]])
; CHECK-NEXT:    [[FDOT0:%.*]] = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> [[VD]], <8 x i8> [[VN]], <16 x i8> [[VM2]], i32 2)
; CHECK-NEXT:    [[FDOT00:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT0]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    br label %[[LOOP:.*]]
; CHECK:       [[LOOP]]:
; CHECK-NEXT:    [[I:%.*]] = phi i8 [ 0, %[[ENTRY]] ], [ [[I_NEXT:%.*]], %[[LOOP]] ]
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM]])
; CHECK-NEXT:    [[FDOT1:%.*]] = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> [[FDOT00]], <8 x i8> [[VN]], <16 x i8> [[VM2]], i32 2)
; CHECK-NEXT:    [[FDOT2:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT1]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    [[FDOT3:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT2]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    [[I_NEXT]] = add i8 [[I]], 1
; CHECK-NEXT:    [[EC:%.*]] = icmp eq i8 [[I]], 20
; CHECK-NEXT:    br i1 [[EC]], label %[[EXIT:.*]], label %[[LOOP]]
; CHECK:       [[EXIT]]:
; CHECK-NEXT:    ret <4 x half> [[FDOT3]]
;
  entry:
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot0 = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> %vd, <8 x i8> %vn, <16 x i8> %vm2, i32 2)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot00 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half>  %fdot0, <8 x i8> %vn, <8 x i8> %vm)
  br label %loop
  loop:
  %i = phi i8 [ 0, %entry ], [ %i.next, %loop ]
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot1 = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> %fdot00, <8 x i8> %vn, <16 x i8> %vm2, i32 2)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot2 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> %fdot1, <8 x i8> %vn, <8 x i8> %vm)
  call void @llvm.aarch64.set.fpmr(i64 %fpm)
  %fdot3 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half>  %fdot2, <8 x i8> %vn, <8 x i8> %vm)
  %i.next = add i8 %i, 1
  %ec = icmp eq i8 %i, 20
  br i1 %ec, label %exit, label %loop

exit:
  ret <4 x half> %fdot3
}


;; Negative tests

define dso_local <4 x half> @neg_test_fpmr_inaccessible(<4 x half>%vd, <8 x i8> %vn, <8 x i8> %vm, <16 x i8> %vm2, i64 %fpm1, i64 %fpm2) {
; CHECK-LABEL: define dso_local <4 x half> @neg_test_fpmr_inaccessible(
; CHECK-SAME: <4 x half> [[VD:%.*]], <8 x i8> [[VN:%.*]], <8 x i8> [[VM:%.*]], <16 x i8> [[VM2:%.*]], i64 [[FPM1:%.*]], i64 [[FPM2:%.*]]) {
; CHECK-NEXT:  [[ENTRY:.*:]]
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM1]])
; CHECK-NEXT:    [[FDOT1:%.*]] = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> [[VD]], <8 x i8> [[VN]], <16 x i8> [[VM2]], i32 2)
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM2]])
; CHECK-NEXT:    [[FDOT2:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT1]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM1]])
; CHECK-NEXT:    [[FDOT3:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT2]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM2]])
; CHECK-NEXT:    [[FDOT4:%.*]] = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> [[FDOT3]], <8 x i8> [[VN]], <8 x i8> [[VM]])
; CHECK-NEXT:    ret <4 x half> [[FDOT4]]
;
entry:
  call void @llvm.aarch64.set.fpmr(i64 %fpm1)
  %fdot1 = tail call <4 x half> @llvm.aarch64.neon.fp8.fdot2.lane.v4f16.v8i8(<4 x half> %vd, <8 x i8> %vn, <16 x i8> %vm2, i32 2)
  call void @llvm.aarch64.set.fpmr(i64 %fpm2)
  %fdot2 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half> %fdot1, <8 x i8> %vn, <8 x i8> %vm)
  call void @llvm.aarch64.set.fpmr(i64 %fpm1)
  %fdot3 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half>  %fdot2, <8 x i8> %vn, <8 x i8> %vm)
  call void @llvm.aarch64.set.fpmr(i64 %fpm2)
  %fdot4 = tail call fast <4 x half> @llvm.aarch64.neon.fp8.fdot2.v4f16.v8i8(<4 x half>  %fdot3, <8 x i8> %vn, <8 x i8> %vm)
  ret <4 x half> %fdot4
}


;; sme.fp8.fdot.lane.za16  ZA: write FPMR: read
define void @neg_test_fpmr_za_inaccessible(i32 %slice.0, <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2, <vscale x 16 x i8> %zm, i64 %fpm1, i64 %fpm2) {
; CHECK-LABEL: define void @neg_test_fpmr_za_inaccessible(
; CHECK-SAME: i32 [[SLICE_0:%.*]], <vscale x 16 x i8> [[ZN1:%.*]], <vscale x 16 x i8> [[ZN2:%.*]], <vscale x 16 x i8> [[ZM:%.*]], i64 [[FPM1:%.*]], i64 [[FPM2:%.*]]) {
; CHECK-NEXT:    [[SLICE:%.*]] = add i32 [[SLICE_0]], 7
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM1]])
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZM]], i32 1)
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM2]])
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZM]], i32 0)
; CHECK-NEXT:    call void @llvm.aarch64.set.fpmr(i64 [[FPM1]])
; CHECK-NEXT:    call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 [[SLICE_0]], <vscale x 16 x i8> [[ZN1]], <vscale x 16 x i8> [[ZN2]], <vscale x 16 x i8> [[ZM]], i32 0)
; CHECK-NEXT:    ret void
;
  %slice = add i32 %slice.0, 7
  call void @llvm.aarch64.set.fpmr(i64 %fpm1)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice,
  <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2,
  <vscale x 16 x i8> %zm, i32 1)
  call void @llvm.aarch64.set.fpmr(i64 %fpm2)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice,
  <vscale x 16 x i8> %zn2, <vscale x 16 x i8> %zn1,
  <vscale x 16 x i8> %zm, i32 0)
  call void @llvm.aarch64.set.fpmr(i64 %fpm1)
  call void @llvm.aarch64.sme.fp8.fdot.lane.za16.vg1x2(i32 %slice.0,
  <vscale x 16 x i8> %zn1, <vscale x 16 x i8> %zn2,
  <vscale x 16 x i8> %zm, i32 0)
  ret void
}
